{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码 (Positional Encoding)\n",
    "\n",
    "本 Notebook 讲解 Transformer 中的**位置编码**机制。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 理论解析\n",
    "\n",
    "### 1.1 核心问题：为什么需要位置编码？\n",
    "\n",
    "**RNN/LSTM** 按顺序处理序列，天然具有位置感知能力：\n",
    "```\n",
    "x1 -> x2 -> x3 -> x4  (顺序处理)\n",
    "```\n",
    "\n",
    "**Transformer** 使用自注意力并行处理所有位置：\n",
    "```\n",
    "x1, x2, x3, x4  (同时处理，无顺序信息)\n",
    "```\n",
    "\n",
    "**问题**：对于 Transformer，\"我爱你\" 和 \"你爱我\" 的注意力计算结果完全相同！\n",
    "\n",
    "**解决方案**：在输入 Embedding 中注入位置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 数学原理：正弦/余弦位置编码\n",
    "\n",
    "Transformer 原论文使用固定的正弦/余弦函数生成位置编码：\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "其中：\n",
    "- $pos$：位置索引 (0, 1, 2, ...)\n",
    "- $i$：维度索引 (0, 1, 2, ..., d_model/2 - 1)\n",
    "- $d_{model}$：模型维度\n",
    "\n",
    "**关键洞察**：\n",
    "- 偶数维度使用 $\\sin$，奇数维度使用 $\\cos$\n",
    "- 不同维度对应不同频率的波"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 波长的几何意义\n",
    "\n",
    "分母 $10000^{2i/d_{model}}$ 控制波长：\n",
    "\n",
    "| 维度 $i$ | 波长 | 特点 |\n",
    "|:--------:|:-----|:-----|\n",
    "| $i=0$ | $2\\pi$ | 高频，变化快，区分相邻位置 |\n",
    "| $i=d/2$ | $2\\pi \\times 10000$ | 低频，变化慢，区分远距离位置 |\n",
    "\n",
    "**类比二进制编码**：\n",
    "```\n",
    "位置 0: 0 0 0 0\n",
    "位置 1: 0 0 0 1  <- 最低位变化最快\n",
    "位置 2: 0 0 1 0\n",
    "位置 3: 0 0 1 1\n",
    "位置 4: 0 1 0 0  <- 高位变化慢\n",
    "```\n",
    "\n",
    "正弦编码是二进制的**连续版本**，每个位置都有唯一的\"指纹\"。\n",
    "\n",
    "**优势**：\n",
    "1. 可以外推到训练时未见过的更长序列\n",
    "2. $PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数（相对位置可学习）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"正弦/余弦位置编码。\n",
    "\n",
    "    使用固定的正弦和余弦函数为序列中的每个位置生成唯一编码。\n",
    "\n",
    "    Attributes:\n",
    "        dropout: Dropout 层。\n",
    "        pe: 位置编码矩阵，形状 (1, max_len, d_model)。\n",
    "\n",
    "    Example:\n",
    "        >>> pe = PositionalEncoding(d_model=512, max_len=100)\n",
    "        >>> x = torch.randn(2, 50, 512)\n",
    "        >>> output = pe(x)  # 形状不变\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1) -> None:\n",
    "        \"\"\"初始化位置编码模块。\n",
    "\n",
    "        Args:\n",
    "            d_model: 模型维度。\n",
    "            max_len: 支持的最大序列长度。\n",
    "            dropout: Dropout 概率。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Log Trick: 使用 exp(-log) 代替直接除法，提高数值稳定性\n",
    "        # 原式: 1 / 10000^(2i/d_model) = exp(-2i * log(10000) / d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # 偶数维度: sin, 奇数维度: cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 增加 batch 维度: (max_len, d_model) -> (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer: 不是参数，但会随模型保存/加载\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"将位置编码加到输入上。\n",
    "\n",
    "        Args:\n",
    "            x: 输入张量，形状 (batch, seq_len, d_model)。\n",
    "\n",
    "        Returns:\n",
    "            添加位置编码后的张量，形状不变。\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 可视化验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成位置编码矩阵\n",
    "max_len = 100\n",
    "d_model = 128\n",
    "\n",
    "pe_module = PositionalEncoding(d_model=d_model, max_len=max_len, dropout=0.0)\n",
    "pe_matrix = pe_module.pe.squeeze(0).numpy()  # (max_len, d_model)\n",
    "\n",
    "print(f\"位置编码矩阵形状: {pe_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制热力图\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pe_matrix, cmap=\"RdBu\", aspect=\"auto\", vmin=-1, vmax=1)\n",
    "plt.colorbar(label=\"Encoding Value\")\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.title(\"Positional Encoding Heatmap (max_len=100, d_model=128)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 热力图解读\n",
    "\n",
    "观察上图中的**波浪状纹理**：\n",
    "\n",
    "1. **左侧（低维度）**：波长短，变化快，呈现密集的条纹\n",
    "2. **右侧（高维度）**：波长长，变化慢，呈现稀疏的条纹\n",
    "3. **每一行**：代表一个位置的唯一\"指纹\"\n",
    "4. **相邻行**：编码值相似但不完全相同\n",
    "\n",
    "这种设计让模型能够：\n",
    "- 通过低维度区分相邻位置\n",
    "- 通过高维度感知全局位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制不同维度的正弦波\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "dims = [0, 10, 50, 100]\n",
    "\n",
    "for ax, dim in zip(axes.flat, dims):\n",
    "    ax.plot(pe_matrix[:, dim], label=f\"dim={dim}\")\n",
    "    ax.set_xlabel(\"Position\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.set_title(f\"Dimension {dim}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.suptitle(\"不同维度的位置编码波形\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 波形分析\n",
    "\n",
    "- **Dimension 0**：周期约为 $2\\pi \\approx 6$，变化最快\n",
    "- **Dimension 100**：周期远大于 100，几乎是单调变化\n",
    "- 不同维度的组合构成每个位置的唯一编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 总结\n",
    "\n",
    "| 要点 | 说明 |\n",
    "|:-----|:-----|\n",
    "| **目的** | 为并行计算的 Transformer 注入位置信息 |\n",
    "| **方法** | 正弦/余弦函数，不同维度不同频率 |\n",
    "| **register_buffer** | 存储非参数张量，随模型保存 |\n",
    "| **Log Trick** | 提高大指数计算的数值稳定性 |\n",
    "\n",
    "**下一步**：在 `encoder.ipynb` 中，我们将把位置编码与多头注意力组合成完整的 Encoder 层。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
