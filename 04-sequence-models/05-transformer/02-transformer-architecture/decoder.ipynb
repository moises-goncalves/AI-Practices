{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder 实现\n",
    "\n",
    "本 Notebook 实现 Transformer 的 **Decoder** 部分，包括：\n",
    "- `DecoderLayer`：单层解码器（Masked Self-Attention + Cross-Attention + FFN）\n",
    "- `Decoder`：完整解码器（Embedding + PE + N 层 DecoderLayer + Linear）\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 理论核心\n",
    "\n",
    "### 1.1 DecoderLayer vs EncoderLayer\n",
    "\n",
    "| 组件 | EncoderLayer | DecoderLayer |\n",
    "|:-----|:-------------|:-------------|\n",
    "| Self-Attention | 1 个 (无因果掩码) | 1 个 (**带因果掩码**) |\n",
    "| Cross-Attention | **无** | **1 个** (Encoder-Decoder Attention) |\n",
    "| Feed Forward | 1 个 | 1 个 |\n",
    "| Add & Norm | 2 个 | **3 个** |\n",
    "\n",
    "**为什么 Decoder 需要多一个 Attention 层？**\n",
    "\n",
    "- **Masked Self-Attention**：让 Decoder 关注**已生成的 token**（自回归）\n",
    "- **Cross-Attention**：让 Decoder 关注 **Encoder 的输出**（源序列信息）\n",
    "\n",
    "```\n",
    "DecoderLayer 结构：\n",
    "\n",
    "    Target Input\n",
    "         |\n",
    "         v\n",
    "  +-----------------+\n",
    "  | Masked Self-Attn|  <-- Q, K, V 都来自 target\n",
    "  +-----------------+\n",
    "         |\n",
    "    Add & Norm\n",
    "         |\n",
    "         v\n",
    "  +-----------------+\n",
    "  | Cross-Attention |  <-- Q 来自 Decoder, K/V 来自 Encoder (Memory)\n",
    "  +-----------------+\n",
    "         |\n",
    "    Add & Norm\n",
    "         |\n",
    "         v\n",
    "  +-----------------+\n",
    "  |  Feed Forward   |\n",
    "  +-----------------+\n",
    "         |\n",
    "    Add & Norm\n",
    "         |\n",
    "         v\n",
    "      Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Look-Ahead Mask (因果掩码)\n",
    "\n",
    "**为什么需要因果掩码？**\n",
    "\n",
    "在训练时，Decoder 接收完整的目标序列作为输入。但在预测位置 $t$ 时，模型**不应该看到位置 $t+1, t+2, ...$ 的信息**，否则就是\"作弊\"。\n",
    "\n",
    "**解决方案**：使用上三角掩码，将未来位置的注意力分数设为 $-\\infty$。\n",
    "\n",
    "**上三角掩码示意图** (seq_len=5)：\n",
    "\n",
    "```\n",
    "Query\\Key    t=0   t=1   t=2   t=3   t=4\n",
    "  t=0        0    -inf  -inf  -inf  -inf\n",
    "  t=1        0     0    -inf  -inf  -inf\n",
    "  t=2        0     0     0    -inf  -inf\n",
    "  t=3        0     0     0     0    -inf\n",
    "  t=4        0     0     0     0     0\n",
    "\n",
    "0    = 可以关注 (softmax 后有权重)\n",
    "-inf = 不可关注 (softmax 后为 0)\n",
    "```\n",
    "\n",
    "**效果**：位置 $t$ 只能关注位置 $0, 1, ..., t$，无法\"偷看\"未来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 辅助组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力机制。\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码（正弦/余弦）。\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"位置前馈网络：Linear -> ReLU -> Linear。\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Mask 生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"生成因果掩码（上三角矩阵）。\n",
    "\n",
    "    Args:\n",
    "        seq_len: 序列长度。\n",
    "\n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len)，上三角为 1，其余为 0。\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化因果掩码\n",
    "seq_len = 5\n",
    "causal_mask = generate_square_subsequent_mask(seq_len)\n",
    "print(f\"因果掩码 (seq_len={seq_len}):\")\n",
    "print(causal_mask)\n",
    "print(\"\\n1 = 被遮蔽 (不可关注), 0 = 可关注\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. DecoderLayer 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Transformer Decoder 单层。\n",
    "\n",
    "    结构：Masked Self-Attn -> Add & Norm -> Cross-Attn -> Add & Norm -> FFN -> Add & Norm\n",
    "\n",
    "    Args:\n",
    "        d_model: 模型维度。\n",
    "        n_heads: 注意力头数。\n",
    "        d_ff: FFN 隐藏层维度。\n",
    "        dropout: Dropout 概率。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        # Sub-layer 1: Masked Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Sub-layer 2: Cross-Attention (Encoder-Decoder Attention)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Sub-layer 3: Feed Forward\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        memory_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"前向传播。\n",
    "\n",
    "        Args:\n",
    "            tgt: 目标序列 (batch, tgt_seq, d_model)。\n",
    "            memory: Encoder 输出 (batch, src_seq, d_model)。\n",
    "            tgt_mask: 因果掩码 (tgt_seq, tgt_seq) 或 (batch, 1, tgt_seq, tgt_seq)。\n",
    "            memory_mask: Encoder padding mask (batch, 1, 1, src_seq)。\n",
    "\n",
    "        Returns:\n",
    "            输出张量 (batch, tgt_seq, d_model)。\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Masked Self-Attention + Add & Norm\n",
    "        self_attn_output, _ = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout1(self_attn_output))\n",
    "\n",
    "        # Sub-layer 2: Cross-Attention + Add & Norm\n",
    "        # Q 来自 Decoder (tgt), K/V 来自 Encoder (memory)\n",
    "        cross_attn_output, _ = self.cross_attn(tgt, memory, memory, memory_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
    "\n",
    "        # Sub-layer 3: Feed Forward + Add & Norm\n",
    "        ffn_output = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
    "\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 完整 Decoder 组装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Transformer Decoder。\n",
    "\n",
    "    结构：Embedding -> Positional Encoding -> N x DecoderLayer -> Linear\n",
    "\n",
    "    Args:\n",
    "        vocab_size: 词表大小。\n",
    "        d_model: 模型维度。\n",
    "        n_layers: DecoderLayer 层数。\n",
    "        n_heads: 注意力头数。\n",
    "        d_ff: FFN 隐藏层维度。\n",
    "        max_len: 最大序列长度。\n",
    "        dropout: Dropout 概率。\n",
    "        pad_idx: Padding token 的索引。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int,\n",
    "        max_len: int = 5000,\n",
    "        dropout: float = 0.1,\n",
    "        pad_idx: int = 0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # 输出投影到词表大小\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "    def create_causal_mask(self, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"创建因果掩码。\n",
    "\n",
    "        Args:\n",
    "            tgt: 目标 token IDs (batch, seq)。\n",
    "\n",
    "        Returns:\n",
    "            mask: (1, 1, seq, seq)，上三角为 1。\n",
    "        \"\"\"\n",
    "        seq_len = tgt.size(1)\n",
    "        mask = generate_square_subsequent_mask(seq_len)\n",
    "        return mask.unsqueeze(0).unsqueeze(0).to(tgt.device)\n",
    "\n",
    "    def create_padding_mask(self, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"创建 Padding Mask。\"\"\"\n",
    "        mask = (tgt == self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return mask.to(tgt.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        memory_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"前向传播。\n",
    "\n",
    "        Args:\n",
    "            tgt: 目标 token IDs (batch, tgt_seq)。\n",
    "            memory: Encoder 输出 (batch, src_seq, d_model)。\n",
    "            tgt_mask: 可选的因果掩码，若为 None 则自动生成。\n",
    "            memory_mask: 可选的 Encoder padding mask。\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch, tgt_seq, vocab_size)。\n",
    "        \"\"\"\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.create_causal_mask(tgt)\n",
    "\n",
    "        # Embedding + Scale + Positional Encoding\n",
    "        x = self.embedding(tgt) * self.scale\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Pass through N decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        logits = self.output_projection(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 完整性测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# 超参数\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "batch_size = 2\n",
    "src_seq_len = 12\n",
    "tgt_seq_len = 10\n",
    "\n",
    "# 实例化 Decoder\n",
    "decoder = Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    ")\n",
    "\n",
    "print(f\"Decoder 参数量: {sum(p.numel() for p in decoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造输入\n",
    "tgt = torch.randint(1, vocab_size, (batch_size, tgt_seq_len))\n",
    "memory = torch.randn(batch_size, src_seq_len, d_model)  # 模拟 Encoder 输出\n",
    "\n",
    "print(f\"目标序列 tgt 形状: {tgt.shape}\")\n",
    "print(f\"Encoder 输出 memory 形状: {memory.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    logits = decoder(tgt, memory)\n",
    "\n",
    "print(f\"\\n输出 logits 形状: {logits.shape}\")\n",
    "print(f\"期望形状: (batch={batch_size}, tgt_seq={tgt_seq_len}, vocab_size={vocab_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 断言验证\n",
    "assert logits.shape == (batch_size, tgt_seq_len, vocab_size), \"输出形状不正确！\"\n",
    "print(\"\\n[PASS] 输出形状验证通过: (batch, tgt_seq, vocab_size)\")\n",
    "\n",
    "# 验证因果掩码\n",
    "causal_mask = decoder.create_causal_mask(tgt)\n",
    "print(f\"\\n因果掩码形状: {causal_mask.shape}\")\n",
    "print(f\"因果掩码示例 (squeeze):\")\n",
    "print(causal_mask[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 总结\n",
    "\n",
    "| 组件 | 作用 |\n",
    "|:-----|:-----|\n",
    "| `Masked Self-Attention` | 自回归：只关注已生成的 token |\n",
    "| `Cross-Attention` | 关注 Encoder 输出 (Q 来自 Decoder, K/V 来自 Encoder) |\n",
    "| `因果掩码` | 上三角矩阵，防止\"偷看\"未来 |\n",
    "| `Output Projection` | Linear 层映射到词表大小 |\n",
    "\n",
    "**Decoder 与 Encoder 的关键区别**：\n",
    "1. 多一个 Cross-Attention 层\n",
    "2. Self-Attention 需要因果掩码\n",
    "3. 最后有 Linear 层输出 logits\n",
    "\n",
    "**下一步**：将 Encoder 和 Decoder 组合成完整的 Transformer 模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
