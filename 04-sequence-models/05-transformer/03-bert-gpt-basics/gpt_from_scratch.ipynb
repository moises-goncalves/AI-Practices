{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-GPT: 从零实现 (SOTA 标准)\n",
    "\n",
    "**严谨实现** | 包含权重初始化、温度采样、因果掩码可视化\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 理论基础\n",
    "\n",
    "### 1.1 GPT 架构核心\n",
    "\n",
    "**Decoder-Only Transformer**:\n",
    "- 无 Cross-Attention（不像标准 Decoder）\n",
    "- 因果自注意力（Causal Mask）防止信息泄漏\n",
    "- 自回归生成：每个 token 只能依赖之前的历史\n",
    "\n",
    "### 1.2 权重初始化策略 ⭐ (GPT-2 Paper)\n",
    "\n",
    "**问题**: 随机初始化会导致训练初期梯度不稳定。\n",
    "\n",
    "**解决方案**:\n",
    "\n",
    "**标准层** (Linear/Embedding):\n",
    "$$W \\sim \\mathcal{N}(0, 0.02^2)$$\n",
    "\n",
    "**残差投影层** (Residual Projections):\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{0.02^2}{2 \\times n_{layers}}\\right)$$\n",
    "\n",
    "**直觉**: 残差路径上每层方差应保持一致。$n$ 层的残差连接会让方差累积 $\\sqrt{n}$ 倍，因此需要除以 $\\sqrt{n}$ 补偿。\n",
    "\n",
    "### 1.3 采样策略\n",
    "\n",
    "**Temperature Sampling**:\n",
    "$$P'(x_i) = \\frac{\\exp(\\log P(x_i) / T)}{\\sum_j \\exp(\\log P(x_j) / T)}$$\n",
    "\n",
    "- **T = 0.1**: 分布变尖锐，确定性生成，更保守\n",
    "- **T = 1.0**: 原始分布\n",
    "- **T = 2.0**: 分布变平滑，更随机/混乱\n",
    "\n",
    "**Top-K Sampling**:\n",
    "1. 将概率低于前 K 大的置 0\n",
    "2. 重新归一化\n",
    "3. 采样\n",
    "\n",
    "这避免了采样到极低概率的\"坏\"token。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"GPT 配置类。\"\"\"\n",
    "\n",
    "    vocab_size: int = 65\n",
    "    block_size: int = 256\n",
    "    n_embd: int = 384\n",
    "    n_head: int = 6\n",
    "    n_layer: int = 6\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    # 初始化参数 (GPT-2 风格)\n",
    "    init_std: float = 0.02\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        self.head_size = self.n_embd // self.n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"因果自注意力（单个模块包含多头注意力）。\n",
    "\n",
    "    核心思想:\n",
    "        使用因果掩码确保位置 i 只能看到位置 0~i-1，\n",
    "        防止自回归生成时的信息泄漏。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "        self.scale = 1.0 / math.sqrt(self.head_size)\n",
    "\n",
    "        # 合并的 QKV 投影\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # 输出投影\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 因果掩码 (不参与梯度计算)\n",
    "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "        self.register_buffer(\"mask\", mask.view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # QKV 投影并分头\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # 注意力分数: einsum 'b h i d, b h j d -> b h i j'\n",
    "        scores = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        # 应用因果掩码\n",
    "        scores = scores.masked_fill(self.mask[:, :, :T, :T] == 0, -1e9)\n",
    "\n",
    "        # Softmax + Dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 加权求和: einsum 'b h i j, b h j d -> b h i d'\n",
    "        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn_weights, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.resid_dropout(self.c_proj(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"位置前馈网络，使用 GELU 激活。\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block (Pre-Norm)。\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = FeedForward(config)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"完整的 GPT 语言模型。\n",
    "\n",
    "    核心思想:\n",
    "        Decoder-Only Transformer + 因果掩码 + 自回归生成\n",
    "\n",
    "    初始化策略 (GPT-2):\n",
    "        - 标准层: N(0, 0.02)\n",
    "        - 残差投影: N(0, 0.02 / sqrt(2 * n_layers))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # 权重共享: token embedding 和 lm_head 共享权重\n",
    "        self.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # 残差投影特殊初始化\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                nn.init.normal_(p, mean=0.0, std=config.init_std / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        \"\"\"GPT-2 风格的权重初始化。\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=self.config.init_std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=self.config.init_std)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.zeros_(module.bias)\n",
    "            nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(\n",
    "        self, idx: Tensor, targets: Optional[Tensor] = None\n",
    "    ) -> tuple[Tensor, Optional[Tensor]]:\n",
    "        B, T = idx.shape\n",
    "        assert (\n",
    "            T <= self.config.block_size\n",
    "        ), f\"Sequence length {T} exceeds block_size {self.config.block_size}\"\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        x = self.wte(idx) + self.wpe(pos)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: Tensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"自回归生成文本。\n",
    "\n",
    "        Args:\n",
    "            idx: 上下文 (B, T)\n",
    "            max_new_tokens: 生成 token 数量\n",
    "            temperature: 温度参数，<1 更保守，>1 更随机\n",
    "            top_k: 只从概率最高的 K 个 token 中采样\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:, -self.config.block_size :]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 因果掩码可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_causal_mask(seq_len: int = 10) -> None:\n",
    "    \"\"\"可视化因果掩码如何阻止信息泄漏。\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # 左图：掩码矩阵\n",
    "    ax1 = axes[0]\n",
    "    im1 = ax1.imshow(mask.numpy(), cmap=\"Blues\", origin=\"upper\")\n",
    "    ax1.set_xticks(range(seq_len))\n",
    "    ax1.set_yticks(range(seq_len))\n",
    "    ax1.set_xlabel(\"Key 位置 (j)\")\n",
    "    ax1.set_ylabel(\"Query 位置 (i)\")\n",
    "    ax1.set_title(\"因果掩码矩阵\\n白色=屏蔽, 蓝色=可见\")\n",
    "\n",
    "    # 标注关键点\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if mask[i, j] == 1:\n",
    "                ax1.text(j, i, \"✓\", ha=\"center\", va=\"center\", color=\"green\", fontsize=10)\n",
    "            else:\n",
    "                ax1.text(j, i, \"✗\", ha=\"center\", va=\"center\", color=\"red\", fontsize=10)\n",
    "\n",
    "    # 右图：信息流向图\n",
    "    ax2 = axes[1]\n",
    "    im2 = ax2.imshow(mask.numpy(), cmap=\"Greens\", origin=\"upper\")\n",
    "    ax2.set_xticks(range(seq_len))\n",
    "    ax2.set_yticks(range(seq_len))\n",
    "    ax2.set_xlabel(\"来源位置\")\n",
    "    ax2.set_ylabel(\"目标位置\")\n",
    "    ax2.set_title(\"信息流向图\\n位置 i 可以从哪些位置获取信息\")\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        ax2.text(\n",
    "            i,\n",
    "            i,\n",
    "            f\"t={i}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\" if mask[i, i] > 0.5 else \"white\",\n",
    "            fontsize=8,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n关键观察:\")\n",
    "    print(\"  - 下三角 (含对角线) 为 1 (蓝色/绿色): 允许的注意力连接\")\n",
    "    print(\"  - 上三角为 0 (白色): 被屏蔽的连接\")\n",
    "    print(\"  - 位置 0 只能看到自己\")\n",
    "    print(f\"  - 位置 {seq_len-1} 可以看到所有位置 0~{seq_len-1}\")\n",
    "\n",
    "\n",
    "visualize_causal_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_information_leakage() -> None:\n",
    "    \"\"\"解释因果掩码如何防止信息泄漏。\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"信息泄漏 (Information Leakage) 解释\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"问题场景: 自回归生成\")\n",
    "    print(\"  假设我们要生成句子: 'The cat sat on the ...'\")\n",
    "    print()\n",
    "    print(\"  步骤 1: 已生成 'The'\")\n",
    "    print(\"  步骤 2: 预测下一个词 → 得到 'cat'\")\n",
    "    print(\"  步骤 3: 预测下一个词 → 得到 'sat'\")\n",
    "    print()\n",
    "    print(\"如果没有因果掩码:\")\n",
    "    print(\"  - 'The' 在步骤 3 时能看到 'sat' (未来信息！)\")\n",
    "    print(\"  - 这相当于考试时偷看答案\")\n",
    "    print(\"  - 模型无法学会真正的预测\")\n",
    "    print()\n",
    "    print(\"有因果掩码:\")\n",
    "    print(\"  - 'The' 只能看到自己\")\n",
    "    print(\"  - 'cat' 只能看到 'The' 和 'cat'\")\n",
    "    print(\"  - 每个位置只能基于历史信息进行预测\")\n",
    "    print(\"  - 保证训练和生成的一致性\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "explain_information_leakage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 测试与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass() -> None:\n",
    "    \"\"\"验证前向传播形状正确性。\"\"\"\n",
    "    config = GPTConfig(vocab_size=100, block_size=32, n_embd=64, n_head=4, n_layer=2)\n",
    "    model = GPT(config)\n",
    "\n",
    "    B, T = 2, 16\n",
    "    idx = torch.randint(0, config.vocab_size, (B, T))\n",
    "    targets = torch.randint(0, config.vocab_size, (B, T))\n",
    "\n",
    "    logits, loss = model(idx, targets)\n",
    "\n",
    "    assert logits.shape == (B, T, config.vocab_size)\n",
    "    assert loss is not None\n",
    "\n",
    "    print(\"[PASS] test_forward_pass\")\n",
    "    print(f\"  Logits shape: {logits.shape}\")\n",
    "    print(f\"  Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "test_forward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_initialization() -> None:\n",
    "    \"\"\"验证权重初始化是否符合 GPT-2 标准。\"\"\"\n",
    "    config = GPTConfig(n_layer=6)\n",
    "    model = GPT(config)\n",
    "\n",
    "    # 检查标准层的初始化\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"c_proj.weight\" in name:\n",
    "            expected_std = config.init_std / math.sqrt(2 * config.n_layer)\n",
    "            actual_std = param.std().item()\n",
    "            print(f\"{name}: std={actual_std:.6f} (expected ~{expected_std:.6f})\")\n",
    "        elif \"wte.weight\" in name or \"c_attn.weight\" in name:\n",
    "            print(f\"{name}: std={param.std().item():.6f} (expected ~{config.init_std:.6f})\")\n",
    "\n",
    "    print(\"[PASS] test_initialization\")\n",
    "\n",
    "\n",
    "test_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 采样策略对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_temperature_sampling() -> None:\n",
    "    \"\"\"演示不同温度对采样结果的影响。\"\"\"\n",
    "    # 模拟一个 logit 分布 (10 个 token)\n",
    "    logits = torch.tensor([2.0, 1.5, 1.0, 0.5, 0.0, -0.5, -1.0, -1.5, -2.0, -3.0])\n",
    "    tokens = [f\"token_{i}\" for i in range(len(logits))]\n",
    "\n",
    "    temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(temperatures), figsize=(16, 4))\n",
    "\n",
    "    for ax, temp in zip(axes, temperatures):\n",
    "        probs = F.softmax(logits / temp, dim=0)\n",
    "        ax.bar(range(len(tokens)), probs.numpy(), color=\"steelblue\")\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha=\"right\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "        ax.set_title(f\"Temperature = {temp}\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        # 标注最高概率\n",
    "        top_idx = probs.argmax().item()\n",
    "        ax.bar(top_idx, probs[top_idx].item(), color=\"orange\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n温度影响分析:\")\n",
    "    print(\"  T=0.1: 分布极尖锐，几乎总是选择 token_0\")\n",
    "    print(\"  T=0.5: 分布较集中，偏向高概率 token\")\n",
    "    print(\"  T=1.0: 原始分布\")\n",
    "    print(\"  T=2.0: 分布平滑，低概率 token 也有机会\")\n",
    "\n",
    "\n",
    "demo_temperature_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_top_k_sampling() -> None:\n",
    "    \"\"\"演示 Top-K 采样。\"\"\"\n",
    "    logits = torch.randn(20)\n",
    "    k_values = [1, 3, 5, 10]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(k_values), figsize=(16, 4))\n",
    "\n",
    "    for ax, k in zip(axes, k_values):\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "\n",
    "        # Top-K 处理\n",
    "        v, _ = torch.topk(logits, k)\n",
    "        logits_masked = logits.clone()\n",
    "        logits_masked[logits < v[-1]] = -float(\"Inf\")\n",
    "        probs_topk = F.softmax(logits_masked, dim=0)\n",
    "\n",
    "        colors = [\"orange\" if logits[i] >= v[-1] else \"lightgray\" for i in range(len(logits))]\n",
    "        ax.bar(range(len(logits)), probs.numpy(), color=\"lightgray\")\n",
    "        ax.bar(range(len(logits)), probs_topp.numpy(), color=\"orange\")\n",
    "        ax.set_title(f\"Top-K={k}\")\n",
    "        ax.set_xlabel(\"Token Index\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop-K 采样分析:\")\n",
    "    print(\"  K=1: 等价于贪心采样 (Greedy)\")\n",
    "    print(\"  K=3: 只从前 3 个高概率 token 中采样\")\n",
    "    print(\"  K=10: 从前 10 个中采样，多样性增加\")\n",
    "    print(\"  优势: 避免采样到极低概率的'坏' token\")\n",
    "\n",
    "\n",
    "demo_top_k_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 完整生成 Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_generation_with_strategies() -> None:\n",
    "    \"\"\"对比不同采样策略的生成结果。\"\"\"\n",
    "    # 使用简单词表\n",
    "    vocab = {i: c for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz \")}\n",
    "    char_to_idx = {c: i for i, c in vocab.items()}\n",
    "\n",
    "    config = GPTConfig(\n",
    "        vocab_size=len(vocab),\n",
    "        block_size=32,\n",
    "        n_embd=64,\n",
    "        n_head=2,\n",
    "        n_layer=2,\n",
    "    )\n",
    "\n",
    "    # 随机初始化模型 (未训练)\n",
    "    model = GPT(config).eval()\n",
    "\n",
    "    prompt = \"the \"\n",
    "    context = torch.tensor([[char_to_idx[c] for c in prompt]], dtype=torch.long)\n",
    "\n",
    "    strategies = [\n",
    "        (\"Conservative (T=0.1, K=1)\", {\"temperature\": 0.1, \"top_k\": 1}),\n",
    "        (\"Balanced (T=1.0, K=5)\", {\"temperature\": 1.0, \"top_k\": 5}),\n",
    "        (\"Creative (T=2.0, K=10)\", {\"temperature\": 2.0, \"top_k\": 10}),\n",
    "    ]\n",
    "\n",
    "    print(\"生成结果对比 (未训练模型，仅供格式演示):\\n\")\n",
    "    for name, kwargs in strategies:\n",
    "        generated = model.generate(context, max_new_tokens=20, **kwargs)\n",
    "        text = \"\".join([vocab[i] for i in generated[0].tolist()])\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  {text}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "demo_generation_with_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. KV Cache: 高效推理 ⭐⭐\n",
    "\n",
    "### 7.1 核心思想\n",
    "\n",
    "**问题**: 自回归生成时，每生成一个 token 都要重新计算所有历史 token 的 K, V。\n",
    "\n",
    "**解决方案**: 缓存历史 token 的 K, V，只计算新 token 的 K, V。\n",
    "\n",
    "**复杂度优化**:\n",
    "- 无缓存: 生成 $n$ 个 token 需要 $O(n^3)$ 计算\n",
    "- 有缓存: 生成 $n$ 个 token 需要 $O(n^2)$ 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"KV Cache 用于高效自回归生成。\n",
    "\n",
    "    核心思想:\n",
    "        缓存历史 token 的 Key 和 Value，避免重复计算。\n",
    "\n",
    "    复杂度:\n",
    "        - 无缓存: O(n^3) 生成 n 个 token\n",
    "        - 有缓存: O(n^2) 生成 n 个 token\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_batch_size: int, max_seq_len: int, n_heads: int, head_dim: int) -> None:\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # 预分配缓存空间\n",
    "        self.k_cache = torch.zeros(max_batch_size, n_heads, max_seq_len, head_dim)\n",
    "        self.v_cache = torch.zeros(max_batch_size, n_heads, max_seq_len, head_dim)\n",
    "        self.seq_len = 0\n",
    "\n",
    "    def update(self, k: Tensor, v: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"更新缓存并返回完整的 K, V。\n",
    "\n",
    "        Args:\n",
    "            k: 新的 Key (batch, n_heads, new_len, head_dim)\n",
    "            v: 新的 Value (batch, n_heads, new_len, head_dim)\n",
    "\n",
    "        Returns:\n",
    "            完整的 K, V (包含历史和新的)\n",
    "        \"\"\"\n",
    "        batch_size, n_heads, new_len, head_dim = k.shape\n",
    "\n",
    "        # 更新缓存\n",
    "        self.k_cache[:batch_size, :, self.seq_len : self.seq_len + new_len, :] = k\n",
    "        self.v_cache[:batch_size, :, self.seq_len : self.seq_len + new_len, :] = v\n",
    "        self.seq_len += new_len\n",
    "\n",
    "        # 返回完整的 K, V\n",
    "        return (\n",
    "            self.k_cache[:batch_size, :, : self.seq_len, :],\n",
    "            self.v_cache[:batch_size, :, : self.seq_len, :],\n",
    "        )\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"重置缓存。\"\"\"\n",
    "        self.seq_len = 0\n",
    "\n",
    "\n",
    "class CausalSelfAttentionWithKVCache(nn.Module):\n",
    "    \"\"\"支持 KV Cache 的因果自注意力。\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "        self.scale = 1.0 / math.sqrt(self.head_size)\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        start_pos: int = 0,\n",
    "    ) -> Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # QKV 投影\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # 使用 KV Cache\n",
    "        if kv_cache is not None:\n",
    "            k, v = kv_cache.update(k, v)\n",
    "\n",
    "        # 注意力计算\n",
    "        scores = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        # 因果掩码 (只对新 token 应用)\n",
    "        seq_len = k.shape[2]\n",
    "        mask = torch.triu(torch.ones(T, seq_len, device=x.device), diagonal=seq_len - T + 1)\n",
    "        scores = scores.masked_fill(mask.bool().unsqueeze(0).unsqueeze(0), -1e9)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.c_proj(out)\n",
    "\n",
    "\n",
    "# 测试 KV Cache\n",
    "def test_kv_cache() -> None:\n",
    "    cache = KVCache(max_batch_size=2, max_seq_len=64, n_heads=4, head_dim=32)\n",
    "\n",
    "    # 模拟逐 token 生成\n",
    "    for i in range(5):\n",
    "        k_new = torch.randn(2, 4, 1, 32)  # 每次一个新 token\n",
    "        v_new = torch.randn(2, 4, 1, 32)\n",
    "        k_full, v_full = cache.update(k_new, v_new)\n",
    "        print(f\"Step {i+1}: K shape = {k_full.shape}, cached_len = {cache.seq_len}\")\n",
    "\n",
    "    print(\"[PASS] KV Cache 测试通过\")\n",
    "\n",
    "\n",
    "test_kv_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Nucleus (Top-P) Sampling ⭐\n",
    "\n",
    "### 8.1 核心思想\n",
    "\n",
    "**Top-K 的问题**: 固定 K 值，无法适应不同的概率分布。\n",
    "\n",
    "**Nucleus Sampling**: 动态选择累积概率达到 $p$ 的最小 token 集合。\n",
    "\n",
    "$$\\text{Nucleus}(p) = \\min\\{V' \\subseteq V : \\sum_{v \\in V'} P(v) \\geq p\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(logits: Tensor, top_p: float = 0.9, temperature: float = 1.0) -> Tensor:\n",
    "    \"\"\"Nucleus (Top-P) Sampling。\n",
    "\n",
    "    核心思想:\n",
    "        动态选择累积概率达到 p 的最小 token 集合，\n",
    "        比 Top-K 更灵活，能适应不同的概率分布。\n",
    "\n",
    "    Args:\n",
    "        logits: 模型输出 (batch, vocab_size)\n",
    "        top_p: 累积概率阈值 (0.9 表示保留 90% 概率质量)\n",
    "        temperature: 温度参数\n",
    "\n",
    "    Returns:\n",
    "        采样的 token 索引 (batch, 1)\n",
    "    \"\"\"\n",
    "    # 温度缩放\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # 排序\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "\n",
    "    # 计算累积概率\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # 找到累积概率超过 top_p 的位置\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # 保留第一个超过阈值的 token\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = False\n",
    "\n",
    "    # 将被移除的 token 的 logits 设为 -inf\n",
    "    sorted_logits[sorted_indices_to_remove] = -float(\"Inf\")\n",
    "\n",
    "    # 恢复原始顺序\n",
    "    logits = torch.gather(sorted_logits, -1, sorted_indices.argsort(-1))\n",
    "\n",
    "    # 采样\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "# 可视化 Top-P vs Top-K\n",
    "def compare_sampling_methods() -> None:\n",
    "    \"\"\"对比 Top-K 和 Top-P 采样。\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 两种不同的分布\n",
    "    # 分布1: 尖锐 (一个 token 主导)\n",
    "    logits_sharp = torch.tensor([[5.0, 1.0, 0.5, 0.2, 0.1, -1.0, -2.0, -3.0, -4.0, -5.0]])\n",
    "    # 分布2: 平坦 (多个 token 概率相近)\n",
    "    logits_flat = torch.tensor([[1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "    for row, (logits, name) in enumerate([(logits_sharp, \"尖锐分布\"), (logits_flat, \"平坦分布\")]):\n",
    "        probs = F.softmax(logits, dim=-1).squeeze()\n",
    "\n",
    "        # 原始分布\n",
    "        axes[row, 0].bar(range(10), probs.numpy(), color=\"steelblue\")\n",
    "        axes[row, 0].set_title(f\"{name}\\n原始分布\")\n",
    "        axes[row, 0].set_ylim(0, 1)\n",
    "\n",
    "        # Top-K=3\n",
    "        k = 3\n",
    "        v, _ = torch.topk(logits, k)\n",
    "        mask = logits < v[:, -1:]\n",
    "        logits_topk = logits.clone()\n",
    "        logits_topk[mask] = -float(\"Inf\")\n",
    "        probs_topk = F.softmax(logits_topk, dim=-1).squeeze()\n",
    "        colors = [\"orange\" if p > 0 else \"lightgray\" for p in probs_topk]\n",
    "        axes[row, 1].bar(range(10), probs_topk.numpy(), color=colors)\n",
    "        axes[row, 1].set_title(f\"Top-K=3\\n保留 {(probs_topk > 0).sum().item()} 个 token\")\n",
    "        axes[row, 1].set_ylim(0, 1)\n",
    "\n",
    "        # Top-P=0.9\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "        n_keep = (cumsum < 0.9).sum().item() + 1\n",
    "        probs_topp = probs.clone()\n",
    "        threshold = sorted_probs[n_keep - 1]\n",
    "        probs_topp[probs < threshold] = 0\n",
    "        probs_topp = probs_topp / probs_topp.sum()\n",
    "        colors = [\"green\" if p > 0 else \"lightgray\" for p in probs_topp]\n",
    "        axes[row, 2].bar(range(10), probs_topp.numpy(), color=colors)\n",
    "        axes[row, 2].set_title(f\"Top-P=0.9\\n保留 {(probs_topp > 0).sum().item()} 个 token\")\n",
    "        axes[row, 2].set_ylim(0, 1)\n",
    "\n",
    "    plt.suptitle(\"Top-K vs Top-P 采样对比\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n观察:\")\n",
    "    print(\"  - 尖锐分布: Top-K=3 和 Top-P=0.9 效果相似\")\n",
    "    print(\"  - 平坦分布: Top-P 自动保留更多 token，更灵活\")\n",
    "\n",
    "\n",
    "compare_sampling_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 完整训练循环 (Shakespeare 数据集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mini_gpt(epochs: int = 3, batch_size: int = 32, block_size: int = 64) -> GPT:\n",
    "    \"\"\"在 Shakespeare 数据集上训练 Mini-GPT。\"\"\"\n",
    "    # 下载数据\n",
    "    import urllib.request\n",
    "\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    )\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=10) as response:\n",
    "            text = response.read().decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        # 使用模拟数据\n",
    "        text = \"To be or not to be, that is the question. \" * 1000\n",
    "        print(\"使用模拟数据进行演示\")\n",
    "\n",
    "    print(f\"数据集大小: {len(text):,} 字符\")\n",
    "\n",
    "    # 构建词表\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    itos = {i: ch for i, ch in enumerate(chars)}\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "    print(f\"词表大小: {vocab_size}\")\n",
    "\n",
    "    # 编码数据\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    # 数据加载函数\n",
    "    def get_batch(split: str) -> Tuple[Tensor, Tensor]:\n",
    "        data_split = train_data if split == \"train\" else val_data\n",
    "        ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "        x = torch.stack([data_split[i : i + block_size] for i in ix])\n",
    "        y = torch.stack([data_split[i + 1 : i + block_size + 1] for i in ix])\n",
    "        return x.to(device), y.to(device)\n",
    "\n",
    "    # 创建模型\n",
    "    config = GPTConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=block_size,\n",
    "        n_embd=128,\n",
    "        n_head=4,\n",
    "        n_layer=4,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "    model = GPT(config).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # 训练循环\n",
    "    steps_per_epoch = 100\n",
    "    print(\"\\n开始训练...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step in range(steps_per_epoch):\n",
    "            xb, yb = get_batch(\"train\")\n",
    "            _, loss = model(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / steps_per_epoch\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xv, yv = get_batch(\"val\")\n",
    "            _, val_loss = model(xv, yv)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 生成示例\n",
    "    print(\"\\n生成示例:\")\n",
    "    model.eval()\n",
    "    context = torch.tensor([encode(\"To be or not\")], dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=100, temperature=0.8, top_k=40)\n",
    "    print(decode(generated[0].tolist()))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 运行训练 (减少 epoch 用于演示)\n",
    "trained_model = train_mini_gpt(epochs=2, batch_size=16, block_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 总结\n",
    "\n",
    "| 特性 | 说明 |\n",
    "|:-----|:-----|\n",
    "| **因果掩码** | 防止自回归生成时的信息泄漏 |\n",
    "| **权重初始化** | GPT-2 风格，残差层特殊处理 |\n",
    "| **温度采样** | 控制生成保守度 (T<1 保守, T>1 创造) |\n",
    "| **Top-K 采样** | 固定数量的高概率 token |\n",
    "| **Top-P (Nucleus)** | 动态选择累积概率达到 p 的 token |\n",
    "| **KV Cache** | 缓存历史 K, V，推理加速 $O(n^3) \\to O(n^2)$ |\n",
    "| **Pre-Norm** | 梯度流更稳定，GPT 标配 |\n",
    "\n",
    "**进阶学习**: Speculative Decoding, Continuous Batching, PagedAttention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
