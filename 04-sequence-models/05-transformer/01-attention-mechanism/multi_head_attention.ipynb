{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力机制 (Multi-Head Attention)\n",
    "\n",
    "本 Notebook 讲解 Transformer 的核心组件：**多头注意力 (Multi-Head Attention)**。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 理论讲解\n",
    "\n",
    "### 1.1 为什么需要多头？\n",
    "\n",
    "单头注意力只能学习一种\"关注模式\"。但语言中的关系是多维度的：\n",
    "\n",
    "| 关注维度 | 示例 |\n",
    "|:--------|:-----|\n",
    "| 语法结构 | 主语关注谓语 |\n",
    "| 指代关系 | 代词关注其指代的名词 |\n",
    "| 语义相似 | 同义词之间相互关注 |\n",
    "| 位置关系 | 相邻词之间的关注 |\n",
    "\n",
    "**类比**：就像用**多组不同的滤镜**观察同一张图像：\n",
    "- 滤镜 1 捕捉边缘\n",
    "- 滤镜 2 捕捉颜色\n",
    "- 滤镜 3 捕捉纹理\n",
    "- 最后将所有信息融合\n",
    "\n",
    "多头注意力让模型能够**同时**从不同的表示子空间学习信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 数学公式\n",
    "\n",
    "**多头注意力**的定义：\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O$$\n",
    "\n",
    "其中每个头的计算为：\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "**参数维度**：\n",
    "- $W_i^Q, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$\n",
    "- $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$\n",
    "- 通常设置 $d_k = d_v = d_{model} / h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 维度变化流程\n",
    "\n",
    "```\n",
    "输入: [batch, seq, d_model]\n",
    "         |\n",
    "         v\n",
    "    Linear(Q/K/V)\n",
    "         |\n",
    "         v\n",
    "[batch, seq, d_model]\n",
    "         |\n",
    "         v\n",
    "    view + transpose\n",
    "         |\n",
    "         v\n",
    "[batch, n_heads, seq, d_k]   <-- 并行计算多头\n",
    "         |\n",
    "         v\n",
    "  Scaled Dot-Product Attention\n",
    "         |\n",
    "         v\n",
    "[batch, n_heads, seq, d_k]\n",
    "         |\n",
    "         v\n",
    "    transpose + view\n",
    "         |\n",
    "         v\n",
    "[batch, seq, d_model]   <-- 拼接所有头\n",
    "         |\n",
    "         v\n",
    "    Linear(W_o)\n",
    "         |\n",
    "         v\n",
    "输出: [batch, seq, d_model]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力机制。\n",
    "\n",
    "    将输入投影到多个子空间，分别计算注意力后拼接输出。\n",
    "\n",
    "    Attributes:\n",
    "        d_model: 模型总维度。\n",
    "        n_heads: 注意力头数。\n",
    "        d_k: 每个头的维度 (d_model // n_heads)。\n",
    "        W_q, W_k, W_v: Q/K/V 的线性投影层。\n",
    "        W_o: 输出线性投影层。\n",
    "\n",
    "    Example:\n",
    "        >>> mha = MultiHeadAttention(d_model=512, n_heads=8)\n",
    "        >>> x = torch.randn(2, 10, 512)\n",
    "        >>> output, weights = mha(x, x, x)\n",
    "        >>> assert output.shape == (2, 10, 512)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.0) -> None:\n",
    "        \"\"\"初始化多头注意力模块。\n",
    "\n",
    "        Args:\n",
    "            d_model: 模型总维度，必须能被 n_heads 整除。\n",
    "            n_heads: 注意力头的数量。\n",
    "            dropout: Dropout 概率，默认为 0.0。\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: 当 d_model 不能被 n_heads 整除时。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, f\"d_model({d_model}) 必须能被 n_heads({n_heads}) 整除\"\n",
    "\n",
    "        self.d_model: int = d_model\n",
    "        self.n_heads: int = n_heads\n",
    "        self.d_k: int = d_model // n_heads\n",
    "\n",
    "        # Q, K, V 线性投影\n",
    "        self.W_q: nn.Linear = nn.Linear(d_model, d_model)\n",
    "        self.W_k: nn.Linear = nn.Linear(d_model, d_model)\n",
    "        self.W_v: nn.Linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 输出投影\n",
    "        self.W_o: nn.Linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout: nn.Dropout = nn.Dropout(p=dropout)\n",
    "        self.scale: float = 1.0 / math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"前向传播。\n",
    "\n",
    "        Args:\n",
    "            query: Query 张量，形状 (batch, seq_q, d_model)。\n",
    "            key: Key 张量，形状 (batch, seq_k, d_model)。\n",
    "            value: Value 张量，形状 (batch, seq_k, d_model)。\n",
    "            mask: 可选掩码，形状 (batch, 1, seq_q, seq_k) 或可广播形状。\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - output: 输出张量，形状 (batch, seq_q, d_model)。\n",
    "                - attention_weights: 注意力权重，形状 (batch, n_heads, seq_q, seq_k)。\n",
    "        \"\"\"\n",
    "        batch_size: int = query.size(0)\n",
    "\n",
    "        # Step 1: 线性变换 -> (batch, seq, d_model)\n",
    "        q: torch.Tensor = self.W_q(query)\n",
    "        k: torch.Tensor = self.W_k(key)\n",
    "        v: torch.Tensor = self.W_v(value)\n",
    "\n",
    "        # Step 2: 重塑为多头 -> (batch, n_heads, seq, d_k)\n",
    "        q = q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Step 3: 计算注意力分数 -> (batch, n_heads, seq_q, seq_k)\n",
    "        scores: torch.Tensor = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Step 4: 应用掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        # Step 5: Softmax + Dropout\n",
    "        attention_weights: torch.Tensor = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Step 6: 加权求和 -> (batch, n_heads, seq_q, d_k)\n",
    "        context: torch.Tensor = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # Step 7: 拼接多头 -> (batch, seq_q, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Step 8: 输出投影\n",
    "        output: torch.Tensor = self.W_o(context)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 验证与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# 超参数\n",
    "d_model: int = 512\n",
    "n_heads: int = 8\n",
    "batch_size: int = 2\n",
    "seq_len: int = 10\n",
    "\n",
    "# 初始化模型\n",
    "mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "print(f\"模型参数量: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "print(f\"每个头的维度 d_k: {mha.d_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输入\n",
    "x: torch.Tensor = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "\n",
    "# 前向传播（自注意力：Q=K=V=x）\n",
    "output, attention_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 断言检查：输入输出形状一致\n",
    "assert x.shape == output.shape, \"输入输出形状不匹配！\"\n",
    "print(\"✓ 断言通过：input.shape == output.shape\")\n",
    "\n",
    "# 验证注意力权重归一化\n",
    "weight_sum = attention_weights.sum(dim=-1)\n",
    "assert torch.allclose(weight_sum, torch.ones_like(weight_sum), atol=1e-6)\n",
    "print(\"✓ 断言通过：注意力权重每行之和为 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 可视化：不同头的注意力模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"8 个注意力头的关注模式\", fontsize=14)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    sns.heatmap(\n",
    "        attention_weights[0, i].detach().numpy(),\n",
    "        ax=ax,\n",
    "        cmap=\"viridis\",\n",
    "        cbar=False,\n",
    "        square=True,\n",
    "    )\n",
    "    ax.set_title(f\"Head {i+1}\")\n",
    "    ax.set_xlabel(\"Key\")\n",
    "    ax.set_ylabel(\"Query\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察要点\n",
    "\n",
    "- 不同的头学习到**不同的注意力模式**\n",
    "- 有些头可能关注局部（对角线附近）\n",
    "- 有些头可能关注全局（分布更均匀）\n",
    "- 这种多样性让模型能捕获更丰富的语义关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 总结\n",
    "\n",
    "| 要点 | 说明 |\n",
    "|:-----|:-----|\n",
    "| **多头并行** | 将 d_model 拆分为 h 个子空间并行计算 |\n",
    "| **参数效率** | 总参数量与单头相当 (4 × d_model²) |\n",
    "| **表达能力** | 不同头捕获不同类型的依赖关系 |\n",
    "| **维度不变** | 输入输出形状保持一致 |\n",
    "\n",
    "**下一步**：在 `02-transformer-architecture/` 中，我们将学习如何将多头注意力组装成完整的 Encoder 和 Decoder。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
