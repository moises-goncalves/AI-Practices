{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 缩放点积自注意力机制 (Scaled Dot-Product Self-Attention)\n",
    "\n",
    "**SOTA 教育标准实现** | Transformer 架构核心组件\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 理论基础与数学推导\n",
    "\n",
    "### 1.1 核心公式\n",
    "\n",
    "缩放点积注意力的标准形式：\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "其中：\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$：Query 矩阵\n",
    "- $K \\in \\mathbb{R}^{m \\times d_k}$：Key 矩阵  \n",
    "- $V \\in \\mathbb{R}^{m \\times d_v}$：Value 矩阵\n",
    "- $d_k$：Key/Query 的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 缩放因子的数学证明 ⭐\n",
    "\n",
    "**定理**：假设 $Q, K$ 的元素独立同分布，均值为 0，方差为 1，则 $Q \\cdot K^T$ 的方差为 $d_k$。\n",
    "\n",
    "**证明**：\n",
    "\n",
    "设 $q = (q_1, q_2, \\ldots, q_{d_k})$ 和 $k = (k_1, k_2, \\ldots, k_{d_k})$ 分别为 $Q$ 和 $K$ 的某一行向量。\n",
    "\n",
    "点积定义为：\n",
    "$$z = q \\cdot k = \\sum_{i=1}^{d_k} q_i k_i$$\n",
    "\n",
    "**Step 1: 计算期望**\n",
    "\n",
    "由于 $q_i$ 和 $k_i$ 独立且 $\\mathbb{E}[q_i] = \\mathbb{E}[k_i] = 0$：\n",
    "\n",
    "$$\\mathbb{E}[z] = \\mathbb{E}\\left[\\sum_{i=1}^{d_k} q_i k_i\\right] = \\sum_{i=1}^{d_k} \\mathbb{E}[q_i] \\cdot \\mathbb{E}[k_i] = 0$$\n",
    "\n",
    "**Step 2: 计算方差**\n",
    "\n",
    "$$\\text{Var}(z) = \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i k_i\\right)$$\n",
    "\n",
    "由于各项独立：\n",
    "$$= \\sum_{i=1}^{d_k} \\text{Var}(q_i k_i)$$\n",
    "\n",
    "对于独立随机变量的乘积，有：\n",
    "$$\\text{Var}(q_i k_i) = \\mathbb{E}[q_i^2 k_i^2] - (\\mathbb{E}[q_i k_i])^2 = \\mathbb{E}[q_i^2] \\cdot \\mathbb{E}[k_i^2] - 0 = 1 \\cdot 1 = 1$$\n",
    "\n",
    "因此：\n",
    "$$\\boxed{\\text{Var}(z) = \\sum_{i=1}^{d_k} 1 = d_k}$$\n",
    "\n",
    "**直觉解释**：\n",
    "\n",
    "当 $d_k = 512$ 时，点积的标准差为 $\\sqrt{512} \\approx 22.6$。如此大的数值会使 Softmax 输出趋近于 one-hot 分布，导致：\n",
    "1. **梯度消失**：Softmax 在极端值处梯度接近 0\n",
    "2. **注意力坍缩**：模型只关注单一位置，丧失全局建模能力\n",
    "\n",
    "除以 $\\sqrt{d_k}$ 后，方差重新归一化为 1，Softmax 保持在健康的梯度区间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 代码实现 (SOTA 标准)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttentionConfig:\n",
    "    \"\"\"注意力机制配置类。\n",
    "\n",
    "    使用 dataclass 管理所有超参数，避免魔术数字散落在代码中。\n",
    "\n",
    "    Attributes:\n",
    "        d_k: Key/Query 维度，用于计算缩放因子\n",
    "        d_v: Value 维度，决定输出维度\n",
    "        dropout: Dropout 概率，用于注意力权重正则化\n",
    "        mask_value: 掩码填充值，使用 -1e9 而非 -inf 保证数值稳定性\n",
    "    \"\"\"\n",
    "\n",
    "    d_k: int = 64\n",
    "    d_v: int = 64\n",
    "    dropout: float = 0.0\n",
    "    mask_value: float = -1e9  # 数值稳定性：避免使用 -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力机制 (Scaled Dot-Product Attention)。\n",
    "\n",
    "    核心思想 (Core Idea):\n",
    "        通过 Query 与 Key 的点积计算相似度，经 Softmax 归一化后对 Value 加权求和。\n",
    "        缩放因子 1/sqrt(d_k) 防止高维点积导致的梯度消失。\n",
    "\n",
    "    数学原理 (Mathematical Theory):\n",
    "        Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "\n",
    "        其中 einsum 约定:\n",
    "        - 'b i d, b j d -> b i j': Q @ K^T (注意力分数)\n",
    "        - 'b i j, b j d -> b i d': weights @ V (加权求和)\n",
    "\n",
    "    复杂度 (Complexity):\n",
    "        - 时间: O(n^2 * d) 其中 n 为序列长度\n",
    "        - 空间: O(n^2) 用于存储注意力矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: AttentionConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.scale: float = 1.0 / math.sqrt(config.d_k)\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: Tensor,  # shape: (batch, seq_q, d_k)\n",
    "        k: Tensor,  # shape: (batch, seq_k, d_k)\n",
    "        v: Tensor,  # shape: (batch, seq_k, d_v)\n",
    "        mask: Optional[Tensor] = None,  # shape: (batch, seq_q, seq_k)\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"前向传播。\n",
    "\n",
    "        Args:\n",
    "            q: Query 张量\n",
    "            k: Key 张量\n",
    "            v: Value 张量\n",
    "            mask: 掩码张量，True/1 的位置将被屏蔽\n",
    "\n",
    "        Returns:\n",
    "            output: 注意力输出 (batch, seq_q, d_v)\n",
    "            attention_weights: 注意力权重 (batch, seq_q, seq_k)\n",
    "        \"\"\"\n",
    "        # Step 1: 计算注意力分数 (使用 einsum 提升可读性)\n",
    "        # einsum: 'b i d, b j d -> b i j' 表示 batch 矩阵乘法 Q @ K^T\n",
    "        scores: Tensor = torch.einsum(\"b i d, b j d -> b i j\", q, k)\n",
    "\n",
    "        # Step 2: 缩放 (防止方差过大导致 Softmax 饱和)\n",
    "        scores = scores * self.scale\n",
    "\n",
    "        # Step 3: 应用掩码 (使用 -1e9 而非 -inf 保证数值稳定)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.bool(), self.config.mask_value)\n",
    "\n",
    "        # Step 4: Softmax 归一化\n",
    "        attention_weights: Tensor = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Step 5: Dropout 正则化\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Step 6: 加权求和\n",
    "        # einsum: 'b i j, b j d -> b i d' 表示 weights @ V\n",
    "        output: Tensor = torch.einsum(\"b i j, b j d -> b i d\", attention_weights, v)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 验证测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass() -> None:\n",
    "    \"\"\"验证前向传播的输出形状正确性。\"\"\"\n",
    "    config = AttentionConfig(d_k=64, d_v=64)\n",
    "    attention = ScaledDotProductAttention(config)\n",
    "\n",
    "    batch_size, seq_len = 2, 10\n",
    "    q = torch.randn(batch_size, seq_len, config.d_k)\n",
    "    k = torch.randn(batch_size, seq_len, config.d_k)\n",
    "    v = torch.randn(batch_size, seq_len, config.d_v)\n",
    "\n",
    "    output, weights = attention(q, k, v)\n",
    "\n",
    "    assert output.shape == (batch_size, seq_len, config.d_v), f\"输出形状错误: {output.shape}\"\n",
    "    assert weights.shape == (batch_size, seq_len, seq_len), f\"权重形状错误: {weights.shape}\"\n",
    "\n",
    "    # 验证 Softmax 归一化 (每行和为 1)\n",
    "    row_sums = weights.sum(dim=-1)\n",
    "    assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5), \"Softmax 归一化失败\"\n",
    "\n",
    "    print(\"✓ test_forward_pass 通过\")\n",
    "\n",
    "\n",
    "test_forward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_variance_scaling() -> None:\n",
    "    \"\"\"验证缩放因子确实将方差归一化。\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    d_k = 512\n",
    "    n_samples = 10000\n",
    "\n",
    "    # 生成标准正态分布的 Q, K\n",
    "    q = torch.randn(n_samples, d_k)\n",
    "    k = torch.randn(n_samples, d_k)\n",
    "\n",
    "    # 未缩放的点积\n",
    "    unscaled = torch.einsum(\"i d, i d -> i\", q, k)\n",
    "    print(f\"未缩放点积 - 均值: {unscaled.mean():.4f}, 方差: {unscaled.var():.4f} (理论值: {d_k})\")\n",
    "\n",
    "    # 缩放后的点积\n",
    "    scaled = unscaled / math.sqrt(d_k)\n",
    "    print(f\"缩放后点积 - 均值: {scaled.mean():.4f}, 方差: {scaled.var():.4f} (理论值: 1.0)\")\n",
    "\n",
    "    assert abs(unscaled.var().item() - d_k) < 50, \"未缩放方差偏离理论值过大\"\n",
    "    assert abs(scaled.var().item() - 1.0) < 0.1, \"缩放后方差未归一化\"\n",
    "\n",
    "    print(\"✓ test_variance_scaling 通过\")\n",
    "\n",
    "\n",
    "test_variance_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 高标验证：真实句子 Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_on_sentence(\n",
    "    sentence: str = \"Thinking strictly is hard\",\n",
    "    d_k: int = 64,\n",
    "    seed: int = 42,\n",
    ") -> None:\n",
    "    \"\"\"在真实句子上可视化注意力权重。\n",
    "\n",
    "    Args:\n",
    "        sentence: 输入句子\n",
    "        d_k: 嵌入维度\n",
    "        seed: 随机种子\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    tokens = sentence.split()\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    # 模拟词嵌入 (实际应用中使用预训练嵌入)\n",
    "    embeddings = torch.randn(1, seq_len, d_k)\n",
    "\n",
    "    # 线性投影生成 Q, K, V\n",
    "    W_q = nn.Linear(d_k, d_k, bias=False)\n",
    "    W_k = nn.Linear(d_k, d_k, bias=False)\n",
    "    W_v = nn.Linear(d_k, d_k, bias=False)\n",
    "\n",
    "    q = W_q(embeddings)\n",
    "    k = W_k(embeddings)\n",
    "    v = W_v(embeddings)\n",
    "\n",
    "    # 计算注意力\n",
    "    config = AttentionConfig(d_k=d_k)\n",
    "    attention = ScaledDotProductAttention(config)\n",
    "    _, weights = attention(q, k, v)\n",
    "\n",
    "    # 绘制热力图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        weights[0].detach().numpy(),\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar_kws={\"label\": \"Attention Weight\"},\n",
    "        square=True,\n",
    "    )\n",
    "    plt.xlabel(\"Key (被关注的词)\", fontsize=12)\n",
    "    plt.ylabel(\"Query (发起查询的词)\", fontsize=12)\n",
    "    plt.title(f'Self-Attention Weights: \"{sentence}\"', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 分析结果\n",
    "    print(\"\\n注意力分析:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        top_k = 2\n",
    "        top_indices = weights[0, i].topk(top_k).indices.tolist()\n",
    "        top_tokens = [tokens[j] for j in top_indices]\n",
    "        print(f\"  '{token}' 最关注: {top_tokens}\")\n",
    "\n",
    "\n",
    "visualize_attention_on_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 热力图解读\n",
    "\n",
    "- **行 (Y轴)**：Query 位置，表示\"谁在发起查询\"\n",
    "- **列 (X轴)**：Key 位置，表示\"被关注的对象\"\n",
    "- **颜色深浅**：注意力权重大小，越深表示关注度越高\n",
    "- **每行之和 = 1**：Softmax 保证归一化\n",
    "\n",
    "在训练后的模型中，我们期望看到：\n",
    "- \"Thinking\" 可能关注 \"hard\" (语义关联)\n",
    "- \"strictly\" 可能关注 \"Thinking\" (修饰关系)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 因果掩码 (Causal Mask) 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_causal_mask(seq_len: int = 5) -> Tensor:\n",
    "    \"\"\"可视化因果掩码的工作原理。\n",
    "\n",
    "    因果掩码阻止位置 i 关注位置 j > i，防止信息泄漏。\n",
    "    \"\"\"\n",
    "    # 创建因果掩码 (上三角为 True)\n",
    "    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # 左图：掩码矩阵\n",
    "    ax1 = axes[0]\n",
    "    mask_visual = causal_mask.float().numpy()\n",
    "    sns.heatmap(\n",
    "        mask_visual,\n",
    "        ax=ax1,\n",
    "        annot=True,\n",
    "        fmt=\".0f\",\n",
    "        cmap=\"Reds\",\n",
    "        cbar_kws={\"label\": \"Masked (1) / Visible (0)\"},\n",
    "        square=True,\n",
    "        xticklabels=[f\"t={i}\" for i in range(seq_len)],\n",
    "        yticklabels=[f\"t={i}\" for i in range(seq_len)],\n",
    "    )\n",
    "    ax1.set_xlabel(\"Key 位置 (被关注)\")\n",
    "    ax1.set_ylabel(\"Query 位置 (发起查询)\")\n",
    "    ax1.set_title(\"因果掩码矩阵\\n(1=屏蔽未来, 0=可见)\")\n",
    "\n",
    "    # 右图：信息流向示意\n",
    "    ax2 = axes[1]\n",
    "    allowed = (~causal_mask).float().numpy()\n",
    "    sns.heatmap(\n",
    "        allowed,\n",
    "        ax=ax2,\n",
    "        annot=True,\n",
    "        fmt=\".0f\",\n",
    "        cmap=\"Greens\",\n",
    "        cbar_kws={\"label\": \"Allowed (1) / Blocked (0)\"},\n",
    "        square=True,\n",
    "        xticklabels=[f\"t={i}\" for i in range(seq_len)],\n",
    "        yticklabels=[f\"t={i}\" for i in range(seq_len)],\n",
    "    )\n",
    "    ax2.set_xlabel(\"Key 位置\")\n",
    "    ax2.set_ylabel(\"Query 位置\")\n",
    "    ax2.set_title(\"允许的注意力连接\\n(下三角 + 对角线)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n信息流解释:\")\n",
    "    print(\"  - 位置 0 只能看到自己\")\n",
    "    print(\"  - 位置 1 可以看到位置 0, 1\")\n",
    "    print(\"  - 位置 i 可以看到位置 0, 1, ..., i\")\n",
    "    print(\"  - 这防止了自回归生成时的信息泄漏\")\n",
    "\n",
    "    return causal_mask\n",
    "\n",
    "\n",
    "causal_mask = visualize_causal_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_masked_attention(sentence: str = \"The cat sat on\") -> None:\n",
    "    \"\"\"演示带因果掩码的注意力。\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    tokens = sentence.split()\n",
    "    seq_len = len(tokens)\n",
    "    d_k = 64\n",
    "\n",
    "    # 创建输入\n",
    "    q = torch.randn(1, seq_len, d_k)\n",
    "    k = torch.randn(1, seq_len, d_k)\n",
    "    v = torch.randn(1, seq_len, d_k)\n",
    "\n",
    "    # 因果掩码\n",
    "    mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)\n",
    "\n",
    "    # 计算注意力\n",
    "    config = AttentionConfig(d_k=d_k)\n",
    "    attention = ScaledDotProductAttention(config)\n",
    "    _, weights = attention(q, k, v, mask=mask)\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        weights[0].detach().numpy(),\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"Oranges\",\n",
    "        cbar_kws={\"label\": \"Attention Weight\"},\n",
    "        square=True,\n",
    "    )\n",
    "    plt.xlabel(\"Key (被关注的词)\")\n",
    "    plt.ylabel(\"Query (发起查询的词)\")\n",
    "    plt.title(f'Masked Self-Attention: \"{sentence}\"\\n(上三角被屏蔽)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n观察: 上三角区域权重为 0，每个词只能关注自己和之前的词\")\n",
    "\n",
    "\n",
    "demo_masked_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 多头注意力机制 (Multi-Head Attention) ⭐\n",
    "\n",
    "### 6.1 核心思想\n",
    "\n",
    "**问题**: 单头注意力只能学习一种\"关注模式\"。\n",
    "\n",
    "**解决方案**: 并行运行多个注意力头，每个头学习不同的关注模式。\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "其中 $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "**直觉**: 不同的头可以关注不同的语义关系：\n",
    "- Head 1: 语法依赖 (主谓关系)\n",
    "- Head 2: 指代消解 (代词指向)\n",
    "- Head 3: 位置关系 (相邻词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力机制 (Multi-Head Attention)。\n",
    "\n",
    "    核心思想:\n",
    "        将 d_model 维度分成 n_heads 个子空间，每个子空间独立计算注意力，\n",
    "        最后拼接并投影回原始维度。\n",
    "\n",
    "    数学原理:\n",
    "        MultiHead(Q, K, V) = Concat(head_1, ..., head_h) @ W_O\n",
    "        head_i = Attention(Q @ W_Q_i, K @ W_K_i, V @ W_V_i)\n",
    "\n",
    "    复杂度:\n",
    "        - 时间: O(n^2 * d) 其中 n 为序列长度，d 为维度\n",
    "        - 空间: O(n^2 * h) 用于存储 h 个头的注意力矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            d_model % n_heads == 0\n",
    "        ), f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
    "\n",
    "        # 合并的 QKV 投影 (更高效)\n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=bias)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        return_attention: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"前向传播。\n",
    "\n",
    "        Args:\n",
    "            x: 输入张量 (batch, seq_len, d_model)\n",
    "            mask: 掩码张量 (batch, 1, seq_len, seq_len) 或 (batch, seq_len, seq_len)\n",
    "            return_attention: 是否返回注意力权重\n",
    "\n",
    "        Returns:\n",
    "            output: 输出张量 (batch, seq_len, d_model)\n",
    "            attention_weights: 可选的注意力权重 (batch, n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # 合并投影并分割 QKV\n",
    "        qkv = self.W_qkv(x)  # (B, T, 3*d_model)\n",
    "        qkv = qkv.reshape(B, T, 3, self.n_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, n_heads, T, d_k)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # 注意力分数: einsum 'b h i d, b h j d -> b h i j'\n",
    "        scores = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        # 应用掩码\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)  # (B, 1, T, T)\n",
    "            scores = scores.masked_fill(mask.bool(), -1e9)\n",
    "\n",
    "        # Softmax + Dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 加权求和: einsum 'b h i j, b h j d -> b h i d'\n",
    "        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn_weights, v)\n",
    "\n",
    "        # 合并多头: (B, n_heads, T, d_k) -> (B, T, d_model)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # 输出投影\n",
    "        output = self.W_o(out)\n",
    "\n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "        return output, None\n",
    "\n",
    "\n",
    "# 测试多头注意力\n",
    "def test_multi_head_attention() -> None:\n",
    "    mha = MultiHeadAttention(d_model=256, n_heads=8)\n",
    "    x = torch.randn(2, 10, 256)\n",
    "    out, attn = mha(x, return_attention=True)\n",
    "\n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"输出形状: {out.shape}\")\n",
    "    print(f\"注意力形状: {attn.shape}\")\n",
    "    print(f\"参数量: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "\n",
    "\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Flash Attention 原理与简化实现 ⭐⭐\n",
    "\n",
    "### 7.1 标准注意力的内存瓶颈\n",
    "\n",
    "**问题**: 标准注意力需要存储完整的 $n \\times n$ 注意力矩阵。\n",
    "\n",
    "- 序列长度 $n = 4096$, 头数 $h = 32$, batch $B = 8$\n",
    "- 注意力矩阵大小: $B \\times h \\times n \\times n = 8 \\times 32 \\times 4096 \\times 4096 \\approx 17$ GB (FP32)\n",
    "\n",
    "### 7.2 Flash Attention 核心思想\n",
    "\n",
    "**关键洞察**: 利用 GPU 内存层次结构，分块计算注意力。\n",
    "\n",
    "1. **分块 (Tiling)**: 将 Q, K, V 分成小块，每次只处理一块\n",
    "2. **重计算 (Recomputation)**: 反向传播时重新计算注意力，而非存储\n",
    "3. **在线 Softmax**: 使用数值稳定的在线算法计算 Softmax\n",
    "\n",
    "### 7.3 在线 Softmax 算法\n",
    "\n",
    "标准 Softmax 需要两次遍历:\n",
    "1. 第一遍: 找最大值 $m = \\max(x)$\n",
    "2. 第二遍: 计算 $\\exp(x - m) / \\sum \\exp(x - m)$\n",
    "\n",
    "**在线算法** (单次遍历):\n",
    "$$m_{new} = \\max(m_{old}, x_i)$$\n",
    "$$d_{new} = d_{old} \\cdot e^{m_{old} - m_{new}} + e^{x_i - m_{new}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_simplified(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    block_size: int = 64,\n",
    ") -> Tensor:\n",
    "    \"\"\"Flash Attention 简化实现 (教学版本)。\n",
    "\n",
    "    注意: 这是简化的 Python 实现，用于理解原理。\n",
    "    实际 Flash Attention 使用 CUDA kernel 实现，速度快 2-4x。\n",
    "\n",
    "    核心思想:\n",
    "        1. 分块处理 K, V，避免存储完整注意力矩阵\n",
    "        2. 使用在线 Softmax 算法，数值稳定\n",
    "        3. 累积输出，最后归一化\n",
    "\n",
    "    Args:\n",
    "        q: Query (batch, seq_q, d_k)\n",
    "        k: Key (batch, seq_k, d_k)\n",
    "        v: Value (batch, seq_k, d_v)\n",
    "        block_size: 分块大小\n",
    "\n",
    "    Returns:\n",
    "        output: (batch, seq_q, d_v)\n",
    "    \"\"\"\n",
    "    B, N, d_k = q.shape\n",
    "    _, M, d_v = v.shape\n",
    "    scale = 1.0 / math.sqrt(d_k)\n",
    "\n",
    "    # 初始化输出和归一化因子\n",
    "    output = torch.zeros(B, N, d_v, device=q.device, dtype=q.dtype)\n",
    "    row_max = torch.full((B, N, 1), float(\"-inf\"), device=q.device, dtype=q.dtype)\n",
    "    row_sum = torch.zeros(B, N, 1, device=q.device, dtype=q.dtype)\n",
    "\n",
    "    # 分块遍历 K, V\n",
    "    for j in range(0, M, block_size):\n",
    "        j_end = min(j + block_size, M)\n",
    "        k_block = k[:, j:j_end, :]  # (B, block, d_k)\n",
    "        v_block = v[:, j:j_end, :]  # (B, block, d_v)\n",
    "\n",
    "        # 计算当前块的注意力分数\n",
    "        scores = torch.einsum(\"b i d, b j d -> b i j\", q, k_block) * scale  # (B, N, block)\n",
    "\n",
    "        # 在线 Softmax: 更新最大值\n",
    "        block_max = scores.max(dim=-1, keepdim=True).values  # (B, N, 1)\n",
    "        new_max = torch.maximum(row_max, block_max)\n",
    "\n",
    "        # 重新缩放之前的累积值\n",
    "        exp_diff = torch.exp(row_max - new_max)\n",
    "        output = output * exp_diff\n",
    "        row_sum = row_sum * exp_diff\n",
    "\n",
    "        # 计算当前块的贡献\n",
    "        exp_scores = torch.exp(scores - new_max)  # (B, N, block)\n",
    "        block_sum = exp_scores.sum(dim=-1, keepdim=True)  # (B, N, 1)\n",
    "\n",
    "        # 累积输出和归一化因子\n",
    "        output = output + torch.einsum(\"b i j, b j d -> b i d\", exp_scores, v_block)\n",
    "        row_sum = row_sum + block_sum\n",
    "        row_max = new_max\n",
    "\n",
    "    # 最终归一化\n",
    "    output = output / row_sum\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# 验证 Flash Attention 与标准实现的一致性\n",
    "def verify_flash_attention() -> None:\n",
    "    torch.manual_seed(42)\n",
    "    B, N, d = 2, 128, 64\n",
    "\n",
    "    q = torch.randn(B, N, d)\n",
    "    k = torch.randn(B, N, d)\n",
    "    v = torch.randn(B, N, d)\n",
    "\n",
    "    # 标准注意力\n",
    "    config = AttentionConfig(d_k=d, d_v=d)\n",
    "    std_attn = ScaledDotProductAttention(config)\n",
    "    std_out, _ = std_attn(q, k, v)\n",
    "\n",
    "    # Flash Attention\n",
    "    flash_out = flash_attention_simplified(q, k, v, block_size=32)\n",
    "\n",
    "    # 比较\n",
    "    diff = (std_out - flash_out).abs().max().item()\n",
    "    print(f\"标准注意力 vs Flash Attention 最大差异: {diff:.2e}\")\n",
    "    print(f\"验证通过: {diff < 1e-5}\")\n",
    "\n",
    "\n",
    "verify_flash_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 复杂度分析与优化策略\n",
    "\n",
    "### 8.1 标准注意力复杂度\n",
    "\n",
    "| 操作 | 时间复杂度 | 空间复杂度 |\n",
    "|:-----|:----------|:-----------|\n",
    "| QK^T 计算 | $O(n^2 d)$ | $O(n^2)$ |\n",
    "| Softmax | $O(n^2)$ | $O(n^2)$ |\n",
    "| Attention @ V | $O(n^2 d)$ | $O(nd)$ |\n",
    "| **总计** | $O(n^2 d)$ | $O(n^2)$ |\n",
    "\n",
    "### 8.2 长序列优化方法\n",
    "\n",
    "| 方法 | 时间复杂度 | 核心思想 |\n",
    "|:-----|:----------|:---------|\n",
    "| **Flash Attention** | $O(n^2 d)$ | 分块计算，减少内存 |\n",
    "| **Sparse Attention** | $O(n \\sqrt{n})$ | 稀疏注意力模式 |\n",
    "| **Linear Attention** | $O(nd^2)$ | 核方法近似 |\n",
    "| **Sliding Window** | $O(nwd)$ | 局部窗口注意力 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_complexity() -> None:\n",
    "    \"\"\"分析不同序列长度下的注意力复杂度。\"\"\"\n",
    "    seq_lengths = [128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    batch_size = 1\n",
    "\n",
    "    print(\"注意力机制复杂度分析\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'序列长度':<12} {'注意力矩阵大小':<20} {'内存 (FP32)':<15} {'FLOPs':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for n in seq_lengths:\n",
    "        # 注意力矩阵大小: B * h * n * n\n",
    "        attn_size = batch_size * n_heads * n * n\n",
    "        memory_bytes = attn_size * 4  # FP32 = 4 bytes\n",
    "        memory_mb = memory_bytes / (1024 * 1024)\n",
    "\n",
    "        # FLOPs: 2 * B * h * n^2 * d_k (QK^T) + 2 * B * h * n^2 * d_k (Attn @ V)\n",
    "        d_k = d_model // n_heads\n",
    "        flops = 4 * batch_size * n_heads * n * n * d_k\n",
    "        gflops = flops / 1e9\n",
    "\n",
    "        print(f\"{n:<12} {attn_size:<20,} {memory_mb:<15.2f} MB {gflops:<15.2f} GFLOPs\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"观察: 内存和计算量随序列长度呈 O(n^2) 增长\")\n",
    "\n",
    "\n",
    "analyze_attention_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 总结\n",
    "\n",
    "| 要点 | 说明 |\n",
    "|:-----|:-----|\n",
    "| **缩放因子** | $1/\\sqrt{d_k}$ 将点积方差从 $d_k$ 归一化为 1 |\n",
    "| **数值稳定性** | 掩码使用 `-1e9` 而非 `-inf` |\n",
    "| **einsum** | 提升矩阵运算的数学可读性 |\n",
    "| **因果掩码** | 防止自回归模型的信息泄漏 |\n",
    "| **多头注意力** | 并行学习多种关注模式 |\n",
    "| **Flash Attention** | 分块计算，内存效率提升 2-4x |\n",
    "| **复杂度** | 时间 $O(n^2 d)$，空间 $O(n^2)$ |\n",
    "\n",
    "**进阶学习**: RoPE 位置编码、ALiBi、Grouped Query Attention (GQA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
