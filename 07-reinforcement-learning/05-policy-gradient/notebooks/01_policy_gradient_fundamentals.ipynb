{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods: Fundamentals\n",
    "\n",
    "## Overview\n",
    "This notebook introduces the fundamental concepts of policy gradient methods in reinforcement learning.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand the policy gradient theorem\n",
    "2. Learn the difference between on-policy and off-policy methods\n",
    "3. Implement a simple policy gradient algorithm\n",
    "4. Understand the role of baselines in variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Policy Gradient Theorem\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The policy gradient theorem states:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) Q(s,a)]$$\n",
    "\n",
    "where:\n",
    "- $J(\\theta)$ is the objective function (expected return)\n",
    "- $\\pi_\\theta(a|s)$ is the policy parameterized by $\\theta$\n",
    "- $Q(s,a)$ is the action-value function\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ is the score function\n",
    "\n",
    "### Key Insight\n",
    "The gradient of the expected return can be expressed as an expectation of the product of:\n",
    "1. **Score function**: $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ - direction to increase policy probability\n",
    "2. **Return signal**: $Q(s,a)$ - how good the action is\n",
    "\n",
    "This allows us to use gradient ascent to directly optimize the policy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Policy Gradient Methods?\n",
    "\n",
    "### Advantages\n",
    "- **Direct optimization**: Directly optimize the policy instead of value function\n",
    "- **Continuous actions**: Handle continuous action spaces naturally\n",
    "- **Stochastic policies**: Can learn stochastic (exploratory) policies\n",
    "- **Convergence guarantees**: Guaranteed convergence to local optima\n",
    "\n",
    "### Disadvantages\n",
    "- **High variance**: Requires many samples for stable estimates\n",
    "- **Sample inefficiency**: Only uses on-policy data\n",
    "- **Slow convergence**: Can be slow in high-dimensional spaces\n",
    "- **Hyperparameter sensitivity**: Sensitive to learning rate and other hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. On-Policy vs Off-Policy\n",
    "\n",
    "### On-Policy Methods\n",
    "- Learn from data generated by the current policy\n",
    "- Must collect new data after each policy update\n",
    "- Examples: REINFORCE, Actor-Critic, A3C\n",
    "- **Advantage**: Stable, direct policy optimization\n",
    "- **Disadvantage**: Sample inefficient\n",
    "\n",
    "### Off-Policy Methods\n",
    "- Learn from data generated by any policy (behavior policy)\n",
    "- Can reuse old data (experience replay)\n",
    "- Examples: Q-Learning, DQN\n",
    "- **Advantage**: Sample efficient\n",
    "- **Disadvantage**: Can be unstable, requires importance sampling\n",
    "\n",
    "**Policy gradient methods are typically on-policy**, which is why they require frequent data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Role of Baselines\n",
    "\n",
    "### Problem: High Variance\n",
    "Using raw returns $G_t$ as the signal leads to high variance:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) G_t]$$\n",
    "\n",
    "### Solution: Subtract a Baseline\n",
    "We can subtract any baseline $b(s)$ that depends only on the state:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) (G_t - b(s))]$$\n",
    "\n",
    "This is mathematically equivalent (the baseline term has zero expectation) but reduces variance!\n",
    "\n",
    "### Optimal Baseline\n",
    "The optimal baseline is the value function:\n",
    "\n",
    "$$b^*(s) = V(s) = \\mathbb{E}[G_t | s]$$\n",
    "\n",
    "This gives us the **advantage function**:\n",
    "\n",
    "$$A(s,a) = Q(s,a) - V(s)$$\n",
    "\n",
    "which measures how much better action $a$ is compared to the average action in state $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Demonstrate variance reduction with baseline\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate returns from an episode\n",
    "returns = np.array([10, 15, 8, 12, 20, 18, 14, 16, 11, 13])\n",
    "baseline = np.mean(returns)  # Simple baseline: mean return\n",
    "\n",
    "# Compute advantages\n",
    "advantages = returns - baseline\n",
    "\n",
    "print(f\"Returns: {returns}\")\n",
    "print(f\"Baseline (mean): {baseline:.2f}\")\n",
    "print(f\"Advantages: {advantages}\")\n",
    "print(f\"\\nVariance without baseline: {np.var(returns):.2f}\")\n",
    "print(f\"Variance with baseline: {np.var(advantages):.2f}\")\n",
    "print(f\"Variance reduction: {(1 - np.var(advantages)/np.var(returns))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "### Key Concepts\n",
    "1. **Policy Gradient Theorem**: Provides a way to compute gradients of the expected return\n",
    "2. **On-Policy Learning**: Policy gradient methods learn from current policy data\n",
    "3. **Baselines**: Reduce variance without introducing bias\n",
    "4. **Advantage Function**: Measures relative quality of actions\n",
    "\n",
    "### Next Steps\n",
    "- Implement REINFORCE algorithm\n",
    "- Add value function baseline\n",
    "- Explore Actor-Critic methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
