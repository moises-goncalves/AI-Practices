{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods: Combining Policy and Value Learning\n",
    "\n",
    "## Overview\n",
    "Actor-Critic methods combine policy gradient (actor) with value function learning (critic) to reduce variance while maintaining on-policy learning.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand the actor-critic architecture\n",
    "2. Learn how value functions reduce variance\n",
    "3. Implement Actor-Critic algorithm\n",
    "4. Compare with REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Actor-Critic Architecture\n",
    "\n",
    "### Two Components\n",
    "\n",
    "**Actor (Policy Network)**\n",
    "- Learns the policy π_θ(a|s)\n",
    "- Updated using policy gradient\n",
    "- Selects actions\n",
    "\n",
    "**Critic (Value Network)**\n",
    "- Learns the value function V_φ(s)\n",
    "- Updated using temporal difference (TD) learning\n",
    "- Provides baseline for variance reduction\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "Policy gradient with advantage:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) A(s,a)]$$\n",
    "\n",
    "where the advantage is estimated as:\n",
    "$$A(s,a) \\approx r + \\gamma V_\\phi(s') - V_\\phi(s)$$\n",
    "\n",
    "This is the **Temporal Difference (TD) error** or **TD residual**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Difference Learning\n",
    "\n",
    "### TD Error\n",
    "\n",
    "The TD error measures the difference between predicted and actual value:\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "### Advantages of TD Learning\n",
    "1. **Lower variance**: Uses one-step lookahead instead of full trajectory\n",
    "2. **Online learning**: Can update after each step\n",
    "3. **Bootstrapping**: Uses value estimates to bootstrap\n",
    "\n",
    "### Critic Update\n",
    "\n",
    "The critic is trained to minimize TD error:\n",
    "\n",
    "$$L_{critic} = \\mathbb{E}[(\\delta_t)^2]$$\n",
    "\n",
    "$$\\phi \\leftarrow \\phi - \\beta \\nabla_\\phi (\\delta_t)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Actor-Critic Algorithm\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "```\n",
    "Initialize policy π_θ and value function V_φ\n",
    "for episode = 1 to num_episodes:\n",
    "    state ← env.reset()\n",
    "    for t = 0 to T:\n",
    "        # Actor: sample action\n",
    "        action ~ π_θ(·|state)\n",
    "        \n",
    "        # Environment step\n",
    "        next_state, reward ← env.step(action)\n",
    "        \n",
    "        # Critic: compute TD error\n",
    "        δ ← reward + γV_φ(next_state) - V_φ(state)\n",
    "        \n",
    "        # Critic update\n",
    "        φ ← φ + β∇_φ V_φ(state) δ\n",
    "        \n",
    "        # Actor update\n",
    "        θ ← θ + α∇_θ log π_θ(action|state) δ\n",
    "        \n",
    "        state ← next_state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variance Reduction Analysis\n",
    "\n",
    "### Comparison: REINFORCE vs Actor-Critic\n",
    "\n",
    "**REINFORCE**\n",
    "- Uses full trajectory return: $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k}$\n",
    "- High variance (depends on entire trajectory)\n",
    "- Unbiased estimate\n",
    "\n",
    "**Actor-Critic**\n",
    "- Uses TD error: $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "- Lower variance (one-step lookahead)\n",
    "- Biased estimate (depends on value function accuracy)\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "- **More bias**: Faster convergence, but may converge to suboptimal policy\n",
    "- **Less bias**: Slower convergence, but better final policy\n",
    "- Actor-Critic finds a good balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Demonstrate variance reduction\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate trajectory\n",
    "rewards = np.array([1.0, 1.0, 1.0, 0.0, 1.0, 1.0])\n",
    "values = np.array([2.5, 2.0, 1.5, 0.5, 1.5, 1.0, 0.0])  # V(s_t) for each state\n",
    "gamma = 0.99\n",
    "\n",
    "# REINFORCE: use full returns\n",
    "returns = np.zeros(len(rewards))\n",
    "cumulative = 0\n",
    "for t in reversed(range(len(rewards))):\n",
    "    cumulative = rewards[t] + gamma * cumulative\n",
    "    returns[t] = cumulative\n",
    "\n",
    "# Actor-Critic: use TD errors\n",
    "td_errors = np.zeros(len(rewards))\n",
    "for t in range(len(rewards)):\n",
    "    td_errors[t] = rewards[t] + gamma * values[t+1] - values[t]\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"\\nREINFORCE (full returns):\")\n",
    "print(f\"  Returns: {returns}\")\n",
    "print(f\"  Variance: {np.var(returns):.4f}\")\n",
    "\n",
    "print(\"\\nActor-Critic (TD errors):\")\n",
    "print(f\"  TD errors: {td_errors}\")\n",
    "print(f\"  Variance: {np.var(td_errors):.4f}\")\n",
    "\n",
    "print(f\"\\nVariance reduction: {(1 - np.var(td_errors)/np.var(returns))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementation\n",
    "\n",
    "### Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Policy network (actor)\"\"\"\n",
    "    def __init__(self, state_dim=4, action_dim=2, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def get_action_and_log_prob(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        logits = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.squeeze(), log_prob.squeeze()\n",
    "\n",
    "print(\"Actor network defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Value function network (critic)\"\"\"\n",
    "    def __init__(self, state_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        return self.net(state)\n",
    "\n",
    "print(\"Critic network defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_step(actor, critic, state, action, reward, next_state, done, \n",
    "                      actor_optimizer, critic_optimizer, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Perform one Actor-Critic training step.\n",
    "    \n",
    "    Args:\n",
    "        actor: Policy network\n",
    "        critic: Value network\n",
    "        state: Current state\n",
    "        action: Action taken\n",
    "        reward: Reward received\n",
    "        next_state: Next state\n",
    "        done: Whether episode is done\n",
    "        actor_optimizer: Optimizer for actor\n",
    "        critic_optimizer: Optimizer for critic\n",
    "        gamma: Discount factor\n",
    "    \"\"\"\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "    \n",
    "    # Critic: compute TD error\n",
    "    with torch.no_grad():\n",
    "        current_value = critic(state_tensor)\n",
    "        next_value = critic(next_state_tensor) if not done else torch.tensor([[0.0]])\n",
    "        td_target = reward + gamma * next_value\n",
    "    \n",
    "    # Critic loss and update\n",
    "    critic_loss = torch.nn.functional.mse_loss(current_value, td_target)\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    # Actor: compute policy loss using TD error as advantage\n",
    "    td_error = (td_target - current_value).detach()\n",
    "    \n",
    "    logits = actor(state_tensor)\n",
    "    dist = Categorical(logits=logits)\n",
    "    log_prob = dist.log_prob(torch.tensor(action))\n",
    "    \n",
    "    actor_loss = -log_prob * td_error.squeeze()\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "print(\"Actor-Critic training step defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advantages of Actor-Critic\n",
    "\n",
    "### vs. REINFORCE\n",
    "1. **Lower variance**: Uses TD error instead of full returns\n",
    "2. **Faster convergence**: Fewer samples needed\n",
    "3. **Online learning**: Can update after each step\n",
    "4. **Better stability**: Value function provides stable baseline\n",
    "\n",
    "### vs. Pure Value-Based Methods\n",
    "1. **Direct policy optimization**: Learns policy directly\n",
    "2. **Continuous actions**: Handles continuous action spaces naturally\n",
    "3. **Stochastic policies**: Can learn exploratory policies\n",
    "4. **Convergence guarantees**: Theoretical convergence properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Concepts\n",
    "1. **Actor-Critic**: Combines policy gradient with value learning\n",
    "2. **TD Learning**: Uses one-step lookahead for lower variance\n",
    "3. **Advantage Function**: Measures relative quality of actions\n",
    "4. **Bias-Variance Trade-off**: Balances between REINFORCE and pure value methods\n",
    "\n",
    "### Next Steps\n",
    "- Implement A2C with Generalized Advantage Estimation (GAE)\n",
    "- Explore parallel variants (A3C)\n",
    "- Study trust region methods (PPO, TRPO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
