{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics: GAE, A2C, and Beyond\n",
    "\n",
    "## Overview\n",
    "This notebook covers advanced policy gradient techniques including Generalized Advantage Estimation (GAE) and Advantage Actor-Critic (A2C).\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand Generalized Advantage Estimation (GAE)\n",
    "2. Learn A2C algorithm with batch updates\n",
    "3. Understand the bias-variance trade-off in advantage estimation\n",
    "4. Explore practical training techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generalized Advantage Estimation (GAE)\n",
    "\n",
    "### Problem: Choosing the Right Advantage Estimator\n",
    "\n",
    "Different advantage estimators have different bias-variance properties:\n",
    "\n",
    "**1-step TD:**\n",
    "$$A_t^{(1)} = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "- Low variance, high bias\n",
    "\n",
    "**n-step TD:**\n",
    "$$A_t^{(n)} = \\sum_{l=0}^{n-1} \\gamma^l r_{t+l} + \\gamma^n V(s_{t+n}) - V(s_t)$$\n",
    "- Medium variance, medium bias\n",
    "\n",
    "**Monte Carlo (∞-step):**\n",
    "$$A_t^{(\\infty)} = G_t - V(s_t)$$\n",
    "- High variance, low bias\n",
    "\n",
    "### Solution: GAE\n",
    "\n",
    "Generalized Advantage Estimation combines all n-step returns:\n",
    "\n",
    "$$A_t^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_t^{(l)}$$\n",
    "\n",
    "where $\\delta_t^{(l)} = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
    "\n",
    "### Efficient Computation\n",
    "\n",
    "$$A_t = \\delta_t + (\\gamma\\lambda) A_{t+1}$$\n",
    "\n",
    "This can be computed efficiently in reverse order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GAE Parameters\n",
    "\n",
    "### Lambda Parameter (λ)\n",
    "\n",
    "- **λ = 0**: Uses only 1-step TD (low variance, high bias)\n",
    "- **λ = 1**: Uses full Monte Carlo returns (high variance, low bias)\n",
    "- **0 < λ < 1**: Interpolates between the two (typical: λ = 0.95)\n",
    "\n",
    "### Gamma Parameter (γ)\n",
    "\n",
    "- **γ = 0**: Only immediate reward matters\n",
    "- **γ = 1**: All future rewards equally important\n",
    "- **Typical**: γ = 0.99 or γ = 0.999\n",
    "\n",
    "### Effect of λ on Advantage Estimates\n",
    "\n",
    "```\n",
    "λ = 0.0:  A_t = δ_t                           (1-step)\n",
    "λ = 0.5:  A_t = δ_t + 0.5γδ_{t+1} + ...      (mixed)\n",
    "λ = 1.0:  A_t = G_t - V(s_t)                 (MC)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_gae(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation.\n",
    "    \n",
    "    Args:\n",
    "        rewards: Array of rewards\n",
    "        values: Array of value estimates (including final state)\n",
    "        gamma: Discount factor\n",
    "        lambda_: GAE lambda parameter\n",
    "    \n",
    "    Returns:\n",
    "        advantages: Array of advantage estimates\n",
    "        returns: Array of returns (advantages + values)\n",
    "    \"\"\"\n",
    "    advantages = np.zeros(len(rewards))\n",
    "    gae = 0.0\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        # TD error\n",
    "        delta = rewards[t] + gamma * values[t+1] - values[t]\n",
    "        \n",
    "        # GAE\n",
    "        gae = delta + gamma * lambda_ * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    returns = advantages + values[:-1]\n",
    "    return advantages, returns\n",
    "\n",
    "# Example\n",
    "rewards = np.array([1.0, 1.0, 1.0, 0.0, 1.0])\n",
    "values = np.array([2.5, 2.0, 1.5, 0.5, 1.5, 1.0])  # V(s_t) for each state\n",
    "\n",
    "# Compare different lambda values\n",
    "lambdas = [0.0, 0.5, 0.95, 1.0]\n",
    "fig, axes = plt.subplots(1, len(lambdas), figsize=(15, 3))\n",
    "\n",
    "for idx, lambda_ in enumerate(lambdas):\n",
    "    advantages, returns = compute_gae(rewards, values, gamma=0.99, lambda_=lambda_)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.bar(range(len(advantages)), advantages, alpha=0.7, label='Advantages')\n",
    "    ax.set_title(f'λ = {lambda_}')\n",
    "    ax.set_xlabel('Timestep')\n",
    "    ax.set_ylabel('Advantage')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"GAE with different lambda values:\")\n",
    "for lambda_ in lambdas:\n",
    "    advantages, _ = compute_gae(rewards, values, gamma=0.99, lambda_=lambda_)\n",
    "    print(f\"λ = {lambda_}: advantages = {advantages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A2C Algorithm\n",
    "\n",
    "### Advantage Actor-Critic (A2C)\n",
    "\n",
    "A2C extends Actor-Critic with:\n",
    "1. **Batch updates**: Collect multiple trajectories before updating\n",
    "2. **GAE**: Use Generalized Advantage Estimation\n",
    "3. **Parallel environments**: Collect data from multiple environments simultaneously\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "```\n",
    "Initialize actor π_θ and critic V_φ\n",
    "for iteration = 1 to num_iterations:\n",
    "    # Collect trajectories from N parallel environments\n",
    "    for env_id = 1 to N:\n",
    "        Collect trajectory from environment\n",
    "    \n",
    "    # Compute advantages using GAE\n",
    "    advantages ← compute_gae(trajectories)\n",
    "    \n",
    "    # Batch update\n",
    "    for epoch = 1 to num_epochs:\n",
    "        # Critic update\n",
    "        L_critic = MSE(V_φ(s), returns)\n",
    "        φ ← φ - β∇_φ L_critic\n",
    "        \n",
    "        # Actor update\n",
    "        L_actor = -log π_θ(a|s) * advantages\n",
    "        θ ← θ + α∇_θ L_actor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Training Techniques\n",
    "\n",
    "### 1. Advantage Normalization\n",
    "\n",
    "Normalize advantages to have zero mean and unit variance:\n",
    "\n",
    "$$A_{norm} = \\frac{A - \\text{mean}(A)}{\\text{std}(A) + \\epsilon}$$\n",
    "\n",
    "**Benefits:**\n",
    "- Stabilizes training\n",
    "- Makes learning rate less sensitive to reward scale\n",
    "- Improves gradient flow\n",
    "\n",
    "### 2. Return Normalization\n",
    "\n",
    "Normalize returns for value function training:\n",
    "\n",
    "$$G_{norm} = \\frac{G - \\text{mean}(G)}{\\text{std}(G) + \\epsilon}$$\n",
    "\n",
    "### 3. Gradient Clipping\n",
    "\n",
    "Clip gradients to prevent large updates:\n",
    "\n",
    "$$\\nabla \\leftarrow \\text{clip}(\\nabla, -\\text{max_norm}, \\text{max_norm})$$\n",
    "\n",
    "### 4. Entropy Regularization\n",
    "\n",
    "Encourage exploration by adding entropy bonus:\n",
    "\n",
    "$$L_{total} = L_{policy} + L_{value} - \\beta H(\\pi)$$\n",
    "\n",
    "where $H(\\pi) = -\\mathbb{E}[\\log \\pi]$ is the policy entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def normalize_advantages(advantages, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Normalize advantages to have zero mean and unit variance.\n",
    "    \n",
    "    Args:\n",
    "        advantages: Tensor of advantages\n",
    "        epsilon: Small constant for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Normalized advantages\n",
    "    \"\"\"\n",
    "    return (advantages - advantages.mean()) / (advantages.std() + epsilon)\n",
    "\n",
    "def compute_policy_loss(log_probs, advantages, entropy, entropy_coeff=0.01):\n",
    "    \"\"\"\n",
    "    Compute policy loss with entropy regularization.\n",
    "    \n",
    "    Args:\n",
    "        log_probs: Log probabilities of actions\n",
    "        advantages: Advantage estimates\n",
    "        entropy: Policy entropy\n",
    "        entropy_coeff: Entropy regularization coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Policy loss\n",
    "    \"\"\"\n",
    "    policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "    entropy_loss = -entropy_coeff * entropy.mean()\n",
    "    return policy_loss + entropy_loss\n",
    "\n",
    "# Example\n",
    "log_probs = torch.randn(32, 1, requires_grad=True)\n",
    "advantages = torch.randn(32, 1)\n",
    "entropy = torch.randn(32)\n",
    "\n",
    "# Normalize advantages\n",
    "advantages_norm = normalize_advantages(advantages)\n",
    "\n",
    "# Compute loss\n",
    "loss = compute_policy_loss(log_probs, advantages_norm, entropy, entropy_coeff=0.01)\n",
    "\n",
    "print(f\"Original advantages - mean: {advantages.mean():.4f}, std: {advantages.std():.4f}\")\n",
    "print(f\"Normalized advantages - mean: {advantages_norm.mean():.4f}, std: {advantages_norm.std():.4f}\")\n",
    "print(f\"Policy loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of Methods\n",
    "\n",
    "### REINFORCE vs Actor-Critic vs A2C\n",
    "\n",
    "| Aspect | REINFORCE | Actor-Critic | A2C |\n",
    "|--------|-----------|--------------|-----|\n",
    "| **Advantage Estimator** | Full return | 1-step TD | GAE |\n",
    "| **Variance** | High | Medium | Low |\n",
    "| **Bias** | Low | Medium | Medium |\n",
    "| **Update Frequency** | Per episode | Per step | Per batch |\n",
    "| **Parallel Envs** | No | No | Yes |\n",
    "| **Sample Efficiency** | Low | Medium | High |\n",
    "| **Convergence Speed** | Slow | Medium | Fast |\n",
    "| **Complexity** | Simple | Medium | Complex |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **REINFORCE**: Educational purposes, simple environments\n",
    "- **Actor-Critic**: Standard choice for most problems\n",
    "- **A2C**: When sample efficiency is critical, parallel training available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "1. **GAE**: Provides flexible bias-variance trade-off via λ parameter\n",
    "2. **A2C**: Combines GAE with batch updates and parallel environments\n",
    "3. **Practical techniques**: Normalization, clipping, entropy regularization\n",
    "4. **Trade-offs**: Variance vs bias, sample efficiency vs convergence speed\n",
    "\n",
    "### Next Steps\n",
    "- Implement trust region methods (PPO, TRPO)\n",
    "- Explore off-policy methods (SAC, TD3)\n",
    "- Study model-based RL approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
