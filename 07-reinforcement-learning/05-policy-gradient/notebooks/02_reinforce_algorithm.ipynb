{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm: Implementation and Analysis\n",
    "\n",
    "## Overview\n",
    "REINFORCE is the foundational policy gradient algorithm that directly uses Monte Carlo returns to estimate policy gradients.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand REINFORCE algorithm\n",
    "2. Implement REINFORCE from scratch\n",
    "3. Train on CartPole environment\n",
    "4. Analyze learning curves and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. REINFORCE Algorithm\n",
    "\n",
    "### Algorithm Description\n",
    "\n",
    "REINFORCE uses the policy gradient theorem with Monte Carlo returns:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) G_t]$$\n",
    "\n",
    "where $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k}$ is the discounted return.\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) G_t$$\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "```\n",
    "Initialize policy π_θ\n",
    "for episode = 1 to num_episodes:\n",
    "    Collect trajectory: (s_0, a_0, r_0, ..., s_T, a_T, r_T)\n",
    "    Compute returns: G_t for each timestep\n",
    "    for t = 0 to T:\n",
    "        θ ← θ + α ∇_θ log π_θ(a_t|s_t) G_t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Characteristics of REINFORCE\n",
    "\n",
    "### Advantages\n",
    "1. **Simple**: Easy to understand and implement\n",
    "2. **Theoretically sound**: Guaranteed convergence to local optima\n",
    "3. **General**: Works with any differentiable policy\n",
    "4. **No value function needed**: Can work without baseline\n",
    "\n",
    "### Disadvantages\n",
    "1. **High variance**: Uses full trajectory returns (high variance)\n",
    "2. **Sample inefficient**: Only uses on-policy data\n",
    "3. **Slow convergence**: Requires many episodes\n",
    "4. **Reward scaling sensitive**: Performance depends on reward scale\n",
    "\n",
    "### Complexity Analysis\n",
    "- **Time per episode**: O(T) where T is episode length\n",
    "- **Space**: O(T) for storing trajectory\n",
    "- **Sample complexity**: O(1/ε²) for ε-optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation Details\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Policy Network**: Maps states to action probabilities\n",
    "   - Input: state\n",
    "   - Output: action logits (for discrete) or mean/std (for continuous)\n",
    "\n",
    "2. **Return Computation**: Compute discounted returns\n",
    "   - $G_t = r_t + \\gamma G_{t+1}$\n",
    "   - Computed in reverse order for efficiency\n",
    "\n",
    "3. **Policy Loss**: Negative log probability weighted by returns\n",
    "   - $L = -\\mathbb{E}[\\log \\pi_\\theta(a|s) G_t]$\n",
    "\n",
    "4. **Entropy Regularization**: Encourage exploration\n",
    "   - $L_{total} = L_{policy} - \\beta H(\\pi)$\n",
    "   - where $H(\\pi) = -\\mathbb{E}[\\log \\pi]$ is entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Simple policy network for CartPole\n",
    "class SimplePolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def sample(self, state):\n",
    "        \"\"\"Sample action and return log probability\"\"\"\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        logits = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.squeeze(), log_prob.squeeze()\n",
    "\n",
    "# Test the policy\n",
    "policy = SimplePolicy()\n",
    "state = torch.randn(4)\n",
    "action, log_prob = policy.sample(state)\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"Action: {action.item()}\")\n",
    "print(f\"Log probability: {log_prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Return Computation\n",
    "\n",
    "### Efficient Computation\n",
    "\n",
    "Computing returns efficiently is crucial for training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns from rewards.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards from trajectory\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        Array of returns for each timestep\n",
    "    \"\"\"\n",
    "    returns = np.zeros(len(rewards))\n",
    "    cumulative_return = 0.0\n",
    "    \n",
    "    # Compute returns in reverse order\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        cumulative_return = rewards[t] + gamma * cumulative_return\n",
    "        returns[t] = cumulative_return\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# Example\n",
    "rewards = [1.0, 1.0, 1.0, 0.0]  # CartPole rewards\n",
    "returns = compute_returns(rewards, gamma=0.99)\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Returns:\", returns)\n",
    "print(\"\\nExplanation:\")\n",
    "print(f\"  G_3 = 0\")\n",
    "print(f\"  G_2 = 1 + 0.99 * 0 = 1.0\")\n",
    "print(f\"  G_1 = 1 + 0.99 * 1.0 = 1.99\")\n",
    "print(f\"  G_0 = 1 + 0.99 * 1.99 = 2.9701\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "### Complete REINFORCE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_episode(policy, env, optimizer, gamma=0.99, entropy_coeff=0.01):\n",
    "    \"\"\"\n",
    "    Train REINFORCE for one episode.\n",
    "    \n",
    "    Args:\n",
    "        policy: Policy network\n",
    "        env: Gym environment\n",
    "        optimizer: PyTorch optimizer\n",
    "        gamma: Discount factor\n",
    "        entropy_coeff: Entropy regularization coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Episode return and loss\n",
    "    \"\"\"\n",
    "    state, _ = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    entropies = []\n",
    "    \n",
    "    # Collect trajectory\n",
    "    done = False\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        \n",
    "        # Sample action\n",
    "        logits = policy(state_tensor.unsqueeze(0))\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        # Take environment step\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        entropies.append(entropy)\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # Compute returns\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    \n",
    "    # Normalize returns\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    \n",
    "    # Compute loss\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    entropies = torch.stack(entropies)\n",
    "    \n",
    "    policy_loss = -(log_probs * returns).mean()\n",
    "    entropy_loss = -entropy_coeff * entropies.mean()\n",
    "    total_loss = policy_loss + entropy_loss\n",
    "    \n",
    "    # Update policy\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return sum(rewards), total_loss.item()\n",
    "\n",
    "print(\"REINFORCE training function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "1. **REINFORCE** is the foundational policy gradient algorithm\n",
    "2. **Simple but high variance** - uses full trajectory returns\n",
    "3. **On-policy** - requires collecting new data after each update\n",
    "4. **Entropy regularization** helps with exploration\n",
    "\n",
    "### Next Steps\n",
    "- Add value function baseline to reduce variance\n",
    "- Explore Actor-Critic methods\n",
    "- Implement advanced variants (PPO, TRPO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
