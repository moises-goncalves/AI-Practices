{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序差分学习教程\n",
    "\n",
    "## Temporal Difference Learning Tutorial\n",
    "\n",
    "本教程系统介绍时序差分(TD)学习的核心概念和算法实现。\n",
    "\n",
    "### 目录\n",
    "\n",
    "1. [TD学习基础](#1-TD学习基础)\n",
    "2. [TD(0)预测](#2-TD0预测)\n",
    "3. [SARSA控制](#3-SARSA控制)\n",
    "4. [Q-Learning控制](#4-Q-Learning控制)\n",
    "5. [算法对比实验](#5-算法对比实验)\n",
    "6. [资格迹方法](#6-资格迹方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加模块路径\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. TD学习基础\n",
    "\n",
    "### 1.1 核心思想\n",
    "\n",
    "时序差分学习结合了两种方法的优点：\n",
    "\n",
    "- **蒙特卡洛方法**：从经验中学习，不需要环境模型\n",
    "- **动态规划**：使用估计值更新估计值（自举）\n",
    "\n",
    "### 1.2 TD误差\n",
    "\n",
    "TD学习的核心是**TD误差**：\n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
    "\n",
    "这表示\"实际获得的回报\"与\"预期回报\"之间的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入核心模块\n",
    "from core import TDConfig, TD0ValueLearner\n",
    "from environments import RandomWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. TD(0)预测\n",
    "\n",
    "### 2.1 RandomWalk环境\n",
    "\n",
    "RandomWalk是TD预测的标准测试环境：\n",
    "\n",
    "```\n",
    "T(0) - A - B - C - D - E - T(1)\n",
    "(左终止)           (右终止)\n",
    "```\n",
    "\n",
    "- 随机策略：左右各50%概率\n",
    "- 到达右终止状态获得+1奖励\n",
    "- 有解析解，便于验证算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建RandomWalk环境\n",
    "env = RandomWalk(n_states=19)\n",
    "\n",
    "# 显示环境信息\n",
    "print(f\"状态数: {env.n_total_states}\")\n",
    "print(f\"非终止状态: {env.n_states}\")\n",
    "\n",
    "# 获取真实价值\n",
    "true_values = env.get_true_values(gamma=1.0)\n",
    "print(f\"\\n真实价值 (中间5个状态):\")\n",
    "for i in [8, 9, 10, 11, 12]:\n",
    "    print(f\"  状态 {i}: {true_values[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TD(0)更新规则\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建TD(0)学习器\n",
    "config = TDConfig(alpha=0.1, gamma=1.0)\n",
    "learner = TD0ValueLearner(config)\n",
    "\n",
    "# 训练\n",
    "metrics = learner.train(\n",
    "    env,\n",
    "    n_episodes=100,\n",
    "    log_interval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 可视化学习结果\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# 估计值 vs 真实值\nstates = range(1, env.n_states + 1)\nestimated = [learner._value_function.get(s, 0.0) for s in states]\ntrue_vals = [true_values[s] for s in states]\n\naxes[0].plot(states, true_vals, 'b-', label='True Values', linewidth=2)\naxes[0].plot(states, estimated, 'r--', label='TD(0) Estimates', linewidth=2)\naxes[0].set_xlabel('State')\naxes[0].set_ylabel('Value')\naxes[0].set_title('TD(0) Value Estimates vs True Values')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 误差\nerrors = [abs(e - t) for e, t in zip(estimated, true_vals)]\naxes[1].bar(states, errors, color='coral', alpha=0.7)\naxes[1].set_xlabel('State')\naxes[1].set_ylabel('Absolute Error')\naxes[1].set_title(f'Estimation Errors (Mean: {np.mean(errors):.4f})')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. SARSA控制\n",
    "\n",
    "### 3.1 SARSA算法\n",
    "\n",
    "SARSA是On-Policy的TD控制算法：\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\n",
    "\n",
    "名称来源：**S**tate-**A**ction-**R**eward-**S**tate-**A**ction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import SARSA\n",
    "from environments import CliffWalkingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建CliffWalking环境\n",
    "env = CliffWalkingEnv()\n",
    "\n",
    "# 显示环境\n",
    "print(\"CliffWalking环境:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建SARSA学习器\n",
    "sarsa_config = TDConfig(alpha=0.5, gamma=1.0, epsilon=0.1)\n",
    "sarsa = SARSA(sarsa_config)\n",
    "\n",
    "# 训练\n",
    "sarsa_metrics = sarsa.train(\n",
    "    env,\n",
    "    n_episodes=500,\n",
    "    log_interval=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练曲线\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "rewards = sarsa_metrics.episode_rewards\n",
    "window = 50\n",
    "\n",
    "ax.plot(rewards, alpha=0.3, color='blue')\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(rewards)), smoothed, color='blue', linewidth=2, label='SARSA')\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('SARSA Training Curve on CliffWalking')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Q-Learning控制\n",
    "\n",
    "### 4.1 Q-Learning算法\n",
    "\n",
    "Q-Learning是Off-Policy的TD控制算法：\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
    "\n",
    "与SARSA的关键区别：使用**max**而不是实际的下一动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Q-Learning学习器\n",
    "qlearn_config = TDConfig(alpha=0.5, gamma=1.0, epsilon=0.1)\n",
    "qlearn = QLearning(qlearn_config)\n",
    "\n",
    "# 训练\n",
    "env = CliffWalkingEnv()\n",
    "qlearn_metrics = qlearn.train(\n",
    "    env,\n",
    "    n_episodes=500,\n",
    "    log_interval=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 算法对比实验\n",
    "\n",
    "### 5.1 SARSA vs Q-Learning\n",
    "\n",
    "在CliffWalking环境中：\n",
    "- **SARSA**：学习远离悬崖的安全路径（考虑探索风险）\n",
    "- **Q-Learning**：学习沿悬崖边缘的最短路径（假设最优执行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较训练曲线\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# SARSA\n",
    "sarsa_rewards = sarsa_metrics.episode_rewards\n",
    "ax.plot(sarsa_rewards, alpha=0.2, color='blue')\n",
    "sarsa_smooth = np.convolve(sarsa_rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(sarsa_rewards)), sarsa_smooth, color='blue', linewidth=2, label='SARSA')\n",
    "\n",
    "# Q-Learning\n",
    "qlearn_rewards = qlearn_metrics.episode_rewards\n",
    "ax.plot(qlearn_rewards, alpha=0.2, color='red')\n",
    "qlearn_smooth = np.convolve(qlearn_rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(qlearn_rewards)), qlearn_smooth, color='red', linewidth=2, label='Q-Learning')\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('SARSA vs Q-Learning on CliffWalking (Training)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估贪婪策略\n",
    "env = CliffWalkingEnv()\n",
    "\n",
    "sarsa_eval = sarsa.evaluate(env, n_episodes=100)\n",
    "qlearn_eval = qlearn.evaluate(env, n_episodes=100)\n",
    "\n",
    "print(\"贪婪策略评估（无探索）:\")\n",
    "print(f\"  SARSA:      {sarsa_eval[0]:.2f} ± {sarsa_eval[1]:.2f}\")\n",
    "print(f\"  Q-Learning: {qlearn_eval[0]:.2f} ± {qlearn_eval[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 关键洞察\n",
    "\n",
    "| 指标 | SARSA | Q-Learning |\n",
    "|------|-------|------------|\n",
    "| 训练奖励 | 更高 | 更低 |\n",
    "| 评估奖励 | 更低 | 更高 |\n",
    "| 学到的路径 | 安全路径 | 最优路径 |\n",
    "| 风险考虑 | 是 | 否 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 资格迹方法\n",
    "\n",
    "### 6.1 TD(λ)算法\n",
    "\n",
    "资格迹允许将TD误差回传给所有\"最近访问过的\"状态：\n",
    "\n",
    "$$e_t(s) = \\gamma \\lambda e_{t-1}(s) + \\mathbf{1}[S_t = s]$$\n",
    "\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import TDLambda, SARSALambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 比较不同λ值\nenv = RandomWalk(n_states=19)\ntrue_values = env.get_true_values(gamma=1.0)\n\nlambda_values = [0.0, 0.4, 0.8, 0.95]\nresults = {}\n\nfor lam in lambda_values:\n    config = TDConfig(alpha=0.1, gamma=1.0, lambda_=lam)\n    learner = TDLambda(config)\n    \n    metrics = learner.train(env, n_episodes=100, log_interval=200)\n    \n    # 计算RMSE\n    from utils import compute_rmse\n    estimated = {s: learner._value_function.get(s, 0.0) for s in range(env.n_total_states)}\n    rmse = compute_rmse(estimated, dict(enumerate(true_values)))\n    results[lam] = rmse\n    print(f\"λ = {lam:.2f}: RMSE = {rmse:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化λ的影响\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(list(results.keys()), list(results.values()), 'bo-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('λ')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Effect of λ on TD(λ) Performance')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 标记最优λ\n",
    "best_lambda = min(results, key=results.get)\n",
    "ax.axvline(best_lambda, color='r', linestyle='--', alpha=0.5, label=f'Best λ = {best_lambda}')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **TD学习**结合了MC的采样和DP的自举\n",
    "2. **SARSA**是On-Policy，考虑探索风险，更安全\n",
    "3. **Q-Learning**是Off-Policy，学习最优策略，可能过估计\n",
    "4. **资格迹**通过λ参数权衡TD(0)和MC\n",
    "\n",
    "### 算法选择建议\n",
    "\n",
    "| 场景 | 推荐算法 |\n",
    "|------|----------|\n",
    "| 安全重要 | SARSA |\n",
    "| 需要最优 | Q-Learning |\n",
    "| 稀疏奖励 | TD(λ)或SARSA(λ) |\n",
    "| 过估计问题 | Double Q-Learning |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}