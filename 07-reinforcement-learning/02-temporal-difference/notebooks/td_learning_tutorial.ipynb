{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序差分学习教程\n",
    "\n",
    "## Temporal Difference Learning Tutorial\n",
    "\n",
    "本教程系统介绍TD学习的核心概念和算法实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "1. [TD学习基础](#1-TD学习基础)\n",
    "2. [TD(0)预测](#2-TD0预测)\n",
    "3. [SARSA控制](#3-SARSA控制)\n",
    "4. [Q-Learning控制](#4-Q-Learning控制)\n",
    "5. [算法对比实验](#5-算法对比实验)\n",
    "6. [资格迹方法](#6-资格迹方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. TD学习基础\n",
    "\n",
    "### 1.1 核心思想\n",
    "\n",
    "TD学习结合了两种方法的优点：\n",
    "- **蒙特卡洛方法**：从经验中学习\n",
    "- **动态规划**：使用估计值更新估计值（自举）\n",
    "\n",
    "### 1.2 TD误差\n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入核心模块\n",
    "from core import TDConfig, TD0ValueLearner, SARSA, QLearning\n",
    "from environments import RandomWalk, CliffWalkingEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. TD(0)预测\n",
    "\n",
    "### 2.1 RandomWalk环境\n",
    "\n",
    "```\n",
    "T(0) - A - B - C - D - E - T(1)\n",
    "```\n",
    "\n",
    "- 随机策略：左右各50%\n",
    "- 右终止+1奖励，左终止0奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建RandomWalk环境\n",
    "env = RandomWalk(n_states=19)\n",
    "print(f\"状态数: {env.n_total_states}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取真实价值\n",
    "true_values = env.get_true_values(gamma=1.0)\n",
    "print(\"真实价值 (中间5个状态):\")\n",
    "for i in [8, 9, 10, 11, 12]:\n",
    "    print(f\"  状态 {i}: {true_values[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TD(0)更新规则\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建TD(0)学习器\n",
    "config = TDConfig(alpha=0.1, gamma=1.0)\n",
    "learner = TD0ValueLearner(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "metrics = learner.train(env, n_episodes=100, log_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化学习结果\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "states = range(1, env.n_states + 1)\n",
    "estimated = [learner._value_function.get(s, 0.0) for s in states]\n",
    "true_vals = [true_values[s] for s in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制估计值 vs 真实值\n",
    "axes[0].plot(states, true_vals, 'b-', label='True', linewidth=2)\n",
    "axes[0].plot(states, estimated, 'r--', label='TD(0)', linewidth=2)\n",
    "axes[0].set_xlabel('State')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_title('TD(0) vs True Values')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制误差\n",
    "errors = [abs(e - t) for e, t in zip(estimated, true_vals)]\n",
    "axes[1].bar(states, errors, color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('State')\n",
    "axes[1].set_ylabel('Absolute Error')\n",
    "axes[1].set_title(f'Errors (Mean: {np.mean(errors):.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. SARSA控制\n",
    "\n",
    "### 3.1 SARSA算法\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\n",
    "\n",
    "名称来源：**S**tate-**A**ction-**R**eward-**S**tate-**A**ction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建CliffWalking环境\n",
    "env = CliffWalkingEnv()\n",
    "print(\"CliffWalking环境:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建SARSA学习器\n",
    "sarsa_config = TDConfig(alpha=0.5, gamma=1.0, epsilon=0.1)\n",
    "sarsa = SARSA(sarsa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练SARSA\n",
    "sarsa_metrics = sarsa.train(env, n_episodes=500, log_interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Q-Learning控制\n",
    "\n",
    "### 4.1 Q-Learning算法\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
    "\n",
    "关键区别：使用**max**而不是实际的下一动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Q-Learning学习器\n",
    "qlearn_config = TDConfig(alpha=0.5, gamma=1.0, epsilon=0.1)\n",
    "qlearn = QLearning(qlearn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练Q-Learning\n",
    "env = CliffWalkingEnv()\n",
    "qlearn_metrics = qlearn.train(env, n_episodes=500, log_interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 算法对比实验\n",
    "\n",
    "### 5.1 SARSA vs Q-Learning\n",
    "\n",
    "在CliffWalking环境中：\n",
    "- **SARSA**：学习远离悬崖的安全路径\n",
    "- **Q-Learning**：学习沿悬崖边缘的最短路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较训练曲线\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "window = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制SARSA曲线\n",
    "sarsa_rewards = sarsa_metrics.episode_rewards\n",
    "ax.plot(sarsa_rewards, alpha=0.2, color='blue')\n",
    "sarsa_smooth = np.convolve(sarsa_rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(sarsa_rewards)), sarsa_smooth, \n",
    "        color='blue', linewidth=2, label='SARSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制Q-Learning曲线\n",
    "qlearn_rewards = qlearn_metrics.episode_rewards\n",
    "ax.plot(qlearn_rewards, alpha=0.2, color='red')\n",
    "qlearn_smooth = np.convolve(qlearn_rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(qlearn_rewards)), qlearn_smooth, \n",
    "        color='red', linewidth=2, label='Q-Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('SARSA vs Q-Learning on CliffWalking')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估贪婪策略\n",
    "env = CliffWalkingEnv()\n",
    "sarsa_eval = sarsa.evaluate(env, n_episodes=100)\n",
    "qlearn_eval = qlearn.evaluate(env, n_episodes=100)\n",
    "\n",
    "print(\"贪婪策略评估（无探索）:\")\n",
    "print(f\"  SARSA:      {sarsa_eval[0]:.2f} +/- {sarsa_eval[1]:.2f}\")\n",
    "print(f\"  Q-Learning: {qlearn_eval[0]:.2f} +/- {qlearn_eval[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 关键洞察\n",
    "\n",
    "| 指标 | SARSA | Q-Learning |\n",
    "|------|-------|------------|\n",
    "| 训练奖励 | 更高 | 更低 |\n",
    "| 评估奖励 | 更低 | 更高 |\n",
    "| 学到的路径 | 安全路径 | 最优路径 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 资格迹方法\n",
    "\n",
    "### 6.1 TD(λ)算法\n",
    "\n",
    "资格迹允许将TD误差回传给所有\"最近访问过的\"状态：\n",
    "\n",
    "$$E_t(s) = \\gamma \\lambda E_{t-1}(s) + \\mathbf{1}[S_t = s]$$\n",
    "\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t E_t(s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import TDLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同λ值\n",
    "env = RandomWalk(n_states=19)\n",
    "true_values = env.get_true_values(gamma=1.0)\n",
    "\n",
    "lambda_values = [0.0, 0.4, 0.8, 0.95]\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in lambda_values:\n",
    "    config = TDConfig(alpha=0.1, gamma=1.0, lambda_=lam)\n",
    "    learner = TDLambda(config)\n",
    "    learner.train(env, n_episodes=100, log_interval=200)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    estimated = {s: learner._value_function.get(s, 0.0) \n",
    "                 for s in range(env.n_total_states)}\n",
    "    rmse = np.sqrt(np.mean([(estimated[s] - true_values[s])**2 \n",
    "                            for s in range(1, env.n_states + 1)]))\n",
    "    results[lam] = rmse\n",
    "    print(f\"lambda = {lam:.2f}: RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化λ的影响\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(list(results.keys()), list(results.values()), 'bo-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Effect of lambda on TD(lambda) Performance')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **TD学习**结合了MC的采样和DP的自举\n",
    "2. **SARSA**是On-Policy，考虑探索风险，更安全\n",
    "3. **Q-Learning**是Off-Policy，学习最优策略\n",
    "4. **资格迹**通过λ参数权衡TD(0)和MC\n",
    "\n",
    "### 算法选择建议\n",
    "\n",
    "| 场景 | 推荐算法 |\n",
    "|------|----------|\n",
    "| 安全重要 | SARSA |\n",
    "| 需要最优 | Q-Learning |\n",
    "| 稀疏奖励 | TD(λ) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
