"""
TDå­¦ä¹ åŸºç±»æ¨¡å— (TD Learning Base Classes)
=========================================

æ ¸å¿ƒæ€æƒ³:
--------
æä¾›TDå­¦ä¹ ç®—æ³•çš„é€šç”¨æ¡†æ¶ï¼ŒåŒ…æ‹¬ä»·å€¼å‡½æ•°ç®¡ç†ã€ç­–ç•¥å®ç°å’Œè®­ç»ƒå¾ªç¯ã€‚
é‡‡ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼Œå­ç±»åªéœ€å®ç°ç‰¹å®šçš„æ›´æ–°è§„åˆ™ã€‚

æ•°å­¦åŸç†:
--------
TDå­¦ä¹ çš„æ ¸å¿ƒæ›´æ–°:
    V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]
              â””â”€æ—§ä¼°è®¡â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€TDç›®æ ‡â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TDè¯¯å·® Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t) æ˜¯å­¦ä¹ çš„é©±åŠ¨åŠ›ã€‚
"""

from __future__ import annotations
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Any, TypeVar, Generic, Protocol
import numpy as np
import logging

from .config import TDConfig, TrainingMetrics

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

State = TypeVar('State')
Action = TypeVar('Action')


class Environment(Protocol[State, Action]):
    """ç¯å¢ƒåè®®ï¼Œå…¼å®¹Gymnasium APIã€‚"""
    def reset(self) -> Tuple[State, Dict[str, Any]]: ...
    def step(self, action: Action) -> Tuple[State, float, bool, bool, Dict[str, Any]]: ...
    @property
    def action_space(self) -> Any: ...
    @property
    def observation_space(self) -> Any: ...


class Policy(Protocol[State, Action]):
    """ç­–ç•¥åè®®ã€‚"""
    def __call__(self, state: State) -> Action: ...
    def action_probabilities(self, state: State) -> Dict[Action, float]: ...


class BaseTDLearner(ABC, Generic[State, Action]):
    """
    æ—¶åºå·®åˆ†å­¦ä¹ ç®—æ³•åŸºç±»ã€‚
    
    æ ¸å¿ƒæ€æƒ³:
    --------
    TDå­¦ä¹ ç»“åˆäº†MCçš„é‡‡æ ·å’ŒDPçš„è‡ªä¸¾ï¼Œæ— éœ€ç­‰å¾…å›åˆç»“æŸå³å¯æ›´æ–°ã€‚
    
    æ•°å­¦åŸç†:
    --------
    æ”¶æ•›æ€§ä¿è¯ (Robbins-Monroæ¡ä»¶):
        Î£Î±_t = âˆ ä¸” Î£Î±_tÂ² < âˆ
    
    åœ¨æ»¡è¶³æ¡ä»¶æ—¶ï¼ŒTD(0)ä»¥æ¦‚ç‡1æ”¶æ•›åˆ°çœŸå®ä»·å€¼å‡½æ•°ã€‚
    """

    def __init__(self, config: TDConfig) -> None:
        self.config = config
        self._value_function: Dict[State, float] = defaultdict(lambda: config.initial_value)
        self._q_function: Dict[Tuple[State, Action], float] = defaultdict(lambda: config.initial_value)
        self.metrics = TrainingMetrics()
        self._action_space: Optional[List[Action]] = None

    @property
    def value_function(self) -> Dict[State, float]:
        return dict(self._value_function)

    @property
    def q_function(self) -> Dict[Tuple[State, Action], float]:
        return dict(self._q_function)

    def get_value(self, state: State) -> float:
        return self._value_function[state]

    def get_q_value(self, state: State, action: Action) -> float:
        return self._q_function[(state, action)]

    def set_action_space(self, actions: List[Action]) -> None:
        self._action_space = actions

    def epsilon_greedy_action(self, state: State) -> Action:
        """
        Îµ-greedyç­–ç•¥: Ï€(a|s) = Îµ/|A| + (1-Îµ)Â·ğŸ™(a = argmax Q)
        """
        if self._action_space is None:
            raise ValueError("æœªè®¾ç½®åŠ¨ä½œç©ºé—´")
        if np.random.random() < self.config.epsilon:
            return np.random.choice(self._action_space)
        q_values = [self.get_q_value(state, a) for a in self._action_space]
        max_q = max(q_values)
        best_actions = [a for a, q in zip(self._action_space, q_values) if np.isclose(q, max_q)]
        return np.random.choice(best_actions)

    def greedy_action(self, state: State) -> Action:
        if self._action_space is None:
            raise ValueError("æœªè®¾ç½®åŠ¨ä½œç©ºé—´")
        q_values = [self.get_q_value(state, a) for a in self._action_space]
        max_q = max(q_values)
        best_actions = [a for a, q in zip(self._action_space, q_values) if np.isclose(q, max_q)]
        return np.random.choice(best_actions)

    @abstractmethod
    def update(self, state: State, action: Action, reward: float,
               next_state: State, next_action: Optional[Action], done: bool) -> float:
        """æ‰§è¡ŒTDæ›´æ–°ï¼Œè¿”å›TDè¯¯å·®ã€‚"""
        pass

    def train_episode(self, env: Environment[State, Action], max_steps: int = 10000) -> Tuple[float, int]:
        state, _ = env.reset()
        action = self.epsilon_greedy_action(state)
        total_reward, td_errors = 0.0, []

        for step in range(max_steps):
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            next_action = None if done else self.epsilon_greedy_action(next_state)
            td_error = self.update(state, action, reward, next_state, next_action, done)
            td_errors.append(abs(td_error))
            total_reward += reward
            if done:
                break
            state, action = next_state, next_action

        self.metrics.add_episode(total_reward, step + 1, np.mean(td_errors) if td_errors else 0.0)
        return total_reward, step + 1

    def train(self, env: Environment[State, Action], n_episodes: int = 1000,
              max_steps_per_episode: int = 10000, log_interval: int = 100,
              early_stop_reward: Optional[float] = None) -> TrainingMetrics:
        if self._action_space is None:
            if hasattr(env.action_space, 'n'):
                self.set_action_space(list(range(env.action_space.n)))
            else:
                raise ValueError("æ— æ³•è‡ªåŠ¨æ¨æ–­åŠ¨ä½œç©ºé—´")

        for episode in range(n_episodes):
            reward, steps = self.train_episode(env, max_steps_per_episode)
            if (episode + 1) % log_interval == 0:
                avg = np.mean(self.metrics.episode_rewards[-log_interval:])
                logger.info(f"Episode {episode+1}/{n_episodes} | Avg: {avg:.2f} | Last: {reward:.2f}")
            if early_stop_reward and len(self.metrics.episode_rewards) >= 100:
                if np.mean(self.metrics.episode_rewards[-100:]) >= early_stop_reward:
                    logger.info(f"æ—©åœ: å¹³å‡å¥–åŠ±è¾¾åˆ° {early_stop_reward}")
                    break
        return self.metrics

    def evaluate(self, env: Environment[State, Action], n_episodes: int = 100,
                 max_steps: int = 10000) -> Tuple[float, float]:
        rewards = []
        for _ in range(n_episodes):
            state, _ = env.reset()
            total_reward = 0.0
            for _ in range(max_steps):
                action = self.greedy_action(state)
                state, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward
                if terminated or truncated:
                    break
            rewards.append(total_reward)
        return np.mean(rewards), np.std(rewards)
