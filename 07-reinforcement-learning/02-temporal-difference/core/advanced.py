"""
é«˜çº§TDç®—æ³•æ¨¡å— (Advanced TD Algorithms)
======================================

æ ¸å¿ƒæ€æƒ³ (Core Idea):
--------------------
æœ¬æ¨¡å—åŒ…å«TDå­¦ä¹ çš„é«˜çº§å˜ä½“ï¼Œè§£å†³åŸºç¡€ç®—æ³•çš„å„ç§å±€é™æ€§:
- Double Q-Learning: è§£å†³æœ€å¤§åŒ–åå·®ï¼ˆè¿‡ä¼°è®¡ï¼‰é—®é¢˜
- N-Step TD: åœ¨TD(0)å’ŒMonte Carloä¹‹é—´æƒè¡¡
- TD(Î»): é€šè¿‡èµ„æ ¼è¿¹ç»Ÿä¸€æ‰€æœ‰n-stepæ–¹æ³•
- SARSA(Î»): On-Policyç‰ˆæœ¬çš„TD(Î»)
- Watkins Q(Î»): Off-Policyå®‰å…¨çš„TD(Î»)

æ•°å­¦åŸç† (Mathematical Theory):
------------------------------
è¿™äº›ç®—æ³•ä»£è¡¨äº†TDå­¦ä¹ åœ¨ä¸åŒç»´åº¦çš„æ‰©å±•:

1. åå·®-æ–¹å·®æƒè¡¡ç»´åº¦:
   TD(0) â†â†’ n-step TD â†â†’ TD(Î») â†â†’ Monte Carlo
   (é«˜åå·®ä½æ–¹å·®)              (ä½åå·®é«˜æ–¹å·®)

2. è¿‡ä¼°è®¡æ ¡æ­£ç»´åº¦:
   Q-Learning â†’ Double Q-Learning
   (æœ‰æœ€å¤§åŒ–åå·®)   (æ— å)

3. ä¿¡ç”¨åˆ†é…ç»´åº¦:
   å•æ­¥æ›´æ–° â†’ èµ„æ ¼è¿¹ï¼ˆå¤šæ­¥ä¿¡ç”¨åˆ†é…ï¼‰

é—®é¢˜èƒŒæ™¯ (Problem Statement):
----------------------------
åŸºç¡€TDç®—æ³•å­˜åœ¨å„ç§å±€é™:
- Q-Learningåœ¨å™ªå£°ç¯å¢ƒä¸­è¿‡ä¼°è®¡Qå€¼
- TD(0)åªç”¨å•æ­¥ä¿¡æ¯ï¼Œæ”¶æ•›å¯èƒ½è¾ƒæ…¢
- ä¿¡ç”¨åˆ†é…èŒƒå›´æœ‰é™

æœ¬æ¨¡å—çš„ç®—æ³•é’ˆå¯¹è¿™äº›é—®é¢˜æä¾›è§£å†³æ–¹æ¡ˆã€‚

å¤æ‚åº¦ (Complexity):
-------------------
- Double Q-Learning: O(|A|) time, O(2Ã—|S|Ã—|A|) space
- N-Step TD: O(1) time, O(n) buffer space
- TD(Î»): O(|S|Ã—|A|) time (éœ€æ›´æ–°æ‰€æœ‰æœ‰èµ„æ ¼çš„çŠ¶æ€)
"""

from __future__ import annotations

import numpy as np
from collections import defaultdict
from typing import Optional, Dict, List, Tuple

from .base import BaseTDLearner, State, Action
from .config import TDConfig, EligibilityTraceType


class DoubleQLearning(BaseTDLearner[State, Action]):
    """
    Double Q-Learningç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    Double Q-Learningé€šè¿‡ç»´æŠ¤ä¸¤ä¸ªç‹¬ç«‹çš„Qè¡¨æ¥è§£å†³Q-Learningçš„æœ€å¤§åŒ–åå·®ã€‚
    ä¸€ä¸ªQè¡¨ç”¨äºé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œå¦ä¸€ä¸ªç”¨äºè¯„ä¼°è¯¥åŠ¨ä½œçš„ä»·å€¼ã€‚
    è¿™ç§"è§£è€¦"ç­–ç•¥æœ‰æ•ˆæ¶ˆé™¤äº†è¿‡ä¼°è®¡é—®é¢˜ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    æ ‡å‡†Q-Learningçš„é—®é¢˜:
        max_a Q(s', a) ä½¿ç”¨åŒä¸€ä¸ªQæ¥é€‰æ‹©å’Œè¯„ä¼°

        å½“Qæœ‰å™ªå£°æ—¶: E[max_a Q(s,a)] â‰¥ max_a E[Q(s,a)]
        è¿™å¯¼è‡´ç³»ç»Ÿæ€§çš„è¿‡ä¼°è®¡ã€‚

    è¿‡ä¼°è®¡çš„ç›´è§‰:
        æƒ³è±¡ä½ ä¼°è®¡å¤šä¸ªéšæœºå˜é‡çš„æœ€å¤§å€¼ã€‚
        å³ä½¿æ¯ä¸ªä¼°è®¡éƒ½æ˜¯æ— åçš„ï¼Œmaxæ“ä½œä¼šåå‘é€‰æ‹©
        é‚£äº›æ°å¥½è¢«é«˜ä¼°çš„å˜é‡ï¼Œå¯¼è‡´æ•´ä½“è¿‡ä¼°è®¡ã€‚

    Double Q-Learningè§£å†³æ–¹æ¡ˆ:
        ä»¥50%æ¦‚ç‡é€‰æ‹©æ›´æ–°Q_Aæˆ–Q_B:

        æ›´æ–°Q_Aæ—¶:
            a* = argmax_a Q_A(S', a)           # ç”¨Q_Aé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            Q_A(S, A) â† Q_A + Î±[R + Î³Q_B(S', a*) - Q_A]  # ç”¨Q_Bè¯„ä¼°

        æ›´æ–°Q_Bæ—¶:
            a* = argmax_a Q_B(S', a)           # ç”¨Q_Bé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            Q_B(S, A) â† Q_B + Î±[R + Î³Q_A(S', a*) - Q_B]  # ç”¨Q_Aè¯„ä¼°

    ä¸ºä»€ä¹ˆæœ‰æ•ˆ:
        å…³é”®æ´å¯Ÿ: E[max(X, Y)] â‰¥ max(E[X], E[Y]) (Jensenä¸ç­‰å¼)

        Q_Aå’ŒQ_Bæ˜¯ç‹¬ç«‹å­¦ä¹ çš„ï¼Œå®ƒä»¬çš„å™ªå£°ä¸ç›¸å…³ã€‚
        å½“ç”¨Q_Aé€‰æ‹©åŠ¨ä½œæ—¶ï¼Œå³ä½¿é€‰åˆ°äº†Q_Aé«˜ä¼°çš„åŠ¨ä½œï¼Œ
        Q_Bå¯¹è¯¥åŠ¨ä½œçš„ä¼°è®¡ï¼ˆç‹¬ç«‹å™ªå£°ï¼‰ä¸ä¼šåŒæ ·é«˜ä¼°ï¼Œ
        å› æ­¤æœŸæœ›æ˜¯æ— åçš„ã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    åœ¨éšæœºç¯å¢ƒä¸­ï¼ŒQ-Learningä¼šç³»ç»Ÿæ€§åœ°è¿‡ä¼°è®¡Qå€¼ã€‚
    ç»å…¸ä¾‹å­ï¼šåœ¨ä¸€ä¸ªçŠ¶æ€æœ‰ä¸¤ä¸ªåŠ¨ä½œï¼Œæ¯ä¸ªåŠ¨ä½œçš„çœŸå®ä»·å€¼éƒ½æ˜¯0ï¼Œ
    ä½†æœ‰éšæœºå™ªå£°ã€‚Q-Learningä¼šé€‰æ‹©ä¼°è®¡è¾ƒé«˜çš„é‚£ä¸ªï¼Œ
    å¯¼è‡´è¯¥çŠ¶æ€çš„max Q > 0ï¼Œäº§ç”Ÿè¿‡ä¼°è®¡ã€‚

    Double Q-Learningé€šè¿‡ä½¿ç”¨ç‹¬ç«‹çš„Qè¡¨è¿›è¡Œé€‰æ‹©å’Œè¯„ä¼°ï¼Œ
    æ‰“ç ´äº†è¿™ç§æ­£å‘åå·®ï¼Œè·å¾—æ— åçš„ä¼°è®¡ã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      ç®—æ³•        â”‚   åå·®     â”‚   æ–¹å·®     â”‚   å†…å­˜     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Q-Learning     â”‚  è¿‡ä¼°è®¡    â”‚    ä¸­      â”‚    1Ã—      â”‚
    â”‚ Double Q-Learningâ”‚   æ— å     â”‚    ä¸­      â”‚    2Ã—      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´å¤æ‚åº¦: O(|A|) per step
    - ç©ºé—´å¤æ‚åº¦: O(2 Ã— |S| Ã— |A|) for two Q-tables

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    Double Q-Learningæ˜¯Q-Learningçš„å»åå·®ç‰ˆæœ¬ã€‚é€šè¿‡ç»´æŠ¤ä¸¤ä¸ªQè¡¨
    å¹¶éšæœºé€‰æ‹©å“ªä¸ªç”¨äºé€‰æ‹©ã€å“ªä¸ªç”¨äºè¯„ä¼°ï¼Œå®ƒæ¶ˆé™¤äº†maxæ“ä½œå¼•å…¥çš„
    ç³»ç»Ÿæ€§è¿‡ä¼°è®¡ã€‚ä»£ä»·æ˜¯åŒå€çš„å†…å­˜æ¶ˆè€—ã€‚è¿™ä¸€æ€æƒ³åæ¥è¢«DQNé‡‡ç”¨
    (Double DQN)ï¼Œæˆä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ ‡å‡†æŠ€æœ¯ã€‚

    Example:
        >>> config = TDConfig(alpha=0.5, gamma=0.99, epsilon=0.1)
        >>> double_q = DoubleQLearning(config)
        >>> # åœ¨å™ªå£°ç¯å¢ƒä¸­ï¼ŒDouble Q-Learningçš„ä¼°è®¡æ›´å‡†ç¡®
    """

    def __init__(self, config: TDConfig) -> None:
        """åˆå§‹åŒ–Double Q-Learningï¼Œåˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„Qè¡¨ã€‚"""
        super().__init__(config)

        # ä¸¤ä¸ªç‹¬ç«‹çš„Qè¡¨
        self._q_function_a: Dict[Tuple[State, Action], float] = defaultdict(
            lambda: config.initial_value
        )
        self._q_function_b: Dict[Tuple[State, Action], float] = defaultdict(
            lambda: config.initial_value
        )

    def get_q_value(self, state: State, action: Action) -> float:
        """
        è·å–åˆå¹¶åçš„Qå€¼ï¼ˆä¸¤ä¸ªQè¡¨çš„å¹³å‡ï¼‰ã€‚

        ç”¨äºç­–ç•¥é€‰æ‹©æ—¶çš„Qå€¼è¯„ä¼°ã€‚
        """
        q_a = self._q_function_a[(state, action)]
        q_b = self._q_function_b[(state, action)]
        return (q_a + q_b) / 2

    @property
    def q_function(self) -> Dict[Tuple[State, Action], float]:
        """è·å–åˆå¹¶åçš„Qå‡½æ•°ã€‚"""
        all_keys = set(self._q_function_a.keys()) | set(self._q_function_b.keys())
        return {
            key: (self._q_function_a[key] + self._q_function_b[key]) / 2
            for key in all_keys
        }

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒDouble Q-Learningæ›´æ–°ã€‚

        ä»¥50%æ¦‚ç‡é€‰æ‹©æ›´æ–°Q_Aæˆ–Q_Bï¼Œäº¤å‰ä½¿ç”¨å¦ä¸€ä¸ªQè¡¨è¿›è¡Œè¯„ä¼°ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆä¸ä½¿ç”¨ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        # éšæœºé€‰æ‹©æ›´æ–°å“ªä¸ªQè¡¨
        update_a = np.random.random() < 0.5

        if update_a:
            # æ›´æ–°Q_Aï¼Œç”¨Q_Bè¯„ä¼°
            q_select = self._q_function_a
            q_eval = self._q_function_b
            q_update = self._q_function_a
        else:
            # æ›´æ–°Q_Bï¼Œç”¨Q_Aè¯„ä¼°
            q_select = self._q_function_b
            q_eval = self._q_function_a
            q_update = self._q_function_b

        current_q = q_update[(state, action)]

        if done:
            td_target = reward
        else:
            # ç”¨ä¸€ä¸ªQè¡¨é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            best_action = max(
                self._action_space,
                key=lambda a: q_select[(next_state, a)]
            )
            # ç”¨å¦ä¸€ä¸ªQè¡¨è¯„ä¼°è¯¥åŠ¨ä½œ
            td_target = reward + self.config.gamma * q_eval[(next_state, best_action)]

        td_error = td_target - current_q
        q_update[(state, action)] += self.config.alpha * td_error

        return td_error


class NStepTD(BaseTDLearner[State, Action]):
    """
    N-Step TDç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    N-Step TDæ˜¯TD(0)å’ŒMonte Carloçš„ä¸­é—´æ–¹æ¡ˆã€‚å®ƒä½¿ç”¨næ­¥çš„å®é™…å¥–åŠ±
    åŠ ä¸Šç¬¬n+1æ­¥çš„ä»·å€¼ä¼°è®¡ä½œä¸ºTDç›®æ ‡ã€‚nè¶Šå¤§ï¼Œè¶Šæ¥è¿‘Monte Carloï¼›
    n=1æ—¶å°±æ˜¯TD(0)ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    n-stepå›æŠ¥:
        G_t^{(n)} = R_{t+1} + Î³R_{t+2} + ... + Î³^{n-1}R_{t+n} + Î³^n V(S_{t+n})
                  = Î£_{k=0}^{n-1} Î³^k R_{t+k+1} + Î³^n V(S_{t+n})

    æ›´æ–°è§„åˆ™:
        V(S_t) â† V(S_t) + Î±[G_t^{(n)} - V(S_t)]

    å¯¹äºQå‡½æ•°ï¼ˆn-step SARSAé£æ ¼ï¼‰:
        G_t^{(n)} = Î£_{k=0}^{n-1} Î³^k R_{t+k+1} + Î³^n Q(S_{t+n}, A_{t+n})

    å…³é”®æ´å¯Ÿ:
        å½“nâ†’âˆï¼ŒG_t^{(n)}å˜æˆå®Œæ•´çš„Monte Carloå›æŠ¥
        å½“n=1ï¼ŒG_t^{(n)}å°±æ˜¯TD(0)ç›®æ ‡
        n-step TDæä¾›äº†å¹³æ»‘çš„è¿‡æ¸¡

    åå·®-æ–¹å·®æƒè¡¡:
        è¾ƒå°çš„n: æ›´å¤šè‡ªä¸¾ï¼Œåå·®å¤§ä½†æ–¹å·®å°
        è¾ƒå¤§çš„n: æ›´å¤šå®é™…å¥–åŠ±ï¼Œåå·®å°ä½†æ–¹å·®å¤§

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    TD(0)åå·®é«˜ã€æ–¹å·®ä½ï¼›Monte Carloåå·®ä½ã€æ–¹å·®é«˜ã€‚
    N-Step TDæä¾›äº†ä¸€ç§åœ¨ä¸¤è€…ä¹‹é—´å¹³æ»‘è¿‡æ¸¡çš„æ–¹å¼ï¼Œ
    å…è®¸æ ¹æ®é—®é¢˜ç‰¹æ€§é€‰æ‹©åˆé€‚çš„nå€¼ã€‚

    å®è·µä¸­ï¼Œæœ€ä¼˜né€šå¸¸åœ¨4-10ä¹‹é—´ï¼Œéœ€è¦é’ˆå¯¹å…·ä½“ä»»åŠ¡è°ƒä¼˜ã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     n     â”‚    åå·®    â”‚    æ–¹å·®    â”‚   å»¶è¿Ÿ     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    1      â”‚     é«˜     â”‚     ä½     â”‚   1 step   â”‚
    â”‚    5      â”‚     ä¸­     â”‚     ä¸­     â”‚   5 steps  â”‚
    â”‚   100     â”‚     ä½     â”‚     é«˜     â”‚  100 steps â”‚
    â”‚    âˆ      â”‚     æ—      â”‚     é«˜     â”‚  episode   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´å¤æ‚åº¦: O(1) per step (æ‘Šé”€)
    - ç©ºé—´å¤æ‚åº¦: O(n) for storing n-step buffer

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    N-Step TDé€šè¿‡è°ƒæ•´nå€¼åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´æƒè¡¡ã€‚è¾ƒå°çš„næ›´æ–°æ›´é¢‘ç¹ä½†åå·®å¤§ï¼Œ
    è¾ƒå¤§çš„nèƒ½åˆ©ç”¨æ›´å¤šçœŸå®å¥–åŠ±ä¿¡æ¯ä½†éœ€è¦ç­‰å¾…æ›´é•¿æ—¶é—´ã€‚
    å®ƒæ˜¯ç†è§£TD(Î»)çš„åŸºç¡€â€”â€”TD(Î»)æœ¬è´¨ä¸Šæ˜¯å¯¹æ‰€æœ‰n-stepå›æŠ¥çš„åŠ æƒç»„åˆã€‚

    Example:
        >>> config = TDConfig(alpha=0.5, gamma=0.99, n_step=3)
        >>> n_step_td = NStepTD(config)
        >>> metrics = n_step_td.train(env, n_episodes=500)
    """

    def __init__(self, config: TDConfig) -> None:
        """åˆå§‹åŒ–N-Step TDã€‚"""
        super().__init__(config)
        # næ­¥ç»éªŒç¼“å†²åŒº: [(state, action, reward), ...]
        self._buffer: List[Tuple[State, Action, float]] = []
        self._states_buffer: List[State] = []

    def on_episode_start(self, episode: int) -> None:
        """å›åˆå¼€å§‹æ—¶æ¸…ç©ºç¼“å†²åŒºã€‚"""
        self._buffer.clear()
        self._states_buffer.clear()

    def _compute_n_step_return(
        self,
        rewards: List[float],
        final_state: State,
        done: bool
    ) -> float:
        """
        è®¡ç®—n-stepå›æŠ¥ã€‚

        G_t^{(n)} = Î£_{k=0}^{n-1} Î³^k R_{t+k+1} + Î³^n V(S_{t+n})

        Args:
            rewards: næ­¥å¥–åŠ±åˆ—è¡¨
            final_state: æœ€ç»ˆçŠ¶æ€
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            n-stepå›æŠ¥
        """
        n_step_return = 0.0
        discount = 1.0

        for reward in rewards:
            n_step_return += discount * reward
            discount *= self.config.gamma

        # å¦‚æœæœªç»ˆæ­¢ï¼ŒåŠ ä¸Šè‡ªä¸¾é¡¹
        if not done:
            # ä½¿ç”¨çŠ¶æ€çš„æœ€å¤§Qå€¼ä½œä¸ºä»·å€¼ä¼°è®¡
            max_q = max(
                self._q_function[(final_state, a)]
                for a in self._action_space
            ) if self._action_space else 0.0
            n_step_return += discount * max_q

        return n_step_return

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒN-Step TDæ›´æ–°ã€‚

        å°†ç»éªŒå­˜å…¥ç¼“å†²åŒºï¼Œå½“ç¼“å†²åŒºæ»¡æˆ–å›åˆç»“æŸæ—¶æ›´æ–°Qå€¼ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œ
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®ï¼ˆå¦‚æœè§¦å‘æ›´æ–°ï¼‰
        """
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self._buffer.append((state, action, reward))
        self._states_buffer.append(next_state)

        td_error = 0.0

        # å½“ç¼“å†²åŒºæ»¡æˆ–å›åˆç»“æŸæ—¶ï¼Œè¿›è¡Œæ›´æ–°
        if len(self._buffer) >= self.config.n_step or done:
            # æå–è¦æ›´æ–°çš„çŠ¶æ€å’Œå¥–åŠ±
            update_state = self._buffer[0][0]
            update_action = self._buffer[0][1]
            rewards = [exp[2] for exp in self._buffer]

            # è®¡ç®—n-stepå›æŠ¥
            n_step_return = self._compute_n_step_return(rewards, next_state, done)

            # æ›´æ–°Qå€¼
            current_q = self._q_function[(update_state, update_action)]
            td_error = n_step_return - current_q
            self._q_function[(update_state, update_action)] += self.config.alpha * td_error

            # ç§»é™¤æœ€æ—§çš„ç»éªŒ
            self._buffer.pop(0)
            if self._states_buffer:
                self._states_buffer.pop(0)

        # å›åˆç»“æŸæ—¶æ›´æ–°ç¼“å†²åŒºä¸­å‰©ä½™çš„çŠ¶æ€
        if done:
            while self._buffer:
                update_state = self._buffer[0][0]
                update_action = self._buffer[0][1]
                rewards = [exp[2] for exp in self._buffer]

                n_step_return = self._compute_n_step_return(rewards, next_state, True)

                current_q = self._q_function[(update_state, update_action)]
                td_error = n_step_return - current_q
                self._q_function[(update_state, update_action)] += self.config.alpha * td_error

                self._buffer.pop(0)
                if self._states_buffer:
                    self._states_buffer.pop(0)

        return td_error


class TDLambda(BaseTDLearner[State, Action]):
    """
    TD(Î»)ç®—æ³•å®ç° (å¸¦èµ„æ ¼è¿¹)ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    TD(Î»)é€šè¿‡èµ„æ ¼è¿¹(Eligibility Traces)ç»Ÿä¸€äº†TD(0)å’ŒMonte Carloã€‚
    èµ„æ ¼è¿¹è¿½è¸ªå“ªäº›çŠ¶æ€"æœ‰èµ„æ ¼"æ¥æ”¶å½“å‰TDè¯¯å·®çš„æ›´æ–°â€”â€”æœ€è¿‘è®¿é—®çš„çŠ¶æ€
    èµ„æ ¼æœ€é«˜ï¼Œéšæ—¶é—´æŒ‡æ•°è¡°å‡ã€‚è¿™ç­‰ä»·äºåœ¨æ‰€æœ‰n-stepå›æŠ¥ä¸Šåšå‡ ä½•åŠ æƒå¹³å‡ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    Î»-å›æŠ¥ (Forward View):
        G_t^Î» = (1-Î») Î£_{n=1}^{âˆ} Î»^{n-1} G_t^{(n)}

    è¿™æ˜¯æ‰€æœ‰n-stepå›æŠ¥çš„å‡ ä½•åŠ æƒå¹³å‡:
    - G_t^{(1)} æƒé‡: (1-Î»)
    - G_t^{(2)} æƒé‡: (1-Î»)Î»
    - G_t^{(n)} æƒé‡: (1-Î»)Î»^{n-1}
    - æƒé‡å’Œ: (1-Î»)(1 + Î» + Î»Â² + ...) = 1

    èµ„æ ¼è¿¹ (Backward View):
        æä¾›äº†é«˜æ•ˆå®ç°Î»-å›æŠ¥çš„æ–¹æ³•ã€‚

        ç´¯ç§¯è¿¹ (Accumulating Trace):
            E_t(s) = Î³Î»E_{t-1}(s) + ğŸ™(S_t = s)
            æ¯æ¬¡è®¿é—®çŠ¶æ€æ—¶è¿¹å€¼ç´¯åŠ ã€‚

        æ›¿æ¢è¿¹ (Replacing Trace):
            E_t(s) = Î³Î»E_{t-1}(s) if s â‰  S_t
            E_t(S_t) = 1
            è®¿é—®æ—¶é‡ç½®ä¸º1ï¼Œé¿å…ç´¯ç§¯è¿‡å¤§ã€‚

        è·å…°è¿¹ (Dutch Trace):
            E_t(s) = Î³Î»E_{t-1}(s) + (1 - Î±Î³Î»E_{t-1}(s))ğŸ™(S_t = s)
            åœ¨å‡½æ•°é€¼è¿‘ä¸‹æœ‰æ›´å¥½çš„ç†è®ºä¿è¯ã€‚

    æ›´æ–°è§„åˆ™:
        Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)
        V(s) â† V(s) + Î±Î´_t E_t(s), âˆ€s

    å‰å‘è§†å›¾ä¸åå‘è§†å›¾ç­‰ä»·æ€§:
        åœ¨ç¦»çº¿æ›´æ–°ï¼ˆå›åˆç»“æŸåæ‰¹é‡æ›´æ–°ï¼‰ä¸‹ï¼Œä¸¤ç§è§†å›¾äº§ç”Ÿç›¸åŒçš„æ›´æ–°é‡ã€‚
        èµ„æ ¼è¿¹æä¾›äº†åœ¨çº¿ã€å¢é‡å¼çš„å®ç°ã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    N-Step TDéœ€è¦é€‰æ‹©ç‰¹å®šçš„nå€¼ï¼Œä¸åŒçš„nåœ¨ä¸åŒç¯å¢ƒä¸­è¡¨ç°å·®å¼‚å¤§ã€‚
    TD(Î»)é€šè¿‡èµ„æ ¼è¿¹å®ç°å¯¹æ‰€æœ‰nçš„åŠ æƒç»„åˆï¼Œç”±å•ä¸€å‚æ•°Î»æ§åˆ¶:
    - Î»=0: ç­‰ä»·äºTD(0)ï¼Œåªçœ‹ä¸€æ­¥
    - Î»=1: ç­‰ä»·äºMonte Carloï¼Œçœ‹å®Œæ•´å›åˆ
    - 0<Î»<1: ä¸¤è€…çš„æ··åˆï¼Œé€šå¸¸Î»=0.9æ˜¯å¥½çš„èµ·ç‚¹

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    ä¸åŒÎ»å€¼çš„ç‰¹æ€§:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     Î»     â”‚   ç­‰ä»·äº   â”‚    åå·®    â”‚      æ–¹å·®      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    0      â”‚   TD(0)    â”‚     é«˜     â”‚       ä½       â”‚
    â”‚   0.5     â”‚   æ··åˆ     â”‚     ä¸­     â”‚       ä¸­       â”‚
    â”‚   0.9     â”‚ æ¥è¿‘MC     â”‚     ä½     â”‚       è¾ƒé«˜     â”‚
    â”‚    1      â”‚   MC       â”‚     æ—      â”‚       é«˜       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    èµ„æ ¼è¿¹ç±»å‹å¯¹æ¯”:
    - ç´¯ç§¯è¿¹: ç»å…¸æ–¹æ³•ï¼Œä½†åœ¨é¢‘ç¹é‡è®¿æ—¶å¯èƒ½ä¸ç¨³å®š
    - æ›¿æ¢è¿¹: åœ¨éƒ¨åˆ†ç¯å¢ƒä¸­æ›´ç¨³å®šï¼Œä½†ç†è®ºä¿è¯è¾ƒå¼±
    - è·å…°è¿¹: æ¨èç”¨äºå‡½æ•°é€¼è¿‘ï¼Œç†è®ºå’Œå®è·µè¡¨ç°éƒ½å¥½

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´å¤æ‚åº¦: O(|S|Ã—|A|) per step (éœ€è¦æ›´æ–°æ‰€æœ‰æœ‰èµ„æ ¼çš„çŠ¶æ€)
    - ç©ºé—´å¤æ‚åº¦: O(|S|Ã—|A|) for eligibility traces

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    TD(Î»)æ˜¯TDå­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ã€‚é€šè¿‡èµ„æ ¼è¿¹ï¼Œå®ƒåœ¨æ¯ä¸€æ­¥å°†TDè¯¯å·®
    åˆ†é…ç»™æ‰€æœ‰æœ€è¿‘è®¿é—®çš„çŠ¶æ€ï¼Œåˆ†é…é‡éšæ—¶é—´å’ŒÎ»æŒ‡æ•°è¡°å‡ã€‚
    è¿™å·§å¦™åœ°ç»„åˆäº†æ‰€æœ‰n-stepæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œç”¨å•ä¸€å‚æ•°Î»æ§åˆ¶æƒè¡¡ã€‚
    åœ¨å®è·µä¸­ï¼ŒÎ»=0.9é€šå¸¸æ˜¯ä¸€ä¸ªå¥½çš„èµ·ç‚¹ã€‚

    Example:
        >>> config = TDConfig(alpha=0.1, gamma=0.99, lambda_=0.9)
        >>> td_lambda = TDLambda(config)
        >>> metrics = td_lambda.train(env, n_episodes=500)
    """

    def __init__(self, config: TDConfig) -> None:
        """åˆå§‹åŒ–TD(Î»)ã€‚"""
        super().__init__(config)
        # èµ„æ ¼è¿¹: (state, action) -> trace value
        self._eligibility_traces: Dict[Tuple[State, Action], float] = defaultdict(float)

    def on_episode_start(self, episode: int) -> None:
        """å›åˆå¼€å§‹æ—¶æ¸…ç©ºèµ„æ ¼è¿¹ã€‚"""
        self._eligibility_traces.clear()

    def _update_traces(self, state: State, action: Action) -> None:
        """
        æ›´æ–°èµ„æ ¼è¿¹ã€‚

        æ ¹æ®é…ç½®çš„è¿¹ç±»å‹æ‰§è¡Œä¸åŒçš„æ›´æ–°è§„åˆ™ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
        """
        gamma_lambda = self.config.gamma * self.config.lambda_

        # è¡°å‡æ‰€æœ‰ç°æœ‰çš„èµ„æ ¼è¿¹
        keys_to_remove = []
        for key in self._eligibility_traces:
            self._eligibility_traces[key] *= gamma_lambda
            # æ¸…é™¤è¿‡å°çš„è¿¹ä»¥èŠ‚çœå†…å­˜å’Œè®¡ç®—
            if self._eligibility_traces[key] < 1e-8:
                keys_to_remove.append(key)

        for key in keys_to_remove:
            del self._eligibility_traces[key]

        # æ›´æ–°å½“å‰çŠ¶æ€-åŠ¨ä½œçš„èµ„æ ¼è¿¹
        if self.config.trace_type == EligibilityTraceType.ACCUMULATING:
            # ç´¯ç§¯è¿¹: E(s,a) += 1
            self._eligibility_traces[(state, action)] += 1.0

        elif self.config.trace_type == EligibilityTraceType.REPLACING:
            # æ›¿æ¢è¿¹: E(s,a) = 1
            self._eligibility_traces[(state, action)] = 1.0

        elif self.config.trace_type == EligibilityTraceType.DUTCH:
            # è·å…°è¿¹: E(s,a) = (1-Î±)Î³Î»E(s,a) + 1
            current_trace = self._eligibility_traces[(state, action)]
            self._eligibility_traces[(state, action)] = (
                (1 - self.config.alpha) * gamma_lambda * current_trace + 1.0
            )

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒTD(Î»)æ›´æ–°ã€‚

        æµç¨‹:
        1. è®¡ç®—TDè¯¯å·®Î´
        2. æ›´æ–°å½“å‰çŠ¶æ€-åŠ¨ä½œçš„èµ„æ ¼è¿¹
        3. ç”¨Î´å’Œèµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå€¼

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œ
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        # è®¡ç®—TDç›®æ ‡
        if done:
            td_target = reward
        else:
            if next_action is None:
                # Q-Learningé£æ ¼: ä½¿ç”¨max
                max_next_q = max(
                    self._q_function[(next_state, a)]
                    for a in self._action_space
                )
                td_target = reward + self.config.gamma * max_next_q
            else:
                # SARSAé£æ ¼: ä½¿ç”¨å®é™…åŠ¨ä½œ
                td_target = reward + self.config.gamma * self._q_function[(next_state, next_action)]

        # è®¡ç®—TDè¯¯å·®
        current_q = self._q_function[(state, action)]
        td_error = td_target - current_q

        # æ›´æ–°èµ„æ ¼è¿¹ï¼ˆåœ¨è®¡ç®—è¯¯å·®ä¹‹åï¼‰
        self._update_traces(state, action)

        # ä½¿ç”¨èµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰ç›¸å…³çš„Qå€¼
        for (s, a), trace in self._eligibility_traces.items():
            self._q_function[(s, a)] += self.config.alpha * td_error * trace

        # å›åˆç»“æŸæ—¶æ¸…ç©ºèµ„æ ¼è¿¹
        if done:
            self._eligibility_traces.clear()

        return td_error


class SARSALambda(TDLambda):
    """
    SARSA(Î»)ç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    SARSA(Î»)æ˜¯SARSAä¸èµ„æ ¼è¿¹çš„ç»“åˆã€‚å®ƒæ˜¯On-Policyçš„TD(Î»)æ§åˆ¶ç®—æ³•ï¼Œ
    ä½¿ç”¨å®é™…ä¸‹ä¸€åŠ¨ä½œè®¡ç®—TDç›®æ ‡ï¼ŒåŒæ—¶é€šè¿‡èµ„æ ¼è¿¹å®ç°å¤šæ­¥ä¿¡ç”¨åˆ†é…ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    TDè¯¯å·® (SARSAé£æ ¼):
        Î´_t = R_{t+1} + Î³Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)

    èµ„æ ¼è¿¹æ›´æ–°:
        E_t(s, a) = Î³Î»E_{t-1}(s, a) + ğŸ™(S_t=s, A_t=a)

    Qå€¼æ›´æ–°:
        Q(s, a) â† Q(s, a) + Î±Î´_t E_t(s, a), âˆ€s, a

    ä¸SARSAçš„å…³ç³»:
        å½“Î»=0æ—¶ï¼Œé€€åŒ–ä¸ºSARSAï¼ˆå•æ­¥æ›´æ–°ï¼‰
        å½“Î»=1æ—¶ï¼Œå˜æˆå®Œæ•´å›åˆçš„On-Policyæ›´æ–°

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    SARSAçš„æ›´æ–°ä»…ä¾èµ–å•æ­¥ä¿¡æ¯ï¼Œä¿¡ç”¨åˆ†é…èŒƒå›´æœ‰é™ã€‚
    SARSA(Î»)é€šè¿‡èµ„æ ¼è¿¹å°†TDè¯¯å·®ä¼ æ’­åˆ°æ‰€æœ‰æœ€è¿‘è®¿é—®çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œ
    å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒOn-Policyçš„å®‰å…¨æ€§ç‰¹ç‚¹ã€‚

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´å¤æ‚åº¦: O(|S|Ã—|A|) per step (æ›´æ–°æ‰€æœ‰æœ‰èµ„æ ¼çš„çŠ¶æ€)
    - ç©ºé—´å¤æ‚åº¦: O(|S|Ã—|A|) for eligibility traces

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    SARSA(Î»)ç»“åˆäº†SARSAçš„On-Policyç‰¹æ€§å’Œèµ„æ ¼è¿¹çš„é«˜æ•ˆä¿¡ç”¨åˆ†é…ã€‚
    å®ƒä¿æŒäº†SARSAçš„å®‰å…¨æ€§ï¼ˆè€ƒè™‘æ¢ç´¢é£é™©ï¼‰ï¼ŒåŒæ—¶é€šè¿‡å¤šæ­¥ä¼ æ’­åŠ é€Ÿå­¦ä¹ ã€‚
    é€‚åˆéœ€è¦å®‰å…¨æ¢ç´¢ä¸”çŠ¶æ€ç©ºé—´è¾ƒå¤§çš„ç¯å¢ƒã€‚

    Example:
        >>> config = TDConfig(alpha=0.1, gamma=0.99, lambda_=0.9, epsilon=0.1)
        >>> sarsa_lambda = SARSALambda(config)
        >>> metrics = sarsa_lambda.train(env, n_episodes=500)
    """

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒSARSA(Î»)æ›´æ–°ã€‚

        å¼ºåˆ¶ä½¿ç”¨SARSAé£æ ¼çš„TDç›®æ ‡ï¼ˆå®é™…ä¸‹ä¸€åŠ¨ä½œï¼‰ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆSARSAå¿…éœ€ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®

        Raises:
            ValueError: å½“éç»ˆæ­¢çŠ¶æ€ç¼ºå°‘next_actionæ—¶
        """
        # è®¡ç®—SARSAé£æ ¼çš„TDè¯¯å·®
        if done:
            td_target = reward
        else:
            if next_action is None:
                raise ValueError("SARSA(Î»)éœ€è¦next_actionå‚æ•°")
            td_target = reward + self.config.gamma * self._q_function[(next_state, next_action)]

        current_q = self._q_function[(state, action)]
        td_error = td_target - current_q

        # æ›´æ–°èµ„æ ¼è¿¹
        self._update_traces(state, action)

        # ä½¿ç”¨èµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰Qå€¼
        for (s, a), trace in self._eligibility_traces.items():
            self._q_function[(s, a)] += self.config.alpha * td_error * trace

        # å›åˆç»“æŸæ—¶æ¸…ç©ºèµ„æ ¼è¿¹
        if done:
            self._eligibility_traces.clear()

        return td_error


class WatkinsQLambda(TDLambda):
    """
    Watkins's Q(Î»)ç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    Watkins's Q(Î»)æ˜¯Q-Learningä¸èµ„æ ¼è¿¹çš„ç»“åˆï¼Œä½†æœ‰ä¸€ä¸ªå…³é”®ç‰¹ç‚¹ï¼š
    å½“é‡‡å–éè´ªå©ªåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰æ—¶ï¼Œèµ„æ ¼è¿¹è¢«æ¸…é›¶ã€‚è¿™ç¡®ä¿äº†ç®—æ³•åœ¨
    Off-Policyè®¾ç½®ä¸‹çš„æ”¶æ•›æ€§ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    TDè¯¯å·® (Q-Learningé£æ ¼):
        Î´_t = R_{t+1} + Î³ max_a Q(S_{t+1}, a) - Q(S_t, A_t)

    èµ„æ ¼è¿¹æ›´æ–° (å…³é”®åŒºåˆ«):
        å¦‚æœ A_{t+1} = argmax_a Q(S_{t+1}, a) (è´ªå©ªåŠ¨ä½œ):
            E_t(s, a) = Î³Î»E_{t-1}(s, a) + ğŸ™(S_t=s, A_t=a)
        å¦åˆ™ (æ¢ç´¢åŠ¨ä½œ):
            E_t(s, a) = 0  âˆ€s, a  (æ¸…é›¶æ‰€æœ‰è¿¹!)
            ç„¶å E_t(S_t, A_t) = 1

    ä¸ºä»€ä¹ˆæ¸…é›¶èµ„æ ¼è¿¹:
        Q-Learningçš„TDç›®æ ‡å‡è®¾åç»­åŠ¨ä½œéƒ½æ˜¯è´ªå©ªçš„ã€‚
        å½“å®é™…é‡‡å–æ¢ç´¢åŠ¨ä½œæ—¶ï¼Œè¿™ä¸ªå‡è®¾è¢«æ‰“ç ´ã€‚
        å¦‚æœç»§ç»­ä¼ æ’­TDè¯¯å·®åˆ°æ›´æ—©çš„çŠ¶æ€ï¼Œä¼šå¼•å…¥åå·®ï¼Œ
        å¯èƒ½å¯¼è‡´ä¸æ”¶æ•›ã€‚æ¸…é›¶èµ„æ ¼è¿¹åˆ‡æ–­é”™è¯¯çš„ä¿¡ç”¨åˆ†é…é“¾ã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    ç®€å•åœ°å°†èµ„æ ¼è¿¹åŠ å…¥Q-Learningä¼šå¯¼è‡´åœ¨Off-Policyè®¾ç½®ä¸‹ä¸æ”¶æ•›ã€‚
    å› ä¸ºQ-Learningå‡è®¾ç›®æ ‡ç­–ç•¥æ˜¯è´ªå©ªçš„ï¼Œä½†èµ„æ ¼è¿¹ä¼ æ’­çš„æ˜¯
    è¡Œä¸ºç­–ç•¥ï¼ˆå«æ¢ç´¢ï¼‰çš„ç»éªŒã€‚

    Watkins's Q(Î»)é€šè¿‡åœ¨æ¢ç´¢æ—¶åˆ‡æ–­èµ„æ ¼è¿¹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚
    ç¼ºç‚¹æ˜¯åœ¨é«˜æ¢ç´¢ç‡ä¸‹ï¼Œèµ„æ ¼è¿¹ç»å¸¸è¢«æ¸…é›¶ï¼Œé€€åŒ–ä¸ºæ¥è¿‘Q-Learningã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     ç®—æ³•       â”‚   æ¢ç´¢æ—¶çš„è¿¹    â”‚    æ”¶æ•›æ€§       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Q(Î») naive   â”‚     ä¿ç•™        â”‚    ä¸ä¿è¯       â”‚
    â”‚  Watkins Q(Î»)  â”‚     æ¸…é›¶        â”‚    ä¿è¯         â”‚
    â”‚   SARSA(Î»)     â”‚     ä¿ç•™        â”‚    ä¿è¯*        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    * SARSA(Î»)æ˜¯On-Policyçš„ï¼Œä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´å¤æ‚åº¦: O(|S|Ã—|A|) per step (æœ€åæƒ…å†µ)
    - ç©ºé—´å¤æ‚åº¦: O(|S|Ã—|A|) for eligibility traces

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    Watkins's Q(Î»)åœ¨Off-Policyå­¦ä¹ ä¸­å®‰å…¨åœ°ä½¿ç”¨èµ„æ ¼è¿¹ã€‚
    ä»£ä»·æ˜¯å½“æ¢ç´¢åŠ¨ä½œå‘ç”Ÿæ—¶ï¼Œæ— æ³•åˆ©ç”¨ä¹‹å‰çš„ç»éªŒè¿›è¡Œä¿¡ç”¨åˆ†é…ã€‚
    åœ¨ä½Îµè®¾ç½®ä¸‹æ•ˆæœè¾ƒå¥½ï¼Œé«˜Îµæ—¶é€€åŒ–ä¸ºè¿‘ä¼¼Q-Learningã€‚

    Example:
        >>> config = TDConfig(alpha=0.1, gamma=0.99, lambda_=0.9, epsilon=0.05)
        >>> watkins_q = WatkinsQLambda(config)
        >>> # ä½æ¢ç´¢ç‡ä¸‹æ•ˆæœæœ€ä½³
    """

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒWatkins's Q(Î»)æ›´æ–°ã€‚

        ä½¿ç”¨Q-Learningç›®æ ‡ï¼Œåœ¨æ¢ç´¢åŠ¨ä½œæ—¶æ¸…é›¶èµ„æ ¼è¿¹ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆç”¨äºæ£€æµ‹æ˜¯å¦æ¢ç´¢ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        # è®¡ç®—Q-Learningé£æ ¼çš„TDè¯¯å·®
        if done:
            td_target = reward
        else:
            max_next_q = max(
                self._q_function[(next_state, a)]
                for a in self._action_space
            )
            td_target = reward + self.config.gamma * max_next_q

        current_q = self._q_function[(state, action)]
        td_error = td_target - current_q

        # æ›´æ–°èµ„æ ¼è¿¹
        self._update_traces(state, action)

        # ä½¿ç”¨èµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰Qå€¼
        for (s, a), trace in self._eligibility_traces.items():
            self._q_function[(s, a)] += self.config.alpha * td_error * trace

        # æ£€æŸ¥æ˜¯å¦é‡‡å–äº†æ¢ç´¢åŠ¨ä½œï¼Œå¦‚æœæ˜¯åˆ™æ¸…é›¶èµ„æ ¼è¿¹
        if not done and next_action is not None:
            # æ‰¾åˆ°è´ªå©ªåŠ¨ä½œ
            max_next_q = max(
                self._q_function[(next_state, a)]
                for a in self._action_space
            )
            greedy_actions = [
                a for a in self._action_space
                if np.isclose(self._q_function[(next_state, a)], max_next_q)
            ]

            # å¦‚æœä¸‹ä¸€åŠ¨ä½œä¸æ˜¯è´ªå©ªåŠ¨ä½œï¼Œæ¸…é›¶èµ„æ ¼è¿¹
            if next_action not in greedy_actions:
                self._eligibility_traces.clear()

        # å›åˆç»“æŸæ—¶æ¸…ç©ºèµ„æ ¼è¿¹
        if done:
            self._eligibility_traces.clear()

        return td_error
