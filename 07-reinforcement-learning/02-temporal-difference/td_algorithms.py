"""
æ—¶åºå·®åˆ†å­¦ä¹ ç®—æ³• (Temporal Difference Learning Algorithms)
============================================================

æ ¸å¿ƒæ€æƒ³ (Core Idea):
--------------------
æ—¶åºå·®åˆ†å­¦ä¹ æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒèŒƒå¼ï¼Œå®ƒç»“åˆäº†è’™ç‰¹å¡æ´›æ–¹æ³•çš„é‡‡æ ·æ€æƒ³å’ŒåŠ¨æ€è§„åˆ’çš„
è‡ªä¸¾(Bootstrapping)æ€æƒ³ã€‚TDæ–¹æ³•æ— éœ€ç­‰å¾…å›åˆç»“æŸï¼Œä»…ä¾èµ–ä¸‹ä¸€æ­¥çš„ä¼°è®¡å€¼å°±èƒ½
æ›´æ–°å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡â€”â€”è¿™æ˜¯"ç”¨çŒœæµ‹æ›´æ–°çŒœæµ‹"çš„ç²¾é«“ã€‚

æ•°å­¦åŸç† (Mathematical Theory):
------------------------------
TD(0)æ›´æ–°è§„åˆ™:
    V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]

å…¶ä¸­:
    - V(S_t): çŠ¶æ€S_tçš„ä»·å€¼ä¼°è®¡
    - Î±: å­¦ä¹ ç‡ (learning rate)
    - R_{t+1}: ä»S_tè½¬ç§»åˆ°S_{t+1}è·å¾—çš„å³æ—¶å¥–åŠ±
    - Î³: æŠ˜æ‰£å› å­ (discount factor), Î³ âˆˆ [0, 1]
    - R_{t+1} + Î³V(S_{t+1}): TDç›®æ ‡ (TD target)
    - Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t): TDè¯¯å·® (TD error)

TD(Î»)èµ„æ ¼è¿¹æ›´æ–°:
    E_t(s) = Î³Î»E_{t-1}(s) + ğŸ™(S_t = s)  (ç´¯ç§¯è¿¹)
    æˆ–
    E_t(s) = (1-Î±)Î³Î»E_{t-1}(s) + ğŸ™(S_t = s)  (è·å…°è¿¹)

    V(s) â† V(s) + Î±Î´_t E_t(s), âˆ€s

é—®é¢˜èƒŒæ™¯ (Problem Statement):
----------------------------
Monte Carloæ–¹æ³•éœ€è¦ç­‰å¾…æ•´ä¸ªå›åˆç»“æŸæ‰èƒ½æ›´æ–°ä»·å€¼ä¼°è®¡ï¼Œè¿™åœ¨ä»¥ä¸‹åœºæ™¯å­˜åœ¨é—®é¢˜:
1. å›åˆå¾ˆé•¿æˆ–æ— é™é•¿
2. éœ€è¦åœ¨çº¿å­¦ä¹ (online learning)
3. éœ€è¦å¿«é€Ÿé€‚åº”ç¯å¢ƒå˜åŒ–

TDæ–¹æ³•é€šè¿‡è‡ªä¸¾è§£å†³äº†è¿™äº›é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†æ— æ¨¡å‹(model-free)çš„ä¼˜åŠ¿ã€‚

ç®—æ³•å¯¹æ¯” (Comparison):
---------------------
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç®—æ³•      â”‚   åå·®       â”‚    æ–¹å·®     â”‚  æ•°æ®æ•ˆç‡   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Monte Carlo â”‚   æ— å       â”‚    é«˜       â”‚    ä½       â”‚
â”‚ TD(0)       â”‚   æœ‰å       â”‚    ä½       â”‚    é«˜       â”‚
â”‚ TD(Î»)       â”‚   å¯è°ƒ       â”‚    å¯è°ƒ     â”‚    å¯è°ƒ     â”‚
â”‚ n-step TD   â”‚   å¯è°ƒ       â”‚    å¯è°ƒ     â”‚    å¯è°ƒ     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¤æ‚åº¦ (Complexity):
-------------------
- TD(0): æ—¶é—´O(1)/æ­¥, ç©ºé—´O(|S|)
- TD(Î»): æ—¶é—´O(|S|)/æ­¥, ç©ºé—´O(|S|)
- SARSA: æ—¶é—´O(1)/æ­¥, ç©ºé—´O(|S|Ã—|A|)

ç®—æ³•æ€»ç»“ (Summary):
-----------------
TDå­¦ä¹ æ˜¯ä¸€ç§åœ¨çº¿ã€å¢é‡å¼çš„ä»·å€¼å‡½æ•°å­¦ä¹ æ–¹æ³•ã€‚å®ƒåœ¨æ¯ä¸€æ­¥éƒ½èƒ½æ›´æ–°ä¼°è®¡å€¼ï¼Œ
æ— éœ€ç­‰å¾…å›åˆç»“æŸã€‚è¿™ä½¿å¾—TDæ–¹æ³•ç‰¹åˆ«é€‚åˆè¿ç»­ä»»åŠ¡å’Œéœ€è¦å¿«é€Ÿå“åº”çš„åœºæ™¯ã€‚
TD(Î»)é€šè¿‡èµ„æ ¼è¿¹ç»Ÿä¸€äº†TD(0)å’ŒMonte Carloï¼Œæä¾›äº†åå·®-æ–¹å·®æƒè¡¡çš„çµæ´»æ€§ã€‚
"""

from __future__ import annotations

import numpy as np
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import (
    Dict, List, Optional, Tuple, Callable,
    Protocol, TypeVar, Generic, Any, Union
)
import warnings
from collections import defaultdict
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =============================================================================
# ç±»å‹å®šä¹‰ä¸åè®®
# =============================================================================

State = TypeVar('State')
Action = TypeVar('Action')


class Environment(Protocol[State, Action]):
    """
    ç¯å¢ƒåè®®ï¼Œå®šä¹‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„æœ€å°æ¥å£ã€‚
    å…¼å®¹OpenAI Gym/Gymnasiumé£æ ¼çš„ç¯å¢ƒã€‚
    """

    def reset(self) -> Tuple[State, Dict[str, Any]]:
        """é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹çŠ¶æ€å’Œä¿¡æ¯å­—å…¸ã€‚"""
        ...

    def step(self, action: Action) -> Tuple[State, float, bool, bool, Dict[str, Any]]:
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›(æ–°çŠ¶æ€, å¥–åŠ±, ç»ˆæ­¢æ ‡å¿—, æˆªæ–­æ ‡å¿—, ä¿¡æ¯å­—å…¸)ã€‚
        """
        ...

    @property
    def action_space(self) -> Any:
        """è¿”å›åŠ¨ä½œç©ºé—´ã€‚"""
        ...

    @property
    def observation_space(self) -> Any:
        """è¿”å›è§‚æµ‹ç©ºé—´ã€‚"""
        ...


class Policy(Protocol[State, Action]):
    """ç­–ç•¥åè®®ï¼Œæ ¹æ®çŠ¶æ€é€‰æ‹©åŠ¨ä½œã€‚"""

    def __call__(self, state: State) -> Action:
        """æ ¹æ®çŠ¶æ€è¿”å›åŠ¨ä½œã€‚"""
        ...

    def action_probabilities(self, state: State) -> Dict[Action, float]:
        """è¿”å›çŠ¶æ€ä¸‹å„åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚"""
        ...


# =============================================================================
# é…ç½®ç±»
# =============================================================================

class EligibilityTraceType(Enum):
    """
    èµ„æ ¼è¿¹ç±»å‹æšä¸¾ã€‚

    èµ„æ ¼è¿¹æ˜¯TD(Î»)çš„æ ¸å¿ƒæœºåˆ¶ï¼Œç”¨äºè¿½è¸ªå“ªäº›çŠ¶æ€å¯¹å½“å‰TDè¯¯å·®"è´Ÿæœ‰è´£ä»»"ã€‚
    """
    ACCUMULATING = auto()  # ç´¯ç§¯è¿¹: E(s) â† Î³Î»E(s) + 1
    REPLACING = auto()      # æ›¿æ¢è¿¹: E(s) â† 1 (è®¿é—®æ—¶é‡ç½®ä¸º1)
    DUTCH = auto()          # è·å…°è¿¹: E(s) â† (1-Î±)Î³Î»E(s) + 1 (è§£å†³ç´¯ç§¯è¿¹çš„å‘æ•£é—®é¢˜)


@dataclass
class TDConfig:
    """
    æ—¶åºå·®åˆ†å­¦ä¹ é…ç½®ç±»ã€‚

    å°è£…æ‰€æœ‰TDç®—æ³•çš„è¶…å‚æ•°ï¼Œä¾¿äºå®éªŒç®¡ç†å’Œå¤ç°ã€‚

    Attributes:
        alpha: å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ–°ä¿¡æ¯å¯¹ä¼°è®¡å€¼çš„å½±å“ç¨‹åº¦ã€‚
               å¤ªå¤§å¯¼è‡´ä¸ç¨³å®šï¼Œå¤ªå°æ”¶æ•›æ…¢ã€‚å…¸å‹å€¼: 0.01-0.5
        gamma: æŠ˜æ‰£å› å­ï¼Œå†³å®šæœªæ¥å¥–åŠ±çš„é‡è¦æ€§ã€‚
               Î³=0è¡¨ç¤ºåªå…³å¿ƒå³æ—¶å¥–åŠ±ï¼ŒÎ³=1è¡¨ç¤ºé•¿è¿œå¥–åŠ±åŒç­‰é‡è¦ã€‚
        lambda_: TD(Î»)çš„Î»å‚æ•°ï¼Œæ§åˆ¶è‡ªä¸¾ç¨‹åº¦ã€‚
                 Î»=0é€€åŒ–ä¸ºTD(0)ï¼ŒÎ»=1é€€åŒ–ä¸ºMonte Carloã€‚
        epsilon: Îµ-greedyç­–ç•¥çš„æ¢ç´¢ç‡ã€‚
        n_step: n-step TDçš„æ­¥æ•°ã€‚
        trace_type: èµ„æ ¼è¿¹ç±»å‹ã€‚
        initial_value: ä»·å€¼å‡½æ•°åˆå§‹åŒ–å€¼ï¼Œä¹è§‚åˆå§‹åŒ–å¯ä¿ƒè¿›æ¢ç´¢ã€‚
    """
    alpha: float = 0.1
    gamma: float = 0.99
    lambda_: float = 0.9
    epsilon: float = 0.1
    n_step: int = 1
    trace_type: EligibilityTraceType = EligibilityTraceType.ACCUMULATING
    initial_value: float = 0.0

    def __post_init__(self) -> None:
        """å‚æ•°éªŒè¯ã€‚"""
        if not 0 < self.alpha <= 1:
            raise ValueError(f"å­¦ä¹ ç‡alphaå¿…é¡»åœ¨(0, 1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {self.alpha}")
        if not 0 <= self.gamma <= 1:
            raise ValueError(f"æŠ˜æ‰£å› å­gammaå¿…é¡»åœ¨[0, 1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {self.gamma}")
        if not 0 <= self.lambda_ <= 1:
            raise ValueError(f"Î»å‚æ•°å¿…é¡»åœ¨[0, 1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {self.lambda_}")
        if not 0 <= self.epsilon <= 1:
            raise ValueError(f"æ¢ç´¢ç‡epsilonå¿…é¡»åœ¨[0, 1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {self.epsilon}")
        if self.n_step < 1:
            raise ValueError(f"n_stepå¿…é¡»è‡³å°‘ä¸º1ï¼Œå½“å‰å€¼: {self.n_step}")


@dataclass
class TrainingMetrics:
    """
    è®­ç»ƒæŒ‡æ ‡è®°å½•ç±»ã€‚

    ç”¨äºè¿½è¸ªå’Œåˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„å„é¡¹æŒ‡æ ‡ã€‚
    """
    episode_rewards: List[float] = field(default_factory=list)
    episode_lengths: List[int] = field(default_factory=list)
    td_errors: List[float] = field(default_factory=list)
    value_changes: List[float] = field(default_factory=list)

    def add_episode(
        self,
        reward: float,
        length: int,
        avg_td_error: float = 0.0,
        avg_value_change: float = 0.0
    ) -> None:
        """è®°å½•ä¸€ä¸ªå›åˆçš„æŒ‡æ ‡ã€‚"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)
        self.td_errors.append(avg_td_error)
        self.value_changes.append(avg_value_change)

    def get_moving_average(self, window: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """è®¡ç®—å¥–åŠ±å’Œå›åˆé•¿åº¦çš„ç§»åŠ¨å¹³å‡ã€‚"""
        if len(self.episode_rewards) < window:
            return np.array(self.episode_rewards), np.array(self.episode_lengths)

        rewards = np.convolve(
            self.episode_rewards,
            np.ones(window) / window,
            mode='valid'
        )
        lengths = np.convolve(
            self.episode_lengths,
            np.ones(window) / window,
            mode='valid'
        )
        return rewards, lengths


# =============================================================================
# åŸºç±»
# =============================================================================

class BaseTDLearner(ABC, Generic[State, Action]):
    """
    æ—¶åºå·®åˆ†å­¦ä¹ ç®—æ³•åŸºç±»ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    æä¾›TDå­¦ä¹ ç®—æ³•çš„é€šç”¨æ¡†æ¶ï¼ŒåŒ…æ‹¬ä»·å€¼å‡½æ•°ç®¡ç†ã€ç­–ç•¥å®ç°å’Œè®­ç»ƒå¾ªç¯ã€‚
    å­ç±»åªéœ€å®ç°ç‰¹å®šçš„æ›´æ–°è§„åˆ™å³å¯ã€‚

    è®¾è®¡æ¨¡å¼:
    --------
    é‡‡ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼(Template Method Pattern)ï¼Œå°†ç®—æ³•éª¨æ¶å®šä¹‰åœ¨åŸºç±»ä¸­ï¼Œ
    å…·ä½“çš„æ›´æ–°æ­¥éª¤å»¶è¿Ÿåˆ°å­ç±»å®ç°ã€‚
    """

    def __init__(self, config: TDConfig) -> None:
        """
        åˆå§‹åŒ–TDå­¦ä¹ å™¨ã€‚

        Args:
            config: TDå­¦ä¹ é…ç½®å¯¹è±¡
        """
        self.config = config
        self._value_function: Dict[State, float] = defaultdict(
            lambda: config.initial_value
        )
        self._q_function: Dict[Tuple[State, Action], float] = defaultdict(
            lambda: config.initial_value
        )
        self.metrics = TrainingMetrics()
        self._action_space: Optional[List[Action]] = None

    @property
    def value_function(self) -> Dict[State, float]:
        """è·å–çŠ¶æ€ä»·å€¼å‡½æ•°V(s)ã€‚"""
        return dict(self._value_function)

    @property
    def q_function(self) -> Dict[Tuple[State, Action], float]:
        """è·å–åŠ¨ä½œä»·å€¼å‡½æ•°Q(s, a)ã€‚"""
        return dict(self._q_function)

    def get_value(self, state: State) -> float:
        """è·å–çŠ¶æ€ä»·å€¼V(s)ã€‚"""
        return self._value_function[state]

    def get_q_value(self, state: State, action: Action) -> float:
        """è·å–åŠ¨ä½œä»·å€¼Q(s, a)ã€‚"""
        return self._q_function[(state, action)]

    def set_action_space(self, actions: List[Action]) -> None:
        """è®¾ç½®åŠ¨ä½œç©ºé—´ã€‚"""
        self._action_space = actions

    def epsilon_greedy_action(self, state: State) -> Action:
        """
        Îµ-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œã€‚

        æ•°å­¦åŸç†:
            Ï€(a|s) = Îµ/|A| + (1-Îµ)Â·ğŸ™(a = argmax Q(s,a'))

        ä»¥æ¦‚ç‡Îµéšæœºé€‰æ‹©åŠ¨ä½œ(æ¢ç´¢)ï¼Œä»¥æ¦‚ç‡1-Îµé€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œ(åˆ©ç”¨)ã€‚

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if self._action_space is None:
            raise ValueError("æœªè®¾ç½®åŠ¨ä½œç©ºé—´ï¼Œè¯·å…ˆè°ƒç”¨set_action_space()")

        if np.random.random() < self.config.epsilon:
            return np.random.choice(self._action_space)

        q_values = [self.get_q_value(state, a) for a in self._action_space]
        max_q = max(q_values)
        best_actions = [
            a for a, q in zip(self._action_space, q_values)
            if np.isclose(q, max_q)
        ]
        return np.random.choice(best_actions)

    def greedy_action(self, state: State) -> Action:
        """
        è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼ˆç”¨äºè¯„ä¼°ï¼‰ã€‚

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            æœ€ä¼˜åŠ¨ä½œ
        """
        if self._action_space is None:
            raise ValueError("æœªè®¾ç½®åŠ¨ä½œç©ºé—´ï¼Œè¯·å…ˆè°ƒç”¨set_action_space()")

        q_values = [self.get_q_value(state, a) for a in self._action_space]
        max_q = max(q_values)
        best_actions = [
            a for a, q in zip(self._action_space, q_values)
            if np.isclose(q, max_q)
        ]
        return np.random.choice(best_actions)

    @abstractmethod
    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒTDæ›´æ–°æ­¥éª¤ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆSARSAéœ€è¦ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®Î´
        """
        pass

    def train_episode(
        self,
        env: Environment[State, Action],
        max_steps: int = 10000
    ) -> Tuple[float, int]:
        """
        è®­ç»ƒä¸€ä¸ªå›åˆã€‚

        Args:
            env: ç¯å¢ƒå®ä¾‹
            max_steps: æœ€å¤§æ­¥æ•°é™åˆ¶

        Returns:
            (å›åˆæ€»å¥–åŠ±, å›åˆæ­¥æ•°)
        """
        state, _ = env.reset()
        action = self.epsilon_greedy_action(state)

        total_reward = 0.0
        td_errors = []

        for step in range(max_steps):
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            next_action = None if done else self.epsilon_greedy_action(next_state)

            td_error = self.update(state, action, reward, next_state, next_action, done)
            td_errors.append(abs(td_error))

            total_reward += reward

            if done:
                break

            state = next_state
            action = next_action

        steps = step + 1
        avg_td_error = np.mean(td_errors) if td_errors else 0.0
        self.metrics.add_episode(total_reward, steps, avg_td_error)

        return total_reward, steps

    def train(
        self,
        env: Environment[State, Action],
        n_episodes: int = 1000,
        max_steps_per_episode: int = 10000,
        log_interval: int = 100,
        early_stop_reward: Optional[float] = None
    ) -> TrainingMetrics:
        """
        æ‰§è¡Œå®Œæ•´è®­ç»ƒè¿‡ç¨‹ã€‚

        Args:
            env: ç¯å¢ƒå®ä¾‹
            n_episodes: è®­ç»ƒå›åˆæ•°
            max_steps_per_episode: æ¯å›åˆæœ€å¤§æ­¥æ•°
            log_interval: æ—¥å¿—è¾“å‡ºé—´éš”
            early_stop_reward: æ—©åœå¥–åŠ±é˜ˆå€¼

        Returns:
            è®­ç»ƒæŒ‡æ ‡
        """
        # è‡ªåŠ¨è®¾ç½®åŠ¨ä½œç©ºé—´
        if self._action_space is None:
            if hasattr(env.action_space, 'n'):
                self.set_action_space(list(range(env.action_space.n)))
            else:
                raise ValueError("æ— æ³•è‡ªåŠ¨æ¨æ–­åŠ¨ä½œç©ºé—´ï¼Œè¯·æ‰‹åŠ¨è®¾ç½®")

        for episode in range(n_episodes):
            reward, steps = self.train_episode(env, max_steps_per_episode)

            if (episode + 1) % log_interval == 0:
                recent_rewards = self.metrics.episode_rewards[-log_interval:]
                avg_reward = np.mean(recent_rewards)
                logger.info(
                    f"Episode {episode + 1}/{n_episodes} | "
                    f"Avg Reward: {avg_reward:.2f} | "
                    f"Last Reward: {reward:.2f} | "
                    f"Steps: {steps}"
                )

            # æ—©åœæ£€æŸ¥
            if early_stop_reward is not None:
                if len(self.metrics.episode_rewards) >= 100:
                    recent_avg = np.mean(self.metrics.episode_rewards[-100:])
                    if recent_avg >= early_stop_reward:
                        logger.info(
                            f"è¾¾åˆ°æ—©åœæ¡ä»¶: å¹³å‡å¥–åŠ± {recent_avg:.2f} >= {early_stop_reward}"
                        )
                        break

        return self.metrics

    def evaluate(
        self,
        env: Environment[State, Action],
        n_episodes: int = 100,
        max_steps: int = 10000
    ) -> Tuple[float, float]:
        """
        è¯„ä¼°å½“å‰ç­–ç•¥æ€§èƒ½ã€‚

        Args:
            env: ç¯å¢ƒå®ä¾‹
            n_episodes: è¯„ä¼°å›åˆæ•°
            max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°

        Returns:
            (å¹³å‡å¥–åŠ±, å¥–åŠ±æ ‡å‡†å·®)
        """
        rewards = []

        for _ in range(n_episodes):
            state, _ = env.reset()
            total_reward = 0.0

            for _ in range(max_steps):
                action = self.greedy_action(state)
                state, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward

                if terminated or truncated:
                    break

            rewards.append(total_reward)

        return np.mean(rewards), np.std(rewards)


# =============================================================================
# TD(0) çŠ¶æ€ä»·å€¼å­¦ä¹ 
# =============================================================================

class TD0ValueLearner(BaseTDLearner[State, Action]):
    """
    TD(0)çŠ¶æ€ä»·å€¼å­¦ä¹ ç®—æ³•ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    TD(0)æ˜¯æœ€ç®€å•çš„TDæ–¹æ³•ï¼Œä½¿ç”¨å•æ­¥è‡ªä¸¾æ¥æ›´æ–°ä»·å€¼ä¼°è®¡ã€‚
    å®ƒåªçœ‹ä¸‹ä¸€æ­¥çš„å¥–åŠ±å’Œä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ï¼Œä¸ç­‰å¾…å®Œæ•´å›åˆã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    æ›´æ–°è§„åˆ™:
        V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]

    TDè¯¯å·®:
        Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)

    æ”¶æ•›æ€§:
        åœ¨æ»¡è¶³Robbins-Monroæ¡ä»¶(Î£Î±=âˆ, Î£Î±Â²<âˆ)ä¸”ç­–ç•¥å›ºå®šæ—¶ï¼Œ
        TD(0)ä»¥æ¦‚ç‡1æ”¶æ•›åˆ°çœŸå®ä»·å€¼å‡½æ•°ã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    ç»™å®šä¸€ä¸ªå›ºå®šç­–ç•¥Ï€ï¼Œä¼°è®¡è¯¥ç­–ç•¥ä¸‹çš„çŠ¶æ€ä»·å€¼å‡½æ•°V^Ï€(s)ã€‚
    è¿™æ˜¯ç­–ç•¥è¯„ä¼°(Policy Evaluation)é—®é¢˜ï¼Œæ˜¯ç­–ç•¥è¿­ä»£ç®—æ³•çš„åŸºç¡€ã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    ä¸Monte Carloç›¸æ¯”:
    - ä¼˜åŠ¿: æ— éœ€ç­‰å¾…å›åˆç»“æŸï¼Œæ–¹å·®ä½ï¼Œæ•°æ®æ•ˆç‡é«˜
    - åŠ£åŠ¿: å¼•å…¥åå·®(å› ä¸ºV(S_{t+1})æœ¬èº«æ˜¯ä¼°è®¡å€¼)

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(1) per step
    - ç©ºé—´: O(|S|) for value function
    """

    def __init__(self, config: TDConfig, policy: Optional[Policy[State, Action]] = None):
        """
        åˆå§‹åŒ–TD(0)ä»·å€¼å­¦ä¹ å™¨ã€‚

        Args:
            config: TDå­¦ä¹ é…ç½®
            policy: å¾…è¯„ä¼°çš„ç­–ç•¥ï¼ŒNoneåˆ™ä½¿ç”¨Îµ-greedy
        """
        super().__init__(config)
        self._policy = policy

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒTD(0)æ›´æ–°ã€‚

        Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)
        V(S_t) â† V(S_t) + Î±Î´_t

        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œï¼ˆæœ¬ç®—æ³•ä¸ä½¿ç”¨ï¼‰
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆæœ¬ç®—æ³•ä¸ä½¿ç”¨ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®Î´_t
        """
        # è®¡ç®—TDç›®æ ‡
        if done:
            td_target = reward
        else:
            td_target = reward + self.config.gamma * self._value_function[next_state]

        # è®¡ç®—TDè¯¯å·®
        td_error = td_target - self._value_function[state]

        # æ›´æ–°ä»·å€¼ä¼°è®¡
        self._value_function[state] += self.config.alpha * td_error

        return td_error


# =============================================================================
# SARSA (State-Action-Reward-State-Action)
# =============================================================================

class SARSA(BaseTDLearner[State, Action]):
    """
    SARSAç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    SARSAæ˜¯ä¸€ç§on-policy TDæ§åˆ¶ç®—æ³•ã€‚å…¶åç§°æ¥æºäºæ›´æ–°æ‰€éœ€çš„äº”å…ƒç»„:
    (State, Action, Reward, State', Action')ã€‚å…³é”®ç‰¹ç‚¹æ˜¯ä½¿ç”¨å®é™…æ‰§è¡Œçš„
    ä¸‹ä¸€åŠ¨ä½œA'æ¥è®¡ç®—TDç›®æ ‡ï¼Œå› æ­¤å­¦ä¹ çš„æ˜¯è¡Œä¸ºç­–ç•¥æœ¬èº«çš„ä»·å€¼ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    æ›´æ–°è§„åˆ™:
        Q(S_t, A_t) â† Q(S_t, A_t) + Î±[R_{t+1} + Î³Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]

    TDç›®æ ‡:
        G_t^{(1)} = R_{t+1} + Î³Q(S_{t+1}, A_{t+1})

    æ”¶æ•›æ€§:
        åœ¨æ»¡è¶³GLIE(Greedy in the Limit with Infinite Exploration)æ¡ä»¶æ—¶ï¼Œ
        SARSAæ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    SARSAè§£å†³çš„æ˜¯æ§åˆ¶é—®é¢˜(Control Problem)ï¼šæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥Ï€*ã€‚
    ä¸Q-Learningçš„å…³é”®åŒºåˆ«åœ¨äºSARSAæ˜¯on-policyçš„â€”â€”å®ƒè¯„ä¼°å’Œæ”¹è¿›çš„æ˜¯
    å®é™…æ‰§è¡Œçš„ç­–ç•¥ï¼ŒåŒ…æ‹¬æ¢ç´¢è¡Œä¸ºã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    SARSA vs Q-Learning:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    ç‰¹æ€§        â”‚     SARSA       â”‚    Q-Learning   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    ç±»å‹        â”‚    on-policy    â”‚    off-policy   â”‚
    â”‚  ä¸‹ä¸€åŠ¨ä½œ      â”‚  å®é™…é‡‡æ ·A'     â”‚    max_a Q      â”‚
    â”‚    å®‰å…¨æ€§      â”‚      é«˜         â”‚       ä½        â”‚
    â”‚  æ”¶æ•›é€Ÿåº¦      â”‚      æ…¢         â”‚       å¿«        â”‚
    â”‚  æœ€ç»ˆç­–ç•¥      â”‚    ä¿å®ˆ         â”‚      æ¿€è¿›       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    åœ¨cliff walkingç­‰å±é™©ç¯å¢ƒä¸­ï¼ŒSARSAä¼šå­¦åˆ°æ›´å®‰å…¨çš„è·¯å¾„ï¼Œ
    å› ä¸ºå®ƒè€ƒè™‘äº†æ¢ç´¢æ—¶å¯èƒ½æ‰è½çš„é£é™©ã€‚

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(1) per step
    - ç©ºé—´: O(|S| Ã— |A|) for Q-table

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    SARSAé€šè¿‡äº”å…ƒç»„(S,A,R,S',A')è¿›è¡Œå­¦ä¹ ã€‚å®ƒå¿ å®åœ°è¯„ä¼°å½“å‰ç­–ç•¥
    ï¼ˆåŒ…æ‹¬æ¢ç´¢è¡Œä¸ºï¼‰çš„ä»·å€¼ï¼Œå› æ­¤åœ¨éœ€è¦è€ƒè™‘æ¢ç´¢é£é™©çš„ç¯å¢ƒä¸­
    å¾€å¾€èƒ½å­¦åˆ°æ›´å®‰å…¨ã€æ›´ä¿å®ˆçš„ç­–ç•¥ã€‚
    """

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒSARSAæ›´æ–°ã€‚

        Q(S_t, A_t) â† Q(S_t, A_t) + Î±[R + Î³Q(S', A') - Q(S_t, A_t)]

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆSARSAå¿…éœ€ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        current_q = self._q_function[(state, action)]

        if done:
            td_target = reward
        else:
            if next_action is None:
                raise ValueError("SARSAéœ€è¦next_actionå‚æ•°")
            td_target = reward + self.config.gamma * self._q_function[(next_state, next_action)]

        td_error = td_target - current_q
        self._q_function[(state, action)] += self.config.alpha * td_error

        return td_error


# =============================================================================
# Expected SARSA
# =============================================================================

class ExpectedSARSA(BaseTDLearner[State, Action]):
    """
    Expected SARSAç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    Expected SARSAæ˜¯SARSAçš„å˜ä½“ï¼Œä½¿ç”¨ä¸‹ä¸€çŠ¶æ€æ‰€æœ‰åŠ¨ä½œQå€¼çš„æœŸæœ›
    ï¼ˆæŒ‰ç­–ç•¥æ¦‚ç‡åŠ æƒï¼‰ä½œä¸ºTDç›®æ ‡ï¼Œè€Œä¸æ˜¯å•ä¸€é‡‡æ ·åŠ¨ä½œçš„Qå€¼ã€‚
    è¿™æ¶ˆé™¤äº†åŠ¨ä½œé‡‡æ ·å¸¦æ¥çš„æ–¹å·®ï¼Œä½¿å­¦ä¹ æ›´åŠ ç¨³å®šã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    æ›´æ–°è§„åˆ™:
        Q(S_t, A_t) â† Q(S_t, A_t) + Î±[R + Î³ğ”¼_Ï€[Q(S', A')] - Q(S_t, A_t)]

    æœŸæœ›è®¡ç®—:
        ğ”¼_Ï€[Q(S', A')] = Î£_a Ï€(a|S') Ã— Q(S', a)

    å¯¹äºÎµ-greedyç­–ç•¥:
        ğ”¼_Ï€[Q(S', A')] = Îµ/|A| Ã— Î£_a Q(S', a) + (1-Îµ) Ã— max_a Q(S', a)

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    SARSAçš„æ›´æ–°ä¾èµ–äºé‡‡æ ·çš„ä¸‹ä¸€åŠ¨ä½œï¼Œå¼•å…¥äº†é¢å¤–æ–¹å·®ã€‚
    Expected SARSAé€šè¿‡è®¡ç®—æœŸæœ›æ¶ˆé™¤è¿™ä¸€æ–¹å·®æºï¼Œè·å¾—æ›´ç¨³å®šçš„å­¦ä¹ ã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    Expected SARSAä½äºSARSAå’ŒQ-Learningä¹‹é—´:
    - å½“Îµ=0æ—¶ï¼Œé€€åŒ–ä¸ºQ-Learningï¼ˆç¡®å®šæ€§greedyç­–ç•¥ï¼‰
    - å½“ä»…è€ƒè™‘å•ä¸€åŠ¨ä½œæ—¶ï¼Œé€€åŒ–ä¸ºSARSA
    - ç»“åˆäº†SARSAçš„on-policyç‰¹æ€§å’Œæ›´ä½çš„æ–¹å·®

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     ç®—æ³•        â”‚    æ–¹å·®    â”‚   åå·®     â”‚    è®¡ç®—æˆæœ¬    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    SARSA        â”‚    é«˜      â”‚    ä½      â”‚      O(1)      â”‚
    â”‚  Expected SARSA â”‚    ä½      â”‚    ä½      â”‚     O(|A|)     â”‚
    â”‚   Q-Learning    â”‚    ä¸­      â”‚    æœ‰      â”‚      O(|A|)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(|A|) per step (éœ€è¦éå†æ‰€æœ‰åŠ¨ä½œè®¡ç®—æœŸæœ›)
    - ç©ºé—´: O(|S| Ã— |A|) for Q-table

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    Expected SARSAé€šè¿‡è®¡ç®—ç­–ç•¥åœ¨ä¸‹ä¸€çŠ¶æ€çš„æœŸæœ›ä»·å€¼ï¼Œæ¶ˆé™¤äº†SARSAä¸­
    åŠ¨ä½œé‡‡æ ·çš„æ–¹å·®ã€‚å®ƒåœ¨ä¿æŒon-policyç‰¹æ€§çš„åŒæ—¶è·å¾—æ›´ç¨³å®šçš„æ›´æ–°ï¼Œ
    æ˜¯SARSAå’ŒQ-Learningä¹‹é—´çš„ä¼˜é›…æŠ˜ä¸­ã€‚
    """

    def _compute_expected_q(self, state: State) -> float:
        """
        è®¡ç®—çŠ¶æ€ä¸‹Qå€¼çš„æœŸæœ›ã€‚

        å¯¹äºÎµ-greedyç­–ç•¥:
        ğ”¼[Q(s,Â·)] = Îµ/|A| Ã— Î£_a Q(s,a) + (1-Îµ) Ã— max_a Q(s,a)

        Args:
            state: çŠ¶æ€

        Returns:
            æœŸæœ›Qå€¼
        """
        if self._action_space is None:
            raise ValueError("æœªè®¾ç½®åŠ¨ä½œç©ºé—´")

        q_values = [self._q_function[(state, a)] for a in self._action_space]
        n_actions = len(self._action_space)

        # Îµ-greedyç­–ç•¥çš„æœŸæœ›è®¡ç®—
        # æ¢ç´¢éƒ¨åˆ†: æ¯ä¸ªåŠ¨ä½œæ¦‚ç‡ Îµ/|A|
        exploration_value = (self.config.epsilon / n_actions) * sum(q_values)

        # åˆ©ç”¨éƒ¨åˆ†: æœ€ä¼˜åŠ¨ä½œæ¦‚ç‡ (1-Îµ)
        exploitation_value = (1 - self.config.epsilon) * max(q_values)

        return exploration_value + exploitation_value

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒExpected SARSAæ›´æ–°ã€‚

        Q(S, A) â† Q(S, A) + Î±[R + Î³ğ”¼[Q(S', Â·)] - Q(S, A)]

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆæœ¬ç®—æ³•ä¸ä½¿ç”¨ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        current_q = self._q_function[(state, action)]

        if done:
            td_target = reward
        else:
            expected_q = self._compute_expected_q(next_state)
            td_target = reward + self.config.gamma * expected_q

        td_error = td_target - current_q
        self._q_function[(state, action)] += self.config.alpha * td_error

        return td_error


# =============================================================================
# Q-Learning
# =============================================================================

class QLearning(BaseTDLearner[State, Action]):
    """
    Q-Learningç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    Q-Learningæ˜¯æœ€è‘—åçš„off-policy TDæ§åˆ¶ç®—æ³•ã€‚æ— è®ºè¡Œä¸ºç­–ç•¥å¦‚ä½•ï¼Œ
    å®ƒæ€»æ˜¯å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„Qå€¼â€”â€”ä½¿ç”¨maxæ“ä½œé€‰æ‹©ä¸‹ä¸€çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œï¼Œ
    è€Œä¸æ˜¯å®é™…æ‰§è¡Œçš„åŠ¨ä½œã€‚è¿™ç§"ä¹è§‚ä¸»ä¹‰"ä½¿å…¶èƒ½å¤Ÿç›´æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    æ›´æ–°è§„åˆ™:
        Q(S_t, A_t) â† Q(S_t, A_t) + Î±[R_{t+1} + Î³ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]

    TDç›®æ ‡ (æœ€ä¼˜Bellmanæ–¹ç¨‹çš„é‡‡æ ·ç‰ˆæœ¬):
        G_t = R_{t+1} + Î³ max_a Q(S_{t+1}, a)

    è¿™ç›´æ¥å¯¹åº”æœ€ä¼˜Bellmanæ–¹ç¨‹:
        Q*(s, a) = ğ”¼[R + Î³ max_{a'} Q*(s', a') | s, a]

    æ”¶æ•›æ€§å®šç† (Watkins, 1989):
        åœ¨ä»¥ä¸‹æ¡ä»¶ä¸‹Q-Learningä»¥æ¦‚ç‡1æ”¶æ•›åˆ°Q*:
        1. æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹è¢«æ— é™æ¬¡è®¿é—®
        2. å­¦ä¹ ç‡æ»¡è¶³: Î£Î±_t = âˆ ä¸” Î£Î±_tÂ² < âˆ

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    Q-Learningè§£å†³æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œç›´æ¥å­¦ä¹ æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•°Q*ã€‚
    å…¶off-policyç‰¹æ€§å…è®¸ä½¿ç”¨ä»»æ„æ¢ç´¢ç­–ç•¥æ”¶é›†æ•°æ®ï¼ŒåŒæ—¶å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    Q-Learning vs SARSA:
    - Q-Learning: off-policy, æ›´æ¿€è¿›, å¯èƒ½ä¸å®‰å…¨çš„æ¢ç´¢
    - SARSA: on-policy, æ›´ä¿å®ˆ, è€ƒè™‘æ¢ç´¢é£é™©

    æœ€å¤§åŒ–åå·® (Maximization Bias):
        Q-Learningçš„maxæ“ä½œä¼šå¯¼è‡´ç³»ç»Ÿæ€§çš„è¿‡ä¼°è®¡ã€‚
        åœ¨å™ªå£°ç¯å¢ƒä¸­ï¼Œmaxä¼šé€‰ä¸­ä¼°è®¡å€¼åé«˜çš„åŠ¨ä½œï¼Œå¯¼è‡´è¿‡åº¦ä¹è§‚ã€‚
        Double Q-Learningé€šè¿‡è§£è€¦é€‰æ‹©å’Œè¯„ä¼°æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(|A|) per step (éœ€è¦æ‰¾max)
    - ç©ºé—´: O(|S| Ã— |A|) for Q-table

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    Q-Learningé€šè¿‡"å‡è£…"è¡Œä¸ºç­–ç•¥æ˜¯è´ªå©ªçš„æ¥ç›´æ¥å­¦ä¹ æœ€ä¼˜Qå‡½æ•°ã€‚
    è¿™ç§off-policyç‰¹æ€§ä½¿å…¶å¯ä»¥ä»ä»»ä½•æ•°æ®æºå­¦ä¹ ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´
    åœ¨å±é™©ç¯å¢ƒä¸­å­¦åˆ°ä¸å®‰å…¨çš„ç­–ç•¥ï¼Œä»¥åŠåœ¨å™ªå£°ç¯å¢ƒä¸­è¿‡ä¼°è®¡Qå€¼ã€‚
    """

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒQ-Learningæ›´æ–°ã€‚

        Q(S, A) â† Q(S, A) + Î±[R + Î³ max_a Q(S', a) - Q(S, A)]

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆQ-Learningä¸ä½¿ç”¨ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        current_q = self._q_function[(state, action)]

        if done:
            td_target = reward
        else:
            # å…³é”®åŒºåˆ«: ä½¿ç”¨maxè€Œä¸æ˜¯å®é™…ä¸‹ä¸€åŠ¨ä½œ
            max_next_q = max(
                self._q_function[(next_state, a)]
                for a in self._action_space
            )
            td_target = reward + self.config.gamma * max_next_q

        td_error = td_target - current_q
        self._q_function[(state, action)] += self.config.alpha * td_error

        return td_error


# =============================================================================
# Double Q-Learning
# =============================================================================

class DoubleQLearning(BaseTDLearner[State, Action]):
    """
    Double Q-Learningç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    Double Q-Learningé€šè¿‡ç»´æŠ¤ä¸¤ä¸ªç‹¬ç«‹çš„Qè¡¨æ¥è§£å†³Q-Learningçš„æœ€å¤§åŒ–åå·®ã€‚
    ä¸€ä¸ªQè¡¨ç”¨äºé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼Œå¦ä¸€ä¸ªç”¨äºè¯„ä¼°è¯¥åŠ¨ä½œçš„ä»·å€¼ã€‚
    è¿™ç§"è§£è€¦"ç­–ç•¥æœ‰æ•ˆæ¶ˆé™¤äº†è¿‡ä¼°è®¡é—®é¢˜ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    æ ‡å‡†Q-Learningçš„é—®é¢˜:
        max_a Q(s', a) ä½¿ç”¨åŒä¸€ä¸ªQæ¥é€‰æ‹©å’Œè¯„ä¼°ï¼Œå½“Qæœ‰å™ªå£°æ—¶å¯¼è‡´è¿‡ä¼°è®¡ã€‚

    Double Q-Learningè§£å†³æ–¹æ¡ˆ:
        ä»¥0.5æ¦‚ç‡æ›´æ–°Q_Aæˆ–Q_B:

        æ›´æ–°Q_A:
            a* = argmax_a Q_A(S', a)           (ç”¨Q_Aé€‰æ‹©)
            Q_A(S, A) â† Q_A(S, A) + Î±[R + Î³Q_B(S', a*) - Q_A(S, A)]  (ç”¨Q_Bè¯„ä¼°)

        æ›´æ–°Q_B:
            a* = argmax_a Q_B(S', a)           (ç”¨Q_Bé€‰æ‹©)
            Q_B(S, A) â† Q_B(S, A) + Î±[R + Î³Q_A(S', a*) - Q_B(S, A)]  (ç”¨Q_Aè¯„ä¼°)

    ä¸ºä»€ä¹ˆæœ‰æ•ˆ:
        å…³é”®æ´å¯Ÿ: ğ”¼[max(X, Y)] â‰¥ max(ğ”¼[X], ğ”¼[Y])
        å½“ä¼°è®¡æœ‰å™ªå£°æ—¶ï¼Œmaxæ€»æ˜¯åå‘é«˜ä¼°ã€‚
        é€šè¿‡ç”¨ç‹¬ç«‹çš„ä¼°è®¡å™¨è¯„ä¼°é€‰ä¸­çš„åŠ¨ä½œï¼ŒæœŸæœ›å€¼å˜å¾—æ— åã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    åœ¨éšæœºç¯å¢ƒä¸­ï¼ŒQ-Learningä¼šç³»ç»Ÿæ€§åœ°è¿‡ä¼°è®¡Qå€¼ï¼Œå¯¼è‡´æ¬¡ä¼˜ç­–ç•¥ã€‚
    ç»å…¸ä¾‹å­: åœ¨æœ‰éšæœºå¥–åŠ±çš„MDPä¸­ï¼ŒQ-Learningå¯èƒ½å­¦åˆ°é”™è¯¯ç­–ç•¥ã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      ç®—æ³•        â”‚   åå·®     â”‚   æ–¹å·®     â”‚   å†…å­˜     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Q-Learning     â”‚   è¿‡ä¼°è®¡   â”‚    ä¸­      â”‚    1Ã—      â”‚
    â”‚ Double Q-Learningâ”‚   æ— å     â”‚    ä¸­      â”‚    2Ã—      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(|A|) per step
    - ç©ºé—´: O(2 Ã— |S| Ã— |A|) for two Q-tables

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    Double Q-Learningæ˜¯Q-Learningçš„å»åå·®ç‰ˆæœ¬ã€‚é€šè¿‡ç»´æŠ¤ä¸¤ä¸ªQè¡¨
    å¹¶éšæœºé€‰æ‹©å“ªä¸ªç”¨äºé€‰æ‹©ã€å“ªä¸ªç”¨äºè¯„ä¼°ï¼Œå®ƒæ¶ˆé™¤äº†maxæ“ä½œå¼•å…¥çš„
    ç³»ç»Ÿæ€§è¿‡ä¼°è®¡ã€‚ä»£ä»·æ˜¯åŒå€çš„å†…å­˜æ¶ˆè€—ã€‚
    """

    def __init__(self, config: TDConfig) -> None:
        """åˆå§‹åŒ–Double Q-Learningã€‚"""
        super().__init__(config)
        # ç»´æŠ¤ä¸¤ä¸ªç‹¬ç«‹çš„Qè¡¨
        self._q_function_a: Dict[Tuple[State, Action], float] = defaultdict(
            lambda: config.initial_value
        )
        self._q_function_b: Dict[Tuple[State, Action], float] = defaultdict(
            lambda: config.initial_value
        )

    def get_q_value(self, state: State, action: Action) -> float:
        """è·å–åˆå¹¶åçš„Qå€¼ï¼ˆä¸¤ä¸ªQè¡¨çš„å¹³å‡ï¼‰ã€‚"""
        q_a = self._q_function_a[(state, action)]
        q_b = self._q_function_b[(state, action)]
        return (q_a + q_b) / 2

    @property
    def q_function(self) -> Dict[Tuple[State, Action], float]:
        """è·å–åˆå¹¶åçš„Qå‡½æ•°ã€‚"""
        all_keys = set(self._q_function_a.keys()) | set(self._q_function_b.keys())
        return {
            key: (self._q_function_a[key] + self._q_function_b[key]) / 2
            for key in all_keys
        }

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒDouble Q-Learningæ›´æ–°ã€‚

        ä»¥0.5æ¦‚ç‡é€‰æ‹©æ›´æ–°Q_Aæˆ–Q_Bï¼Œäº¤å‰ä½¿ç”¨å¦ä¸€ä¸ªQè¡¨è¿›è¡Œè¯„ä¼°ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆä¸ä½¿ç”¨ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        # éšæœºé€‰æ‹©æ›´æ–°å“ªä¸ªQè¡¨
        if np.random.random() < 0.5:
            # æ›´æ–°Q_Aï¼Œç”¨Q_Bè¯„ä¼°
            q_select = self._q_function_a
            q_eval = self._q_function_b
            q_update = self._q_function_a
        else:
            # æ›´æ–°Q_Bï¼Œç”¨Q_Aè¯„ä¼°
            q_select = self._q_function_b
            q_eval = self._q_function_a
            q_update = self._q_function_b

        current_q = q_update[(state, action)]

        if done:
            td_target = reward
        else:
            # ç”¨ä¸€ä¸ªQè¡¨é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            best_action = max(
                self._action_space,
                key=lambda a: q_select[(next_state, a)]
            )
            # ç”¨å¦ä¸€ä¸ªQè¡¨è¯„ä¼°
            td_target = reward + self.config.gamma * q_eval[(next_state, best_action)]

        td_error = td_target - current_q
        q_update[(state, action)] += self.config.alpha * td_error

        return td_error


# =============================================================================
# N-Step TD
# =============================================================================

class NStepTD(BaseTDLearner[State, Action]):
    """
    N-Step TDç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    N-Step TDæ˜¯TD(0)å’ŒMonte Carloçš„ä¸­é—´æ–¹æ¡ˆã€‚å®ƒä½¿ç”¨næ­¥çš„å®é™…å¥–åŠ±
    åŠ ä¸Šç¬¬n+1æ­¥çš„ä»·å€¼ä¼°è®¡ä½œä¸ºTDç›®æ ‡ã€‚nè¶Šå¤§ï¼Œè¶Šæ¥è¿‘Monte Carloï¼›
    n=1æ—¶å°±æ˜¯TD(0)ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    n-stepå›æŠ¥:
        G_t^{(n)} = R_{t+1} + Î³R_{t+2} + ... + Î³^{n-1}R_{t+n} + Î³^n V(S_{t+n})
                  = Î£_{k=0}^{n-1} Î³^k R_{t+k+1} + Î³^n V(S_{t+n})

    æ›´æ–°è§„åˆ™:
        V(S_t) â† V(S_t) + Î±[G_t^{(n)} - V(S_t)]

    å¯¹äºQå‡½æ•°:
        G_t^{(n)} = Î£_{k=0}^{n-1} Î³^k R_{t+k+1} + Î³^n Q(S_{t+n}, A_{t+n})

    å…³é”®æ´å¯Ÿ:
        å½“nâ†’âˆï¼ŒG_t^{(n)}å˜æˆMonte Carloå›æŠ¥
        å½“n=1ï¼ŒG_t^{(n)}å°±æ˜¯TD(0)ç›®æ ‡

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    TD(0)åå·®é«˜ã€æ–¹å·®ä½ï¼›Monte Carloåå·®ä½ã€æ–¹å·®é«˜ã€‚
    N-Step TDæä¾›äº†ä¸€ç§åœ¨ä¸¤è€…ä¹‹é—´å¹³æ»‘è¿‡æ¸¡çš„æ–¹å¼ï¼Œ
    å…è®¸æ ¹æ®é—®é¢˜ç‰¹æ€§é€‰æ‹©åˆé€‚çš„nå€¼ã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     n     â”‚    åå·®    â”‚    æ–¹å·®    â”‚   å»¶è¿Ÿ     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    1      â”‚     é«˜     â”‚     ä½     â”‚   1 step   â”‚
    â”‚    5      â”‚     ä¸­     â”‚     ä¸­     â”‚   5 steps  â”‚
    â”‚   100     â”‚     ä½     â”‚     é«˜     â”‚  100 steps â”‚
    â”‚    âˆ      â”‚     æ—      â”‚     é«˜     â”‚  episode   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    å®è·µä¸­æœ€ä¼˜né€šå¸¸åœ¨4-10ä¹‹é—´ã€‚

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(1) per step (æ‘Šé”€)
    - ç©ºé—´: O(n) for storing n-step buffer

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    N-Step TDé€šè¿‡è°ƒæ•´nå€¼åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´æƒè¡¡ã€‚è¾ƒå°çš„næ›´æ–°æ›´é¢‘ç¹ä½†åå·®å¤§ï¼Œ
    è¾ƒå¤§çš„nèƒ½åˆ©ç”¨æ›´å¤šçœŸå®å¥–åŠ±ä¿¡æ¯ä½†éœ€è¦ç­‰å¾…æ›´é•¿æ—¶é—´ã€‚
    å®ƒæ˜¯ç†è§£TD(Î»)çš„åŸºç¡€ã€‚
    """

    def __init__(self, config: TDConfig) -> None:
        """åˆå§‹åŒ–N-Step TDã€‚"""
        super().__init__(config)
        # å­˜å‚¨næ­¥ç»éªŒçš„ç¼“å†²åŒº
        self._buffer: List[Tuple[State, Action, float]] = []
        self._states_buffer: List[State] = []

    def _compute_n_step_return(
        self,
        rewards: List[float],
        final_state: State,
        done: bool
    ) -> float:
        """
        è®¡ç®—n-stepå›æŠ¥ã€‚

        G_t^{(n)} = Î£Î³^k R_{t+k+1} + Î³^n V(S_{t+n})

        Args:
            rewards: næ­¥å¥–åŠ±åˆ—è¡¨
            final_state: æœ€ç»ˆçŠ¶æ€
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            n-stepå›æŠ¥
        """
        n_step_return = 0.0
        discount = 1.0

        for reward in rewards:
            n_step_return += discount * reward
            discount *= self.config.gamma

        if not done:
            n_step_return += discount * self._value_function[final_state]

        return n_step_return

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒN-Step TDæ›´æ–°ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œ
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®ï¼ˆå¦‚æœè§¦å‘æ›´æ–°ï¼‰
        """
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self._buffer.append((state, action, reward))
        self._states_buffer.append(next_state)

        td_error = 0.0

        # å½“ç¼“å†²åŒºæ»¡æˆ–å›åˆç»“æŸæ—¶ï¼Œè¿›è¡Œæ›´æ–°
        if len(self._buffer) >= self.config.n_step or done:
            # æå–è¦æ›´æ–°çš„çŠ¶æ€å’Œå¥–åŠ±
            update_state = self._buffer[0][0]
            update_action = self._buffer[0][1]
            rewards = [exp[2] for exp in self._buffer]

            # è®¡ç®—n-stepå›æŠ¥
            n_step_return = self._compute_n_step_return(rewards, next_state, done)

            # æ›´æ–°Qå€¼
            current_q = self._q_function[(update_state, update_action)]
            td_error = n_step_return - current_q
            self._q_function[(update_state, update_action)] += self.config.alpha * td_error

            # ç§»é™¤æœ€æ—§çš„ç»éªŒ
            self._buffer.pop(0)
            self._states_buffer.pop(0)

        # å›åˆç»“æŸæ—¶æ¸…ç©ºç¼“å†²åŒºå¹¶æ›´æ–°å‰©ä½™çŠ¶æ€
        if done:
            while self._buffer:
                update_state = self._buffer[0][0]
                update_action = self._buffer[0][1]
                rewards = [exp[2] for exp in self._buffer]

                n_step_return = self._compute_n_step_return(rewards, next_state, True)

                current_q = self._q_function[(update_state, update_action)]
                td_error = n_step_return - current_q
                self._q_function[(update_state, update_action)] += self.config.alpha * td_error

                self._buffer.pop(0)
                if self._states_buffer:
                    self._states_buffer.pop(0)

            # æ¸…ç©ºç¼“å†²åŒº
            self._buffer = []
            self._states_buffer = []

        return td_error


# =============================================================================
# TD(Î») with Eligibility Traces
# =============================================================================

class TDLambda(BaseTDLearner[State, Action]):
    """
    TD(Î»)ç®—æ³•å®ç° (å¸¦èµ„æ ¼è¿¹)ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    TD(Î»)é€šè¿‡èµ„æ ¼è¿¹(Eligibility Traces)ç»Ÿä¸€äº†TD(0)å’ŒMonte Carloã€‚
    èµ„æ ¼è¿¹è¿½è¸ªå“ªäº›çŠ¶æ€"æœ‰èµ„æ ¼"æ¥æ”¶å½“å‰TDè¯¯å·®çš„æ›´æ–°â€”â€”æœ€è¿‘è®¿é—®çš„çŠ¶æ€
    èµ„æ ¼æœ€é«˜ï¼Œéšæ—¶é—´æŒ‡æ•°è¡°å‡ã€‚è¿™ç­‰ä»·äºåœ¨æ‰€æœ‰n-stepå›æŠ¥ä¸Šåšå‡ ä½•åŠ æƒå¹³å‡ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    Î»-å›æŠ¥ (Forward View):
        G_t^Î» = (1-Î») Î£_{n=1}^{âˆ} Î»^{n-1} G_t^{(n)}

    è¿™æ˜¯æ‰€æœ‰n-stepå›æŠ¥çš„å‡ ä½•åŠ æƒå¹³å‡ï¼Œæƒé‡(1-Î»)Î»^{n-1}ã€‚

    èµ„æ ¼è¿¹ (Backward View):
        ç´¯ç§¯è¿¹: E_t(s) = Î³Î»E_{t-1}(s) + ğŸ™(S_t = s)
        æ›¿æ¢è¿¹: E_t(s) = Î³Î»E_{t-1}(s); E_t(S_t) = 1
        è·å…°è¿¹: E_t(s) = Î³Î»E_{t-1}(s) + (1-Î±Î³Î»E_{t-1}(s))ğŸ™(S_t = s)

    æ›´æ–°è§„åˆ™:
        Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)
        V(s) â† V(s) + Î±Î´_t E_t(s), âˆ€s

    å‰å‘è§†å›¾ä¸åå‘è§†å›¾ç­‰ä»·æ€§:
        åœ¨ç¦»çº¿æ›´æ–°ä¸‹ï¼ŒTD(Î»)çš„åå‘è§†å›¾äº§ç”Ÿçš„æ€»æ›´æ–°é‡
        ç­‰äºä½¿ç”¨Î»-å›æŠ¥çš„å‰å‘è§†å›¾ã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    N-Step TDéœ€è¦é€‰æ‹©ç‰¹å®šçš„nå€¼ï¼Œä¸åŒçš„nåœ¨ä¸åŒç¯å¢ƒä¸­è¡¨ç°å·®å¼‚å¤§ã€‚
    TD(Î»)é€šè¿‡èµ„æ ¼è¿¹å®ç°å¯¹æ‰€æœ‰nçš„åŠ æƒç»„åˆï¼Œç”±å•ä¸€å‚æ•°Î»æ§åˆ¶ã€‚
    Î»=0ç­‰ä»·äºTD(0)ï¼ŒÎ»=1ç­‰ä»·äºMonte Carloã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    ä¸åŒÎ»å€¼çš„ç‰¹æ€§:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     Î»     â”‚   ç­‰ä»·äº   â”‚    åå·®    â”‚      æ–¹å·®      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    0      â”‚   TD(0)    â”‚     é«˜     â”‚       ä½       â”‚
    â”‚   0.5     â”‚   æ··åˆ     â”‚     ä¸­     â”‚       ä¸­       â”‚
    â”‚   0.9     â”‚ æ¥è¿‘MC     â”‚     ä½     â”‚       è¾ƒé«˜     â”‚
    â”‚    1      â”‚   MC       â”‚     æ—      â”‚       é«˜       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    å®è·µä¸­Î»=0.9æ˜¯å¸¸ç”¨èµ·ç‚¹ã€‚

    èµ„æ ¼è¿¹ç±»å‹å¯¹æ¯”:
    - ç´¯ç§¯è¿¹: ç»å…¸æ–¹æ³•ï¼Œä½†å¯èƒ½å‘æ•£
    - æ›¿æ¢è¿¹: åœ¨éƒ¨åˆ†çŠ¶æ€é‡è®¿é—®å¤šçš„ä»»åŠ¡ä¸­æ›´ç¨³å®š
    - è·å…°è¿¹: ç»“åˆä¸¤è€…ä¼˜ç‚¹ï¼Œç†è®ºä¿è¯æ›´å¥½

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(|S|) per step (éœ€è¦æ›´æ–°æ‰€æœ‰æœ‰èµ„æ ¼çš„çŠ¶æ€)
    - ç©ºé—´: O(|S|) for eligibility traces

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    TD(Î»)æ˜¯TDå­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ã€‚é€šè¿‡èµ„æ ¼è¿¹ï¼Œå®ƒåœ¨æ¯ä¸€æ­¥å°†TDè¯¯å·®
    åˆ†é…ç»™æ‰€æœ‰æœ€è¿‘è®¿é—®çš„çŠ¶æ€ï¼Œåˆ†é…é‡éšæ—¶é—´å’ŒÎ»æŒ‡æ•°è¡°å‡ã€‚
    è¿™å·§å¦™åœ°ç»„åˆäº†æ‰€æœ‰n-stepæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œç”¨å•ä¸€å‚æ•°Î»æ§åˆ¶æƒè¡¡ã€‚
    """

    def __init__(self, config: TDConfig) -> None:
        """åˆå§‹åŒ–TD(Î»)ã€‚"""
        super().__init__(config)
        # èµ„æ ¼è¿¹
        self._eligibility_traces: Dict[Tuple[State, Action], float] = defaultdict(float)

    def _update_traces(
        self,
        state: State,
        action: Action
    ) -> None:
        """
        æ›´æ–°èµ„æ ¼è¿¹ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
        """
        gamma_lambda = self.config.gamma * self.config.lambda_

        # è¡°å‡æ‰€æœ‰ç°æœ‰çš„èµ„æ ¼è¿¹
        for key in list(self._eligibility_traces.keys()):
            self._eligibility_traces[key] *= gamma_lambda
            # æ¸…é™¤è¿‡å°çš„è¿¹ä»¥èŠ‚çœå†…å­˜
            if self._eligibility_traces[key] < 1e-8:
                del self._eligibility_traces[key]

        # æ›´æ–°å½“å‰çŠ¶æ€-åŠ¨ä½œçš„èµ„æ ¼è¿¹
        if self.config.trace_type == EligibilityTraceType.ACCUMULATING:
            self._eligibility_traces[(state, action)] += 1.0
        elif self.config.trace_type == EligibilityTraceType.REPLACING:
            self._eligibility_traces[(state, action)] = 1.0
        elif self.config.trace_type == EligibilityTraceType.DUTCH:
            current_trace = self._eligibility_traces[(state, action)]
            self._eligibility_traces[(state, action)] = (
                (1 - self.config.alpha) * gamma_lambda * current_trace + 1.0
            )

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒTD(Î»)æ›´æ–°ã€‚

        1. è®¡ç®—TDè¯¯å·®Î´
        2. æ›´æ–°èµ„æ ¼è¿¹
        3. ç”¨Î´å’Œèµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå€¼

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œ
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        # è®¡ç®—TDè¯¯å·®
        if done:
            td_target = reward
        else:
            if next_action is None:
                # Q-Learningé£æ ¼
                max_next_q = max(
                    self._q_function[(next_state, a)]
                    for a in self._action_space
                )
                td_target = reward + self.config.gamma * max_next_q
            else:
                # SARSAé£æ ¼
                td_target = reward + self.config.gamma * self._q_function[(next_state, next_action)]

        current_q = self._q_function[(state, action)]
        td_error = td_target - current_q

        # æ›´æ–°èµ„æ ¼è¿¹
        self._update_traces(state, action)

        # ä½¿ç”¨èµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰ç›¸å…³çš„Qå€¼
        for (s, a), trace in self._eligibility_traces.items():
            self._q_function[(s, a)] += self.config.alpha * td_error * trace

        # å›åˆç»“æŸæ—¶æ¸…ç©ºèµ„æ ¼è¿¹
        if done:
            self._eligibility_traces.clear()

        return td_error


# =============================================================================
# SARSA(Î»)
# =============================================================================

class SARSALambda(TDLambda):
    """
    SARSA(Î»)ç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    SARSA(Î»)æ˜¯SARSAä¸èµ„æ ¼è¿¹çš„ç»“åˆã€‚å®ƒæ˜¯on-policyçš„TD(Î»)æ§åˆ¶ç®—æ³•ï¼Œ
    ä½¿ç”¨å®é™…ä¸‹ä¸€åŠ¨ä½œè®¡ç®—TDç›®æ ‡ï¼ŒåŒæ—¶é€šè¿‡èµ„æ ¼è¿¹å®ç°å¤šæ­¥ä¿¡ç”¨åˆ†é…ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    TDè¯¯å·®:
        Î´_t = R_{t+1} + Î³Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)

    èµ„æ ¼è¿¹æ›´æ–°:
        E_t(s, a) = Î³Î»E_{t-1}(s, a) + ğŸ™(S_t=s, A_t=a)

    Qå€¼æ›´æ–°:
        Q(s, a) â† Q(s, a) + Î±Î´_t E_t(s, a), âˆ€s, a

    ä¸SARSAçš„å…³ç³»:
        å½“Î»=0æ—¶ï¼Œé€€åŒ–ä¸ºSARSA
        å½“Î»=1æ—¶ï¼Œå˜æˆå®Œæ•´å›åˆçš„on-policyæ›´æ–°

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    SARSAçš„æ›´æ–°ä»…ä¾èµ–å•æ­¥ä¿¡æ¯ï¼Œä¿¡ç”¨åˆ†é…èŒƒå›´æœ‰é™ã€‚
    SARSA(Î»)é€šè¿‡èµ„æ ¼è¿¹å°†TDè¯¯å·®ä¼ æ’­åˆ°æ‰€æœ‰æœ€è¿‘è®¿é—®çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œ
    å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ ã€‚

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(|S|Ã—|A|) per step (æœ€åæƒ…å†µï¼Œå®é™…å–å†³äºæ´»è·ƒè¿¹æ•°é‡)
    - ç©ºé—´: O(|S|Ã—|A|) for eligibility traces

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    SARSA(Î»)ç»“åˆäº†SARSAçš„on-policyç‰¹æ€§å’Œèµ„æ ¼è¿¹çš„é«˜æ•ˆä¿¡ç”¨åˆ†é…ã€‚
    å®ƒä¿æŒäº†SARSAçš„å®‰å…¨æ€§ï¼ˆè€ƒè™‘æ¢ç´¢é£é™©ï¼‰ï¼ŒåŒæ—¶é€šè¿‡å¤šæ­¥ä¼ æ’­åŠ é€Ÿå­¦ä¹ ã€‚
    """

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒSARSA(Î»)æ›´æ–°ã€‚

        ä½¿ç”¨å®é™…ä¸‹ä¸€åŠ¨ä½œè®¡ç®—TDç›®æ ‡ï¼Œé…åˆèµ„æ ¼è¿¹æ›´æ–°ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆSARSAå¿…éœ€ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        # è®¡ç®—SARSAé£æ ¼çš„TDè¯¯å·®
        if done:
            td_target = reward
        else:
            if next_action is None:
                raise ValueError("SARSA(Î»)éœ€è¦next_actionå‚æ•°")
            td_target = reward + self.config.gamma * self._q_function[(next_state, next_action)]

        current_q = self._q_function[(state, action)]
        td_error = td_target - current_q

        # æ›´æ–°èµ„æ ¼è¿¹
        self._update_traces(state, action)

        # ä½¿ç”¨èµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰Qå€¼
        for (s, a), trace in self._eligibility_traces.items():
            self._q_function[(s, a)] += self.config.alpha * td_error * trace

        # å›åˆç»“æŸæ—¶æ¸…ç©ºèµ„æ ¼è¿¹
        if done:
            self._eligibility_traces.clear()

        return td_error


# =============================================================================
# Watkins's Q(Î»)
# =============================================================================

class WatkinsQLambda(TDLambda):
    """
    Watkins's Q(Î»)ç®—æ³•å®ç°ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    Watkins's Q(Î»)æ˜¯Q-Learningä¸èµ„æ ¼è¿¹çš„ç»“åˆï¼Œä½†æœ‰ä¸€ä¸ªå…³é”®ç‰¹ç‚¹ï¼š
    å½“é‡‡å–éè´ªå©ªåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰æ—¶ï¼Œèµ„æ ¼è¿¹è¢«æ¸…é›¶ã€‚è¿™ç¡®ä¿äº†ç®—æ³•åœ¨
    off-policyè®¾ç½®ä¸‹çš„æ”¶æ•›æ€§ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    TDè¯¯å·® (Q-Learningé£æ ¼):
        Î´_t = R_{t+1} + Î³ max_a Q(S_{t+1}, a) - Q(S_t, A_t)

    èµ„æ ¼è¿¹æ›´æ–° (å…³é”®åŒºåˆ«):
        å¦‚æœ A_{t+1} = argmax_a Q(S_{t+1}, a):
            E_t(s, a) = Î³Î»E_{t-1}(s, a) + ğŸ™(S_t=s, A_t=a)
        å¦åˆ™ (æ¢ç´¢åŠ¨ä½œ):
            E_t(s, a) = ğŸ™(S_t=s, A_t=a)  // æ¸…é™¤å†å²è¿¹

    ä¸ºä»€ä¹ˆæ¸…é›¶èµ„æ ¼è¿¹:
        Q-Learningå‡è®¾åç»­åŠ¨ä½œéƒ½æ˜¯è´ªå©ªçš„ã€‚å½“å®é™…é‡‡å–æ¢ç´¢åŠ¨ä½œæ—¶ï¼Œ
        è¿™ä¸ªå‡è®¾è¢«æ‰“ç ´ï¼Œç»§ç»­ä¼ æ’­TDè¯¯å·®åˆ°æ›´æ—©çš„çŠ¶æ€-åŠ¨ä½œå¯¹ä¼šå¼•å…¥åå·®ã€‚
        æ¸…é›¶èµ„æ ¼è¿¹åˆ‡æ–­è¿™ç§é”™è¯¯çš„ä¿¡ç”¨åˆ†é…é“¾ã€‚

    é—®é¢˜èƒŒæ™¯ (Problem Statement):
    ----------------------------
    ç®€å•åœ°å°†èµ„æ ¼è¿¹åŠ å…¥Q-Learningä¼šå¯¼è‡´åœ¨off-policyè®¾ç½®ä¸‹ä¸æ”¶æ•›ã€‚
    Watkins's Q(Î»)é€šè¿‡åœ¨æ¢ç´¢æ—¶åˆ‡æ–­èµ„æ ¼è¿¹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚
    ç¼ºç‚¹æ˜¯åœ¨é«˜æ¢ç´¢ç‡ä¸‹ï¼Œèµ„æ ¼è¿¹ç»å¸¸è¢«æ¸…é›¶ï¼Œé€€åŒ–ä¸ºè¿‘ä¼¼Q-Learningã€‚

    ç®—æ³•å¯¹æ¯” (Comparison):
    ---------------------
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     ç®—æ³•       â”‚   æ¢ç´¢æ—¶çš„è¿¹    â”‚    æ”¶æ•›æ€§       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   Q(Î») naive   â”‚     ä¿ç•™        â”‚    ä¸ä¿è¯       â”‚
    â”‚  Watkins Q(Î»)  â”‚     æ¸…é›¶        â”‚    ä¿è¯         â”‚
    â”‚    Peng's Q(Î») â”‚   éƒ¨åˆ†ä¿ç•™      â”‚    å¼±ä¿è¯       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    å¤æ‚åº¦ (Complexity):
    -------------------
    - æ—¶é—´: O(|S|Ã—|A|) per step (æœ€å)
    - ç©ºé—´: O(|S|Ã—|A|) for eligibility traces

    ç®—æ³•æ€»ç»“ (Summary):
    -----------------
    Watkins's Q(Î»)åœ¨off-policyå­¦ä¹ ä¸­å®‰å…¨åœ°ä½¿ç”¨èµ„æ ¼è¿¹ã€‚
    ä»£ä»·æ˜¯å½“æ¢ç´¢åŠ¨ä½œå‘ç”Ÿæ—¶ï¼Œæ— æ³•åˆ©ç”¨ä¹‹å‰çš„ç»éªŒè¿›è¡Œä¿¡ç”¨åˆ†é…ã€‚
    åœ¨ä½Îµè®¾ç½®ä¸‹æ•ˆæœè¾ƒå¥½ï¼Œé«˜Îµæ—¶é€€åŒ–ä¸ºQ-Learningã€‚
    """

    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒWatkins's Q(Î»)æ›´æ–°ã€‚

        ä½¿ç”¨Q-Learningç›®æ ‡ï¼Œä½†åœ¨æ¢ç´¢åŠ¨ä½œæ—¶æ¸…é›¶èµ„æ ¼è¿¹ã€‚

        Args:
            state: å½“å‰çŠ¶æ€
            action: å½“å‰åŠ¨ä½œ
            reward: å³æ—¶å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            next_action: ä¸‹ä¸€åŠ¨ä½œï¼ˆç”¨äºæ£€æµ‹æ˜¯å¦æ¢ç´¢ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·®
        """
        # è®¡ç®—Q-Learningé£æ ¼çš„TDè¯¯å·®
        if done:
            td_target = reward
        else:
            max_next_q = max(
                self._q_function[(next_state, a)]
                for a in self._action_space
            )
            td_target = reward + self.config.gamma * max_next_q

        current_q = self._q_function[(state, action)]
        td_error = td_target - current_q

        # æ›´æ–°èµ„æ ¼è¿¹
        self._update_traces(state, action)

        # ä½¿ç”¨èµ„æ ¼è¿¹æ›´æ–°æ‰€æœ‰Qå€¼
        for (s, a), trace in self._eligibility_traces.items():
            self._q_function[(s, a)] += self.config.alpha * td_error * trace

        # æ£€æŸ¥æ˜¯å¦é‡‡å–äº†æ¢ç´¢åŠ¨ä½œï¼Œå¦‚æœæ˜¯åˆ™æ¸…é›¶èµ„æ ¼è¿¹
        if not done and next_action is not None:
            # æ‰¾åˆ°è´ªå©ªåŠ¨ä½œ
            max_next_q = max(
                self._q_function[(next_state, a)]
                for a in self._action_space
            )
            greedy_actions = [
                a for a in self._action_space
                if np.isclose(self._q_function[(next_state, a)], max_next_q)
            ]

            # å¦‚æœä¸‹ä¸€åŠ¨ä½œä¸æ˜¯è´ªå©ªåŠ¨ä½œï¼Œæ¸…é›¶èµ„æ ¼è¿¹
            if next_action not in greedy_actions:
                self._eligibility_traces.clear()

        # å›åˆç»“æŸæ—¶æ¸…ç©ºèµ„æ ¼è¿¹
        if done:
            self._eligibility_traces.clear()

        return td_error


# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_td_learner(
    algorithm: str,
    config: Optional[TDConfig] = None,
    **kwargs
) -> BaseTDLearner:
    """
    TDå­¦ä¹ ç®—æ³•å·¥å‚å‡½æ•°ã€‚

    Args:
        algorithm: ç®—æ³•åç§°ï¼Œå¯é€‰:
            - 'td0': TD(0)çŠ¶æ€ä»·å€¼å­¦ä¹ 
            - 'sarsa': SARSA
            - 'expected_sarsa': Expected SARSA
            - 'q_learning': Q-Learning
            - 'double_q': Double Q-Learning
            - 'n_step': N-Step TD
            - 'td_lambda': TD(Î»)
            - 'sarsa_lambda': SARSA(Î»)
            - 'watkins_q_lambda': Watkins's Q(Î»)
        config: TDå­¦ä¹ é…ç½®ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        **kwargs: ä¼ é€’ç»™TDConfigçš„é¢å¤–å‚æ•°

    Returns:
        å¯¹åº”çš„TDå­¦ä¹ å™¨å®ä¾‹

    Example:
        >>> learner = create_td_learner('sarsa', alpha=0.1, gamma=0.99)
        >>> learner = create_td_learner('td_lambda', config=TDConfig(lambda_=0.9))
    """
    if config is None:
        config = TDConfig(**kwargs)

    algorithm_map = {
        'td0': TD0ValueLearner,
        'sarsa': SARSA,
        'expected_sarsa': ExpectedSARSA,
        'q_learning': QLearning,
        'double_q': DoubleQLearning,
        'n_step': NStepTD,
        'td_lambda': TDLambda,
        'sarsa_lambda': SARSALambda,
        'watkins_q_lambda': WatkinsQLambda,
    }

    algorithm = algorithm.lower()
    if algorithm not in algorithm_map:
        raise ValueError(
            f"æœªçŸ¥ç®—æ³•: {algorithm}. æ”¯æŒçš„ç®—æ³•: {list(algorithm_map.keys())}"
        )

    return algorithm_map[algorithm](config)


# =============================================================================
# å•å…ƒæµ‹è¯•
# =============================================================================

if __name__ == '__main__':
    import gymnasium as gym

    print("=" * 70)
    print("æ—¶åºå·®åˆ†å­¦ä¹ ç®—æ³•æµ‹è¯•")
    print("=" * 70)

    # æµ‹è¯•é…ç½®
    config = TDConfig(
        alpha=0.1,
        gamma=0.99,
        epsilon=0.1,
        lambda_=0.9,
        n_step=3
    )

    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('CliffWalking-v0')

    # æµ‹è¯•å„ç®—æ³•ï¼ˆä½¿ç”¨æå°å‚æ•°å¿«é€ŸéªŒè¯ï¼‰
    algorithms = ['sarsa', 'expected_sarsa', 'q_learning', 'double_q', 'n_step', 'sarsa_lambda']

    for algo_name in algorithms:
        print(f"\næµ‹è¯• {algo_name}...")

        # åˆ›å»ºå­¦ä¹ å™¨
        test_config = TDConfig(
            alpha=0.5,
            gamma=0.99,
            epsilon=0.2,
            lambda_=0.8,
            n_step=3
        )
        learner = create_td_learner(algo_name, config=test_config)

        # å¿«é€Ÿæµ‹è¯•ï¼šä»…è¿è¡Œå°‘é‡å›åˆéªŒè¯ä»£ç æ­£ç¡®æ€§
        try:
            metrics = learner.train(
                env,
                n_episodes=5,  # æå°å€¼ç”¨äºæµ‹è¯•
                max_steps_per_episode=100,
                log_interval=5
            )
            print(f"  âœ“ {algo_name} æµ‹è¯•é€šè¿‡")
            print(f"    æœ€å5å›åˆå¹³å‡å¥–åŠ±: {np.mean(metrics.episode_rewards[-5:]):.2f}")
        except Exception as e:
            print(f"  âœ— {algo_name} æµ‹è¯•å¤±è´¥: {e}")

    print("\n" + "=" * 70)
    print("å®Œæ•´è®­ç»ƒæµ‹è¯• (SARSA on CliffWalking)")
    print("=" * 70)

    # ç”Ÿäº§ç¯å¢ƒå‚æ•°çš„å®Œæ•´è®­ç»ƒ
    production_config = TDConfig(
        alpha=0.5,
        gamma=0.99,
        epsilon=0.1
    )

    sarsa_learner = create_td_learner('sarsa', config=production_config)

    metrics = sarsa_learner.train(
        env,
        n_episodes=500,
        max_steps_per_episode=200,
        log_interval=100,
        early_stop_reward=-20.0
    )

    # è¯„ä¼°
    mean_reward, std_reward = sarsa_learner.evaluate(env, n_episodes=100)
    print(f"\nè¯„ä¼°ç»“æœ: {mean_reward:.2f} Â± {std_reward:.2f}")

    env.close()
    print("\næ‰€æœ‰æµ‹è¯•å®Œæˆ!")
