# 深度强化学习知识要点

> 从 DQN 到 PPO：核心概念与算法精要

---

## 一、知识框架概览

```
深度强化学习
├── 价值方法 (Value-Based)
│   ├── DQN
│   ├── Double DQN
│   ├── Dueling DQN
│   └── Rainbow
├── 策略方法 (Policy-Based)
│   ├── REINFORCE
│   └── Actor-Critic
│       ├── A2C/A3C
│       └── PPO
└── 混合方法
    ├── SAC
    └── TD3
```

---

## 二、DQN 核心知识点

### 2.1 为什么需要深度强化学习？

| 表格方法局限 | 深度 RL 解决方案 |
|-------------|-----------------|
| 状态空间爆炸 | 神经网络压缩表示 |
| 无法处理连续状态 | 函数近似泛化 |
| 无法处理高维输入 | CNN 自动特征提取 |

**核心公式**：
$$Q(s, a) \approx Q(s, a; \theta)$$

### 2.2 DQN 两大核心技术

#### 经验回放 (Experience Replay)

**问题**：连续样本违反 i.i.d. 假设

**解决**：
$$D = \{(s_t, a_t, r_t, s_{t+1}, d_t)\}_{t=1}^{N}$$
$$\text{Batch} \sim \text{Uniform}(D)$$

**优势**：
- 打破时序相关性
- 提高样本效率
- 稳定训练过程

#### 目标网络 (Target Network)

**问题**：更新目标不断漂移

**解决**：
$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

- $\theta^-$: 滞后的网络参数
- 定期更新: $\theta^- \leftarrow \theta$

### 2.3 DQN 损失函数

$$L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D}\left[(y - Q(s, a; \theta))^2\right]$$

其中目标值:
$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

### 2.4 DQN 变体

#### Double DQN

**问题**：max 操作导致过估计
$$\mathbb{E}[\max_a Q] \geq \max_a \mathbb{E}[Q]$$

**解决**：分离选择与评估
$$y^{DDQN} = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta); \theta^-)$$

#### Dueling DQN

**思想**：分解 Q = V + A

$$Q(s, a) = V(s) + A(s, a) - \frac{1}{|A|}\sum_{a'} A(s, a')$$

| 流 | 学习内容 | 作用 |
|----|---------|------|
| V(s) | 状态价值 | 状态好坏 |
| A(s,a) | 动作优势 | 动作相对优劣 |

---

## 三、策略梯度核心知识点

### 3.1 策略梯度定理

**目标**：最大化期望回报
$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]$$

**梯度**：
$$\nabla_\theta J = \mathbb{E}_{\pi}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi}(s, a)\right]$$

**直觉**：好动作概率增加，坏动作概率减少

### 3.2 方差缩减技术

#### 基线 (Baseline)

$$\nabla_\theta J = \mathbb{E}\left[\nabla_\theta \log \pi(a|s) \cdot (Q(s, a) - b(s))\right]$$

最优基线：$b(s) = V(s)$

#### 优势函数 (Advantage)

$$A(s, a) = Q(s, a) - V(s) \approx r + \gamma V(s') - V(s)$$

### 3.3 广义优势估计 (GAE)

$$\hat{A}_t^{GAE} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$

其中 TD 误差：
$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

| λ 值 | 效果 | 偏差 | 方差 |
|------|------|------|------|
| 0 | 1-step TD | 高 | 低 |
| 1 | Monte Carlo | 低 | 高 |
| 0.95 | 平衡 | 中 | 中 |

---

## 四、Actor-Critic 架构

### 4.1 双网络协作

```
Actor (策略网络)           Critic (价值网络)
       ↓                         ↓
    π(a|s)                     V(s)
       ↓                         ↓
   选择动作              计算优势 A = r + γV' - V
       ↓                         ↓
   环境交互  ←──────  策略梯度更新
```

### 4.2 A2C 损失函数

$$L = L_{policy} + c_v \cdot L_{value} - c_e \cdot H[\pi]$$

| 项 | 公式 | 作用 |
|----|------|------|
| $L_{policy}$ | $-\mathbb{E}[\log \pi \cdot \hat{A}]$ | 策略优化 |
| $L_{value}$ | $\mathbb{E}[(V - R)^2]$ | 价值拟合 |
| $H[\pi]$ | $-\mathbb{E}[\pi \log \pi]$ | 鼓励探索 |

---

## 五、PPO 核心思想

### 5.1 信任域约束

**问题**：策略更新步长难以控制
- 太大 → 崩溃
- 太小 → 缓慢

### 5.2 PPO-Clip

**策略比率**：
$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

**裁剪目标**：
$$L^{CLIP} = \mathbb{E}_t\left[\min(r_t \hat{A}_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) \hat{A}_t)\right]$$

### 5.3 裁剪效果

| 情况 | 效果 |
|------|------|
| A > 0, r > 1+ε | 阻止概率过度增加 |
| A < 0, r < 1-ε | 阻止概率过度减少 |

---

## 六、算法对比与选择

### 6.1 特性对比

| 算法 | 类型 | 样本效率 | 稳定性 | 适用场景 |
|------|------|----------|--------|----------|
| DQN | Off-Policy | 高 | 中 | 离散动作 |
| A2C | On-Policy | 低 | 中 | 简单任务 |
| PPO | On-Policy | 中 | 高 | 通用 |
| SAC | Off-Policy | 高 | 高 | 连续控制 |

### 6.2 选择指南

```
开始
  │
  ├── 动作空间离散?
  │   ├── 是 → DQN 家族
  │   └── 否 → 连续控制
  │             ├── 样本效率优先? → SAC/TD3
  │             └── 稳定性优先? → PPO
```

---

## 七、实践要点

### 7.1 超参数推荐

| 参数 | DQN | A2C | PPO |
|------|-----|-----|-----|
| 学习率 | 1e-4 | 7e-4 | 3e-4 |
| γ | 0.99 | 0.99 | 0.99 |
| Batch Size | 32-256 | N-steps | Mini-batch |
| 目标更新 | 1000步 | - | - |
| GAE λ | - | 0.95 | 0.95 |
| Clip ε | - | - | 0.2 |

### 7.2 常见问题

| 现象 | 可能原因 | 解决方案 |
|------|---------|---------|
| 奖励不增长 | 探索不足 | 增大 ε，加熵正则 |
| 奖励波动大 | 学习率太大 | 降低学习率，增大 batch |
| 训练崩溃 | 梯度爆炸 | 梯度裁剪，检查奖励尺度 |
| 过拟合环境 | 样本多样性不足 | 增大 buffer，环境随机化 |

### 7.3 调试技巧

1. **先验证环境**：用随机策略测试
2. **小规模测试**：epoch=1, batch=2 快速迭代
3. **监控指标**：损失、梯度范数、熵
4. **可视化**：学习曲线、Q 值分布

---

## 八、思考题

### 基础理解

1. 为什么 DQN 需要经验回放？如果去掉会怎样？
2. 目标网络和在线网络的区别是什么？
3. Double DQN 如何解决过估计问题？

### 进阶思考

4. 为什么 PPO 比 TRPO 更流行？
5. GAE 的 λ 参数如何影响偏差-方差权衡？
6. 在什么情况下应该选择 off-policy 而非 on-policy？

### 实践应用

7. 如何为新环境选择合适的算法？
8. 奖励塑形 (Reward Shaping) 有什么利弊？
9. 如何判断训练是否已经收敛？

---

## 九、延伸阅读

### 经典论文

1. **DQN**: Mnih et al., "Human-level control through deep RL", Nature 2015
2. **Double DQN**: van Hasselt et al., AAAI 2016
3. **Dueling DQN**: Wang et al., ICML 2016
4. **A3C**: Mnih et al., ICML 2016
5. **PPO**: Schulman et al., 2017
6. **Rainbow**: Hessel et al., AAAI 2018

### 实现资源

- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3): 高质量实现
- [CleanRL](https://github.com/vwxyzjn/cleanrl): 单文件实现
- [RLlib](https://docs.ray.io/en/latest/rllib/): 分布式框架

---

## 十、快速复习卡片

### DQN 三句话

1. 用神经网络近似 Q 函数
2. 经验回放打破样本相关性
3. 目标网络稳定训练目标

### Actor-Critic 三句话

1. Actor 学策略，Critic 学价值
2. 优势函数减少方差
3. GAE 平衡偏差-方差

### PPO 三句话

1. 限制策略更新幅度
2. 裁剪比率防止剧烈变化
3. 多轮复用提高效率

---

[返回上级](../README.md)
