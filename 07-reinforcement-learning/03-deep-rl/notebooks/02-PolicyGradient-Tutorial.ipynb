{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic 与 PPO 策略梯度实战教程\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "| 目标 | 描述 |\n",
    "|------|------|\n",
    "| 策略梯度 | 理解策略梯度定理的数学原理 |\n",
    "| Actor-Critic | 掌握演员-评论家架构 |\n",
    "| GAE | 理解广义优势估计的偏差-方差权衡 |\n",
    "| PPO | 实现 Proximal Policy Optimization |\n",
    "| 实战 | 在 CartPole 训练并对比不同算法 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: 策略梯度理论基础\n",
    "\n",
    "### 1.1 从 Q-Learning 到策略梯度\n",
    "\n",
    "| 方法 | 学习目标 | 优点 | 缺点 |\n",
    "|------|---------|------|------|\n",
    "| DQN | Q(s,a) | 样本效率高 | 只能处理离散动作 |\n",
    "| 策略梯度 | π(a\\|s) | 连续动作、随机策略 | 高方差 |\n",
    "\n",
    "### 1.2 策略梯度定理\n",
    "\n",
    "**目标**: 最大化期望回报\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "**梯度**: \n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s, a)\\right]$$\n",
    "\n",
    "**直觉**: 增加高回报动作的概率，减少低回报动作的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 方差缩减: 基线 (Baseline)\n",
    "\n",
    "**问题**: 原始策略梯度方差很大\n",
    "\n",
    "**解决**: 减去基线 b(s)\n",
    "\n",
    "$$\\nabla_\\theta J = \\mathbb{E}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (Q(s, a) - b(s))\\right]$$\n",
    "\n",
    "**最优基线**: $b(s) = V(s)$\n",
    "\n",
    "**优势函数**: $A(s, a) = Q(s, a) - V(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Actor-Critic 架构\n",
    "\n",
    "### 2.1 双网络协作\n",
    "\n",
    "```\n",
    "状态 s\n",
    "    │\n",
    "    ├──────────────┬────────────────┐\n",
    "    ↓              ↓                │\n",
    "┌───────┐     ┌───────┐            │\n",
    "│ Actor │     │ Critic│            │\n",
    "│ π(a|s)│     │ V(s)  │            │\n",
    "└───┬───┘     └───┬───┘            │\n",
    "    │             │                 │\n",
    "    ↓             ↓                 │\n",
    " 动作 a      优势 A = r + γV' - V   │\n",
    "    │             │                 │\n",
    "    ↓             ↓                 │\n",
    " 环境交互 ←── 策略梯度更新 ←────────┘\n",
    "```\n",
    "\n",
    "### 2.2 组件职责\n",
    "\n",
    "| 组件 | 输入 | 输出 | 作用 |\n",
    "|------|------|------|------|\n",
    "| Actor | s | π(a\\|s) | 决定如何行动 |\n",
    "| Critic | s | V(s) | 评估状态价值 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: 广义优势估计 (GAE)\n",
    "\n",
    "### 3.1 偏差-方差权衡\n",
    "\n",
    "| 方法 | 公式 | 偏差 | 方差 |\n",
    "|------|------|------|------|\n",
    "| 1-step TD | $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ | 高 | 低 |\n",
    "| MC | $A_t = R_t - V(s_t)$ | 低 | 高 |\n",
    "\n",
    "### 3.2 GAE 公式\n",
    "\n",
    "$$\\hat{A}_t^{GAE} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "其中 TD 误差:\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "### 3.3 λ 参数效果\n",
    "\n",
    "| λ | 效果 | 适用场景 |\n",
    "|---|------|----------|\n",
    "| 0 | 单步 TD | V(s) 准确时 |\n",
    "| 1 | 蒙特卡洛 | V(s) 不准确时 |\n",
    "| 0.95 | 最佳平衡 | 推荐默认值 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入核心库\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"请安装 gymnasium: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"计算设备: {DEVICE}\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: 轨迹缓冲区实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    On-Policy 轨迹缓冲区\n",
    "    \n",
    "    与 DQN 的经验回放不同:\n",
    "    - 存储完整轨迹\n",
    "    - 数据用后即弃 (on-policy 约束)\n",
    "    - 计算 GAE 优势估计\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma: float = 0.99, gae_lambda: float = 0.95):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"清空缓冲区\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def add(self, state, action, log_prob, reward, value, done):\n",
    "        \"\"\"添加单步数据\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(self, last_value: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    计算广义优势估计 (GAE)\n",
    "    \n",
    "    递推公式:\n",
    "        δ_t = r_t + γ(1-d_t)V(s_{t+1}) - V(s_t)\n",
    "        Â_t = δ_t + γλ(1-d_t)Â_{t+1}\n",
    "    \n",
    "    Returns:\n",
    "        (returns, advantages)\n",
    "    \"\"\"\n",
    "    rewards = np.array(self.rewards)\n",
    "    values = np.array(self.values)\n",
    "    dones = np.array(self.dones)\n",
    "    n_steps = len(rewards)\n",
    "    \n",
    "    # 添加最后价值用于 bootstrap\n",
    "    values = np.append(values, last_value)\n",
    "    \n",
    "    # GAE 递推计算\n",
    "    advantages = np.zeros(n_steps, dtype=np.float32)\n",
    "    gae = 0.0\n",
    "    \n",
    "    for t in reversed(range(n_steps)):\n",
    "        delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    # 回报 = 优势 + 价值\n",
    "    returns = advantages + values[:-1]\n",
    "    \n",
    "    return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "\n",
    "RolloutBuffer.compute_gae = compute_gae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 GAE 计算\n",
    "buffer = RolloutBuffer(gamma=0.99, gae_lambda=0.95)\n",
    "for i in range(10):\n",
    "    buffer.add(\n",
    "        state=np.random.randn(4),\n",
    "        action=0,\n",
    "        log_prob=-0.5,\n",
    "        reward=1.0,\n",
    "        value=0.5,\n",
    "        done=(i == 9)\n",
    "    )\n",
    "\n",
    "returns, advantages = buffer.compute_gae(last_value=0.0)\n",
    "print(f\"缓冲区大小: {len(buffer)}\")\n",
    "print(f\"回报形状: {returns.shape}\")\n",
    "print(f\"优势形状: {advantages.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Actor-Critic 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    共享参数的 Actor-Critic 网络\n",
    "    \n",
    "    架构:\n",
    "        State → 共享层 → Actor 头 → π(a|s)\n",
    "                      → Critic 头 → V(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 共享特征层\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Actor 头 (策略)\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Critic 头 (价值)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # Actor 输出用小增益\n",
    "        nn.init.orthogonal_(self.actor.weight, gain=0.01)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.shared(x)\n",
    "        action_logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return action_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_and_value(self, state, action=None):\n",
    "    \"\"\"\n",
    "    获取动作、对数概率、熵和价值\n",
    "    \n",
    "    用于:\n",
    "    1. 动作选择 (action=None): 从 π 采样\n",
    "    2. 计算对数概率 (action 给定): 计算 log π\n",
    "    \"\"\"\n",
    "    logits, value = self(state)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    dist = Categorical(probs)\n",
    "    \n",
    "    if action is None:\n",
    "        action = dist.sample()\n",
    "    \n",
    "    log_prob = dist.log_prob(action)\n",
    "    entropy = dist.entropy()\n",
    "    \n",
    "    return action, log_prob, entropy, value.squeeze(-1)\n",
    "\n",
    "ActorCriticNetwork.get_action_and_value = get_action_and_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试网络\n",
    "net = ActorCriticNetwork(state_dim=4, action_dim=2).to(DEVICE)\n",
    "x = torch.randn(32, 4).to(DEVICE)\n",
    "\n",
    "action, log_prob, entropy, value = net.get_action_and_value(x)\n",
    "print(f\"动作: {action.shape}\")\n",
    "print(f\"对数概率: {log_prob.shape}\")\n",
    "print(f\"熵: {entropy.shape}\")\n",
    "print(f\"价值: {value.shape}\")\n",
    "print(f\"参数量: {sum(p.numel() for p in net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: A2C 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic 智能体\n",
    "    \n",
    "    损失函数:\n",
    "        L = L_policy + c_v × L_value - c_e × H[π]\n",
    "    \n",
    "    其中:\n",
    "        L_policy = -E[log π × Â]  (策略梯度)\n",
    "        L_value = E[(V - R)²]     (价值函数误差)\n",
    "        H[π] = -E[π log π]        (熵正则化)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        lr: float = 7e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        value_coef: float = 0.5,\n",
    "        entropy_coef: float = 0.01,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        device: str = 'auto'\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.device = torch.device('cuda' if device == 'auto' and torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.buffer = RolloutBuffer(gamma=gamma, gae_lambda=gae_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state: np.ndarray) -> Tuple[int, float, float]:\n",
    "    \"\"\"选择动作\"\"\"\n",
    "    state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = self.network.get_action_and_value(state_t)\n",
    "    return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "def store(self, state, action, log_prob, reward, value, done):\n",
    "    \"\"\"存储转换\"\"\"\n",
    "    self.buffer.add(state, action, log_prob, reward, value, done)\n",
    "\n",
    "A2CAgent.get_action = get_action\n",
    "A2CAgent.store = store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_a2c(self, last_value: float) -> Dict[str, float]:\n",
    "    \"\"\"A2C 更新\"\"\"\n",
    "    returns, advantages = self.buffer.compute_gae(last_value)\n",
    "    \n",
    "    states = torch.FloatTensor(np.array(self.buffer.states)).to(self.device)\n",
    "    actions = torch.LongTensor(self.buffer.actions).to(self.device)\n",
    "    returns = returns.to(self.device)\n",
    "    advantages = advantages.to(self.device)\n",
    "    \n",
    "    # 标准化优势\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    # 计算新策略\n",
    "    _, new_log_probs, entropy, values = self.network.get_action_and_value(states, actions)\n",
    "    \n",
    "    # 策略损失\n",
    "    policy_loss = -(new_log_probs * advantages.detach()).mean()\n",
    "    \n",
    "    # 价值损失\n",
    "    value_loss = F.mse_loss(values, returns)\n",
    "    \n",
    "    # 熵损失\n",
    "    entropy_loss = -entropy.mean()\n",
    "    \n",
    "    # 总损失\n",
    "    loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "    \n",
    "    # 优化\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "    self.optimizer.step()\n",
    "    \n",
    "    self.buffer.reset()\n",
    "    \n",
    "    return {\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'value_loss': value_loss.item(),\n",
    "        'entropy': -entropy_loss.item()\n",
    "    }\n",
    "\n",
    "A2CAgent.update = update_a2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: PPO 核心思想\n",
    "\n",
    "### 8.1 信任域约束\n",
    "\n",
    "**问题**: 策略梯度更新步长难以控制\n",
    "- 步长太大 → 策略崩溃\n",
    "- 步长太小 → 学习缓慢\n",
    "\n",
    "### 8.2 PPO-Clip 目标函数\n",
    "\n",
    "**策略比率**:\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "**裁剪目标**:\n",
    "$$L^{CLIP} = \\mathbb{E}_t\\left[\\min\\left(r_t \\hat{A}_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "### 8.3 裁剪效果\n",
    "\n",
    "| 情况 | r_t 范围 | 效果 |\n",
    "|------|---------|------|\n",
    "| A > 0, r > 1+ε | 裁剪 | 阻止过度增加概率 |\n",
    "| A < 0, r < 1-ε | 裁剪 | 阻止过度减少概率 |\n",
    "| 其他 | 不裁剪 | 正常梯度 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: PPO 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization 智能体\n",
    "    \n",
    "    与 A2C 的关键差异:\n",
    "    1. 多轮 epoch: 每批数据训练多次\n",
    "    2. Mini-batch: 将大批次分成小批次\n",
    "    3. PPO-Clip: 裁剪策略比率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_epsilon: float = 0.2,\n",
    "        value_coef: float = 0.5,\n",
    "        entropy_coef: float = 0.01,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        n_epochs: int = 10,\n",
    "        mini_batch_size: int = 64,\n",
    "        device: str = 'auto'\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.n_epochs = n_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        self.device = torch.device('cuda' if device == 'auto' and torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr, eps=1e-5)\n",
    "        \n",
    "        self.buffer = RolloutBuffer(gamma=gamma, gae_lambda=gae_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复用动作选择和存储方法\n",
    "PPOAgent.get_action = get_action\n",
    "PPOAgent.store = store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ppo(self, last_value: float) -> Dict[str, float]:\n",
    "    \"\"\"PPO 更新\"\"\"\n",
    "    returns, advantages = self.buffer.compute_gae(last_value)\n",
    "    \n",
    "    states = torch.FloatTensor(np.array(self.buffer.states)).to(self.device)\n",
    "    actions = torch.LongTensor(self.buffer.actions).to(self.device)\n",
    "    old_log_probs = torch.FloatTensor(self.buffer.log_probs).to(self.device)\n",
    "    returns = returns.to(self.device)\n",
    "    advantages = advantages.to(self.device)\n",
    "    \n",
    "    # 标准化优势\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    batch_size = len(states)\n",
    "    total_losses = {'policy': 0, 'value': 0, 'entropy': 0}\n",
    "    n_updates = 0\n",
    "    \n",
    "    # 多轮 epoch\n",
    "    for _ in range(self.n_epochs):\n",
    "        indices = np.random.permutation(batch_size)\n",
    "        \n",
    "        # Mini-batch 更新\n",
    "        for start in range(0, batch_size, self.mini_batch_size):\n",
    "            end = start + self.mini_batch_size\n",
    "            mb_idx = indices[start:end]\n",
    "            \n",
    "            mb_states = states[mb_idx]\n",
    "            mb_actions = actions[mb_idx]\n",
    "            mb_old_log_probs = old_log_probs[mb_idx]\n",
    "            mb_returns = returns[mb_idx]\n",
    "            mb_advantages = advantages[mb_idx]\n",
    "            \n",
    "            # 计算新策略\n",
    "            _, new_log_probs, entropy, values = self.network.get_action_and_value(\n",
    "                mb_states, mb_actions\n",
    "            )\n",
    "            \n",
    "            # 计算比率\n",
    "            ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
    "            \n",
    "            # PPO-Clip 目标\n",
    "            surr1 = ratio * mb_advantages\n",
    "            surr2 = torch.clamp(\n",
    "                ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon\n",
    "            ) * mb_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # 价值损失\n",
    "            value_loss = F.mse_loss(values, mb_returns)\n",
    "            \n",
    "            # 熵损失\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            # 总损失\n",
    "            loss = (policy_loss + \n",
    "                   self.value_coef * value_loss + \n",
    "                   self.entropy_coef * entropy_loss)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_losses['policy'] += policy_loss.item()\n",
    "            total_losses['value'] += value_loss.item()\n",
    "            total_losses['entropy'] += entropy.mean().item()\n",
    "            n_updates += 1\n",
    "    \n",
    "    self.buffer.reset()\n",
    "    \n",
    "    return {\n",
    "        'policy_loss': total_losses['policy'] / n_updates,\n",
    "        'value_loss': total_losses['value'] / n_updates,\n",
    "        'entropy': total_losses['entropy'] / n_updates\n",
    "    }\n",
    "\n",
    "PPOAgent.update = update_ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: 训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_gradient(\n",
    "    agent,\n",
    "    env_name: str = 'CartPole-v1',\n",
    "    total_steps: int = 50000,\n",
    "    n_steps: int = 128,\n",
    "    seed: int = 42,\n",
    "    algo_name: str = 'Agent',\n",
    "    verbose: bool = True\n",
    ") -> List[float]:\n",
    "    \"\"\"训练策略梯度智能体\"\"\"\n",
    "    if not HAS_GYM:\n",
    "        return []\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"训练 {algo_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "    rewards_history = []\n",
    "    episode_reward = 0.0\n",
    "    best_avg = float('-inf')\n",
    "    \n",
    "    state, _ = env.reset(seed=seed)\n",
    "    step = 0\n",
    "    \n",
    "    while step < total_steps:\n",
    "        for _ in range(n_steps):\n",
    "            action, log_prob, value = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store(state, action, log_prob, reward, value, done)\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "            \n",
    "            if done:\n",
    "                rewards_history.append(episode_reward)\n",
    "                episode_reward = 0.0\n",
    "                state, _ = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "            if step >= total_steps:\n",
    "                break\n",
    "        \n",
    "        _, _, last_value = agent.get_action(state)\n",
    "        agent.update(last_value)\n",
    "        \n",
    "        if verbose and len(rewards_history) >= 20 and len(rewards_history) % 20 == 0:\n",
    "            avg = np.mean(rewards_history[-20:])\n",
    "            best_avg = max(best_avg, avg)\n",
    "            print(f\"回合 {len(rewards_history):4d} | 平均: {avg:7.2f} | 最佳: {best_avg:7.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 A2C\n",
    "if HAS_GYM:\n",
    "    a2c_agent = A2CAgent(state_dim=4, action_dim=2)\n",
    "    rewards_a2c = train_policy_gradient(\n",
    "        a2c_agent,\n",
    "        total_steps=50000,\n",
    "        n_steps=5,\n",
    "        algo_name='A2C'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 PPO\n",
    "if HAS_GYM:\n",
    "    ppo_agent = PPOAgent(state_dim=4, action_dim=2)\n",
    "    rewards_ppo = train_policy_gradient(\n",
    "        ppo_agent,\n",
    "        total_steps=50000,\n",
    "        n_steps=128,\n",
    "        algo_name='PPO'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(results: dict, window: int = 20):\n",
    "    \"\"\"绘制算法对比\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    colors = ['#1f77b4', '#ff7f0e']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results.items()):\n",
    "        if len(rewards) >= window:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            plt.plot(smoothed, label=name, color=colors[idx], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episode', fontsize=12)\n",
    "    plt.ylabel('Total Reward', fontsize=12)\n",
    "    plt.title('A2C vs PPO 学习曲线对比', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM and rewards_a2c and rewards_ppo:\n",
    "    plot_comparison({\n",
    "        'A2C': rewards_a2c,\n",
    "        'PPO': rewards_ppo\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 12: PPO 裁剪可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ppo_clip(epsilon: float = 0.2):\n",
    "    \"\"\"可视化 PPO 裁剪机制\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ratios = np.linspace(0.5, 1.5, 100)\n",
    "    \n",
    "    # 正优势\n",
    "    ax = axes[0]\n",
    "    advantage = 1.0\n",
    "    unclipped = ratios * advantage\n",
    "    clipped = np.clip(ratios, 1-epsilon, 1+epsilon) * advantage\n",
    "    objective = np.minimum(unclipped, clipped)\n",
    "    \n",
    "    ax.plot(ratios, unclipped, 'b--', label='未裁剪', alpha=0.7)\n",
    "    ax.plot(ratios, clipped, 'r--', label='裁剪后', alpha=0.7)\n",
    "    ax.plot(ratios, objective, 'g-', linewidth=2, label='PPO 目标')\n",
    "    ax.axvline(1-epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(1+epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('策略比率 r(θ)')\n",
    "    ax.set_ylabel('目标值')\n",
    "    ax.set_title(f'正优势 (A > 0), ε = {epsilon}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 负优势\n",
    "    ax = axes[1]\n",
    "    advantage = -1.0\n",
    "    unclipped = ratios * advantage\n",
    "    clipped = np.clip(ratios, 1-epsilon, 1+epsilon) * advantage\n",
    "    objective = np.minimum(unclipped, clipped)\n",
    "    \n",
    "    ax.plot(ratios, unclipped, 'b--', label='未裁剪', alpha=0.7)\n",
    "    ax.plot(ratios, clipped, 'r--', label='裁剪后', alpha=0.7)\n",
    "    ax.plot(ratios, objective, 'g-', linewidth=2, label='PPO 目标')\n",
    "    ax.axvline(1-epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(1+epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('策略比率 r(θ)')\n",
    "    ax.set_ylabel('目标值')\n",
    "    ax.set_title(f'负优势 (A < 0), ε = {epsilon}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ppo_clip(epsilon=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 13: 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"运行单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = failed = 0\n",
    "    \n",
    "    # 测试1: RolloutBuffer\n",
    "    try:\n",
    "        buf = RolloutBuffer(gamma=0.99, gae_lambda=0.95)\n",
    "        for i in range(10):\n",
    "            buf.add(np.random.randn(4), 0, -0.5, 1.0, 0.5, i == 9)\n",
    "        returns, advantages = buf.compute_gae(0.0)\n",
    "        assert returns.shape == (10,)\n",
    "        print(\"测试1通过: RolloutBuffer\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: ActorCriticNetwork\n",
    "    try:\n",
    "        net = ActorCriticNetwork(4, 2, 64)\n",
    "        x = torch.randn(32, 4)\n",
    "        action, log_prob, entropy, value = net.get_action_and_value(x)\n",
    "        assert action.shape == (32,)\n",
    "        assert value.shape == (32,)\n",
    "        print(\"测试2通过: ActorCriticNetwork\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: A2CAgent\n",
    "    try:\n",
    "        agent = A2CAgent(4, 2, device='cpu')\n",
    "        state = np.random.randn(4).astype(np.float32)\n",
    "        action, lp, val = agent.get_action(state)\n",
    "        assert 0 <= action < 2\n",
    "        for i in range(10):\n",
    "            agent.store(state, 0, -0.5, 1.0, 0.5, i == 9)\n",
    "        agent.update(0.0)\n",
    "        print(\"测试3通过: A2CAgent\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试4: PPOAgent\n",
    "    try:\n",
    "        agent = PPOAgent(4, 2, mini_batch_size=32, device='cpu')\n",
    "        state = np.random.randn(4).astype(np.float32)\n",
    "        action, lp, val = agent.get_action(state)\n",
    "        assert 0 <= action < 2\n",
    "        for i in range(64):\n",
    "            agent.store(state, 0, -0.5, 1.0, 0.5, i == 63)\n",
    "        agent.update(0.0)\n",
    "        print(\"测试4通过: PPOAgent\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"结果: {passed} 通过, {failed} 失败\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 特性 | A2C | PPO |\n",
    "|------|-----|-----|\n",
    "| 数据使用 | 用一次即弃 | 多轮复用 |\n",
    "| 更新方式 | 直接梯度 | 裁剪约束 |\n",
    "| 稳定性 | 中等 | 高 |\n",
    "| 调参难度 | 中等 | 简单 |\n",
    "\n",
    "### 调参建议\n",
    "\n",
    "| 参数 | A2C | PPO |\n",
    "|------|-----|-----|\n",
    "| 学习率 | 7e-4 | 3e-4 |\n",
    "| γ | 0.99 | 0.99 |\n",
    "| GAE λ | 0.95 | 0.95 |\n",
    "| Clip ε | - | 0.2 |\n",
    "| N-steps | 5 | 2048 |\n",
    "| Epochs | 1 | 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Mnih et al., \"Asynchronous Methods for Deep RL\", ICML 2016\n",
    "2. Schulman et al., \"High-Dimensional Control Using GAE\", ICLR 2016\n",
    "3. Schulman et al., \"Proximal Policy Optimization\", 2017\n",
    "\n",
    "---\n",
    "\n",
    "[返回上级](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
