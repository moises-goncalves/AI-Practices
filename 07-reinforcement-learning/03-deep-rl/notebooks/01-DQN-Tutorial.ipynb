{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) 深度实战教程\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "| 目标 | 描述 |\n",
    "|------|------|\n",
    "| 理论基础 | 理解 DQN 的数学原理与核心创新 |\n",
    "| 经验回放 | 掌握 Experience Replay 原理与实现 |\n",
    "| 目标网络 | 理解 Target Network 稳定训练的机制 |\n",
    "| 算法变体 | 实现 Double DQN、Dueling DQN |\n",
    "| 实战能力 | 在 CartPole 环境训练和评估智能体 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: 为什么需要深度强化学习？\n",
    "\n",
    "### 1.1 表格型 Q-Learning 的局限\n",
    "\n",
    "| 问题 | 说明 | 影响 |\n",
    "|------|------|------|\n",
    "| 状态空间爆炸 | 围棋有 $10^{170}$ 状态 | 无法存储 Q 表 |\n",
    "| 连续状态空间 | 机器人关节角度是连续值 | 无法离散化索引 |\n",
    "| 无法泛化 | 相似状态需独立学习 | 样本效率极低 |\n",
    "| 高维输入 | 图像有百万像素 | 无法直接作为索引 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 函数近似：核心解决方案\n",
    "\n",
    "**核心思想**：用参数化函数近似价值函数\n",
    "\n",
    "$$Q(s, a) \\approx Q(s, a; \\theta)$$\n",
    "\n",
    "其中 $\\theta$ 是神经网络参数。\n",
    "\n",
    "**为什么选择神经网络？**\n",
    "\n",
    "1. **通用近似定理**：可以逼近任意连续函数\n",
    "2. **自动特征提取**：从原始输入学习有用表示\n",
    "3. **泛化能力**：相似状态共享表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: DQN 核心技术解析\n",
    "\n",
    "### 2.1 经验回放 (Experience Replay)\n",
    "\n",
    "**问题**：在线学习时连续样本高度相关\n",
    "\n",
    "```\n",
    "时序相关：s_1 → s_2 → s_3 → s_4 → ...\n",
    "          ↓     ↓     ↓     ↓\n",
    "SGD假设：i.i.d. 独立同分布\n",
    "```\n",
    "\n",
    "**解决方案**：存储 → 打乱 → 采样\n",
    "\n",
    "$$\\text{Buffer}: D = \\{(s_t, a_t, r_t, s_{t+1}, d_t)\\}_{t=1}^{N}$$\n",
    "\n",
    "$$\\text{Sample}: \\{\\tau_i\\}_{i=1}^{B} \\sim \\text{Uniform}(D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 目标网络 (Target Network)\n",
    "\n",
    "**问题**：更新 Q 网络时目标也在变化\n",
    "\n",
    "$$\\text{Target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta) \\quad \\text{(不稳定)}$$\n",
    "\n",
    "**解决方案**：使用滞后的目标网络 $\\theta^-$\n",
    "\n",
    "$$\\text{Target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) \\quad \\text{(稳定)}$$\n",
    "\n",
    "**更新策略**：\n",
    "- 每 $C$ 步硬更新：$\\theta^- \\leftarrow \\theta$\n",
    "- 或软更新：$\\theta^- \\leftarrow \\tau\\theta + (1-\\tau)\\theta^-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DQN 损失函数\n",
    "\n",
    "**TD 误差**：\n",
    "\n",
    "$$\\delta = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta)$$\n",
    "\n",
    "**损失函数（MSE）**：\n",
    "\n",
    "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D}\\left[\\delta^2\\right]$$\n",
    "\n",
    "**梯度更新**：\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入核心库\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"请安装 gymnasium: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置设置\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"计算设备: {DEVICE}\")\n",
    "\n",
    "# 绘图配置\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CartPole 环境探索\n",
    "\n",
    "CartPole 是经典控制问题：通过左右移动小车平衡竖直的杆子。\n",
    "\n",
    "| 属性 | 值 |\n",
    "|------|----|\n",
    "| 状态空间 | 4 维连续 |\n",
    "| 动作空间 | 2 个离散 |\n",
    "| 奖励 | 每步 +1 |\n",
    "| 成功标准 | 平均回报 ≥ 475 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索环境\n",
    "if HAS_GYM:\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    print(\"CartPole-v1 环境信息:\")\n",
    "    print(f\"  状态空间: {env.observation_space}\")\n",
    "    print(f\"  动作空间: {env.action_space}\")\n",
    "    \n",
    "    state, _ = env.reset(seed=SEED)\n",
    "    print(f\"\\n状态含义:\")\n",
    "    print(f\"  [0] 小车位置: {state[0]:.4f}\")\n",
    "    print(f\"  [1] 小车速度: {state[1]:.4f}\")\n",
    "    print(f\"  [2] 杆子角度: {state[2]:.4f}\")\n",
    "    print(f\"  [3] 杆子角速度: {state[3]:.4f}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 经验回放缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Transition:\n",
    "    \"\"\"单步转换数据\"\"\"\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    经验回放缓冲区\n",
    "    \n",
    "    复杂度:\n",
    "        push: O(1) 摊销\n",
    "        sample: O(batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, ...]:\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = np.array([t.state for t in batch], dtype=np.float32)\n",
    "        actions = np.array([t.action for t in batch], dtype=np.int64)\n",
    "        rewards = np.array([t.reward for t in batch], dtype=np.float32)\n",
    "        next_states = np.array([t.next_state for t in batch], dtype=np.float32)\n",
    "        dones = np.array([t.done for t in batch], dtype=np.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def is_ready(self, batch_size: int) -> bool:\n",
    "        return len(self) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试缓冲区\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "for i in range(100):\n",
    "    buffer.push(\n",
    "        state=np.random.randn(4),\n",
    "        action=i % 2,\n",
    "        reward=1.0,\n",
    "        next_state=np.random.randn(4),\n",
    "        done=False\n",
    "    )\n",
    "\n",
    "states, actions, rewards, _, _ = buffer.sample(32)\n",
    "print(f\"缓冲区大小: {len(buffer)}\")\n",
    "print(f\"采样形状: states={states.shape}, actions={actions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: DQN 网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    标准 DQN 网络\n",
    "    \n",
    "    架构: State → FC → ReLU → FC → ReLU → FC → Q-values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试网络\n",
    "net = DQNNetwork(state_dim=4, action_dim=2).to(DEVICE)\n",
    "x = torch.randn(32, 4).to(DEVICE)\n",
    "q_values = net(x)\n",
    "\n",
    "print(f\"输入: {x.shape}\")\n",
    "print(f\"输出: {q_values.shape}\")\n",
    "print(f\"参数量: {sum(p.numel() for p in net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Dueling DQN\n",
    "\n",
    "### 6.1 核心思想\n",
    "\n",
    "将 Q 值分解为**状态价值** V(s) 和**优势函数** A(s,a)：\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'} A(s, a')$$\n",
    "\n",
    "**为什么有效？**\n",
    "\n",
    "- V(s) 独立学习状态的整体价值\n",
    "- A(s,a) 专注于比较动作的相对优劣\n",
    "- 在动作影响不大的状态下学习更高效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling DQN 网络\n",
    "    \n",
    "    架构:\n",
    "        State → 共享层 → 价值流 → V(s)\n",
    "                      → 优势流 → A(s,a)\n",
    "        Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 共享特征层\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 价值流 V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # 优势流 A(s,a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        \n",
    "        # 聚合: Q = V + (A - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=-1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 Dueling 网络\n",
    "dueling_net = DuelingDQNNetwork(state_dim=4, action_dim=2).to(DEVICE)\n",
    "q_values = dueling_net(x)\n",
    "\n",
    "print(f\"Dueling DQN 输出: {q_values.shape}\")\n",
    "print(f\"参数量: {sum(p.numel() for p in dueling_net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: DQN 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN 智能体\n",
    "    \n",
    "    支持:\n",
    "    - 标准 DQN\n",
    "    - Double DQN (解耦动作选择与评估)\n",
    "    - Dueling DQN (V-A 分解)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        lr: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.01,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        buffer_size: int = 100000,\n",
    "        batch_size: int = 64,\n",
    "        target_update_freq: int = 100,\n",
    "        double_dqn: bool = False,\n",
    "        dueling: bool = False,\n",
    "        device: str = 'auto'\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.double_dqn = double_dqn\n",
    "        \n",
    "        # 设备\n",
    "        self.device = torch.device('cuda' if device == 'auto' and torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 网络\n",
    "        NetworkClass = DuelingDQNNetwork if dueling else DQNNetwork\n",
    "        self.q_network = NetworkClass(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.target_network = NetworkClass(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # 经验回放\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # 计数器\n",
    "        self.update_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动作选择方法\n",
    "def get_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "    \"\"\"ε-greedy 动作选择\"\"\"\n",
    "    if training and random.random() < self.epsilon:\n",
    "        return random.randint(0, self.action_dim - 1)\n",
    "    \n",
    "    state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "    with torch.no_grad():\n",
    "        q_values = self.q_network(state_t)\n",
    "    return q_values.argmax(dim=1).item()\n",
    "\n",
    "DQNAgent.get_action = get_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新方法\n",
    "def update(self, state, action, reward, next_state, done) -> Optional[float]:\n",
    "    \"\"\"存储经验并训练\"\"\"\n",
    "    self.buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    if not self.buffer.is_ready(self.batch_size):\n",
    "        return None\n",
    "    \n",
    "    # 采样\n",
    "    states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "    \n",
    "    states_t = torch.FloatTensor(states).to(self.device)\n",
    "    actions_t = torch.LongTensor(actions).to(self.device)\n",
    "    rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
    "    next_states_t = torch.FloatTensor(next_states).to(self.device)\n",
    "    dones_t = torch.FloatTensor(dones).to(self.device)\n",
    "    \n",
    "    # 当前 Q 值\n",
    "    current_q = self.q_network(states_t).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # 目标 Q 值\n",
    "    with torch.no_grad():\n",
    "        if self.double_dqn:\n",
    "            # Double DQN: 在线网络选动作，目标网络评估\n",
    "            next_actions = self.q_network(next_states_t).argmax(dim=1)\n",
    "            next_q = self.target_network(next_states_t).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            next_q = self.target_network(next_states_t).max(dim=1)[0]\n",
    "        \n",
    "        target_q = rewards_t + self.gamma * next_q * (1 - dones_t)\n",
    "    \n",
    "    # 损失与优化\n",
    "    loss = F.mse_loss(current_q, target_q)\n",
    "    \n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)\n",
    "    self.optimizer.step()\n",
    "    \n",
    "    # 更新目标网络\n",
    "    self.update_count += 1\n",
    "    if self.update_count % self.target_update_freq == 0:\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "DQNAgent.update = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索率衰减\n",
    "def decay_epsilon(self):\n",
    "    self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "DQNAgent.decay_epsilon = decay_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: 训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(\n",
    "    env_name: str = 'CartPole-v1',\n",
    "    num_episodes: int = 200,\n",
    "    double_dqn: bool = False,\n",
    "    dueling: bool = False,\n",
    "    seed: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[DQNAgent, List[float]]:\n",
    "    \"\"\"训练 DQN 智能体\"\"\"\n",
    "    if not HAS_GYM:\n",
    "        return None, []\n",
    "    \n",
    "    # 设置种子\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # 创建环境\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # 算法名称\n",
    "    algo_name = \"DQN\"\n",
    "    if double_dqn:\n",
    "        algo_name = \"Double \" + algo_name\n",
    "    if dueling:\n",
    "        algo_name = \"Dueling \" + algo_name\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"训练 {algo_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "    # 创建智能体\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        double_dqn=double_dqn,\n",
    "        dueling=dueling\n",
    "    )\n",
    "    \n",
    "    # 训练\n",
    "    rewards_history = []\n",
    "    best_avg = float('-inf')\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if verbose and (episode + 1) % 25 == 0:\n",
    "            avg = np.mean(rewards_history[-25:])\n",
    "            best_avg = max(best_avg, avg)\n",
    "            print(f\"回合 {episode+1:4d} | 平均: {avg:7.2f} | 最佳: {best_avg:7.2f} | ε: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练基础 DQN\n",
    "agent_dqn, rewards_dqn = train_dqn(\n",
    "    num_episodes=150,\n",
    "    double_dqn=False,\n",
    "    dueling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 Double Dueling DQN\n",
    "agent_dd, rewards_dd = train_dqn(\n",
    "    num_episodes=150,\n",
    "    double_dqn=True,\n",
    "    dueling=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(results: dict, window: int = 20):\n",
    "    \"\"\"绘制学习曲线对比\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results.items()):\n",
    "        if len(rewards) >= window:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            plt.plot(smoothed, label=name, color=colors[idx % len(colors)], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episode', fontsize=12)\n",
    "    plt.ylabel('Total Reward', fontsize=12)\n",
    "    plt.title('DQN 变体学习曲线对比', fontsize=14)\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制对比\n",
    "if rewards_dqn and rewards_dd:\n",
    "    plot_learning_curves({\n",
    "        'DQN': rewards_dqn,\n",
    "        'Double Dueling DQN': rewards_dd\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: 交互式实验\n",
    "\n",
    "### 实验 1: ε 衰减策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_epsilon_decay(decay_rates: List[float], episodes: int = 200):\n",
    "    \"\"\"可视化不同衰减率\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for decay in decay_rates:\n",
    "        epsilons = []\n",
    "        eps = 1.0\n",
    "        for _ in range(episodes):\n",
    "            epsilons.append(eps)\n",
    "            eps = max(0.01, eps * decay)\n",
    "        plt.plot(epsilons, label=f'decay={decay}')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('ε-greedy 探索率衰减')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "visualize_epsilon_decay([0.99, 0.995, 0.999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验 2: Double DQN 消除过估计\n",
    "\n",
    "**过估计问题**：\n",
    "\n",
    "$$\\mathbb{E}[\\max_a Q(s, a)] \\geq \\max_a \\mathbb{E}[Q(s, a)]$$\n",
    "\n",
    "**Double DQN 解决方案**：\n",
    "\n",
    "$$y = r + \\gamma Q(s', \\arg\\max_{a'} Q(s', a'; \\theta); \\theta^-)$$\n",
    "\n",
    "- 在线网络选择最优动作\n",
    "- 目标网络评估该动作的价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"运行单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = failed = 0\n",
    "    \n",
    "    # 测试1: ReplayBuffer\n",
    "    try:\n",
    "        buf = ReplayBuffer(100)\n",
    "        for i in range(50):\n",
    "            buf.push(np.random.randn(4), 0, 1.0, np.random.randn(4), False)\n",
    "        assert len(buf) == 50\n",
    "        s, a, r, ns, d = buf.sample(32)\n",
    "        assert s.shape == (32, 4)\n",
    "        print(\"测试1通过: ReplayBuffer\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: DQNNetwork\n",
    "    try:\n",
    "        net = DQNNetwork(4, 2, 64)\n",
    "        x = torch.randn(32, 4)\n",
    "        out = net(x)\n",
    "        assert out.shape == (32, 2)\n",
    "        print(\"测试2通过: DQNNetwork\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: DuelingDQNNetwork\n",
    "    try:\n",
    "        net = DuelingDQNNetwork(4, 2, 64)\n",
    "        x = torch.randn(32, 4)\n",
    "        out = net(x)\n",
    "        assert out.shape == (32, 2)\n",
    "        print(\"测试3通过: DuelingDQNNetwork\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试4: DQNAgent\n",
    "    try:\n",
    "        agent = DQNAgent(4, 2, batch_size=32, device='cpu')\n",
    "        state = np.random.randn(4).astype(np.float32)\n",
    "        action = agent.get_action(state)\n",
    "        assert 0 <= action < 2\n",
    "        for _ in range(50):\n",
    "            agent.update(state, 0, 1.0, state, False)\n",
    "        print(\"测试4通过: DQNAgent\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"结果: {passed} 通过, {failed} 失败\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "| 技术 | 解决的问题 | 方法 |\n",
    "|------|-----------|------|\n",
    "| 经验回放 | 样本相关性 | 存储并随机采样 |\n",
    "| 目标网络 | 目标不稳定 | 滞后更新的网络副本 |\n",
    "| Double DQN | Q 值过估计 | 分离动作选择与评估 |\n",
    "| Dueling DQN | 学习效率 | V-A 值分解 |\n",
    "\n",
    "### 调参建议\n",
    "\n",
    "| 参数 | 推荐范围 | 说明 |\n",
    "|------|----------|------|\n",
    "| 学习率 | 1e-4 ~ 1e-3 | 太大不稳定 |\n",
    "| Batch Size | 32 ~ 256 | 视内存而定 |\n",
    "| γ | 0.99 | 短期任务可用 0.95 |\n",
    "| 目标更新 | 100 ~ 1000 步 | 或软更新 τ=0.005 |\n",
    "| ε 衰减 | 0.995 ~ 0.9999 | 视任务难度 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Mnih et al., \"Playing Atari with Deep Reinforcement Learning\", 2013\n",
    "2. Mnih et al., \"Human-level control through deep RL\", Nature 2015\n",
    "3. van Hasselt et al., \"Deep RL with Double Q-learning\", AAAI 2016\n",
    "4. Wang et al., \"Dueling Network Architectures\", ICML 2016\n",
    "\n",
    "---\n",
    "\n",
    "[返回上级](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
