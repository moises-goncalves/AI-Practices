{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后见经验回放 (Hindsight Experience Replay) 深度教程\n",
    "\n",
    "## 目录\n",
    "1. [问题背景：目标条件RL的困境](#1-问题背景目标条件rl的困境)\n",
    "2. [HER核心思想](#2-her核心思想)\n",
    "3. [目标选择策略](#3-目标选择策略)\n",
    "4. [HER实现详解](#4-her实现详解)\n",
    "5. [进阶变体：优先级HER与课程HER](#5-进阶变体优先级her与课程her)\n",
    "6. [实验与分析](#6-实验与分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 问题背景：目标条件RL的困境\n",
    "\n",
    "### 1.1 目标条件强化学习\n",
    "\n",
    "**标准RL**：学习 $\\pi(a|s)$，最大化 $\\mathbb{E}[\\sum_t \\gamma^t r_t]$\n",
    "\n",
    "**目标条件RL**：学习 $\\pi(a|s, g)$，最大化 $\\mathbb{E}[\\sum_t \\gamma^t r(s_t, a_t, g)]$\n",
    "\n",
    "**应用场景**：\n",
    "- 机器人操作：抓取物体放到指定位置\n",
    "- 导航：到达任意目标点\n",
    "- 游戏：完成指定任务\n",
    "\n",
    "### 1.2 稀疏奖励的灾难\n",
    "\n",
    "典型奖励函数（稀疏）：\n",
    "$$r(s, a, g) = \\begin{cases} 0 & \\text{if } \\|s_{achieved} - g\\| < \\epsilon \\\\ -1 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**问题**：\n",
    "- 随机策略几乎永远达不到目标\n",
    "- 所有样本的奖励都是 -1\n",
    "- 没有梯度信号来改进策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional, Callable\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sparse_reward_problem():\n",
    "    \"\"\"可视化稀疏奖励问题。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 图1: 目标条件任务示意\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    \n",
    "    # 绘制起点、目标和轨迹\n",
    "    start = np.array([1, 1])\n",
    "    goal = np.array([8, 8])\n",
    "    \n",
    "    # 随机轨迹（未到达目标）\n",
    "    traj = [start]\n",
    "    pos = start.copy()\n",
    "    for _ in range(20):\n",
    "        pos = pos + np.random.randn(2) * 0.5\n",
    "        pos = np.clip(pos, 0, 10)\n",
    "        traj.append(pos.copy())\n",
    "    traj = np.array(traj)\n",
    "    \n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'b-', alpha=0.7, linewidth=2)\n",
    "    ax.plot(start[0], start[1], 'go', markersize=15, label='起点')\n",
    "    ax.plot(goal[0], goal[1], 'r*', markersize=20, label='目标')\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 'bs', markersize=12, label='终点')\n",
    "    \n",
    "    # 目标区域\n",
    "    circle = plt.Circle(goal, 0.5, color='red', alpha=0.2)\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('目标条件任务: 到达指定目标')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 图2: 奖励分布（稀疏）\n",
    "    ax = axes[1]\n",
    "    rewards = np.array([-1] * 100)  # 全是失败\n",
    "    ax.hist(rewards, bins=20, color='red', alpha=0.7)\n",
    "    ax.axvline(x=0, color='green', linestyle='--', label='成功阈值')\n",
    "    ax.set_xlabel('奖励')\n",
    "    ax.set_ylabel('频次')\n",
    "    ax.set_title('稀疏奖励分布 (所有样本都失败)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 图3: HER重标注后\n",
    "    ax = axes[2]\n",
    "    rewards_her = np.concatenate([\n",
    "        np.array([-1] * 20),   # 原始失败样本\n",
    "        np.array([0] * 80),    # HER重标注的\"成功\"样本\n",
    "    ])\n",
    "    ax.hist(rewards_her, bins=20, color='green', alpha=0.7)\n",
    "    ax.set_xlabel('奖励')\n",
    "    ax.set_ylabel('频次')\n",
    "    ax.set_title('HER后的奖励分布 (大部分\"成功\")')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sparse_reward_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HER核心思想\n",
    "\n",
    "### 2.1 核心洞察\n",
    "\n",
    "**失败的轨迹包含有价值的信息**：\n",
    "- 虽然没达到原始目标 $g$\n",
    "- 但确实到达了某个状态 $s_T$\n",
    "- 如果目标是 $g' = s_T$，这就是一条\"成功\"的轨迹！\n",
    "\n",
    "### 2.2 HER的数学表述\n",
    "\n",
    "**原始转移**：$(s_t, a_t, g, r_t, s_{t+1})$，其中 $r_t = -1$（失败）\n",
    "\n",
    "**重标注转移**：$(s_t, a_t, g', r'_t, s_{t+1})$，其中：\n",
    "- $g' = \\text{achieved\\_goal}(s_{t+1})$ 或其他策略选择的目标\n",
    "- $r'_t = R(s_{t+1}, g') = 0$（成功！）\n",
    "\n",
    "### 2.3 为什么HER有效？\n",
    "\n",
    "1. **样本效率**：每条轨迹产生 $(1 + k)$ 倍的学习样本\n",
    "2. **正样本比例**：从 ~0% 提升到 ~$\\frac{k}{k+1}$\n",
    "3. **值函数学习**：Q网络学会了\"如何到达各种状态\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_her_relabeling():\n",
    "    \"\"\"演示HER重标注过程。\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"HER重标注演示\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 模拟一个失败的episode\n",
    "    original_goal = np.array([8.0, 8.0])\n",
    "    trajectory = [\n",
    "        {'state': np.array([0.0, 0.0]), 'achieved': np.array([0.0, 0.0])},\n",
    "        {'state': np.array([1.0, 0.5]), 'achieved': np.array([1.0, 0.5])},\n",
    "        {'state': np.array([2.0, 1.5]), 'achieved': np.array([2.0, 1.5])},\n",
    "        {'state': np.array([3.0, 2.0]), 'achieved': np.array([3.0, 2.0])},  # 最终状态\n",
    "    ]\n",
    "    \n",
    "    epsilon = 0.5  # 成功阈值\n",
    "    \n",
    "    print(f\"\\n原始目标: {original_goal}\")\n",
    "    print(f\"最终到达: {trajectory[-1]['achieved']}\")\n",
    "    print(f\"距离目标: {np.linalg.norm(trajectory[-1]['achieved'] - original_goal):.2f}\")\n",
    "    print(f\"成功阈值: {epsilon}\")\n",
    "    print(\"结果: ❌ 失败 (未到达目标)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"HER重标注 (使用'future'策略, k=4)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    k = 4\n",
    "    for t, step in enumerate(trajectory[:-1]):\n",
    "        print(f\"\\n转移 {t}: s={step['state']} -> s'={trajectory[t+1]['state']}\")\n",
    "        \n",
    "        # 原始转移\n",
    "        distance_to_goal = np.linalg.norm(trajectory[t+1]['achieved'] - original_goal)\n",
    "        original_reward = 0 if distance_to_goal < epsilon else -1\n",
    "        print(f\"  原始: g={original_goal}, r={original_reward}\")\n",
    "        \n",
    "        # 重标注：从future中采样\n",
    "        future_goals = [trajectory[i]['achieved'] for i in range(t+1, len(trajectory))]\n",
    "        \n",
    "        for i in range(min(k, len(future_goals))):\n",
    "            new_goal = future_goals[i % len(future_goals)]\n",
    "            distance = np.linalg.norm(trajectory[t+1]['achieved'] - new_goal)\n",
    "            new_reward = 0 if distance < epsilon else -1\n",
    "            status = \"✓\" if new_reward == 0 else \"✗\"\n",
    "            print(f\"  HER {i+1}: g'={new_goal}, r'={new_reward} {status}\")\n",
    "\n",
    "demonstrate_her_relabeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 目标选择策略\n",
    "\n",
    "### 3.1 四种主要策略\n",
    "\n",
    "| 策略 | 描述 | 数学表示 |\n",
    "|------|------|----------|\n",
    "| **final** | 使用轨迹最终达成的目标 | $g' = \\text{ag}_{T}$ |\n",
    "| **future** | 从当前时刻之后的达成目标中采样 | $g' \\sim \\{\\text{ag}_i : i > t\\}$ |\n",
    "| **episode** | 从整个轨迹的达成目标中采样 | $g' \\sim \\{\\text{ag}_i : i \\in [0,T]\\}$ |\n",
    "| **random** | 从所有历史达成目标中采样 | $g' \\sim \\mathcal{D}_{\\text{achieved}}$ |\n",
    "\n",
    "### 3.2 策略选择建议\n",
    "\n",
    "- **future** 是默认最佳选择，提供良好的时间一致性\n",
    "- **final** 最简单，适合快速原型\n",
    "- **random** 提供最大多样性，可能有助于泛化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_goal_strategies():\n",
    "    \"\"\"可视化不同目标选择策略。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 模拟轨迹\n",
    "    np.random.seed(42)\n",
    "    T = 10\n",
    "    trajectory = np.cumsum(np.random.randn(T, 2) * 0.5, axis=0)\n",
    "    current_t = 3  # 当前时刻\n",
    "    \n",
    "    strategies = [\n",
    "        ('final', 'Final: 使用最终达成目标'),\n",
    "        ('future', 'Future: 从未来达成目标采样'),\n",
    "        ('episode', 'Episode: 从全轨迹达成目标采样'),\n",
    "        ('random', 'Random: 从历史达成目标采样'),\n",
    "    ]\n",
    "    \n",
    "    for ax, (strategy, title) in zip(axes.flat, strategies):\n",
    "        # 绘制轨迹\n",
    "        ax.plot(trajectory[:, 0], trajectory[:, 1], 'b-', alpha=0.5, linewidth=2)\n",
    "        \n",
    "        # 标记所有状态\n",
    "        for i in range(T):\n",
    "            color = 'red' if i == current_t else 'blue'\n",
    "            size = 100 if i == current_t else 30\n",
    "            ax.scatter(trajectory[i, 0], trajectory[i, 1], c=color, s=size, zorder=5)\n",
    "        \n",
    "        # 根据策略高亮候选目标\n",
    "        if strategy == 'final':\n",
    "            candidates = [trajectory[-1]]\n",
    "        elif strategy == 'future':\n",
    "            candidates = trajectory[current_t+1:]\n",
    "        elif strategy == 'episode':\n",
    "            candidates = trajectory\n",
    "        else:  # random\n",
    "            candidates = np.random.randn(10, 2) * 2 + trajectory.mean(axis=0)\n",
    "        \n",
    "        for cand in candidates:\n",
    "            ax.scatter(cand[0], cand[1], c='green', s=80, marker='*', \n",
    "                      alpha=0.7, edgecolors='black')\n",
    "        \n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 图例\n",
    "        ax.scatter([], [], c='red', s=100, label=f'当前状态 t={current_t}')\n",
    "        ax.scatter([], [], c='green', s=80, marker='*', label='候选HER目标')\n",
    "        ax.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_goal_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HER实现详解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hindsight_experience_replay import (\n",
    "    GoalSelectionStrategy,\n",
    "    Transition,\n",
    "    Episode,\n",
    "    HERConfig,\n",
    "    GoalConditionedReplayBuffer,\n",
    "    HindsightExperienceReplay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义奖励函数\n",
    "def sparse_reward(achieved_goal, desired_goal, threshold=0.5):\n",
    "    \"\"\"稀疏奖励函数。\"\"\"\n",
    "    distance = np.linalg.norm(np.asarray(achieved_goal) - np.asarray(desired_goal))\n",
    "    return 0.0 if distance < threshold else -1.0\n",
    "\n",
    "# 创建HER配置\n",
    "config = HERConfig(\n",
    "    strategy=GoalSelectionStrategy.FUTURE,\n",
    "    n_sampled_goal=4,  # 每个转移采样4个HER目标\n",
    "    reward_function=sparse_reward,\n",
    ")\n",
    "\n",
    "print(\"HER配置:\")\n",
    "print(f\"  目标选择策略: {config.strategy.value}\")\n",
    "print(f\"  每转移采样目标数: {config.n_sampled_goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建HER回放缓冲区\n",
    "state_dim = 6  # (pos_x, pos_y, vel_x, vel_y, grip, obj)\n",
    "action_dim = 3\n",
    "goal_dim = 2  # (target_x, target_y)\n",
    "\n",
    "her = HindsightExperienceReplay(\n",
    "    capacity=10000,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    goal_dim=goal_dim,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"HER缓冲区容量: {her.capacity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟目标条件任务的Episode\n",
    "def simulate_episode(goal, episode_length=20):\n",
    "    \"\"\"模拟一个目标条件任务的episode。\"\"\"\n",
    "    \n",
    "    transitions = []\n",
    "    \n",
    "    # 初始状态\n",
    "    state = np.zeros(state_dim)\n",
    "    state[:2] = np.random.randn(2) * 0.5  # 随机起始位置\n",
    "    \n",
    "    for _ in range(episode_length):\n",
    "        # 随机动作（简化）\n",
    "        action = np.random.randn(action_dim) * 0.1\n",
    "        \n",
    "        # 简单的动力学\n",
    "        next_state = state.copy()\n",
    "        next_state[:2] += action[:2]  # 位置更新\n",
    "        \n",
    "        # 达成的目标 = 当前位置\n",
    "        achieved_goal = next_state[:2].copy()\n",
    "        \n",
    "        # 计算奖励\n",
    "        reward = sparse_reward(achieved_goal, goal)\n",
    "        \n",
    "        # 检查是否成功\n",
    "        done = reward == 0.0\n",
    "        \n",
    "        transition = Transition(\n",
    "            state=state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            next_state=next_state,\n",
    "            done=done,\n",
    "            goal=goal,\n",
    "            achieved_goal=achieved_goal,\n",
    "        )\n",
    "        transitions.append(transition)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "    \n",
    "    return Episode(transitions=transitions, goal=goal)\n",
    "\n",
    "# 模拟多个episode\n",
    "print(\"模拟目标条件任务...\")\n",
    "\n",
    "successes = 0\n",
    "for i in range(50):\n",
    "    goal = np.random.randn(goal_dim) * 3  # 随机目标\n",
    "    episode = simulate_episode(goal)\n",
    "    \n",
    "    # 存储到HER\n",
    "    her.store_episode(episode)\n",
    "    \n",
    "    if episode.transitions[-1].done:\n",
    "        successes += 1\n",
    "\n",
    "print(f\"\\n原始成功率: {successes}/50 = {successes/50:.1%}\")\n",
    "print(f\"HER缓冲区大小: {len(her)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样并分析HER样本\n",
    "batch_size = 256\n",
    "batch = her.sample(batch_size)\n",
    "\n",
    "# 统计奖励分布\n",
    "positive_rewards = np.sum(batch.rewards >= 0)\n",
    "negative_rewards = np.sum(batch.rewards < 0)\n",
    "\n",
    "print(\"\\nHER采样分析:\")\n",
    "print(f\"  批次大小: {batch_size}\")\n",
    "print(f\"  正奖励样本: {positive_rewards} ({positive_rewards/batch_size:.1%})\")\n",
    "print(f\"  负奖励样本: {negative_rewards} ({negative_rewards/batch_size:.1%})\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(['成功样本 (r≥0)', '失败样本 (r<0)'], \n",
    "        [positive_rewards, negative_rewards],\n",
    "        color=['green', 'red'])\n",
    "plt.ylabel('样本数')\n",
    "plt.title('HER采样后的奖励分布')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n关键洞察: HER将原本~0%的正样本比例提升到了很高的水平！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 进阶变体：优先级HER与课程HER\n",
    "\n",
    "### 5.1 优先级HER (Prioritized HER)\n",
    "\n",
    "结合优先级经验回放，根据TD误差优先采样：\n",
    "\n",
    "$$P(i) \\propto |\\delta_i|^\\alpha$$\n",
    "\n",
    "### 5.2 课程HER (Curriculum HER)\n",
    "\n",
    "**动态调整目标难度**：\n",
    "- 初期：选择\"容易\"的目标（接近当前能力）\n",
    "- 后期：选择更\"困难\"的目标（扩展能力边界）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hindsight_experience_replay import PrioritizedHER, CurriculumHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示课程学习的目标采样\n",
    "def demonstrate_curriculum():\n",
    "    \"\"\"演示课程HER的目标采样。\"\"\"\n",
    "    \n",
    "    # 模拟智能体当前能力\n",
    "    agent_reach = 2.0  # 智能体能可靠到达的距离\n",
    "    \n",
    "    # 模拟目标池\n",
    "    easy_goals = np.random.randn(50, 2) * 1.0  # 简单目标\n",
    "    medium_goals = np.random.randn(50, 2) * 3.0  # 中等目标\n",
    "    hard_goals = np.random.randn(50, 2) * 5.0  # 困难目标\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    training_stages = [\n",
    "        ('初期: 主要采样简单目标', 0.8, 0.15, 0.05),\n",
    "        ('中期: 混合采样', 0.3, 0.5, 0.2),\n",
    "        ('后期: 主要采样困难目标', 0.1, 0.3, 0.6),\n",
    "    ]\n",
    "    \n",
    "    for ax, (title, easy_prob, med_prob, hard_prob) in zip(axes, training_stages):\n",
    "        # 按概率采样目标\n",
    "        n_samples = 100\n",
    "        sampled = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            r = np.random.random()\n",
    "            if r < easy_prob:\n",
    "                idx = np.random.randint(len(easy_goals))\n",
    "                sampled.append(('easy', easy_goals[idx]))\n",
    "            elif r < easy_prob + med_prob:\n",
    "                idx = np.random.randint(len(medium_goals))\n",
    "                sampled.append(('medium', medium_goals[idx]))\n",
    "            else:\n",
    "                idx = np.random.randint(len(hard_goals))\n",
    "                sampled.append(('hard', hard_goals[idx]))\n",
    "        \n",
    "        # 绘制\n",
    "        colors = {'easy': 'green', 'medium': 'orange', 'hard': 'red'}\n",
    "        for difficulty, goal in sampled:\n",
    "            ax.scatter(goal[0], goal[1], c=colors[difficulty], alpha=0.5, s=30)\n",
    "        \n",
    "        # 智能体能力圈\n",
    "        circle = plt.Circle((0, 0), agent_reach, fill=False, \n",
    "                           color='blue', linestyle='--', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        ax.scatter(0, 0, c='blue', s=100, marker='s', label='智能体')\n",
    "        ax.set_xlim(-8, 8)\n",
    "        ax.set_ylim(-8, 8)\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_title(title)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 图例\n",
    "        ax.scatter([], [], c='green', label='简单目标')\n",
    "        ax.scatter([], [], c='orange', label='中等目标')\n",
    "        ax.scatter([], [], c='red', label='困难目标')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_curriculum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 实验与分析\n",
    "\n",
    "### 6.1 HER vs 标准RL 学习曲线对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_learning_comparison():\n",
    "    \"\"\"模拟HER vs 标准RL的学习曲线。\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_episodes = 200\n",
    "    \n",
    "    # 标准RL（稀疏奖励，学习缓慢）\n",
    "    standard_rl = []\n",
    "    success_prob = 0.01  # 初始成功率很低\n",
    "    for ep in range(n_episodes):\n",
    "        # 缓慢学习\n",
    "        success_prob = min(0.95, success_prob + 0.002 * (1 - success_prob))\n",
    "        success_prob += np.random.randn() * 0.01\n",
    "        success_prob = np.clip(success_prob, 0, 1)\n",
    "        standard_rl.append(success_prob)\n",
    "    \n",
    "    # HER（快速学习）\n",
    "    her_rl = []\n",
    "    success_prob = 0.01\n",
    "    for ep in range(n_episodes):\n",
    "        # 快速学习\n",
    "        success_prob = min(0.95, success_prob + 0.02 * (1 - success_prob))\n",
    "        success_prob += np.random.randn() * 0.02\n",
    "        success_prob = np.clip(success_prob, 0, 1)\n",
    "        her_rl.append(success_prob)\n",
    "    \n",
    "    # 绘图\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 学习曲线\n",
    "    ax = axes[0]\n",
    "    ax.plot(standard_rl, 'b-', label='标准RL (稀疏奖励)', alpha=0.8)\n",
    "    ax.plot(her_rl, 'r-', label='HER', alpha=0.8)\n",
    "    ax.set_xlabel('训练回合')\n",
    "    ax.set_ylabel('成功率')\n",
    "    ax.set_title('学习曲线对比')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 达到阈值所需回合数\n",
    "    ax = axes[1]\n",
    "    thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    standard_episodes = []\n",
    "    her_episodes = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        # 找到首次达到阈值的回合\n",
    "        std_ep = next((i for i, v in enumerate(standard_rl) if v >= thresh), n_episodes)\n",
    "        her_ep = next((i for i, v in enumerate(her_rl) if v >= thresh), n_episodes)\n",
    "        standard_episodes.append(std_ep)\n",
    "        her_episodes.append(her_ep)\n",
    "    \n",
    "    x = np.arange(len(thresholds))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, standard_episodes, width, label='标准RL', color='blue')\n",
    "    ax.bar(x + width/2, her_episodes, width, label='HER', color='red')\n",
    "    ax.set_xlabel('目标成功率')\n",
    "    ax.set_ylabel('所需训练回合数')\n",
    "    ax.set_title('样本效率对比')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'{t:.0%}' for t in thresholds])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 计算加速比\n",
    "    print(\"\\n样本效率提升:\")\n",
    "    for thresh, std, her_ep in zip(thresholds, standard_episodes, her_episodes):\n",
    "        if her_ep > 0:\n",
    "            speedup = std / her_ep\n",
    "            print(f\"  达到{thresh:.0%}成功率: 标准RL={std}回合, HER={her_ep}回合, 加速{speedup:.1f}x\")\n",
    "\n",
    "simulate_learning_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印总结\n",
    "summary = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════╗\n",
    "║                  后见经验回放 (HER) - 核心总结                           ║\n",
    "╠══════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                          ║\n",
    "║  核心思想：                                                              ║\n",
    "║    失败的轨迹 + 重新定义的目标 = 成功的经验                              ║\n",
    "║                                                                          ║\n",
    "║  重标注公式：                                                            ║\n",
    "║    原始: (s, a, g, r=-1, s')                                            ║\n",
    "║    HER:  (s, a, g'=achieved(s'), r'=0, s')                              ║\n",
    "║                                                                          ║\n",
    "║  目标选择策略：                                                          ║\n",
    "║    • final:   使用轨迹终点                                              ║\n",
    "║    • future:  从未来状态采样 (推荐)                                      ║\n",
    "║    • episode: 从全轨迹采样                                              ║\n",
    "║    • random:  从历史采样                                                ║\n",
    "║                                                                          ║\n",
    "║  样本效率提升：                                                          ║\n",
    "║    正样本比例: ~0% → ~k/(k+1)                                           ║\n",
    "║    训练速度:   通常提升 5-10倍                                          ║\n",
    "║                                                                          ║\n",
    "║  适用场景：                                                              ║\n",
    "║    ✓ 目标条件任务                                                       ║\n",
    "║    ✓ 稀疏奖励环境                                                       ║\n",
    "║    ✓ 机器人操作                                                         ║\n",
    "║                                                                          ║\n",
    "╚══════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "1. Andrychowicz, M., et al. (2017). Hindsight Experience Replay. NeurIPS.\n",
    "2. Plappert, M., et al. (2018). Multi-Goal RL: Challenging Robotics Environments.\n",
    "3. Fang, M., et al. (2019). Curriculum-guided Hindsight Experience Replay. NeurIPS.\n",
    "4. Zhao, R., & Tresp, V. (2019). Energy-Based Hindsight Experience Prioritization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
