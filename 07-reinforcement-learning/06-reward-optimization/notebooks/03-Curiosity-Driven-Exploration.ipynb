{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 好奇心驱动探索 (Curiosity-Driven Exploration) 深度教程\n",
    "\n",
    "## 目录\n",
    "1. [问题背景：稀疏奖励与探索困境](#1-问题背景稀疏奖励与探索困境)\n",
    "2. [内在动机理论基础](#2-内在动机理论基础)\n",
    "3. [ICM: 内在好奇心模块](#3-icm-内在好奇心模块)\n",
    "4. [RND: 随机网络蒸馏](#4-rnd-随机网络蒸馏)\n",
    "5. [基于计数的探索](#5-基于计数的探索)\n",
    "6. [实验对比与分析](#6-实验对比与分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 问题背景：稀疏奖励与探索困境\n",
    "\n",
    "### 1.1 为什么需要内在动机？\n",
    "\n",
    "**稀疏奖励环境的挑战**：\n",
    "- 随机探索效率极低：$P(\\text{success}) \\approx (1/|A|)^T$\n",
    "- 无法获得学习信号来改进策略\n",
    "- 例如：Montezuma's Revenge 游戏需要数千步精确操作才能获得第一个奖励\n",
    "\n",
    "### 1.2 内在动机的生物学启发\n",
    "\n",
    "人类和动物具有**内在好奇心**：\n",
    "- 婴儿探索环境不是为了外在奖励\n",
    "- 科学家追求知识的本能驱动\n",
    "- 新颖性本身就是一种奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 内在动机理论基础\n",
    "\n",
    "### 2.1 核心公式\n",
    "\n",
    "总奖励由外在和内在奖励组成：\n",
    "\n",
    "$$r_{total} = r_{extrinsic} + \\beta \\cdot r_{intrinsic}$$\n",
    "\n",
    "其中 $\\beta$ 控制探索与利用的平衡。\n",
    "\n",
    "### 2.2 内在奖励的设计原则\n",
    "\n",
    "1. **新颖性**：访问新状态应该获得奖励\n",
    "2. **可学习性**：智能体能从探索中学到知识\n",
    "3. **消退性**：重复访问同一状态的奖励应减少\n",
    "\n",
    "### 2.3 内在奖励的分类\n",
    "\n",
    "| 类型 | 信号来源 | 代表方法 |\n",
    "|------|----------|----------|\n",
    "| 预测误差 | 世界模型预测失败 | ICM, RND |\n",
    "| 状态计数 | 访问次数 | Count-based |\n",
    "| 信息增益 | 模型不确定性降低 | VIME |\n",
    "| 能力获取 | 新技能学习 | Empowerment |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_intrinsic_motivation():\n",
    "    \"\"\"可视化内在动机的直觉。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 图1: 预测误差作为新颖性\n",
    "    ax = axes[0]\n",
    "    x = np.linspace(0, 100, 100)\n",
    "    # 访问次数增加，预测误差降低\n",
    "    prediction_error = 1.0 / (1 + 0.1 * x) + 0.1 * np.random.randn(100) * 0.1\n",
    "    ax.plot(x, prediction_error, 'b-', linewidth=2)\n",
    "    ax.fill_between(x, prediction_error, alpha=0.3)\n",
    "    ax.set_xlabel('访问次数')\n",
    "    ax.set_ylabel('预测误差 (内在奖励)')\n",
    "    ax.set_title('预测误差随熟悉度下降')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 图2: 探索覆盖对比\n",
    "    ax = axes[1]\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 随机探索（集中在起点附近）\n",
    "    random_explore = np.random.randn(200, 2) * 0.5\n",
    "    \n",
    "    # 好奇心探索（更广泛分布）\n",
    "    curious_explore = np.random.randn(200, 2) * 2.0\n",
    "    \n",
    "    ax.scatter(random_explore[:, 0], random_explore[:, 1], \n",
    "               c='blue', alpha=0.5, label='随机探索', s=20)\n",
    "    ax.scatter(curious_explore[:, 0], curious_explore[:, 1],\n",
    "               c='red', alpha=0.5, label='好奇心探索', s=20)\n",
    "    ax.set_xlabel('状态维度1')\n",
    "    ax.set_ylabel('状态维度2')\n",
    "    ax.set_title('探索覆盖对比')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    \n",
    "    # 图3: 学习曲线对比\n",
    "    ax = axes[2]\n",
    "    episodes = np.arange(500)\n",
    "    \n",
    "    # 无内在动机：学习缓慢\n",
    "    no_intrinsic = 1 - np.exp(-episodes / 300)\n",
    "    no_intrinsic += np.random.randn(500) * 0.05\n",
    "    \n",
    "    # 有内在动机：学习更快\n",
    "    with_intrinsic = 1 - np.exp(-episodes / 100)\n",
    "    with_intrinsic += np.random.randn(500) * 0.03\n",
    "    \n",
    "    ax.plot(episodes, np.clip(no_intrinsic, 0, 1), 'b-', \n",
    "            label='无内在动机', alpha=0.8)\n",
    "    ax.plot(episodes, np.clip(with_intrinsic, 0, 1), 'r-',\n",
    "            label='有内在动机', alpha=0.8)\n",
    "    ax.set_xlabel('训练回合')\n",
    "    ax.set_ylabel('成功率')\n",
    "    ax.set_title('学习效率对比')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_intrinsic_motivation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ICM: 内在好奇心模块\n",
    "\n",
    "### 3.1 核心思想\n",
    "\n",
    "ICM使用预测误差作为内在奖励：\n",
    "\n",
    "$$r_i(s_t, a_t, s_{t+1}) = \\eta \\cdot \\|f(s_{t+1}) - \\hat{f}(s_t, a_t)\\|^2$$\n",
    "\n",
    "### 3.2 架构组成\n",
    "\n",
    "```\n",
    "观测 s ──→ [特征编码器 φ] ──→ f(s)\n",
    "              │\n",
    "              ├──→ [前向模型] ──→ f̂(s')\n",
    "              │     ↑ 输入: f(s), a\n",
    "              │     ↓ 输出: 预测的下一状态特征\n",
    "              │\n",
    "              └──→ [逆向模型] ──→ â\n",
    "                    ↑ 输入: f(s), f(s')\n",
    "                    ↓ 输出: 预测的动作\n",
    "```\n",
    "\n",
    "### 3.3 为什么在特征空间预测？\n",
    "\n",
    "原始观测空间包含大量**不可控噪声**：\n",
    "- 游戏中的云朵移动\n",
    "- 树叶的随机摆动\n",
    "- 与智能体动作无关的变化\n",
    "\n",
    "特征空间只保留**动作相关信息**，由逆向模型强制约束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curiosity_driven import (\n",
    "    CuriosityConfig,\n",
    "    FeatureEncoder,\n",
    "    ForwardDynamicsModel,\n",
    "    InverseDynamicsModel,\n",
    "    IntrinsicCuriosityModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建ICM组件\n",
    "obs_dim = 16\n",
    "action_dim = 4\n",
    "feature_dim = 32\n",
    "\n",
    "# 特征编码器\n",
    "encoder = FeatureEncoder(\n",
    "    input_dim=obs_dim,\n",
    "    feature_dim=feature_dim,\n",
    "    hidden_dims=(64, 32),\n",
    ")\n",
    "\n",
    "# 前向动力学模型\n",
    "forward_model = ForwardDynamicsModel(\n",
    "    feature_dim=feature_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dims=(64, 32),\n",
    ")\n",
    "\n",
    "# 逆向动力学模型\n",
    "inverse_model = InverseDynamicsModel(\n",
    "    feature_dim=feature_dim,\n",
    "    action_dim=action_dim,\n",
    "    discrete_actions=True,\n",
    ")\n",
    "\n",
    "print(\"ICM组件创建成功！\")\n",
    "print(f\"特征编码器: {obs_dim} → {feature_dim}\")\n",
    "print(f\"前向模型: (特征{feature_dim}, 动作{action_dim}) → 特征{feature_dim}\")\n",
    "print(f\"逆向模型: (特征{feature_dim}, 特征{feature_dim}) → 动作概率{action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示ICM的工作流程\n",
    "def demonstrate_icm_workflow():\n",
    "    \"\"\"展示ICM的完整工作流程。\"\"\"\n",
    "    \n",
    "    # 模拟一个转移\n",
    "    state = np.random.randn(obs_dim)\n",
    "    action = 2  # 选择动作2\n",
    "    next_state = state + 0.1 * np.random.randn(obs_dim)  # 微小变化\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ICM工作流程演示\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 步骤1: 编码状态\n",
    "    print(\"\\n[步骤1] 特征编码\")\n",
    "    features_s = encoder.encode(state)\n",
    "    features_s_next = encoder.encode(next_state)\n",
    "    print(f\"  状态特征 f(s): shape={features_s.shape}, norm={np.linalg.norm(features_s):.4f}\")\n",
    "    print(f\"  下一状态特征 f(s'): shape={features_s_next.shape}\")\n",
    "    \n",
    "    # 步骤2: 前向预测\n",
    "    print(\"\\n[步骤2] 前向模型预测\")\n",
    "    predicted_features = forward_model.predict(features_s, np.array([action]))\n",
    "    print(f\"  预测特征 f̂(s'): shape={predicted_features.shape}\")\n",
    "    \n",
    "    # 步骤3: 计算预测误差（内在奖励）\n",
    "    print(\"\\n[步骤3] 计算预测误差\")\n",
    "    prediction_error = np.sum((features_s_next - predicted_features) ** 2)\n",
    "    print(f\"  预测误差 ||f(s') - f̂(s')||²: {prediction_error:.6f}\")\n",
    "    \n",
    "    # 步骤4: 逆向模型预测动作\n",
    "    print(\"\\n[步骤4] 逆向模型预测动作\")\n",
    "    action_probs = inverse_model.predict(features_s, features_s_next)\n",
    "    predicted_action = np.argmax(action_probs)\n",
    "    print(f\"  动作概率分布: {action_probs}\")\n",
    "    print(f\"  真实动作: {action}, 预测动作: {predicted_action}\")\n",
    "    \n",
    "    # 步骤5: 计算内在奖励\n",
    "    print(\"\\n[步骤5] 内在奖励\")\n",
    "    eta = 0.01  # 缩放系数\n",
    "    intrinsic_reward = eta * prediction_error\n",
    "    print(f\"  r_i = η × ||f(s') - f̂(s')||² = {eta} × {prediction_error:.4f} = {intrinsic_reward:.6f}\")\n",
    "    \n",
    "    return intrinsic_reward\n",
    "\n",
    "intrinsic_r = demonstrate_icm_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用完整的ICM模块\n",
    "config = CuriosityConfig(\n",
    "    intrinsic_reward_scale=0.01,\n",
    "    feature_dim=32,\n",
    "    learning_rate=0.001,\n",
    "    forward_loss_weight=0.2,\n",
    "    inverse_loss_weight=0.8,\n",
    "    normalize_rewards=True,\n",
    ")\n",
    "\n",
    "icm = IntrinsicCuriosityModule(\n",
    "    observation_dim=obs_dim,\n",
    "    action_dim=action_dim,\n",
    "    config=config,\n",
    "    discrete_actions=True,\n",
    ")\n",
    "\n",
    "print(\"ICM模块配置:\")\n",
    "print(f\"  内在奖励缩放: {config.intrinsic_reward_scale}\")\n",
    "print(f\"  特征维度: {config.feature_dim}\")\n",
    "print(f\"  前向损失权重: {config.forward_loss_weight}\")\n",
    "print(f\"  逆向损失权重: {config.inverse_loss_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟ICM在训练中的行为\n",
    "def simulate_icm_training(icm, n_episodes=50, episode_length=100):\n",
    "    \"\"\"模拟ICM训练过程。\"\"\"\n",
    "    \n",
    "    history = {\n",
    "        'intrinsic_rewards': [],\n",
    "        'forward_losses': [],\n",
    "        'inverse_losses': [],\n",
    "    }\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state = np.random.randn(obs_dim)\n",
    "        ep_intrinsic = []\n",
    "        \n",
    "        for _ in range(episode_length):\n",
    "            action = np.array([np.random.randint(0, action_dim)])\n",
    "            next_state = state + 0.1 * np.random.randn(obs_dim)\n",
    "            \n",
    "            # 计算内在奖励\n",
    "            intrinsic = icm.compute_intrinsic_reward(state, action, next_state)\n",
    "            ep_intrinsic.append(intrinsic)\n",
    "            \n",
    "            # 更新模型\n",
    "            stats = icm.update(state, action, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        history['intrinsic_rewards'].append(np.mean(ep_intrinsic))\n",
    "        history['forward_losses'].append(stats['forward_loss'])\n",
    "        history['inverse_losses'].append(stats['inverse_loss'])\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"开始ICM训练模拟...\")\n",
    "icm_history = simulate_icm_training(icm)\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化ICM训练过程\n",
    "def plot_icm_training(history):\n",
    "    \"\"\"绘制ICM训练曲线。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 内在奖励\n",
    "    axes[0].plot(history['intrinsic_rewards'], 'b-', alpha=0.8)\n",
    "    axes[0].set_xlabel('回合')\n",
    "    axes[0].set_ylabel('平均内在奖励')\n",
    "    axes[0].set_title('内在奖励变化')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 前向损失\n",
    "    axes[1].plot(history['forward_losses'], 'g-', alpha=0.8)\n",
    "    axes[1].set_xlabel('回合')\n",
    "    axes[1].set_ylabel('前向模型损失')\n",
    "    axes[1].set_title('前向预测损失')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 逆向损失\n",
    "    axes[2].plot(history['inverse_losses'], 'r-', alpha=0.8)\n",
    "    axes[2].set_xlabel('回合')\n",
    "    axes[2].set_ylabel('逆向模型损失')\n",
    "    axes[2].set_title('逆向预测损失')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"观察结果:\")\n",
    "    print(\"  - 内在奖励随训练下降 → 模型学会了预测熟悉的状态\")\n",
    "    print(\"  - 前向损失下降 → 世界模型在改进\")\n",
    "    print(\"  - 逆向损失下降 → 特征编码器在学习动作相关信息\")\n",
    "\n",
    "plot_icm_training(icm_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RND: 随机网络蒸馏\n",
    "\n",
    "### 4.1 核心思想\n",
    "\n",
    "RND使用一个**固定的随机网络**作为目标，训练预测网络去匹配它：\n",
    "\n",
    "$$r_i(s) = \\|f_{rand}(s) - \\hat{f}(s)\\|^2$$\n",
    "\n",
    "### 4.2 为什么有效？\n",
    "\n",
    "1. 预测网络只在**访问过的状态**上训练\n",
    "2. 新状态 → 预测误差高 → 高内在奖励\n",
    "3. 旧状态 → 预测误差低 → 低内在奖励\n",
    "\n",
    "### 4.3 RND vs ICM\n",
    "\n",
    "| 特性 | ICM | RND |\n",
    "|------|-----|-----|\n",
    "| 目标网络 | 学习的特征 | 固定随机 |\n",
    "| 逆向模型 | 需要 | 不需要 |\n",
    "| 实现复杂度 | 较高 | 较低 |\n",
    "| 随机噪声鲁棒性 | 较好 | 较差 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curiosity_driven import RandomNetworkDistillation\n",
    "\n",
    "# 创建RND\n",
    "rnd = RandomNetworkDistillation(\n",
    "    observation_dim=obs_dim,\n",
    "    feature_dim=32,\n",
    "    hidden_dims=(64, 32),\n",
    "    learning_rate=0.001,\n",
    "    intrinsic_reward_scale=0.01,\n",
    ")\n",
    "\n",
    "print(\"RND模块创建成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示RND的核心特性：新状态有高奖励\n",
    "def demonstrate_rnd_novelty():\n",
    "    \"\"\"演示RND对新颖性的响应。\"\"\"\n",
    "    \n",
    "    # 创建一个\"熟悉\"的状态区域\n",
    "    familiar_states = np.random.randn(100, obs_dim) * 0.5  # 集中在原点附近\n",
    "    \n",
    "    # 训练RND在熟悉状态上\n",
    "    print(\"训练RND在熟悉状态区域...\")\n",
    "    for _ in range(200):\n",
    "        for state in familiar_states:\n",
    "            rnd.update(state)\n",
    "    \n",
    "    # 测试不同区域的内在奖励\n",
    "    test_regions = {\n",
    "        '熟悉区域 (原点附近)': np.random.randn(20, obs_dim) * 0.5,\n",
    "        '边缘区域': np.random.randn(20, obs_dim) * 2.0,\n",
    "        '远离区域': np.random.randn(20, obs_dim) * 5.0,\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, states in test_regions.items():\n",
    "        rewards = [rnd.compute_intrinsic_reward(s) for s in states]\n",
    "        results[name] = np.mean(rewards)\n",
    "        print(f\"  {name}: 平均内在奖励 = {results[name]:.6f}\")\n",
    "    \n",
    "    # 可视化\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(results.keys(), results.values(), color=['green', 'orange', 'red'])\n",
    "    plt.ylabel('平均内在奖励')\n",
    "    plt.title('RND: 新颖状态获得更高奖励')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n关键洞察: 远离训练分布的状态获得更高的内在奖励！\")\n",
    "\n",
    "demonstrate_rnd_novelty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 基于计数的探索\n",
    "\n",
    "### 5.1 核心公式\n",
    "\n",
    "$$r_i(s) = \\frac{\\beta}{\\sqrt{N(s)}}$$\n",
    "\n",
    "其中 $N(s)$ 是状态 $s$ 的访问次数。\n",
    "\n",
    "### 5.2 优缺点\n",
    "\n",
    "**优点**：\n",
    "- 理论基础扎实（UCB bounds）\n",
    "- 计算简单 O(1)\n",
    "- 对随机环境鲁棒\n",
    "\n",
    "**缺点**：\n",
    "- 需要状态离散化\n",
    "- 连续空间维度灾难"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curiosity_driven import CountBasedExploration\n",
    "\n",
    "# 创建计数探索器\n",
    "counter = CountBasedExploration(\n",
    "    state_discretization=10,  # 每个维度10个bin\n",
    "    bonus_coefficient=1.0,\n",
    "    intrinsic_reward_scale=0.1,\n",
    ")\n",
    "\n",
    "# 模拟探索过程\n",
    "print(\"模拟基于计数的探索...\\n\")\n",
    "\n",
    "# 重复访问同一区域\n",
    "repeated_state = np.array([0.5, 0.5])\n",
    "rewards_over_visits = []\n",
    "\n",
    "for i in range(10):\n",
    "    reward = counter.compute_intrinsic_reward(repeated_state)\n",
    "    rewards_over_visits.append(reward)\n",
    "    counter.update(repeated_state)\n",
    "    print(f\"访问 {i+1}: N(s)={counter.get_count(repeated_state)}, r_i={reward:.4f}\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, 11), rewards_over_visits, 'bo-', markersize=8)\n",
    "plt.xlabel('访问次数')\n",
    "plt.ylabel('内在奖励')\n",
    "plt.title('基于计数的探索: 重复访问奖励递减')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 实验对比与分析\n",
    "\n",
    "### 6.1 不同方法在GridWorld上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的GridWorld环境\n",
    "class SimpleGridWorld:\n",
    "    \"\"\"简单的GridWorld用于测试探索算法。\"\"\"\n",
    "    \n",
    "    def __init__(self, size=10):\n",
    "        self.size = size\n",
    "        self.state = np.array([0, 0])\n",
    "        self.goal = np.array([size-1, size-1])\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.array([0, 0])\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # 返回one-hot编码的位置\n",
    "        obs = np.zeros(self.size * self.size)\n",
    "        idx = self.state[0] * self.size + self.state[1]\n",
    "        obs[idx] = 1.0\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 动作: 0=上, 1=下, 2=左, 3=右\n",
    "        moves = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "        new_state = self.state + np.array(moves[action])\n",
    "        new_state = np.clip(new_state, 0, self.size - 1)\n",
    "        self.state = new_state\n",
    "        \n",
    "        done = np.array_equal(self.state, self.goal)\n",
    "        reward = 1.0 if done else 0.0  # 稀疏奖励\n",
    "        \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "# 测试环境\n",
    "env = SimpleGridWorld(size=5)\n",
    "print(f\"环境状态空间大小: {env.size * env.size}\")\n",
    "print(f\"目标位置: {env.goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_exploration_methods(env, n_episodes=100, max_steps=50):\n",
    "    \"\"\"对比不同探索方法的覆盖率。\"\"\"\n",
    "    \n",
    "    obs_dim = env.size * env.size\n",
    "    action_dim = 4\n",
    "    \n",
    "    # 方法1: 随机探索\n",
    "    random_coverage = set()\n",
    "    \n",
    "    # 方法2: ICM探索\n",
    "    icm = IntrinsicCuriosityModule(\n",
    "        observation_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        config=CuriosityConfig(intrinsic_reward_scale=0.1),\n",
    "    )\n",
    "    icm_coverage = set()\n",
    "    \n",
    "    # 方法3: 计数探索\n",
    "    counter = CountBasedExploration(\n",
    "        state_discretization=env.size,\n",
    "        intrinsic_reward_scale=0.1,\n",
    "    )\n",
    "    count_coverage = set()\n",
    "    \n",
    "    # 运行实验\n",
    "    for ep in range(n_episodes):\n",
    "        # 随机探索\n",
    "        state = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            action = np.random.randint(0, action_dim)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            random_coverage.add(tuple(env.state))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # ICM探索（偏向高内在奖励的动作）\n",
    "        state = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            # 简化：随机动作但更新ICM\n",
    "            action = np.random.randint(0, action_dim)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            icm.compute_intrinsic_reward(state, np.array([action]), next_state)\n",
    "            icm_coverage.add(tuple(env.state))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 计数探索\n",
    "        state = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            action = np.random.randint(0, action_dim)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            counter.update(env.state.astype(float) / env.size)  # 归一化\n",
    "            count_coverage.add(tuple(env.state))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    total_states = env.size * env.size\n",
    "    results = {\n",
    "        '随机探索': len(random_coverage) / total_states,\n",
    "        'ICM探索': len(icm_coverage) / total_states,\n",
    "        '计数探索': len(count_coverage) / total_states,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"运行探索方法对比实验...\")\n",
    "coverage_results = compare_exploration_methods(env)\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(coverage_results.keys(), coverage_results.values(),\n",
    "               color=['blue', 'green', 'orange'])\n",
    "plt.ylabel('状态覆盖率')\n",
    "plt.title('不同探索方法的状态空间覆盖率对比')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "for bar, val in zip(bars, coverage_results.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{val:.1%}', ha='center')\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **内在动机**解决稀疏奖励问题，通过生成额外的学习信号\n",
    "\n",
    "2. **ICM**使用预测误差作为新颖性度量，逆向模型确保特征与动作相关\n",
    "\n",
    "3. **RND**更简单，使用固定随机网络作为目标，但对随机噪声敏感\n",
    "\n",
    "4. **计数探索**理论基础扎实，但需要状态离散化\n",
    "\n",
    "### 方法选择指南\n",
    "\n",
    "| 场景 | 推荐方法 |\n",
    "|------|----------|\n",
    "| 视觉观测（图像）| ICM |\n",
    "| 低维状态空间 | 计数探索 |\n",
    "| 快速原型验证 | RND |\n",
    "| 随机环境 | ICM 或 集成方法 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印总结\n",
    "summary = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║                    好奇心驱动探索 - 核心总结                         ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                      ║\n",
    "║  内在奖励公式：                                                      ║\n",
    "║    r_total = r_extrinsic + β · r_intrinsic                          ║\n",
    "║                                                                      ║\n",
    "║  ICM内在奖励：                                                       ║\n",
    "║    r_i = η · ||f(s') - f̂(s, a)||²                                  ║\n",
    "║                                                                      ║\n",
    "║  RND内在奖励：                                                       ║\n",
    "║    r_i = ||f_rand(s) - f̂(s)||²                                     ║\n",
    "║                                                                      ║\n",
    "║  计数探索奖励：                                                      ║\n",
    "║    r_i = β / √N(s)                                                  ║\n",
    "║                                                                      ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
