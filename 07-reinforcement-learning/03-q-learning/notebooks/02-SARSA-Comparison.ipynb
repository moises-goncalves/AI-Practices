{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA 与 Q-Learning 对比\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将掌握：\n",
    "- SARSA 算法的原理与实现\n",
    "- On-Policy 与 Off-Policy 的本质区别\n",
    "- Expected SARSA 的方差减少机制\n",
    "- 悬崖行走环境中的行为差异分析\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- Q-Learning 算法基础\n",
    "- 时序差分学习概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第一部分：On-Policy vs Off-Policy\n",
    "\n",
    "### 1.1 基本概念\n",
    "\n",
    "| 术语 | 定义 |\n",
    "|------|------|\n",
    "| **行为策略** (Behavior Policy) | 实际用来与环境交互的策略 |\n",
    "| **目标策略** (Target Policy) | 正在学习和优化的策略 |\n",
    "\n",
    "### 1.2 Off-Policy: Q-Learning\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\max_a Q(S',a) - Q(S,A)]$$\n",
    "\n",
    "- 使用 **max** 选择下一状态的最优动作\n",
    "- 不管实际采取什么动作\n",
    "- 行为策略 ≠ 目标策略\n",
    "\n",
    "### 1.3 On-Policy: SARSA\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma Q(S',A') - Q(S,A)]$$\n",
    "\n",
    "- 使用**实际采取的动作** $A'$\n",
    "- 行为策略 = 目标策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：SARSA 算法\n",
    "\n",
    "### 2.1 算法名称来源\n",
    "\n",
    "**S**tate-**A**ction-**R**eward-**S**tate-**A**ction\n",
    "\n",
    "更新需要的五元组：$(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "\n",
    "### 2.2 算法伪代码\n",
    "\n",
    "```\n",
    "算法: SARSA\n",
    "\n",
    "1. 初始化 Q(s, a) = 0\n",
    "2. 对于每个回合:\n",
    "   a. 初始化状态 S\n",
    "   b. 使用策略选择动作 A  ← 关键：在循环外先选择\n",
    "   c. 重复:\n",
    "      i.   执行 A，观察 R, S'\n",
    "      ii.  使用策略选择 A'  ← 关键：更新前先选择下一动作\n",
    "      iii. Q(S, A) ← Q(S, A) + α[R + γ Q(S', A') - Q(S, A)]\n",
    "      iv.  S ← S', A ← A'  ← 关键：动作传递\n",
    "   d. 直到终止\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "print(\"库导入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\"悬崖行走环境\"\"\"\n",
    "    ACTIONS = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "    \n",
    "    def __init__(self, height=4, width=12):\n",
    "        self.height, self.width = height, width\n",
    "        self.start = (height - 1, 0)\n",
    "        self.goal = (height - 1, width - 1)\n",
    "        self.cliff = [(height - 1, j) for j in range(1, width - 1)]\n",
    "        self.state = self.start\n",
    "        self.n_actions = 4\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        di, dj = self.ACTIONS[action]\n",
    "        new_i = int(np.clip(self.state[0] + di, 0, self.height - 1))\n",
    "        new_j = int(np.clip(self.state[1] + dj, 0, self.width - 1))\n",
    "        next_state = (new_i, new_j)\n",
    "        \n",
    "        if next_state in self.cliff:\n",
    "            self.state = self.start\n",
    "            return self.state, -100.0, False\n",
    "        \n",
    "        self.state = next_state\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 0.0, True\n",
    "        return self.state, -1.0, False\n",
    "    \n",
    "    def render(self, path=None):\n",
    "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        for pos in self.cliff: grid[pos[0]][pos[1]] = 'C'\n",
    "        grid[self.start[0]][self.start[1]] = 'S'\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'\n",
    "        if path:\n",
    "            for pos in path[1:-1]:\n",
    "                if pos not in self.cliff and pos != self.start and pos != self.goal:\n",
    "                    grid[pos[0]][pos[1]] = '*'\n",
    "        print(\"┌\" + \"─\" * (self.width * 2 + 1) + \"┐\")\n",
    "        for row in grid: print(\"│ \" + \" \".join(row) + \" │\")\n",
    "        print(\"└\" + \"─\" * (self.width * 2 + 1) + \"┘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()\n",
    "print(\"悬崖行走环境:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SARSA 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"\n",
    "    SARSA 智能体 (On-Policy TD Control)\n",
    "    \n",
    "    与 Q-Learning 的关键区别:\n",
    "    - 更新使用实际采取的下一个动作 A'\n",
    "    - 学习当前策略的价值函数，而非最优策略\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"ε-greedy 动作选择\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        return np.random.choice(np.where(np.isclose(q_values, np.max(q_values)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"\n",
    "        SARSA 更新规则\n",
    "        \n",
    "        Q(S,A) ← Q(S,A) + α[R + γ Q(S',A') - Q(S,A)]\n",
    "        \n",
    "        注意：需要 next_action 参数\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        target = reward if done else reward + self.gamma * self.q_table[next_state][next_action]\n",
    "        self.q_table[state][action] += self.lr * (target - current_q)\n",
    "        return target - current_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# 组装类\n",
    "SARSAAgent.get_action = get_action\n",
    "SARSAAgent.update = update\n",
    "SARSAAgent.decay_epsilon = decay_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Q-Learning 智能体 (对比用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning 智能体 (Off-Policy)\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        return np.random.choice(np.where(np.isclose(q_values, np.max(q_values)))[0])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-Learning: 使用 max 而非实际动作\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        target = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.lr * (target - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, agent, episodes=500, max_steps=200, verbose=False):\n",
    "    \"\"\"训练 Q-Learning\"\"\"\n",
    "    history = {'rewards': [], 'steps': []}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward, steps = 0.0, 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state, total_reward, steps = next_state, total_reward + reward, steps + 1\n",
    "            if done: break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['steps'].append(steps)\n",
    "        \n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            print(f\"Q-Learning Episode {episode+1}: Avg = {np.mean(history['rewards'][-100:]):.2f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env, agent, episodes=500, max_steps=200, verbose=False):\n",
    "    \"\"\"训练 SARSA - 注意动作选择的时机\"\"\"\n",
    "    history = {'rewards': [], 'steps': []}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        action = agent.get_action(state, training=True)  # 关键：先选择初始动作\n",
    "        total_reward, steps = 0.0, 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = agent.get_action(next_state, training=True)  # 关键：更新前选择下一动作\n",
    "            agent.update(state, action, reward, next_state, next_action, done)\n",
    "            state, action = next_state, next_action  # 关键：动作传递\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if done: break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['steps'].append(steps)\n",
    "        \n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            print(f\"SARSA Episode {episode+1}: Avg = {np.mean(history['rewards'][-100:]):.2f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：对比实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"悬崖行走: Q-Learning vs SARSA 对比实验\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "EPISODES = 500\n",
    "LEARNING_RATE = 0.5\n",
    "EPSILON = 0.1  # 固定探索率\n",
    "\n",
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 Q-Learning\n",
    "print(\"\\n训练 Q-Learning...\")\n",
    "q_agent = QLearningAgent(n_actions=4, learning_rate=LEARNING_RATE, epsilon=EPSILON,\n",
    "                          epsilon_decay=1.0, epsilon_min=EPSILON)\n",
    "q_history = train_q_learning(env, q_agent, episodes=EPISODES, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 SARSA\n",
    "print(\"\\n训练 SARSA...\")\n",
    "sarsa_agent = SARSAAgent(n_actions=4, learning_rate=LEARNING_RATE, epsilon=EPSILON,\n",
    "                          epsilon_decay=1.0, epsilon_min=EPSILON)\n",
    "sarsa_history = train_sarsa(env, sarsa_agent, episodes=EPISODES, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 学习曲线对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(q_history, sarsa_history, window=10):\n",
    "    \"\"\"绘制对比曲线\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 奖励曲线\n",
    "    q_smooth = np.convolve(q_history['rewards'], np.ones(window)/window, mode='valid')\n",
    "    sarsa_smooth = np.convolve(sarsa_history['rewards'], np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[0].plot(q_smooth, label='Q-Learning', color='blue', alpha=0.8)\n",
    "    axes[0].plot(sarsa_smooth, label='SARSA', color='red', alpha=0.8)\n",
    "    axes[0].axhline(y=-13, color='green', linestyle='--', alpha=0.5, label='最优 (-13)')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title('学习曲线对比')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 步数曲线\n",
    "    q_steps = np.convolve(q_history['steps'], np.ones(window)/window, mode='valid')\n",
    "    sarsa_steps = np.convolve(sarsa_history['steps'], np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[1].plot(q_steps, label='Q-Learning', color='blue', alpha=0.8)\n",
    "    axes[1].plot(sarsa_steps, label='SARSA', color='red', alpha=0.8)\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('回合步数对比')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(q_history, sarsa_history)\n",
    "\n",
    "print(\"\\n最后100回合统计:\")\n",
    "print(f\"Q-Learning: 平均奖励 = {np.mean(q_history['rewards'][-100:]):.2f}\")\n",
    "print(f\"SARSA:      平均奖励 = {np.mean(sarsa_history['rewards'][-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 策略路径对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(agent, env, max_steps=50):\n",
    "    \"\"\"提取贪心策略路径\"\"\"\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_action(state, training=False)\n",
    "        next_state, _, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        if done: break\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"学到的策略路径\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nQ-Learning (倾向沿悬崖边的最短路径):\")\n",
    "q_path = extract_path(q_agent, env)\n",
    "env.reset()\n",
    "env.render(q_path)\n",
    "print(f\"路径长度: {len(q_path) - 1} 步\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSARSA (倾向远离悬崖的安全路径):\")\n",
    "sarsa_path = extract_path(sarsa_agent, env)\n",
    "env.reset()\n",
    "env.render(sarsa_path)\n",
    "print(f\"路径长度: {len(sarsa_path) - 1} 步\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第五部分：行为差异分析\n",
    "\n",
    "### 5.1 为什么 Q-Learning 选择悬崖边？\n",
    "\n",
    "Q-Learning 更新使用 $\\max$，学习**最优策略的价值**：\n",
    "\n",
    "- 假设执行最优策略，不会掉入悬崖\n",
    "- 沿悬崖边的路径最短，奖励最高\n",
    "- 但训练时的 ε-greedy 探索会导致掉崖\n",
    "\n",
    "### 5.2 为什么 SARSA 选择安全路径？\n",
    "\n",
    "SARSA 使用实际动作，学习**当前策略的价值**：\n",
    "\n",
    "- 考虑到探索时可能随机选择动作\n",
    "- 靠近悬崖时，探索可能导致掉落\n",
    "- 因此远离悬崖的路径价值更高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_values(q_agent, sarsa_agent, env):\n",
    "    \"\"\"对比价值函数\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for idx, (agent, name) in enumerate([(q_agent, 'Q-Learning'), (sarsa_agent, 'SARSA')]):\n",
    "        v_table = np.zeros((env.height, env.width))\n",
    "        for i in range(env.height):\n",
    "            for j in range(env.width):\n",
    "                if (i, j) in agent.q_table:\n",
    "                    v_table[i, j] = np.max(agent.q_table[(i, j)])\n",
    "        \n",
    "        im = axes[idx].imshow(v_table, cmap='RdYlGn', aspect='auto')\n",
    "        axes[idx].set_title(f'{name} 价值函数 V(s)')\n",
    "        plt.colorbar(im, ax=axes[idx])\n",
    "        \n",
    "        for pos in env.cliff:\n",
    "            axes[idx].add_patch(plt.Rectangle((pos[1]-0.5, pos[0]-0.5), 1, 1,\n",
    "                                               fill=True, color='black', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_values(q_agent, sarsa_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第六部分：Expected SARSA\n",
    "\n",
    "### 6.1 算法原理\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R + \\gamma \\mathbb{E}_\\pi[Q(S',A')] - Q(S,A) \\right]$$\n",
    "\n",
    "对于 ε-greedy:\n",
    "\n",
    "$$\\mathbb{E}[Q(S',A')] = \\frac{\\epsilon}{|A|} \\sum_a Q(S',a) + (1-\\epsilon) \\max_a Q(S',a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSARSAAgent:\n",
    "    \"\"\"Expected SARSA 智能体\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        return np.random.choice(np.where(np.isclose(q_values, np.max(q_values)))[0])\n",
    "    \n",
    "    def _get_expected_q(self, state):\n",
    "        \"\"\"计算期望 Q 值\"\"\"\n",
    "        q_values = self.q_table[state]\n",
    "        probs = np.ones(self.n_actions) * self.epsilon / self.n_actions\n",
    "        probs[np.argmax(q_values)] += 1 - self.epsilon\n",
    "        return np.dot(probs, q_values)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        current_q = self.q_table[state][action]\n",
    "        target = reward if done else reward + self.gamma * self._get_expected_q(next_state)\n",
    "        self.q_table[state][action] += self.lr * (target - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n训练 Expected SARSA...\")\n",
    "exp_sarsa = ExpectedSARSAAgent(n_actions=4, learning_rate=LEARNING_RATE,\n",
    "                                epsilon=EPSILON, epsilon_decay=1.0, epsilon_min=EPSILON)\n",
    "exp_sarsa_history = train_q_learning(env, exp_sarsa, episodes=EPISODES, verbose=True)\n",
    "\n",
    "print(\"\\nExpected SARSA 学到的路径:\")\n",
    "exp_sarsa_path = extract_path(exp_sarsa, env)\n",
    "env.reset()\n",
    "env.render(exp_sarsa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 特性 | Q-Learning | SARSA | Expected SARSA |\n",
    "|------|------------|-------|----------------|\n",
    "| 类型 | Off-Policy | On-Policy | On-Policy |\n",
    "| 更新目标 | $\\max_a Q(S',a)$ | $Q(S',A')$ | $\\mathbb{E}[Q(S',A')]$ |\n",
    "| 方差 | 低 | 高 | 低 |\n",
    "| 安全性 | 激进 | 保守 | 中等 |\n",
    "\n",
    "### 选择建议\n",
    "\n",
    "- **Q-Learning**: 追求最优性能\n",
    "- **SARSA**: 需要安全探索（机器人控制）\n",
    "- **Expected SARSA**: 平衡方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    \n",
    "    # 测试1: SARSA 更新\n",
    "    agent = SARSAAgent(n_actions=4, learning_rate=0.5, discount_factor=0.9)\n",
    "    agent.q_table[(0,1)] = np.array([1.0, 2.0, 0.0, 0.0])\n",
    "    agent.update((0,0), 0, -1.0, (0,1), 1, False)\n",
    "    expected = 0.5 * (-1 + 0.9 * 2.0)  # 0.4\n",
    "    assert np.isclose(agent.q_table[(0,0)][0], expected), \"SARSA更新错误\"\n",
    "    print(\"✓ 测试1: SARSA 更新正确\")\n",
    "    passed += 1\n",
    "    \n",
    "    # 测试2: Expected SARSA 期望计算\n",
    "    agent = ExpectedSARSAAgent(n_actions=4, epsilon=0.2)\n",
    "    agent.q_table[(0,0)] = np.array([1.0, 2.0, 0.5, 0.5])\n",
    "    expected_q = agent._get_expected_q((0,0))\n",
    "    manual = 0.05*1.0 + 0.85*2.0 + 0.05*0.5 + 0.05*0.5  # 1.8\n",
    "    assert np.isclose(expected_q, manual), f\"期望计算错误: {expected_q}\"\n",
    "    print(\"✓ 测试2: Expected SARSA 期望计算正确\")\n",
    "    passed += 1\n",
    "    \n",
    "    print(f\"\\n全部 {passed} 项测试通过!\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Rummery & Niranjan (1994). On-Line Q-Learning Using Connectionist Systems.\n",
    "2. Sutton & Barto (2018). Reinforcement Learning: An Introduction, Chapter 6.\n",
    "3. Van Seijen et al. (2009). A Theoretical and Empirical Analysis of Expected Sarsa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
