# 神经网络策略 - 核心知识点总结

## 目录
1. [策略梯度方法概述](#1-策略梯度方法概述)
2. [核心数学原理](#2-核心数学原理)
3. [关键算法详解](#3-关键算法详解)
4. [优势估计方法](#4-优势估计方法)
5. [实践技巧与调参指南](#5-实践技巧与调参指南)
6. [常见问题与解决方案](#6-常见问题与解决方案)
7. [算法对比与选择](#7-算法对比与选择)

---

## 1. 策略梯度方法概述

### 1.1 核心思想

**策略梯度方法直接参数化策略 π_θ(a|s)，通过梯度上升优化期望回报。**

与价值方法的对比：

| 特性 | 价值方法 (DQN) | 策略方法 (PG) |
|------|----------------|---------------|
| 学习目标 | Q(s,a) 或 V(s) | π_θ(a\|s) |
| 动作选择 | argmax Q(s,a) | 从 π 采样 |
| 动作空间 | 离散（标准） | 离散/连续 |
| 策略类型 | 确定性 | 随机 |
| 收敛性 | 可能震荡 | 更平滑 |

### 1.2 策略参数化

**离散动作空间 - Softmax策略：**
```
π_θ(a|s) = exp(f_θ(s)_a) / Σ_a' exp(f_θ(s)_a')
```

**连续动作空间 - 高斯策略：**
```
π_θ(a|s) = N(a | μ_θ(s), σ_θ(s)²)
```

**有界连续动作 - Squashed Gaussian：**
```
u ~ N(μ_θ(s), σ_θ(s)²)
a = tanh(u) ∈ (-1, 1)
```

---

## 2. 核心数学原理

### 2.1 策略梯度定理

**目标函数：**
```
J(θ) = E_{τ~π_θ}[R(τ)] = E_{τ~π_θ}[Σ_t γ^t r_t]
```

**策略梯度定理 (Sutton et al., 1999)：**
```
∇_θ J(θ) = E_{π_θ}[Σ_t ∇_θ log π_θ(a_t|s_t) · Q^π(s_t, a_t)]
```

**关键洞察：**
- `∇_θ log π_θ(a|s)` 指向增加 π(a|s) 的方向
- Q^π(s,a) > 0 时增加该动作概率
- Q^π(s,a) < 0 时减少该动作概率

### 2.2 Log-Derivative Trick

**核心公式：**
```
∇_θ π_θ(a|s) = π_θ(a|s) · ∇_θ log π_θ(a|s)
```

**作用：** 将对概率的梯度转换为对log概率的梯度，使得可以通过采样估计梯度。

### 2.3 优势函数

**定义：**
```
A^π(s,a) = Q^π(s,a) - V^π(s)
```

**直觉：** 衡量动作相对于平均水平的好坏程度
- A > 0: 动作优于平均
- A < 0: 动作劣于平均
- A = 0: 动作等于平均

---

## 3. 关键算法详解

### 3.1 REINFORCE

**核心思想：** 使用蒙特卡洛回报 G_t 作为 Q^π 的无偏估计

**梯度估计：**
```
∇_θ J(θ) ≈ (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · G_t
```

**回报计算：**
```
G_t = Σ_{k=0}^{T-t} γ^k r_{t+k}
```

**特点：**
- ✅ 无偏估计
- ✅ 实现简单
- ❌ 高方差
- ❌ 需要完整episode

### 3.2 Actor-Critic (A2C)

**核心思想：** 用学习的价值函数 V(s) 作为基线减少方差

**网络架构：**
```
Actor: π_θ(a|s) - 输出动作分布
Critic: V_φ(s) - 估计状态价值
```

**损失函数：**
```
L = L_policy + c_v · L_value - c_ent · H(π)

L_policy = -E[log π(a|s) · A]
L_value = E[(V(s) - V_target)²]
H(π) = -E[π log π]
```

**优势估计：**
```
A_t ≈ r_t + γV(s_{t+1}) - V(s_t) = δ_t (TD误差)
```

### 3.3 PPO (Proximal Policy Optimization)

**核心思想：** 通过裁剪目标限制策略更新幅度，允许多epoch重用数据

**裁剪目标：**
```
L^CLIP = E[min(r_t · A_t, clip(r_t, 1-ε, 1+ε) · A_t)]

其中 r_t = π_θ(a|s) / π_{θ_old}(a|s)
```

**裁剪机制分析：**
- 当 A > 0 且 r > 1+ε: 使用裁剪值，停止增加概率
- 当 A < 0 且 r < 1-ε: 使用裁剪值，停止减少概率

**关键超参数：**
| 参数 | 典型值 | 作用 |
|------|--------|------|
| clip_epsilon | 0.2 | 裁剪范围 |
| n_epochs | 10 | 每次rollout的更新次数 |
| batch_size | 64 | 小批量大小 |
| gae_lambda | 0.95 | GAE参数 |

### 3.4 TRPO (Trust Region Policy Optimization)

**核心思想：** 在KL散度约束下最大化目标

**优化问题：**
```
max_θ E[r_t · A_t]
s.t. E[KL(π_old || π_new)] ≤ δ
```

**求解方法：**
1. 计算策略梯度
2. 使用共轭梯度求解自然梯度方向
3. 线搜索确定步长

**特点：**
- ✅ 理论保证单调改进
- ❌ 实现复杂
- ❌ 计算开销大

---

## 4. 优势估计方法

### 4.1 方法对比

| 方法 | 公式 | 偏差 | 方差 |
|------|------|------|------|
| MC回报 | G_t - V(s_t) | 无 | 高 |
| TD(0) | δ_t = r + γV(s') - V(s) | 高 | 低 |
| n-step | Σγ^k r_{t+k} + γ^n V(s_{t+n}) - V(s_t) | 中 | 中 |
| GAE | Σ(γλ)^l δ_{t+l} | 可调 | 可调 |

### 4.2 广义优势估计 (GAE)

**公式：**
```
A_t^GAE(γ,λ) = Σ_{l=0}^∞ (γλ)^l δ_{t+l}
```

**递归计算：**
```
A_t = δ_t + γλ · A_{t+1}
```

**λ的影响：**
- λ = 0: 等于TD(0)，高偏差低方差
- λ = 1: 等于MC，无偏差高方差
- λ = 0.95: 推荐默认值，平衡偏差方差

### 4.3 回报目标

**用于训练Critic：**
```
V_target = A^GAE + V(s)
```

---

## 5. 实践技巧与调参指南

### 5.1 超参数调优

**学习率：**
```
策略学习率: 1e-4 ~ 3e-4 (推荐3e-4)
价值学习率: 可以稍大，1e-3
```

**折扣因子 γ：**
```
短期任务: 0.95
长期任务: 0.99 ~ 0.999
```

**GAE λ：**
```
价值函数准确时: 0.9
价值函数不准确时: 0.95 ~ 0.99
```

**熵系数：**
```
离散动作: 0.01 ~ 0.05
连续动作: 0.001 ~ 0.01
可以随训练衰减
```

### 5.2 训练稳定性技巧

1. **优势标准化：**
```python
advantages = (advantages - mean) / (std + 1e-8)
```

2. **梯度裁剪：**
```python
torch.nn.utils.clip_grad_norm_(params, max_norm=0.5)
```

3. **学习率衰减：**
```python
lr = lr_init * (1 - progress)
```

4. **正交初始化：**
```python
nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
```

### 5.3 常见问题诊断

| 现象 | 可能原因 | 解决方案 |
|------|----------|----------|
| 奖励不增长 | 学习率太小/太大 | 调整学习率 |
| 奖励震荡 | 更新步长太大 | 减小clip_epsilon |
| 策略崩溃 | 熵太低 | 增加熵系数 |
| 价值损失不降 | 学习率不匹配 | 调整value_coef |

---

## 6. 常见问题与解决方案

### Q1: 为什么REINFORCE需要完整episode?

**答：** 因为需要计算蒙特卡洛回报 G_t = Σγ^k r_{t+k}，这需要知道从当前时刻到episode结束的所有奖励。

### Q2: 基线为什么不改变梯度期望?

**答：**
```
E[∇log π(a|s) · b(s)] = b(s) · E[∇log π(a|s)]
                      = b(s) · ∇Σ_a π(a|s)
                      = b(s) · ∇1
                      = 0
```

### Q3: PPO的裁剪如何防止策略崩溃?

**答：** 裁剪限制了概率比率 r_t 的范围在 [1-ε, 1+ε]，这意味着新策略与旧策略的差异被限制在一定范围内，防止单次更新导致策略剧烈变化。

### Q4: 共享网络 vs 分离网络?

**答：**
- **共享网络：** 参数效率高，特征复用，但可能存在梯度干扰
- **分离网络：** 更稳定，可独立调参，但参数量翻倍

推荐：简单任务用共享，复杂任务用分离

### Q5: 如何选择GAE的λ?

**答：**
- 如果价值函数估计准确，用较小的λ（如0.9）减少方差
- 如果价值函数估计不准确，用较大的λ（如0.95-0.99）减少偏差
- 默认推荐0.95

---

## 7. 算法对比与选择

### 7.1 算法特性对比

| 算法 | 样本效率 | 稳定性 | 实现复杂度 | 适用场景 |
|------|----------|--------|------------|----------|
| REINFORCE | 低 | 中 | 简单 | 教学、简单任务 |
| A2C | 中 | 好 | 简单 | 中等任务 |
| PPO | 高 | 优秀 | 简单 | 生产环境 |
| TRPO | 高 | 优秀 | 复杂 | 研究 |

### 7.2 选择建议

```
新手入门 → REINFORCE
快速原型 → A2C
生产部署 → PPO
理论研究 → TRPO
```

### 7.3 环境适配

| 环境类型 | 推荐算法 | 关键配置 |
|----------|----------|----------|
| 离散简单 (CartPole) | PPO | hidden=[64,64], lr=3e-4 |
| 离散复杂 (Atari) | PPO | CNN backbone, frame stack |
| 连续简单 (Pendulum) | PPO | hidden=[64,64], entropy=0.001 |
| 连续复杂 (MuJoCo) | PPO | hidden=[256,256], normalize_obs |

---

## 快速参考卡片

### 核心公式

```
策略梯度: ∇J = E[∇log π · A]
TD误差:   δ = r + γV(s') - V(s)
GAE:      A = Σ(γλ)^l δ_{t+l}
PPO:      L = min(r·A, clip(r)·A)
```

### 默认超参数

```python
config = {
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'lr': 3e-4,
    'clip_epsilon': 0.2,
    'entropy_coef': 0.01,
    'value_coef': 0.5,
    'max_grad_norm': 0.5,
    'n_epochs': 10,
    'batch_size': 64,
}
```

### 调试检查清单

- [ ] 奖励是否正确归一化?
- [ ] 优势是否标准化?
- [ ] 梯度是否裁剪?
- [ ] 学习率是否合适?
- [ ] 熵是否在合理范围?
- [ ] 价值损失是否在下降?
