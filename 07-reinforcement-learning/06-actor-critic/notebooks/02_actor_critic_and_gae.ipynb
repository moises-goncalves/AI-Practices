{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic与广义优势估计 (GAE)\n",
    "\n",
    "本教程深入讲解Actor-Critic架构和GAE的数学原理。\n",
    "\n",
    "## 目录\n",
    "1. [从REINFORCE到Actor-Critic](#1-从reinforce到actor-critic)\n",
    "2. [TD误差与优势估计](#2-td误差与优势估计)\n",
    "3. [广义优势估计GAE](#3-广义优势估计gae)\n",
    "4. [A2C算法实现](#4-a2c算法实现)\n",
    "5. [实践练习](#5-实践练习)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 从REINFORCE到Actor-Critic\n",
    "\n",
    "### 1.1 REINFORCE的局限\n",
    "\n",
    "**问题1: 高方差**\n",
    "- MC回报 $G_t$ 包含整个轨迹的随机性\n",
    "- 方差随轨迹长度增加\n",
    "\n",
    "**问题2: 必须等待episode结束**\n",
    "- 无法在线学习\n",
    "- 长episode效率低\n",
    "\n",
    "### 1.2 Actor-Critic的解决方案\n",
    "\n",
    "**核心思想**: 用学习的价值函数 $V(s)$ 替代MC回报\n",
    "\n",
    "- **Actor**: 策略网络 $\\pi_\\theta(a|s)$\n",
    "- **Critic**: 价值网络 $V_\\phi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TD误差与优势估计\n",
    "\n",
    "### 2.1 时序差分(TD)误差\n",
    "\n",
    "**TD误差定义**:\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "**直觉理解**:\n",
    "- $r_t + \\gamma V(s_{t+1})$: 实际获得的奖励 + 下一状态的估计价值\n",
    "- $V(s_t)$: 当前状态的估计价值\n",
    "- $\\delta_t > 0$: 实际比预期好\n",
    "- $\\delta_t < 0$: 实际比预期差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_error(reward, value, next_value, done, gamma=0.99):\n",
    "    \"\"\"\n",
    "    计算TD误差\n",
    "    \n",
    "    δ_t = r_t + γ(1-done)V(s_{t+1}) - V(s_t)\n",
    "    \"\"\"\n",
    "    if done:\n",
    "        return reward - value\n",
    "    return reward + gamma * next_value - value\n",
    "\n",
    "# 演示\n",
    "print(\"场景1: 获得奖励1, V(s)=5, V(s')=5\")\n",
    "delta1 = compute_td_error(1, 5, 5, False)\n",
    "print(f\"TD误差: {delta1:.4f} (略好于预期)\")\n",
    "\n",
    "print(\"\\n场景2: 获得奖励10, V(s)=5, V(s')=5\")\n",
    "delta2 = compute_td_error(10, 5, 5, False)\n",
    "print(f\"TD误差: {delta2:.4f} (远好于预期)\")\n",
    "\n",
    "print(\"\\n场景3: 获得奖励-5, V(s)=5, V(s')=5\")\n",
    "delta3 = compute_td_error(-5, 5, 5, False)\n",
    "print(f\"TD误差: {delta3:.4f} (差于预期)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TD误差作为优势估计\n",
    "\n",
    "**关键洞察**: TD误差是优势函数的有偏估计\n",
    "\n",
    "$$\\mathbb{E}[\\delta_t | s_t, a_t] = \\mathbb{E}[r_t + \\gamma V(s_{t+1}) | s_t, a_t] - V(s_t)$$\n",
    "\n",
    "如果 $V = V^\\pi$ (真实价值函数):\n",
    "$$\\mathbb{E}[\\delta_t | s_t, a_t] = Q^\\pi(s_t, a_t) - V^\\pi(s_t) = A^\\pi(s_t, a_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 广义优势估计 (GAE)\n",
    "\n",
    "### 3.1 偏差-方差权衡\n",
    "\n",
    "| 方法 | 公式 | 偏差 | 方差 |\n",
    "|------|------|------|------|\n",
    "| TD(0) | $\\delta_t$ | 高 | 低 |\n",
    "| MC | $G_t - V(s_t)$ | 无 | 高 |\n",
    "| n-step | $\\sum_{k=0}^{n-1}\\gamma^k r_{t+k} + \\gamma^n V(s_{t+n}) - V(s_t)$ | 中 | 中 |\n",
    "\n",
    "### 3.2 GAE公式\n",
    "\n",
    "**GAE是TD误差的指数加权平均**:\n",
    "\n",
    "$$A_t^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "**递归形式** (高效计算):\n",
    "$$A_t = \\delta_t + \\gamma\\lambda A_{t+1}$$\n",
    "\n",
    "**特殊情况**:\n",
    "- $\\lambda = 0$: $A_t = \\delta_t$ (TD(0))\n",
    "- $\\lambda = 1$: $A_t = G_t - V(s_t)$ (MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, next_value, dones, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"\n",
    "    计算广义优势估计 (GAE)\n",
    "    \n",
    "    数学公式:\n",
    "        δ_t = r_t + γ(1-done)V(s_{t+1}) - V(s_t)\n",
    "        A_t = δ_t + γλ(1-done)A_{t+1}\n",
    "    \n",
    "    使用反向递归高效计算\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = np.zeros(T)\n",
    "    gae = 0\n",
    "    \n",
    "    # 反向遍历\n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_val = next_value\n",
    "        else:\n",
    "            next_val = values[t + 1]\n",
    "        \n",
    "        # TD误差\n",
    "        delta = rewards[t] + gamma * (1 - dones[t]) * next_val - values[t]\n",
    "        \n",
    "        # GAE递归\n",
    "        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    # 回报 = 优势 + 价值\n",
    "    returns = advantages + np.array(values)\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示GAE计算\n",
    "rewards = [1, 1, 1, 1, 1]\n",
    "values = [4.5, 4.0, 3.5, 3.0, 2.5]\n",
    "dones = [0, 0, 0, 0, 1]\n",
    "next_value = 0\n",
    "\n",
    "advantages, returns = compute_gae(rewards, values, next_value, dones)\n",
    "\n",
    "print(\"奖励:\", rewards)\n",
    "print(\"价值估计:\", values)\n",
    "print(\"GAE优势:\", advantages.round(4))\n",
    "print(\"回报目标:\", returns.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化不同λ的效果\n",
    "def visualize_gae_lambda():\n",
    "    rewards = [1] * 20\n",
    "    values = list(np.linspace(10, 1, 20))\n",
    "    dones = [0] * 19 + [1]\n",
    "    \n",
    "    lambdas = [0.0, 0.5, 0.9, 0.95, 1.0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    for lam in lambdas:\n",
    "        adv, _ = compute_gae(rewards, values, 0, dones, gae_lambda=lam)\n",
    "        plt.plot(adv, label=f'λ={lam}', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Advantage')\n",
    "    plt.title('GAE with Different λ Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "visualize_gae_lambda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A2C算法实现\n",
    "\n",
    "### 4.1 网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    共享特征的Actor-Critic网络\n",
    "    \n",
    "    架构:\n",
    "        state -> [shared] -> features\n",
    "                               |\n",
    "                    +----------+----------+\n",
    "                    |                     |\n",
    "               [actor_head]          [critic_head]\n",
    "                    |                     |\n",
    "                 policy                 value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 共享特征层\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor头\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Critic头\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.shared(state)\n",
    "        logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return logits, value\n",
    "    \n",
    "    def get_action_and_value(self, state):\n",
    "        logits, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action, log_prob, entropy, value.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 损失函数\n",
    "\n",
    "**A2C总损失**:\n",
    "$$L = L_{policy} + c_v \\cdot L_{value} - c_{ent} \\cdot H(\\pi)$$\n",
    "\n",
    "其中:\n",
    "- $L_{policy} = -\\mathbb{E}[\\log \\pi(a|s) \\cdot A]$\n",
    "- $L_{value} = \\mathbb{E}[(V(s) - G)^2]$\n",
    "- $H(\\pi) = -\\mathbb{E}[\\pi \\log \\pi]$ (熵正则化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_loss(log_probs, values, advantages, returns, entropies,\n",
    "             value_coef=0.5, entropy_coef=0.01):\n",
    "    \"\"\"\n",
    "    计算A2C损失\n",
    "    \n",
    "    L = L_policy + c_v * L_value - c_ent * H(π)\n",
    "    \"\"\"\n",
    "    # 策略损失\n",
    "    policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "    \n",
    "    # 价值损失\n",
    "    value_loss = ((values - returns.detach()) ** 2).mean()\n",
    "    \n",
    "    # 熵损失 (负号因为我们要最大化熵)\n",
    "    entropy_loss = -entropies.mean()\n",
    "    \n",
    "    # 总损失\n",
    "    total_loss = policy_loss + value_coef * value_loss + entropy_coef * entropy_loss\n",
    "    \n",
    "    return total_loss, {\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'value_loss': value_loss.item(),\n",
    "        'entropy': -entropy_loss.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 实践练习\n",
    "\n",
    "### 练习1: 完整A2C训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def train_a2c(env_name='CartPole-v1', num_updates=200, n_steps=128,\n",
    "              gamma=0.99, gae_lambda=0.95, lr=3e-4):\n",
    "    \"\"\"\n",
    "    A2C训练循环\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    current_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    for update in range(num_updates):\n",
    "        # 收集n_steps的数据\n",
    "        states, actions, rewards, dones = [], [], [], []\n",
    "        log_probs, values, entropies = [], [], []\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action, log_prob, entropy, value = model.get_action_and_value(state_t)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action.item())\n",
    "            rewards.append(reward)\n",
    "            dones.append(float(done))\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value.item())\n",
    "            entropies.append(entropy)\n",
    "            \n",
    "            current_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(current_reward)\n",
    "                current_reward = 0\n",
    "                state, _ = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        # 计算bootstrap value\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            _, next_value = model(state_t)\n",
    "            next_value = next_value.item()\n",
    "        \n",
    "        # 计算GAE\n",
    "        advantages, returns = compute_gae(rewards, values, next_value, dones, gamma, gae_lambda)\n",
    "        \n",
    "        # 转换为tensor\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        log_probs = torch.stack(log_probs).squeeze()\n",
    "        entropies = torch.stack(entropies).squeeze()\n",
    "        \n",
    "        # 重新计算values (需要梯度)\n",
    "        states_t = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        _, values_t = model(states_t)\n",
    "        values_t = values_t.squeeze()\n",
    "        \n",
    "        # 标准化优势\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # 计算损失并更新\n",
    "        loss, metrics = a2c_loss(log_probs, values_t, advantages, returns, entropies)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (update + 1) % 20 == 0 and episode_rewards:\n",
    "            mean_reward = np.mean(episode_rewards[-20:])\n",
    "            print(f\"Update {update+1}, Mean Reward: {mean_reward:.2f}, \"\n",
    "                  f\"Policy Loss: {metrics['policy_loss']:.4f}, \"\n",
    "                  f\"Value Loss: {metrics['value_loss']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "# 训练\n",
    "rewards = train_a2c(num_updates=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "if rewards:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, alpha=0.3)\n",
    "    window = min(20, len(rewards))\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), smoothed, linewidth=2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('A2C Training on CartPole')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **Actor-Critic**: 结合策略学习和价值学习\n",
    "\n",
    "2. **TD误差**: $\\delta_t = r_t + \\gamma V(s') - V(s)$\n",
    "\n",
    "3. **GAE**: $A_t = \\sum_l (\\gamma\\lambda)^l \\delta_{t+l}$\n",
    "   - $\\lambda=0$: TD(0), 低方差高偏差\n",
    "   - $\\lambda=1$: MC, 高方差无偏差\n",
    "   - $\\lambda=0.95$: 推荐默认值\n",
    "\n",
    "4. **A2C损失**: $L = L_{policy} + c_v L_{value} - c_{ent} H(\\pi)$\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- PPO: 添加裁剪目标提高稳定性\n",
    "- 多环境并行: 提高采样效率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
