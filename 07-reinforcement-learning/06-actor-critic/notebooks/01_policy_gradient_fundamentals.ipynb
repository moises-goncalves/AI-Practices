{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度方法基础 (Policy Gradient Fundamentals)\n",
    "\n",
    "本教程深入讲解策略梯度方法的数学原理和实现细节。\n",
    "\n",
    "## 目录\n",
    "1. [核心思想](#1-核心思想)\n",
    "2. [数学推导](#2-数学推导)\n",
    "3. [REINFORCE算法](#3-reinforce算法)\n",
    "4. [方差减少技术](#4-方差减少技术)\n",
    "5. [实践练习](#5-实践练习)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 核心思想\n",
    "\n",
    "### 1.1 从价值方法到策略方法\n",
    "\n",
    "**价值方法 (Value-Based)**:\n",
    "- 学习价值函数 $Q(s,a)$ 或 $V(s)$\n",
    "- 从价值函数导出策略: $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    "- 问题: 连续动作空间难以处理\n",
    "\n",
    "**策略方法 (Policy-Based)**:\n",
    "- 直接参数化策略 $\\pi_\\theta(a|s)$\n",
    "- 通过梯度上升优化期望回报\n",
    "- 优势: 天然支持连续动作、随机策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 策略参数化\n",
    "\n",
    "**离散动作空间** - Softmax策略:\n",
    "$$\\pi_\\theta(a|s) = \\frac{\\exp(f_\\theta(s)_a)}{\\sum_{a'} \\exp(f_\\theta(s)_{a'})}$$\n",
    "\n",
    "**连续动作空间** - 高斯策略:\n",
    "$$\\pi_\\theta(a|s) = \\mathcal{N}(a | \\mu_\\theta(s), \\sigma_\\theta(s)^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设置\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"环境准备完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数学推导\n",
    "\n",
    "### 2.1 目标函数\n",
    "\n",
    "策略梯度的目标是最大化期望回报:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\gamma^t r_t\\right]$$\n",
    "\n",
    "其中 $\\tau = (s_0, a_0, r_1, s_1, a_1, ...)$ 是轨迹。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 策略梯度定理\n",
    "\n",
    "**关键问题**: 如何计算 $\\nabla_\\theta J(\\theta)$?\n",
    "\n",
    "**Log-Derivative Trick**:\n",
    "$$\\nabla_\\theta \\pi_\\theta(a|s) = \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s)$$\n",
    "\n",
    "**策略梯度定理** (Sutton et al., 1999):\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot Q^\\pi(s_t, a_t)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示Log-Derivative Trick\n",
    "def demonstrate_log_derivative_trick():\n",
    "    \"\"\"展示log-derivative trick的数值验证\"\"\"\n",
    "    \n",
    "    # 简单的softmax策略\n",
    "    theta = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "    \n",
    "    # 计算概率\n",
    "    probs = torch.softmax(theta, dim=0)\n",
    "    \n",
    "    # 方法1: 直接对概率求梯度\n",
    "    grad_direct = torch.autograd.grad(probs[0], theta, retain_graph=True)[0]\n",
    "    \n",
    "    # 方法2: 使用log-derivative trick\n",
    "    log_prob = torch.log(probs[0])\n",
    "    grad_log = torch.autograd.grad(log_prob, theta, retain_graph=True)[0]\n",
    "    grad_trick = probs[0] * grad_log\n",
    "    \n",
    "    print(\"直接梯度:\", grad_direct.detach().numpy())\n",
    "    print(\"Log-trick:\", grad_trick.detach().numpy())\n",
    "    print(\"差异:\", (grad_direct - grad_trick).abs().max().item())\n",
    "\n",
    "demonstrate_log_derivative_trick()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 推导过程\n",
    "\n",
    "**Step 1**: 轨迹概率\n",
    "$$p(\\tau|\\theta) = p(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "**Step 2**: 对数轨迹概率\n",
    "$$\\log p(\\tau|\\theta) = \\log p(s_0) + \\sum_{t=0}^{T-1} \\left[\\log \\pi_\\theta(a_t|s_t) + \\log p(s_{t+1}|s_t, a_t)\\right]$$\n",
    "\n",
    "**Step 3**: 梯度 (环境动态与θ无关)\n",
    "$$\\nabla_\\theta \\log p(\\tau|\\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$$\n",
    "\n",
    "**Step 4**: 策略梯度\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau}\\left[R(\\tau) \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. REINFORCE算法\n",
    "\n",
    "### 3.1 算法描述\n",
    "\n",
    "REINFORCE使用蒙特卡洛回报 $G_t$ 作为 $Q^\\pi(s_t, a_t)$ 的无偏估计:\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{T-t} \\gamma^k r_{t+k}$$\n",
    "\n",
    "**梯度估计**:\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{T_i} \\nabla_\\theta \\log \\pi_\\theta(a_t^i|s_t^i) \\cdot G_t^i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    计算蒙特卡洛回报\n",
    "    \n",
    "    数学公式:\n",
    "        G_t = r_t + γr_{t+1} + γ²r_{t+2} + ...\n",
    "            = r_t + γG_{t+1}\n",
    "    \n",
    "    使用反向递归高效计算\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return torch.tensor(returns, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示回报计算\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "gamma = 0.99\n",
    "\n",
    "returns = compute_returns(rewards, gamma)\n",
    "print(\"奖励序列:\", rewards)\n",
    "print(\"回报序列:\", returns.numpy().round(4))\n",
    "print()\n",
    "print(\"验证 G_0 = 1 + 0.99×1 + 0.99²×1 + 0.99³×1 + 0.99⁴×1\")\n",
    "print(f\"手动计算: {1 + 0.99 + 0.99**2 + 0.99**3 + 0.99**4:.4f}\")\n",
    "print(f\"函数结果: {returns[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy(nn.Module):\n",
    "    \"\"\"简单的离散策略网络\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"返回动作logits\"\"\"\n",
    "        return self.net(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"采样动作并返回log概率\"\"\"\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_update(policy, optimizer, states, actions, returns):\n",
    "    \"\"\"\n",
    "    REINFORCE更新步骤\n",
    "    \n",
    "    损失函数:\n",
    "        L = -E[log π(a|s) · G]\n",
    "    \n",
    "    负号将梯度上升转换为梯度下降\n",
    "    \"\"\"\n",
    "    # 计算log概率\n",
    "    logits = policy(states)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    log_probs = dist.log_prob(actions)\n",
    "    \n",
    "    # 策略梯度损失\n",
    "    policy_loss = -(log_probs * returns).mean()\n",
    "    \n",
    "    # 优化\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return policy_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 方差减少技术\n",
    "\n",
    "### 4.1 问题: 高方差\n",
    "\n",
    "REINFORCE的梯度估计方差很高:\n",
    "$$\\text{Var}[\\nabla_\\theta J] \\propto \\mathbb{E}[G_t^2]$$\n",
    "\n",
    "这导致:\n",
    "- 训练不稳定\n",
    "- 需要大量样本\n",
    "- 收敛缓慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 基线 (Baseline)\n",
    "\n",
    "**关键洞察**: 减去任何不依赖于动作的基线 $b(s)$ 不改变梯度期望:\n",
    "\n",
    "$$\\mathbb{E}_{a \\sim \\pi}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot b(s)] = b(s) \\mathbb{E}_{a \\sim \\pi}[\\nabla_\\theta \\log \\pi_\\theta(a|s)] = 0$$\n",
    "\n",
    "**最优基线**: $b^*(s) = V^\\pi(s)$\n",
    "\n",
    "**优势函数**:\n",
    "$$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s) \\approx G_t - V(s_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_variance_reduction():\n",
    "    \"\"\"\n",
    "    演示基线如何减少方差\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 模拟回报\n",
    "    returns = np.random.normal(100, 20, 1000)  # 均值100, 标准差20\n",
    "    \n",
    "    # 无基线\n",
    "    var_no_baseline = np.var(returns)\n",
    "    \n",
    "    # 有基线 (减去均值)\n",
    "    baseline = np.mean(returns)\n",
    "    advantages = returns - baseline\n",
    "    var_with_baseline = np.var(advantages)\n",
    "    \n",
    "    print(f\"无基线方差: {var_no_baseline:.2f}\")\n",
    "    print(f\"有基线方差: {var_with_baseline:.2f}\")\n",
    "    print(f\"方差减少: {(1 - var_with_baseline/var_no_baseline)*100:.1f}%\")\n",
    "    \n",
    "    # 可视化\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].hist(returns, bins=30, alpha=0.7, label='Returns')\n",
    "    axes[0].axvline(baseline, color='r', linestyle='--', label=f'Baseline={baseline:.1f}')\n",
    "    axes[0].set_title('无基线: 回报分布')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].hist(advantages, bins=30, alpha=0.7, color='green', label='Advantages')\n",
    "    axes[1].axvline(0, color='r', linestyle='--', label='Zero')\n",
    "    axes[1].set_title('有基线: 优势分布')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_variance_reduction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 回报标准化\n",
    "\n",
    "另一种简单有效的方差减少技术:\n",
    "\n",
    "$$G_t^{\\text{norm}} = \\frac{G_t - \\mu_G}{\\sigma_G + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_returns(returns, eps=1e-8):\n",
    "    \"\"\"\n",
    "    标准化回报\n",
    "    \n",
    "    好处:\n",
    "    1. 减少方差\n",
    "    2. 使梯度尺度一致\n",
    "    3. 对奖励缩放不敏感\n",
    "    \"\"\"\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    return (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "# 演示\n",
    "raw_returns = [100, 120, 80, 150, 90]\n",
    "norm_returns = normalize_returns(raw_returns)\n",
    "\n",
    "print(\"原始回报:\", raw_returns)\n",
    "print(\"标准化后:\", norm_returns.numpy().round(3))\n",
    "print(f\"均值: {norm_returns.mean():.6f}, 标准差: {norm_returns.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 实践练习\n",
    "\n",
    "### 练习1: 实现完整的REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def train_reinforce(env_name='CartPole-v1', num_episodes=500, gamma=0.99, lr=1e-3):\n",
    "    \"\"\"\n",
    "    完整的REINFORCE训练循环\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    policy = SimplePolicy(state_dim, action_dim)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # 收集轨迹\n",
    "        states, actions, rewards, log_probs = [], [], [], []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action, log_prob = policy.get_action(state_t)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action.item())\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # 计算回报\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # 标准化\n",
    "        \n",
    "        # 更新策略\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        policy_loss = -(log_probs * returns).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            mean_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode+1}, Mean Reward: {mean_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "# 训练 (设置较少的episode用于演示)\n",
    "rewards = train_reinforce(num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练曲线\n",
    "def plot_rewards(rewards, window=20):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "    \n",
    "    # 滑动平均\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), smoothed, label=f'{window}-Episode Average')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('REINFORCE Training on CartPole')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2: 思考题\n",
    "\n",
    "1. **为什么REINFORCE需要完整的episode?**\n",
    "   - 提示: 考虑 $G_t$ 的计算方式\n",
    "\n",
    "2. **基线为什么不改变梯度期望?**\n",
    "   - 提示: 证明 $\\mathbb{E}[\\nabla \\log \\pi \\cdot b(s)] = 0$\n",
    "\n",
    "3. **如何选择最优基线?**\n",
    "   - 提示: 最小化方差 $\\text{Var}[\\nabla J]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **策略梯度定理**: $\\nabla_\\theta J = \\mathbb{E}[\\nabla \\log \\pi \\cdot Q]$\n",
    "\n",
    "2. **REINFORCE**: 使用MC回报 $G_t$ 估计 $Q$\n",
    "\n",
    "3. **方差减少**: 基线、回报标准化\n",
    "\n",
    "4. **优势函数**: $A(s,a) = Q(s,a) - V(s)$\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- Actor-Critic: 学习价值函数作为基线\n",
    "- GAE: 更好的优势估计\n",
    "- PPO: 稳定的策略更新"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
