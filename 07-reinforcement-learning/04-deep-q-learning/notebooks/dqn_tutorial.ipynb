{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) 深度强化学习完整教程\n",
    "\n",
    "---\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [引言与核心思想](#1-引言与核心思想)\n",
    "2. [数学原理深度剖析](#2-数学原理深度剖析)\n",
    "3. [核心创新详解](#3-核心创新详解)\n",
    "4. [代码实现与逐行解析](#4-代码实现与逐行解析)\n",
    "5. [训练实验与可视化](#5-训练实验与可视化)\n",
    "6. [算法变体对比分析](#6-算法变体对比分析)\n",
    "7. [超参数敏感性分析](#7-超参数敏感性分析)\n",
    "8. [常见问题诊断](#8-常见问题诊断)\n",
    "9. [总结与进阶方向](#9-总结与进阶方向)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 引言与核心思想\n",
    "\n",
    "### 1.1 什么是 DQN？\n",
    "\n",
    "**Deep Q-Network (DQN)** 是深度强化学习的开山之作，由 DeepMind 于 2013 年提出，2015 年发表于 Nature。它首次证明了神经网络可以直接从原始像素输入学习控制策略，在 49 款 Atari 游戏中达到人类水平。\n",
    "\n",
    "**核心思想一句话概括**：\n",
    "\n",
    "> 用深度神经网络逼近 Q 函数，结合经验回放和目标网络实现稳定训练。\n",
    "\n",
    "### 1.2 历史地位\n",
    "\n",
    "| 时间 | 里程碑 |\n",
    "|------|--------|\n",
    "| 2013 | DQN 首次在 Atari 游戏上超越人类 |\n",
    "| 2015 | Nature 论文发表，深度强化学习正式诞生 |\n",
    "| 2016 | Double DQN、Dueling DQN、PER 相继提出 |\n",
    "| 2017 | Rainbow 组合所有改进，刷新记录 |\n",
    "| 2018 | DQN 思想扩展到连续动作空间 (SAC, TD3) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DQN 解决的核心问题\n",
    "\n",
    "传统 Q-Learning 使用 **Q 表** 存储状态-动作价值：\n",
    "\n",
    "```\n",
    "Q[state][action] = value\n",
    "```\n",
    "\n",
    "**问题**：\n",
    "1. **维度灾难**：状态空间呈指数增长，无法存储\n",
    "   - Atari 像素：210 × 160 × 3 = 100,800 维\n",
    "   - 可能状态数：~$10^{20000}$\n",
    "2. **缺乏泛化**：未见过的状态完全不知道如何处理\n",
    "\n",
    "**DQN 解决方案**：\n",
    "\n",
    "```\n",
    "Q(s, a) ≈ f_θ(s)[a]  （神经网络逼近）\n",
    "```\n",
    "\n",
    "- 网络参数 $\\theta$ 通常只有几百万，而非 $10^{20000}$ 个状态\n",
    "- 相似状态自动共享权重，实现泛化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境配置与依赖导入\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 核心科学计算库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# 深度学习框架\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 强化学习环境\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    print(f\"gymnasium version: {gym.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"请安装 gymnasium: pip install gymnasium\")\n",
    "\n",
    "# 本地模块导入\n",
    "from dqn_core import DQNConfig, DQNAgent, DQNNetwork, DuelingDQNNetwork, ReplayBuffer\n",
    "from training_utils import TrainingConfig, train_dqn, evaluate_agent, plot_training_curves\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "rcParams['font.size'] = 11\n",
    "rcParams['axes.labelsize'] = 12\n",
    "rcParams['axes.titlesize'] = 13\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# 设置随机种子确保可复现\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 检测计算设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 数学原理深度剖析\n",
    "\n",
    "### 2.1 强化学习基本框架：马尔可夫决策过程 (MDP)\n",
    "\n",
    "强化学习问题形式化为 MDP 五元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$：\n",
    "\n",
    "- $\\mathcal{S}$：状态空间\n",
    "- $\\mathcal{A}$：动作空间\n",
    "- $P(s'|s, a)$：状态转移概率\n",
    "- $R(s, a, s')$：奖励函数\n",
    "- $\\gamma \\in [0, 1)$：折扣因子\n",
    "\n",
    "**目标**：找到策略 $\\pi$ 最大化累积折扣奖励：\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 动作价值函数 Q(s, a)\n",
    "\n",
    "**定义**：从状态 $s$ 执行动作 $a$ 后，遵循策略 $\\pi$ 的期望累积奖励：\n",
    "\n",
    "$$\n",
    "Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[ G_t \\mid s_t = s, a_t = a \\right]\n",
    "$$\n",
    "\n",
    "**最优 Q 函数** $Q^*$：所有策略中最大的 Q 值：\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
    "$$\n",
    "\n",
    "一旦知道 $Q^*$，最优策略就是在每个状态选择 Q 值最大的动作：\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 贝尔曼最优方程\n",
    "\n",
    "最优 Q 函数满足递归关系：\n",
    "\n",
    "$$\n",
    "\\boxed{Q^*(s, a) = \\mathbb{E}_{s' \\sim P}\\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a \\right]}\n",
    "$$\n",
    "\n",
    "**直觉理解**：\n",
    "- $Q^*(s, a)$ = 立即奖励 + 折扣后的未来最优价值\n",
    "- 这是动态规划的核心：将长期问题分解为短期决策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：贝尔曼方程的递归结构\n",
    "def visualize_bellman():\n",
    "    \"\"\"\n",
    "    贝尔曼方程的图解说明：\n",
    "    \n",
    "    当前状态 s 执行动作 a 后：\n",
    "    1. 获得即时奖励 r\n",
    "    2. 转移到下一状态 s'\n",
    "    3. 在 s' 选择最优动作 a' = argmax Q(s', ·)\n",
    "    4. 累积未来价值 γ * max_a' Q(s', a')\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # 绘制状态节点\n",
    "    ax.scatter([0], [0.5], s=3000, c='steelblue', alpha=0.8, zorder=5)\n",
    "    ax.scatter([4], [0.5], s=3000, c='coral', alpha=0.8, zorder=5)\n",
    "    ax.scatter([8], [0.5], s=3000, c='seagreen', alpha=0.8, zorder=5)\n",
    "    \n",
    "    # 标签\n",
    "    ax.text(0, 0.5, 's', fontsize=16, ha='center', va='center', fontweight='bold', color='white')\n",
    "    ax.text(4, 0.5, \"s'\", fontsize=16, ha='center', va='center', fontweight='bold', color='white')\n",
    "    ax.text(8, 0.5, '...', fontsize=16, ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    # 箭头和标签\n",
    "    ax.annotate('', xy=(3.3, 0.5), xytext=(0.7, 0.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    ax.text(2, 0.8, 'action a', fontsize=12, ha='center')\n",
    "    ax.text(2, 0.2, 'reward r', fontsize=12, ha='center', color='darkred')\n",
    "    \n",
    "    ax.annotate('', xy=(7.3, 0.5), xytext=(4.7, 0.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    ax.text(6, 0.8, \"argmax Q(s', ·)\", fontsize=12, ha='center')\n",
    "    ax.text(6, 0.2, r'$\\gamma \\max Q(s\\prime, a\\prime)$', fontsize=12, ha='center', color='darkgreen')\n",
    "    \n",
    "    # 公式\n",
    "    ax.text(4, -0.3, r\"$Q^*(s, a) = r + \\gamma \\max_{a'} Q^*(s', a')$\", \n",
    "            fontsize=16, ha='center', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlim(-1.5, 10)\n",
    "    ax.set_ylim(-0.8, 1.3)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Bellman Optimality Equation Recursive Structure', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_bellman()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 DQN 损失函数推导\n",
    "\n",
    "**核心思想**：将 Q-Learning 转化为监督学习\n",
    "\n",
    "1. **样本**：从经验回放缓冲区采样 $(s, a, r, s', \\text{done})$\n",
    "\n",
    "2. **预测值**：当前网络输出\n",
    "   $$\\hat{Q} = Q(s, a; \\theta)$$\n",
    "\n",
    "3. **目标值**：TD 目标（使用目标网络）\n",
    "   $$y = r + \\gamma (1 - \\text{done}) \\cdot \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "4. **损失函数**：均方误差\n",
    "   $$\\boxed{L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}}\\left[ (y - Q(s, a; \\theta))^2 \\right]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_td_target():\n",
    "    \"\"\"\n",
    "    演示 TD 目标计算过程\n",
    "    \n",
    "    TD (Temporal Difference) 学习的核心是 \"自举 (bootstrapping)\"：\n",
    "    使用当前估计更新当前估计，而非等到回合结束。\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TD Target Calculation Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 模拟一个批次的数据\n",
    "    batch_size = 4\n",
    "    gamma = 0.99\n",
    "    \n",
    "    # 模拟数据\n",
    "    rewards = np.array([1.0, 0.0, -1.0, 10.0])  # 即时奖励\n",
    "    dones = np.array([0, 0, 0, 1])  # 终止标志\n",
    "    next_q_max = np.array([5.2, 3.1, 8.7, 0.0])  # 下一状态的最大 Q 值\n",
    "    current_q = np.array([4.5, 3.0, 6.5, 8.0])  # 当前 Q 值估计\n",
    "    \n",
    "    # 计算 TD 目标\n",
    "    td_target = rewards + gamma * (1 - dones) * next_q_max\n",
    "    \n",
    "    # 计算 TD 误差\n",
    "    td_error = td_target - current_q\n",
    "    \n",
    "    print(\"\\nSample Data:\")\n",
    "    print(f\"  Immediate reward r:    {rewards}\")\n",
    "    print(f\"  Terminal flag done:    {dones}\")\n",
    "    print(f\"  max Q(s', a'):         {next_q_max}\")\n",
    "    print(f\"  Current Q(s, a):       {current_q}\")\n",
    "    \n",
    "    print(f\"\\nTD Target Calculation:\")\n",
    "    print(f\"  y = r + gamma(1-done)max Q(s', a')\")\n",
    "    print(f\"  gamma = {gamma}\")\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        print(f\"\\n  Sample {i+1}:\")\n",
    "        print(f\"    y = {rewards[i]:.1f} + {gamma} * (1-{dones[i]}) * {next_q_max[i]:.1f}\")\n",
    "        print(f\"      = {rewards[i]:.1f} + {gamma * (1 - dones[i]) * next_q_max[i]:.2f}\")\n",
    "        print(f\"      = {td_target[i]:.2f}\")\n",
    "        print(f\"    TD Error = {td_target[i]:.2f} - {current_q[i]:.2f} = {td_error[i]:.2f}\")\n",
    "    \n",
    "    print(f\"\\n\\nLoss = mean(TD_Error^2) = {np.mean(td_error**2):.4f}\")\n",
    "\n",
    "demonstrate_td_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 核心创新详解\n",
    "\n",
    "### 3.1 为什么直接用神经网络 + Q-Learning 会失败？\n",
    "\n",
    "**问题 1：样本相关性**\n",
    "- 在线学习中，连续样本 $(s_t, s_{t+1}, ...)$ 高度相关\n",
    "- 违反 SGD 的 i.i.d. 假设\n",
    "- 导致：局部过拟合，灾难性遗忘\n",
    "\n",
    "**问题 2：目标不稳定**\n",
    "- TD 目标依赖当前网络：$y = r + \\gamma \\max Q(s'; \\theta)$\n",
    "- 每次更新网络，目标也在变化\n",
    "- 导致：追逐移动目标，震荡不收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 经验回放 (Experience Replay)\n",
    "\n",
    "**解决问题**：样本相关性\n",
    "\n",
    "**核心思想**：\n",
    "1. 将交互经验 $(s, a, r, s', \\text{done})$ 存入缓冲区\n",
    "2. 训练时随机采样 mini-batch\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(s_1, a_1, r_1, s_1'), ..., (s_N, a_N, r_N, s_N')\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{batch} \\sim \\text{Uniform}(\\mathcal{D})\n",
    "$$\n",
    "\n",
    "**优点**：\n",
    "1. 打破时序相关性\n",
    "2. 每个样本可多次使用（样本效率）\n",
    "3. 梯度方差更低（batch 平均）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_experience_replay():\n",
    "    \"\"\"\n",
    "    演示经验回放的工作原理\n",
    "    \n",
    "    经验回放将 RL 问题转化为监督学习：\n",
    "    - 数据集 = 经验缓冲区\n",
    "    - 输入 = 状态\n",
    "    - 标签 = TD 目标\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Experience Replay Mechanism Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 创建回放缓冲区\n",
    "    buffer = ReplayBuffer(capacity=10)\n",
    "    \n",
    "    # 模拟收集经验\n",
    "    print(\"\\n1. Collecting Experience:\")\n",
    "    for i in range(8):\n",
    "        state = np.random.randn(4).astype(np.float32)\n",
    "        action = np.random.randint(0, 2)\n",
    "        reward = np.random.randn()\n",
    "        next_state = state + np.random.randn(4).astype(np.float32) * 0.1\n",
    "        done = i == 7\n",
    "        \n",
    "        buffer.push(state, action, reward, next_state, done)\n",
    "        print(f\"   Step {i+1}: Store (s, a={action}, r={reward:.2f}, s', done={done})\")\n",
    "    \n",
    "    print(f\"\\n   Buffer status: {len(buffer)}/{buffer.capacity} samples\")\n",
    "    \n",
    "    # 随机采样\n",
    "    print(\"\\n2. Random sample batch_size=4:\")\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(4)\n",
    "    \n",
    "    print(f\"   Sampled actions: {actions}\")\n",
    "    print(f\"   Sampled rewards: {rewards.round(2)}\")\n",
    "    print(f\"   Sampled dones: {dones}\")\n",
    "    \n",
    "    print(\"\\nKey Insight:\")\n",
    "    print(\"   - Random sampling breaks temporal correlation\")\n",
    "    print(\"   - Same experience can be sampled multiple times\")\n",
    "    print(\"   - This makes neural network training more stable\")\n",
    "\n",
    "demonstrate_experience_replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 目标网络 (Target Network)\n",
    "\n",
    "**解决问题**：目标不稳定\n",
    "\n",
    "**核心思想**：\n",
    "1. 维护两套网络：在线网络 $\\theta$，目标网络 $\\theta^-$\n",
    "2. TD 目标使用目标网络计算：$y = r + \\gamma \\max Q(s'; \\theta^-)$\n",
    "3. 周期性同步：每 $C$ 步 $\\theta^- \\leftarrow \\theta$\n",
    "\n",
    "**直觉**：\n",
    "- 固定目标 $C$ 步，给在线网络时间去\"追赶\"\n",
    "- 类似于监督学习中固定标签\n",
    "\n",
    "$$\n",
    "\\text{Without Target Network: } y_t = r_t + \\gamma \\max_a Q(s_{t+1}, a; \\theta_t) \\quad \\text{(Moving target)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{With Target Network: } y_t = r_t + \\gamma \\max_a Q(s_{t+1}, a; \\theta^-) \\quad \\text{(Fixed target)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_target_network():\n",
    "    \"\"\"\n",
    "    演示目标网络的稳定作用\n",
    "    \n",
    "    通过对比有无目标网络的 TD 目标变化来说明其重要性。\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Target Network Stability Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 创建两个简单网络\n",
    "    state_dim, action_dim = 4, 2\n",
    "    online_net = DQNNetwork(state_dim, action_dim, hidden_dims=[32, 32])\n",
    "    target_net = DQNNetwork(state_dim, action_dim, hidden_dims=[32, 32])\n",
    "    target_net.load_state_dict(online_net.state_dict())  # 初始同步\n",
    "    \n",
    "    # 固定输入状态\n",
    "    test_state = torch.randn(1, state_dim)\n",
    "    \n",
    "    print(\"\\nInitial State:\")\n",
    "    with torch.no_grad():\n",
    "        q_online = online_net(test_state)\n",
    "        q_target = target_net(test_state)\n",
    "    print(f\"   Online network Q values: {q_online.numpy().flatten().round(3)}\")\n",
    "    print(f\"   Target network Q values: {q_target.numpy().flatten().round(3)}\")\n",
    "    \n",
    "    # 模拟几次梯度更新（只更新在线网络）\n",
    "    optimizer = torch.optim.Adam(online_net.parameters(), lr=0.01)\n",
    "    \n",
    "    print(\"\\nSimulate 5 gradient updates (only update online network):\")\n",
    "    for step in range(5):\n",
    "        # 随机伪损失来模拟更新\n",
    "        loss = online_net(torch.randn(4, state_dim)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_online = online_net(test_state)\n",
    "            q_target = target_net(test_state)\n",
    "        \n",
    "        print(f\"\\n   Step {step+1}:\")\n",
    "        print(f\"      Online Q: {q_online.numpy().flatten().round(3)}  (changing)\")\n",
    "        print(f\"      Target Q: {q_target.numpy().flatten().round(3)}  (stable)\")\n",
    "    \n",
    "    print(\"\\n\\nKey Insight:\")\n",
    "    print(\"   - Target network provides stable regression target\")\n",
    "    print(\"   - Online network gradually learns, target stays fixed\")\n",
    "    print(\"   - Sync target network every C steps\")\n",
    "\n",
    "demonstrate_target_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 代码实现与逐行解析\n",
    "\n",
    "### 4.1 Q 网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_network_architecture():\n",
    "    \"\"\"\n",
    "    深入解析 DQN 网络架构\n",
    "    \n",
    "    Q 网络是一个从状态到 Q 值的映射：\n",
    "        f_theta: S -> R^{|A|}\n",
    "    \n",
    "    输入：状态向量 s in R^d\n",
    "    输出：所有动作的 Q 值 [Q(s, a_1), Q(s, a_2), ..., Q(s, a_n)]\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Q Network Architecture\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 创建网络\n",
    "    state_dim = 4  # CartPole 状态维度\n",
    "    action_dim = 2  # CartPole 动作数\n",
    "    hidden_dims = [128, 128]\n",
    "    \n",
    "    network = DQNNetwork(state_dim, action_dim, hidden_dims)\n",
    "    \n",
    "    print(f\"\\nNetwork Structure:\")\n",
    "    print(f\"   Input layer:  {state_dim} dim (state)\")\n",
    "    for i, dim in enumerate(hidden_dims):\n",
    "        print(f\"   Hidden {i+1}: {dim} dim + ReLU\")\n",
    "    print(f\"   Output layer: {action_dim} dim (Q values for each action)\")\n",
    "    \n",
    "    # 计算参数量\n",
    "    total_params = sum(p.numel() for p in network.parameters())\n",
    "    trainable_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nParameter Statistics:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # 前向传播演示\n",
    "    print(\"\\nForward Pass Demo:\")\n",
    "    sample_state = torch.tensor([[1.5, 0.0, -0.2, 0.3]])  # 示例状态\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_values = network(sample_state)\n",
    "    \n",
    "    print(f\"   Input state:  {sample_state.numpy().flatten()}\")\n",
    "    print(f\"   Output Q values: {q_values.numpy().flatten().round(3)}\")\n",
    "    print(f\"   Selected action: {q_values.argmax(dim=1).item()} (action with max Q)\")\n",
    "    \n",
    "    # 打印网络层\n",
    "    print(\"\\nNetwork Layer Details:\")\n",
    "    print(network)\n",
    "\n",
    "explain_network_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dueling 架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_dueling_architecture():\n",
    "    \"\"\"\n",
    "    Dueling DQN 架构详解\n",
    "    \n",
    "    核心思想：将 Q 函数分解为状态价值 V(s) 和动作优势 A(s, a)\n",
    "    \n",
    "    Q(s, a) = V(s) + A(s, a) - mean_a A(s, a)\n",
    "    \n",
    "    直觉：\n",
    "    - V(s): 这个状态本身有多好（与动作无关）\n",
    "    - A(s, a): 这个动作比平均好多少\n",
    "    \n",
    "    优势：\n",
    "    - 当动作选择影响不大时，V(s) 可以直接学习\n",
    "    - 更快的收敛，更好的泛化\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Dueling DQN Architecture\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    state_dim = 4\n",
    "    action_dim = 2\n",
    "    \n",
    "    # 创建 Dueling 网络\n",
    "    dueling_net = DuelingDQNNetwork(state_dim, action_dim, hidden_dims=[128, 128])\n",
    "    \n",
    "    print(\"\\nNetwork Structure:\")\n",
    "    print(\"\"\"\n",
    "           +---------------------+\n",
    "           |   Shared Features   |\n",
    "           |   (MLP backbone)    |\n",
    "           +----------+----------+\n",
    "                      |\n",
    "              +-------+-------+\n",
    "              |               |\n",
    "        +-----v-----+   +-----v-----+\n",
    "        | Value     |   | Advantage |\n",
    "        | Stream    |   | Stream    |\n",
    "        | V(s) -> 1 |   | A(s,.) -> |A|\n",
    "        +-----+-----+   +-----+-----+\n",
    "              |               |\n",
    "              +-------+-------+\n",
    "                      |\n",
    "              +-------v-------+\n",
    "              | Q(s,a) = V(s) |\n",
    "              | + A(s,a)      |\n",
    "              | - mean(A)    |\n",
    "              +---------------+\n",
    "    \"\"\")\n",
    "    \n",
    "    # 演示前向传播\n",
    "    print(\"\\nForward Pass Demo:\")\n",
    "    sample_state = torch.tensor([[1.5, 0.0, -0.2, 0.3]])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_values = dueling_net(sample_state)\n",
    "    \n",
    "    print(f\"   Input state:  {sample_state.numpy().flatten()}\")\n",
    "    print(f\"   Output Q values: {q_values.numpy().flatten().round(3)}\")\n",
    "    \n",
    "    print(\"\\nMathematical Formula:\")\n",
    "    print(\"   Q(s, a) = V(s) + A(s, a) - (1/|A|) sum_a' A(s, a')\")\n",
    "    print(\"\")\n",
    "    print(\"Why subtract mean?\")\n",
    "    print(\"   - Ensure identifiability\")\n",
    "    print(\"   - Q = V + A has infinitely many decompositions\")\n",
    "    print(\"   - Constraint sum_a A(s,a) = 0 makes it unique\")\n",
    "\n",
    "explain_dueling_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_epsilon_greedy():\n",
    "    \"\"\"\n",
    "    Epsilon-greedy policy详解\n",
    "    \n",
    "    探索-利用权衡 (Exploration-Exploitation Tradeoff)：\n",
    "    - 利用 (Exploitation): 选择当前已知最优动作\n",
    "    - 探索 (Exploration): 尝试未知动作，可能发现更好的\n",
    "    \n",
    "    Epsilon-greedy 策略：\n",
    "    - 以概率 (1-epsilon) 选择贪心动作 (argmax Q)\n",
    "    - 以概率 epsilon 随机选择动作\n",
    "    \n",
    "    Epsilon 衰减策略：\n",
    "    - 训练初期：高 epsilon (更多探索)\n",
    "    - 训练后期：低 epsilon (更多利用)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Epsilon-Greedy Policy Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 模拟 epsilon 衰减\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 10000\n",
    "    \n",
    "    steps = np.arange(0, 15000)\n",
    "    epsilon_values = []\n",
    "    \n",
    "    for step in steps:\n",
    "        decay_progress = min(1.0, step / epsilon_decay)\n",
    "        epsilon = epsilon_start + (epsilon_end - epsilon_start) * decay_progress\n",
    "        epsilon_values.append(epsilon)\n",
    "    \n",
    "    # 可视化\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Epsilon 衰减曲线\n",
    "    ax1.plot(steps, epsilon_values, 'b-', linewidth=2)\n",
    "    ax1.axhline(y=epsilon_start, color='r', linestyle='--', alpha=0.5, label=f'Start: {epsilon_start}')\n",
    "    ax1.axhline(y=epsilon_end, color='g', linestyle='--', alpha=0.5, label=f'End: {epsilon_end}')\n",
    "    ax1.axvline(x=epsilon_decay, color='orange', linestyle='--', alpha=0.5, label=f'Decay steps: {epsilon_decay}')\n",
    "    ax1.set_xlabel('Training Step')\n",
    "    ax1.set_ylabel('Epsilon')\n",
    "    ax1.set_title('Epsilon Decay Schedule')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 探索 vs 利用比例\n",
    "    checkpoints = [0, 2500, 5000, 10000, 15000]\n",
    "    explore_ratios = []\n",
    "    exploit_ratios = []\n",
    "    \n",
    "    for cp in checkpoints:\n",
    "        idx = min(cp, len(epsilon_values) - 1)\n",
    "        eps = epsilon_values[idx]\n",
    "        explore_ratios.append(eps)\n",
    "        exploit_ratios.append(1 - eps)\n",
    "    \n",
    "    x = np.arange(len(checkpoints))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x - width/2, explore_ratios, width, label='Exploration', color='coral')\n",
    "    ax2.bar(x + width/2, exploit_ratios, width, label='Exploitation', color='steelblue')\n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.set_title('Exploration vs Exploitation Ratio')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([str(cp) for cp in checkpoints])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insight:\")\n",
    "    print(\"   - Initial epsilon=1.0: 100% random exploration, collect diverse experience\")\n",
    "    print(\"   - Linear decay: gradually shift to exploitation\")\n",
    "    print(\"   - Final epsilon=0.01: retain small exploration to avoid local optima\")\n",
    "\n",
    "demonstrate_epsilon_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 训练实验与可视化\n",
    "\n",
    "### 5.1 CartPole 环境介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_cartpole_env():\n",
    "    \"\"\"\n",
    "    探索 CartPole-v1 环境\n",
    "    \n",
    "    CartPole 是强化学习的 \"Hello World\"：\n",
    "    - 目标：通过左右移动小车来平衡杆子\n",
    "    - 状态：[小车位置, 小车速度, 杆子角度, 杆子角速度]\n",
    "    - 动作：0 (向左推), 1 (向右推)\n",
    "    - 奖励：每一步不倒 +1\n",
    "    - 终止：杆子倾斜 >12° 或小车出界\n",
    "    - 成功标准：连续 100 回合平均奖励 >= 475\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CartPole-v1 Environment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    print(\"\\nEnvironment Info:\")\n",
    "    print(f\"   Observation space: {env.observation_space}\")\n",
    "    print(f\"   Action space: {env.action_space}\")\n",
    "    \n",
    "    print(\"\\nState Variables:\")\n",
    "    state_names = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "    for i, name in enumerate(state_names):\n",
    "        low = env.observation_space.low[i]\n",
    "        high = env.observation_space.high[i]\n",
    "        print(f\"   [{i}] {name}: [{low:.2f}, {high:.2f}]\")\n",
    "    \n",
    "    print(\"\\nAction Meanings:\")\n",
    "    print(\"   0: Push cart left\")\n",
    "    print(\"   1: Push cart right\")\n",
    "    \n",
    "    # 运行随机策略\n",
    "    print(\"\\nRandom Policy Demo (5 episodes):\")\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for ep in range(5):\n",
    "        state, _ = env.reset(seed=ep)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(500):\n",
    "            action = env.action_space.sample()  # 随机动作\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"   Episode {ep+1}: Reward = {total_reward:.0f}, Steps = {step+1}\")\n",
    "    \n",
    "    print(f\"\\nRandom policy avg reward: {np.mean(episode_rewards):.1f} +/- {np.std(episode_rewards):.1f}\")\n",
    "    print(\"Success criterion: avg reward >= 475 (max 500 steps)\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "explore_cartpole_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 DQN 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_cartpole(num_episodes: int = 200):\n",
    "    \"\"\"\n",
    "    在 CartPole 上训练 DQN\n",
    "    \n",
    "    使用较少的 episode 进行快速演示。\n",
    "    生产环境建议使用 500-1000 episodes。\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DQN Training Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 获取环境信息\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    env.close()\n",
    "    \n",
    "    # 配置 DQN Agent\n",
    "    config = DQNConfig(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dims=[128, 128],\n",
    "        learning_rate=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=5000,\n",
    "        buffer_size=50000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100,\n",
    "        double_dqn=True,\n",
    "        dueling=False,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    agent = DQNAgent(config)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"   State dim: {state_dim}\")\n",
    "    print(f\"   Action dim: {action_dim}\")\n",
    "    print(f\"   Network: {config.hidden_dims}\")\n",
    "    print(f\"   Double DQN: {config.double_dqn}\")\n",
    "    print(f\"   Device: {agent.device}\")\n",
    "    \n",
    "    # 配置训练参数\n",
    "    training_config = TrainingConfig(\n",
    "        num_episodes=num_episodes,\n",
    "        max_steps_per_episode=500,\n",
    "        eval_frequency=50,\n",
    "        eval_episodes=5,\n",
    "        log_frequency=20,\n",
    "        save_frequency=100,\n",
    "        checkpoint_dir='./dqn_checkpoints',\n",
    "        early_stopping_reward=475,\n",
    "        early_stopping_episodes=10,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStarting training for {num_episodes} episodes...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 训练\n",
    "    metrics = train_dqn(\n",
    "        agent=agent,\n",
    "        env_name='CartPole-v1',\n",
    "        config=training_config,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    return agent, metrics\n",
    "\n",
    "# 运行训练\n",
    "agent, metrics = train_dqn_cartpole(num_episodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 训练结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "plot_training_curves(\n",
    "    metrics,\n",
    "    title=\"DQN Training on CartPole-v1\",\n",
    "    window=10,\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 评估训练好的 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trained_agent(agent: DQNAgent, num_episodes: int = 10):\n",
    "    \"\"\"\n",
    "    评估训练好的 Agent\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Agent Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    mean_reward, std_reward = evaluate_agent(\n",
    "        agent=agent,\n",
    "        env_name='CartPole-v1',\n",
    "        num_episodes=num_episodes,\n",
    "        verbose=True,\n",
    "        deterministic=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"   Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    if mean_reward >= 475:\n",
    "        print(\"   [OK] Reached CartPole success criterion (>=475)\")\n",
    "    else:\n",
    "        print(f\"   [X] Below criterion, gap: {475 - mean_reward:.1f}\")\n",
    "\n",
    "evaluate_trained_agent(agent, num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 算法变体对比分析\n",
    "\n",
    "### 6.1 Double DQN\n",
    "\n",
    "**问题**：标准 DQN 系统性过估计 Q 值\n",
    "\n",
    "数学解释：\n",
    "$$\n",
    "\\mathbb{E}[\\max_a Q(s, a)] \\geq \\max_a \\mathbb{E}[Q(s, a)]\n",
    "$$\n",
    "\n",
    "max 操作会放大噪声，导致过估计。\n",
    "\n",
    "**解决方案**：解耦动作选择和价值评估\n",
    "\n",
    "$$\n",
    "y^{\\text{Double}} = r + \\gamma Q\\left(s', \\underbrace{\\arg\\max_{a'} Q(s', a'; \\theta)}_{\\text{Online network selects}};  \\underbrace{\\theta^-}_{\\text{Target network evaluates}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_overestimation():\n",
    "    \"\"\"\n",
    "    演示 Q 值过估计问题\n",
    "    \n",
    "    过估计来源：\n",
    "    1. 函数近似误差（神经网络不完美）\n",
    "    2. max 操作的统计偏差\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Q Value Overestimation Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 模拟：真实 Q 值 + 噪声\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    num_actions = 4\n",
    "    num_experiments = 1000\n",
    "    noise_std = 0.5\n",
    "    \n",
    "    true_q_values = np.array([1.0, 0.8, 0.6, 0.4])  # 真实 Q 值\n",
    "    true_max = np.max(true_q_values)\n",
    "    \n",
    "    estimated_max_values = []\n",
    "    \n",
    "    for _ in range(num_experiments):\n",
    "        # 添加估计噪声\n",
    "        noisy_q = true_q_values + np.random.randn(num_actions) * noise_std\n",
    "        estimated_max_values.append(np.max(noisy_q))\n",
    "    \n",
    "    estimated_max_mean = np.mean(estimated_max_values)\n",
    "    overestimation = estimated_max_mean - true_max\n",
    "    \n",
    "    print(f\"\\nSimulation Setup:\")\n",
    "    print(f\"   True Q values: {true_q_values}\")\n",
    "    print(f\"   True max Q: {true_max}\")\n",
    "    print(f\"   Noise std: {noise_std}\")\n",
    "    print(f\"   Experiments: {num_experiments}\")\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"   Estimated max Q mean: {estimated_max_mean:.3f}\")\n",
    "    print(f\"   Overestimation: {overestimation:.3f}\")\n",
    "    print(f\"   Overestimation ratio: {overestimation / true_max * 100:.1f}%\")\n",
    "    \n",
    "    # 可视化\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 直方图\n",
    "    ax1.hist(estimated_max_values, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax1.axvline(x=true_max, color='red', linestyle='--', linewidth=2, label=f'True max = {true_max}')\n",
    "    ax1.axvline(x=estimated_max_mean, color='orange', linestyle='-', linewidth=2, \n",
    "                label=f'Estimated mean = {estimated_max_mean:.2f}')\n",
    "    ax1.set_xlabel('Estimated Max Q Value')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Max Q Estimation Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 不同噪声水平的过估计\n",
    "    noise_levels = np.linspace(0.1, 2.0, 20)\n",
    "    overestimations = []\n",
    "    \n",
    "    for noise in noise_levels:\n",
    "        estimates = []\n",
    "        for _ in range(500):\n",
    "            noisy_q = true_q_values + np.random.randn(num_actions) * noise\n",
    "            estimates.append(np.max(noisy_q))\n",
    "        overestimations.append(np.mean(estimates) - true_max)\n",
    "    \n",
    "    ax2.plot(noise_levels, overestimations, 'b-o', linewidth=2, markersize=6)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Noise Standard Deviation')\n",
    "    ax2.set_ylabel('Overestimation')\n",
    "    ax2.set_title('Overestimation vs Noise Level')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insight:\")\n",
    "    print(\"   - max operation produces positive bias in presence of noise\")\n",
    "    print(\"   - More noise leads to more overestimation\")\n",
    "    print(\"   - Double DQN mitigates this by decoupling selection and evaluation\")\n",
    "\n",
    "demonstrate_overestimation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 算法变体对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dqn_variants(num_episodes: int = 150):\n",
    "    \"\"\"\n",
    "    对比不同 DQN 变体的性能\n",
    "    \n",
    "    比较以下变体：\n",
    "    1. Standard DQN\n",
    "    2. Double DQN\n",
    "    3. Dueling DQN\n",
    "    4. Double Dueling DQN\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DQN Variants Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    env.close()\n",
    "    \n",
    "    variants = [\n",
    "        (\"Standard DQN\", False, False),\n",
    "        (\"Double DQN\", True, False),\n",
    "        (\"Dueling DQN\", False, True),\n",
    "        (\"Double Dueling\", True, True),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    training_config = TrainingConfig(\n",
    "        num_episodes=num_episodes,\n",
    "        log_frequency=num_episodes + 1,  # 禁用日志\n",
    "        eval_frequency=num_episodes + 1,  # 禁用中间评估\n",
    "        save_frequency=num_episodes + 1,  # 禁用保存\n",
    "    )\n",
    "    \n",
    "    for name, double_dqn, dueling in variants:\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        config = DQNConfig(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            double_dqn=double_dqn,\n",
    "            dueling=dueling,\n",
    "            seed=42,\n",
    "            epsilon_decay=3000,\n",
    "        )\n",
    "        agent = DQNAgent(config)\n",
    "        \n",
    "        metrics = train_dqn(\n",
    "            agent=agent,\n",
    "            env_name='CartPole-v1',\n",
    "            config=training_config,\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        results[name] = metrics\n",
    "        final_avg = np.mean(metrics.episode_rewards[-30:])\n",
    "        print(f\"   Done! Last 30 episodes avg: {final_avg:.1f}\")\n",
    "    \n",
    "    # 可视化对比\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    window = 10\n",
    "    for idx, (name, metrics) in enumerate(results.items()):\n",
    "        rewards = metrics.episode_rewards\n",
    "        if len(rewards) >= window:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            ax1.plot(range(window-1, len(rewards)), smoothed, \n",
    "                    label=name, color=colors[idx], linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.set_title('Learning Curves Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 最终性能柱状图\n",
    "    names = list(results.keys())\n",
    "    final_rewards = [np.mean(results[n].episode_rewards[-30:]) for n in names]\n",
    "    \n",
    "    bars = ax2.bar(names, final_rewards, color=[colors[i] for i in range(len(names))])\n",
    "    ax2.set_ylabel('Average Reward (Last 30 Episodes)')\n",
    "    ax2.set_title('Final Performance Comparison')\n",
    "    ax2.tick_params(axis='x', rotation=15)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, reward in zip(bars, final_rewards):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{reward:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行对比（使用较少 episodes 进行快速演示）\n",
    "comparison_results = compare_dqn_variants(num_episodes=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 超参数敏感性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hyperparameters():\n",
    "    \"\"\"\n",
    "    超参数敏感性分析\n",
    "    \n",
    "    分析关键超参数对性能的影响：\n",
    "    - learning_rate: 学习率\n",
    "    - target_update_freq: 目标网络更新频率\n",
    "    - epsilon_decay: 探索率衰减速度\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Hyperparameter Sensitivity Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nKey Hyperparameters and Their Effects:\")\n",
    "    print(\"\"\"\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    | Hyperparameter     | Suggested      | Effect                    |\n",
    "    +====================+================+===========================+\n",
    "    | learning_rate      | 1e-4 ~ 1e-3    | Convergence vs Stability  |\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    | gamma              | 0.99 ~ 0.999   | Long-term reward weight   |\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    | buffer_size        | 10K ~ 1M       | Data diversity vs Memory  |\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    | batch_size         | 32 ~ 256       | Gradient stability vs Cost|\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    | target_update_freq | 100 ~ 10000    | Target stability vs Delay |\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    | epsilon_decay      | 1K ~ 100K      | Exploration vs Convergence|\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    | hidden_dims        | [64,64] ~      | Capacity vs Overfitting   |\n",
    "    |                    | [512,512]      |                           |\n",
    "    +--------------------+----------------+---------------------------+\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nRecommended Config for CartPole-v1:\")\n",
    "    print(\"\"\"\n",
    "    config = DQNConfig(\n",
    "        hidden_dims=[128, 128],     # Medium network sufficient\n",
    "        learning_rate=1e-3,         # Relatively high, fast convergence\n",
    "        gamma=0.99,                 # Standard discount factor\n",
    "        epsilon_decay=5000,         # Fast decay\n",
    "        target_update_freq=100,     # Frequent updates\n",
    "        buffer_size=50000,          # Medium buffer\n",
    "        batch_size=64,              # Standard batch\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nRecommended Config for LunarLander-v2:\")\n",
    "    print(\"\"\"\n",
    "    config = DQNConfig(\n",
    "        hidden_dims=[256, 256],     # Larger network\n",
    "        learning_rate=5e-4,         # Smaller learning rate\n",
    "        gamma=0.99,\n",
    "        epsilon_decay=20000,        # Slower decay\n",
    "        target_update_freq=500,     # Less frequent updates\n",
    "        buffer_size=100000,         # Larger buffer\n",
    "        batch_size=64,\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "analyze_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 常见问题诊断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def troubleshooting_guide():\n",
    "    \"\"\"\n",
    "    DQN 训练常见问题诊断指南\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DQN Training Troubleshooting Guide\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    problems = [\n",
    "        {\n",
    "            \"symptom\": \"Reward not improving, persistent oscillation\",\n",
    "            \"causes\": [\n",
    "                \"Learning rate too high\",\n",
    "                \"Target network updated too frequently\",\n",
    "                \"Network capacity insufficient\",\n",
    "                \"Insufficient exploration (epsilon decays too fast)\",\n",
    "            ],\n",
    "            \"solutions\": [\n",
    "                \"Reduce learning rate to 1e-4\",\n",
    "                \"Increase target_update_freq to 1000+\",\n",
    "                \"Increase hidden layer width/depth\",\n",
    "                \"Increase epsilon_decay steps\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"symptom\": \"Q values keep growing to extreme values\",\n",
    "            \"causes\": [\n",
    "                \"Severe overestimation\",\n",
    "                \"Reward scale too large\",\n",
    "                \"Gradient explosion\",\n",
    "            ],\n",
    "            \"solutions\": [\n",
    "                \"Enable Double DQN\",\n",
    "                \"Clip or normalize rewards\",\n",
    "                \"Enable gradient clipping (grad_clip=10.0)\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"symptom\": \"Slow learning, requires too many samples\",\n",
    "            \"causes\": [\n",
    "                \"Low exploration efficiency\",\n",
    "                \"Buffer too small\",\n",
    "                \"Batch size too small\",\n",
    "            ],\n",
    "            \"solutions\": [\n",
    "                \"Increase initial exploration (epsilon_start=1.0)\",\n",
    "                \"Increase buffer capacity\",\n",
    "                \"Increase batch_size to 128\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"symptom\": \"Converged to suboptimal policy\",\n",
    "            \"causes\": [\n",
    "                \"Network capacity insufficient\",\n",
    "                \"Exploration stopped too early\",\n",
    "                \"Stuck in local optimum\",\n",
    "            ],\n",
    "            \"solutions\": [\n",
    "                \"Increase network capacity\",\n",
    "                \"Keep epsilon_end > 0 (e.g., 0.01)\",\n",
    "                \"Run multiple times with different seeds\",\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    for i, problem in enumerate(problems, 1):\n",
    "        print(f\"\\nProblem {i}: {problem['symptom']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        print(\"Possible Causes:\")\n",
    "        for cause in problem['causes']:\n",
    "            print(f\"   * {cause}\")\n",
    "        \n",
    "        print(\"Solutions:\")\n",
    "        for solution in problem['solutions']:\n",
    "            print(f\"   [OK] {solution}\")\n",
    "\n",
    "troubleshooting_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 总结与进阶方向\n",
    "\n",
    "### 9.1 DQN 核心要点回顾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary():\n",
    "    \"\"\"\n",
    "    DQN 学习要点总结\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DQN Key Points Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    1. Core Idea\n",
    "       Approximate Q function with neural network, combined with\n",
    "       experience replay and target network for stable training.\n",
    "    \n",
    "    2. Key Innovations\n",
    "       * Experience Replay: Break sample correlation, improve data efficiency\n",
    "       * Target Network: Stabilize TD target, prevent divergence\n",
    "    \n",
    "    3. Mathematical Foundation\n",
    "       * Bellman Equation: Q*(s,a) = E[r + gamma max Q*(s',a')]\n",
    "       * TD Error: delta = r + gamma max Q(s') - Q(s,a)\n",
    "       * Loss Function: L(theta) = E[(TD_target - Q(s,a;theta))^2]\n",
    "    \n",
    "    4. Algorithm Variants\n",
    "       * Double DQN: Decouple selection and evaluation, reduce overestimation\n",
    "       * Dueling DQN: Decompose V(s) and A(s,a), faster convergence\n",
    "       * PER: Prioritize high TD-error samples\n",
    "    \n",
    "    5. Applicable Scenarios\n",
    "       [OK] Discrete action space\n",
    "       [OK] High-dimensional states (images, sensors)\n",
    "       [OK] Delayed reward tasks\n",
    "       [X] Continuous action space (need DDPG/TD3/SAC)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nLearning Path:\")\n",
    "    print(\"\"\"\n",
    "    DQN -> Double DQN -> Dueling DQN -> PER -> Rainbow\n",
    "                 |\n",
    "            DDPG/TD3/SAC (continuous actions)\n",
    "                 |\n",
    "            PPO/A3C (policy gradient)\n",
    "                 |\n",
    "            Model-based RL (MuZero, Dreamer)\n",
    "    \"\"\")\n",
    "\n",
    "summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 参考文献\n",
    "\n",
    "**核心论文：**\n",
    "1. Mnih, V., et al. (2013). Playing Atari with Deep Reinforcement Learning. *NIPS Workshop*.\n",
    "2. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. *Nature*.\n",
    "3. van Hasselt, H., et al. (2016). Deep Reinforcement Learning with Double Q-learning. *AAAI*.\n",
    "4. Wang, Z., et al. (2016). Dueling Network Architectures for Deep Reinforcement Learning. *ICML*.\n",
    "5. Schaul, T., et al. (2016). Prioritized Experience Replay. *ICLR*.\n",
    "6. Hessel, M., et al. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. *AAAI*.\n",
    "\n",
    "**推荐资源：**\n",
    "- [OpenAI Spinning Up](https://spinningup.openai.com/)\n",
    "- [DeepMind Blog](https://deepmind.com/blog)\n",
    "- [Berkeley Deep RL Course](http://rail.eecs.berkeley.edu/deeprlcourse/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
