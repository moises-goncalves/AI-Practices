{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度Q学习(DQN)完整教程\n",
    "\n",
    "本教程将带你从零开始理解和实现Deep Q-Network算法。\n",
    "\n",
    "## 目录\n",
    "1. [理论基础](#1-理论基础)\n",
    "2. [核心组件](#2-核心组件)\n",
    "3. [实现DQN Agent](#3-实现dqn-agent)\n",
    "4. [训练与评估](#4-训练与评估)\n",
    "5. [算法变体对比](#5-算法变体对比)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 理论基础\n",
    "\n",
    "### 1.1 Q-Learning回顾\n",
    "\n",
    "Q-Learning是一种无模型的强化学习算法，通过学习动作价值函数Q(s,a)来找到最优策略。\n",
    "\n",
    "**贝尔曼最优方程**:\n",
    "$$Q^*(s, a) = \\mathbb{E}\\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a \\right]$$\n",
    "\n",
    "其中:\n",
    "- $Q^*(s, a)$: 最优动作价值函数\n",
    "- $r$: 即时奖励\n",
    "- $\\gamma \\in [0, 1]$: 折扣因子\n",
    "- $s'$: 下一状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 为什么需要Deep Q-Network?\n",
    "\n",
    "传统Q-Learning使用表格存储Q值，面临以下挑战:\n",
    "\n",
    "1. **维度灾难**: 状态空间指数增长 $|S| = O(d^n)$\n",
    "2. **缺乏泛化**: 每个状态独立学习，无法迁移知识\n",
    "3. **连续状态**: 无法处理连续状态空间\n",
    "\n",
    "**DQN解决方案**: 使用神经网络作为函数逼近器\n",
    "$$Q(s, a; \\theta) \\approx Q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DQN的两大创新\n",
    "\n",
    "#### 经验回放 (Experience Replay)\n",
    "- 存储转移 $(s, a, r, s', done)$ 到缓冲区\n",
    "- 随机采样打破时序相关性\n",
    "- 提高数据利用效率\n",
    "\n",
    "#### 目标网络 (Target Network)\n",
    "- 使用独立的目标网络计算TD目标\n",
    "- 定期同步参数: $\\theta^- \\leftarrow \\theta$\n",
    "- 提供稳定的回归目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设置\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 检查GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 核心组件\n",
    "\n",
    "### 2.1 经验回放缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffers import ReplayBuffer\n",
    "\n",
    "# 创建缓冲区\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "print(f'缓冲区容量: {buffer.capacity}')\n",
    "print(f'当前大小: {len(buffer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟存储转移\n",
    "for i in range(100):\n",
    "    state = np.random.randn(4).astype(np.float32)\n",
    "    action = np.random.randint(0, 2)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(4).astype(np.float32)\n",
    "    done = np.random.random() < 0.1\n",
    "    \n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f'存储后大小: {len(buffer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样批次\n",
    "if buffer.is_ready(32):\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "    print(f'状态形状: {states.shape}')\n",
    "    print(f'动作形状: {actions.shape}')\n",
    "    print(f'奖励形状: {rewards.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Q网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import DQNNetwork, DuelingDQNNetwork\n",
    "\n",
    "# 标准DQN网络\n",
    "q_net = DQNNetwork(state_dim=4, action_dim=2, hidden_dims=[128, 128])\n",
    "print('标准DQN网络:')\n",
    "print(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling DQN网络\n",
    "dueling_net = DuelingDQNNetwork(state_dim=4, action_dim=2, hidden_dims=[128, 128])\n",
    "print('Dueling DQN网络:')\n",
    "print(dueling_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试前向传播\n",
    "test_state = torch.randn(1, 4)\n",
    "q_values = q_net(test_state)\n",
    "print(f'Q值输出: {q_values}')\n",
    "print(f'最优动作: {q_values.argmax().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 实现DQN Agent\n",
    "\n",
    "### 3.1 创建Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import DQNAgent, create_dqn_agent\n",
    "from core import DQNConfig\n",
    "\n",
    "# 方法1: 使用配置类\n",
    "config = DQNConfig(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    hidden_dims=[128, 128],\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    "    double_dqn=True,\n",
    "    dueling=False,\n",
    ")\n",
    "agent = DQNAgent(config)\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法2: 使用工厂函数\n",
    "agent = create_dqn_agent(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    double_dqn=True,\n",
    "    dueling=True,\n",
    ")\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 动作选择 (ε-贪婪策略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试动作选择\n",
    "state = np.random.randn(4).astype(np.float32)\n",
    "\n",
    "# 训练模式 (带探索)\n",
    "action_train = agent.select_action(state, training=True)\n",
    "print(f'训练模式动作: {action_train}, epsilon: {agent.epsilon:.3f}')\n",
    "\n",
    "# 评估模式 (贪婪)\n",
    "action_eval = agent.select_action(state, training=False)\n",
    "print(f'评估模式动作: {action_eval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 训练步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟训练步骤\n",
    "losses = []\n",
    "\n",
    "for i in range(200):\n",
    "    state = np.random.randn(4).astype(np.float32)\n",
    "    action = agent.select_action(state, training=True)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(4).astype(np.float32)\n",
    "    done = np.random.random() < 0.1\n",
    "    \n",
    "    loss = agent.train_step(state, action, reward, next_state, done)\n",
    "    if loss is not None:\n",
    "        losses.append(loss)\n",
    "\n",
    "print(f'训练步数: {agent.training_step}')\n",
    "print(f'更新次数: {agent.update_count}')\n",
    "print(f'当前epsilon: {agent.epsilon:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制损失曲线\n",
    "if losses:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('更新步数')\n",
    "    plt.ylabel('损失')\n",
    "    plt.title('训练损失曲线')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练与评估\n",
    "\n",
    "### 4.1 在CartPole环境训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print('gymnasium未安装，跳过环境训练')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    from train import train_dqn, evaluate_agent\n",
    "    from utils.training import TrainingConfig\n",
    "    \n",
    "    # 创建agent\n",
    "    agent = create_dqn_agent(\n",
    "        state_dim=4,\n",
    "        action_dim=2,\n",
    "        double_dqn=True,\n",
    "        epsilon_decay=5000,\n",
    "    )\n",
    "    \n",
    "    # 训练配置 (快速测试)\n",
    "    config = TrainingConfig(\n",
    "        num_episodes=100,  # 生产环境建议300+\n",
    "        max_steps_per_episode=500,\n",
    "        log_frequency=20,\n",
    "        eval_frequency=50,\n",
    "    )\n",
    "    \n",
    "    # 训练\n",
    "    metrics = train_dqn(agent, 'CartPole-v1', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM and 'metrics' in dir():\n",
    "    from utils.visualization import plot_training_curves\n",
    "    \n",
    "    plot_training_curves(\n",
    "        metrics.episode_rewards,\n",
    "        metrics.losses,\n",
    "        metrics.epsilon_history,\n",
    "        metrics.eval_rewards,\n",
    "        title='DQN训练进度',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 算法变体对比\n",
    "\n",
    "### 5.1 Double DQN\n",
    "\n",
    "**问题**: 标准DQN的max操作导致过估计\n",
    "$$\\mathbb{E}[\\max_a Q(s,a)] \\geq \\max_a \\mathbb{E}[Q(s,a)]$$\n",
    "\n",
    "**解决方案**: 解耦动作选择和评估\n",
    "$$y = r + \\gamma Q(s', \\arg\\max_{a'} Q(s', a'; \\theta); \\theta^-)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dueling DQN\n",
    "\n",
    "**核心思想**: 分解Q函数为状态价值和动作优势\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|A|} \\sum_{a'} A(s, a')$$\n",
    "\n",
    "**优势**:\n",
    "- 状态价值从所有动作经验学习\n",
    "- 在动作选择不重要的状态更快收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 优先经验回放 (PER)\n",
    "\n",
    "**核心思想**: 按TD误差大小优先采样\n",
    "$$P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}, \\quad p_i = |\\delta_i| + \\epsilon$$\n",
    "\n",
    "**重要性采样权重**:\n",
    "$$w_i = \\left( \\frac{1}{N \\cdot P(i)} \\right)^\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffers import PrioritizedReplayBuffer\n",
    "\n",
    "# 创建优先回放缓冲区\n",
    "per_buffer = PrioritizedReplayBuffer(\n",
    "    capacity=10000,\n",
    "    alpha=0.6,  # 优先化程度\n",
    "    beta_start=0.4,  # 初始重要性采样\n",
    ")\n",
    "\n",
    "print(f'PER缓冲区: {per_buffer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程介绍了DQN的核心概念和实现:\n",
    "\n",
    "1. **经验回放**: 打破时序相关性，提高数据效率\n",
    "2. **目标网络**: 提供稳定的训练目标\n",
    "3. **Double DQN**: 解决过估计问题\n",
    "4. **Dueling DQN**: 价值-优势分解\n",
    "5. **优先经验回放**: 高效采样重要样本\n",
    "\n",
    "### 下一步\n",
    "- 尝试不同的超参数组合\n",
    "- 在更复杂的环境中测试\n",
    "- 实现Rainbow DQN (组合所有改进)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
