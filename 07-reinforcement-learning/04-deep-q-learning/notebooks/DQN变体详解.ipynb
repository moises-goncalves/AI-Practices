{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Variants: 深度Q网络变体详解\n",
    "\n",
    "---\n",
    "\n",
    "## 模块概述\n",
    "\n",
    "本模块系统地介绍Deep Q-Network (DQN)的主要变体算法，包括理论推导、核心创新点、实现细节和对比分析。\n",
    "\n",
    "### 学习目标\n",
    "\n",
    "完成本模块后，您将能够：\n",
    "\n",
    "1. **理解原始DQN的局限性**及其在实际应用中的失效模式\n",
    "2. **掌握各DQN变体的数学原理**，包括Double DQN、Dueling DQN、Noisy Networks、Categorical DQN等\n",
    "3. **实现各种replay buffer策略**（Uniform、Prioritized、N-step）\n",
    "4. **分析Rainbow算法**如何组合所有改进达到SOTA性能\n",
    "5. **进行算法对比实验**并解读结果\n",
    "\n",
    "### 目录\n",
    "\n",
    "1. [背景知识与问题定义](#1-背景知识与问题定义)\n",
    "2. [Double DQN: 消除过估计偏差](#2-double-dqn-消除过估计偏差)\n",
    "3. [Dueling DQN: 价值-优势分解](#3-dueling-dqn-价值-优势分解)\n",
    "4. [Noisy Networks: 参数化探索](#4-noisy-networks-参数化探索)\n",
    "5. [Categorical DQN (C51): 分布式强化学习](#5-categorical-dqn-c51-分布式强化学习)\n",
    "6. [Prioritized Experience Replay: 优先经验回放](#6-prioritized-experience-replay-优先经验回放)\n",
    "7. [N-step Learning: 多步学习](#7-n-step-learning-多步学习)\n",
    "8. [Rainbow: 集大成者](#8-rainbow-集大成者)\n",
    "9. [实验对比与分析](#9-实验对比与分析)\n",
    "10. [总结与展望](#10-总结与展望)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 环境配置与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 科学计算\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# 深度学习\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 强化学习环境\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"Warning: gymnasium not installed. Run: pip install gymnasium\")\n",
    "\n",
    "# 本地模块\n",
    "from dqn_variants import (\n",
    "    DQNVariant,\n",
    "    DQNVariantConfig,\n",
    "    DQNVariantAgent,\n",
    "    ReplayBuffer,\n",
    "    PrioritizedReplayBuffer,\n",
    "    NStepReplayBuffer,\n",
    "    SumTree,\n",
    "    NoisyLinear,\n",
    "    DQNNetwork,\n",
    "    DuelingNetwork,\n",
    "    NoisyNetwork,\n",
    "    CategoricalNetwork,\n",
    "    RainbowNetwork,\n",
    "    train_agent,\n",
    "    evaluate_agent,\n",
    "    compare_variants,\n",
    "    plot_comparison,\n",
    ")\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 可视化设置\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"Environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 背景知识与问题定义\n",
    "\n",
    "### 1.1 原始DQN回顾\n",
    "\n",
    "Deep Q-Network (Mnih et al., 2015) 是深度强化学习的里程碑式工作，首次实现了从原始像素到动作的端到端学习。\n",
    "\n",
    "**核心思想**：用神经网络 $Q(s, a; \\theta)$ 近似最优动作价值函数 $Q^*(s, a)$。\n",
    "\n",
    "**TD目标**：\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "**损失函数**：\n",
    "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\left[ (Q(s, a; \\theta) - y)^2 \\right]$$\n",
    "\n",
    "**两个关键创新**：\n",
    "1. **Experience Replay**: 打破样本相关性，提高数据效率\n",
    "2. **Target Network**: 稳定学习目标，防止发散\n",
    "\n",
    "### 1.2 原始DQN的局限性\n",
    "\n",
    "尽管DQN取得了突破性成果，但它存在几个根本性的局限："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化DQN的局限性\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. 过估计偏差 (Overestimation Bias)\n",
    "ax1 = axes[0, 0]\n",
    "np.random.seed(42)\n",
    "true_q = np.array([1.0, 1.5, 0.8, 1.2])  # 真实Q值\n",
    "noise = np.random.randn(4, 100) * 0.5  # 估计噪声\n",
    "estimated_q = true_q[:, np.newaxis] + noise\n",
    "max_estimated = np.max(estimated_q, axis=0)\n",
    "\n",
    "ax1.hist(max_estimated, bins=30, alpha=0.7, density=True, label='E[max Q]')\n",
    "ax1.axvline(np.max(true_q), color='red', linestyle='--', linewidth=2, label=f'max Q* = {np.max(true_q)}')\n",
    "ax1.axvline(np.mean(max_estimated), color='blue', linestyle='-', linewidth=2, label=f'E[max Q] = {np.mean(max_estimated):.2f}')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('1. Overestimation Bias\\n(过估计偏差)')\n",
    "ax1.legend()\n",
    "ax1.text(0.05, 0.95, r'$E[\\max_a Q] \\geq \\max_a E[Q]$', transform=ax1.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "# 2. 样本效率问题 (Sample Inefficiency)\n",
    "ax2 = axes[0, 1]\n",
    "buffer_size = 1000\n",
    "td_errors = np.abs(np.random.randn(buffer_size)) * np.exp(-np.arange(buffer_size) / 200)  # TD误差随时间衰减\n",
    "uniform_samples = np.random.choice(buffer_size, 100, replace=False)\n",
    "prioritized_samples = np.random.choice(buffer_size, 100, replace=False, p=td_errors/td_errors.sum())\n",
    "\n",
    "ax2.scatter(np.arange(buffer_size), td_errors, alpha=0.3, s=10, label='All transitions')\n",
    "ax2.scatter(uniform_samples, td_errors[uniform_samples], color='red', s=30, alpha=0.8, label='Uniform sampling')\n",
    "ax2.scatter(prioritized_samples, td_errors[prioritized_samples], color='green', s=30, alpha=0.8, marker='^', label='Prioritized sampling')\n",
    "ax2.set_xlabel('Transition index')\n",
    "ax2.set_ylabel('|TD error|')\n",
    "ax2.set_title('2. Sample Inefficiency\\n(样本效率低)')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. 探索问题 (Exploration Challenge)\n",
    "ax3 = axes[1, 0]\n",
    "steps = np.arange(100000)\n",
    "epsilon_greedy = np.maximum(1.0 - steps / 50000, 0.1)  # ε-greedy decay\n",
    "noisy_exploration = 0.5 * np.exp(-steps / 30000) + 0.1 * np.sin(steps / 5000)  # 噪声网络的state-dependent探索\n",
    "noisy_exploration = np.maximum(noisy_exploration, 0.05)\n",
    "\n",
    "ax3.plot(steps, epsilon_greedy, 'r-', linewidth=2, label='ε-greedy (state-independent)')\n",
    "ax3.plot(steps, noisy_exploration, 'g-', linewidth=2, alpha=0.8, label='Noisy Networks (state-dependent)')\n",
    "ax3.set_xlabel('Training steps')\n",
    "ax3.set_ylabel('Exploration amount')\n",
    "ax3.set_title('3. Exploration Challenge\\n(探索问题)')\n",
    "ax3.legend()\n",
    "ax3.set_xlim(0, 100000)\n",
    "\n",
    "# 4. 标量值局限 (Scalar Value Limitation)\n",
    "ax4 = axes[1, 1]\n",
    "x = np.linspace(-10, 30, 1000)\n",
    "dist1 = 0.5 * np.exp(-(x - 10)**2 / 20) + 0.5 * np.exp(-(x - 10)**2 / 200)  # 低方差分布\n",
    "dist2 = 0.25 * np.exp(-(x - 0)**2 / 10) + 0.75 * np.exp(-(x - 13.3)**2 / 10)  # 高方差分布\n",
    "dist1 /= np.sum(dist1) * (x[1] - x[0])\n",
    "dist2 /= np.sum(dist2) * (x[1] - x[0])\n",
    "\n",
    "ax4.fill_between(x, 0, dist1, alpha=0.5, label=f'Action A (E[R]={np.sum(x * dist1 * (x[1]-x[0])):.1f}, low var)')\n",
    "ax4.fill_between(x, 0, dist2, alpha=0.5, label=f'Action B (E[R]={np.sum(x * dist2 * (x[1]-x[0])):.1f}, high var)')\n",
    "ax4.set_xlabel('Return')\n",
    "ax4.set_ylabel('Probability density')\n",
    "ax4.set_title('4. Scalar Value Limitation\\n(标量值局限: 期望相同但分布不同)')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('DQN的四大局限性', fontsize=14, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 问题总结\n",
    "\n",
    "| 问题 | 原因 | 影响 | 解决方案 |\n",
    "|------|------|------|----------|\n",
    "| **过估计偏差** | max操作同时用于选择和评估 | 不稳定、次优策略 | Double DQN |\n",
    "| **样本效率低** | 均匀随机采样 | 学习慢、数据浪费 | Prioritized Replay |\n",
    "| **探索能力弱** | ε-greedy与状态无关 | 难以逃离局部最优 | Noisy Networks |\n",
    "| **标量值局限** | 只建模期望值 | 丢失分布信息、风险中立 | Categorical DQN |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Double DQN: 消除过估计偏差\n",
    "\n",
    "### 2.1 核心思想\n",
    "\n",
    "**问题根源**: 标准DQN使用同一个网络的max操作同时进行动作选择和动作评估。\n",
    "\n",
    "$$y^{\\text{DQN}} = r + \\gamma \\underbrace{\\max_{a'} Q(s', a'; \\theta^-)}_{\\text{selection = evaluation}}$$\n",
    "\n",
    "当Q值估计有噪声时，max会倾向于选择被高估的动作，导致系统性过估计。\n",
    "\n",
    "**Double DQN解决方案**: 解耦动作选择（使用online network）和动作评估（使用target network）。\n",
    "\n",
    "$$y^{\\text{Double}} = r + \\gamma Q\\left(s', \\underbrace{\\arg\\max_{a'} Q(s', a'; \\theta)}_{\\text{online selects}\\;\\;\\;\\;\\;\\;\\;\\;\\;}; \\theta^-\\right)$$\n",
    "\n",
    "### 2.2 数学推导\n",
    "\n",
    "设真实Q值为 $Q^*(s,a)$，估计为 $Q(s,a) = Q^*(s,a) + \\epsilon(s,a)$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$。\n",
    "\n",
    "**标准DQN的过估计**:\n",
    "$$\\mathbb{E}\\left[\\max_a Q(s,a)\\right] = \\mathbb{E}\\left[\\max_a (Q^*(s,a) + \\epsilon(s,a))\\right] \\geq \\max_a Q^*(s,a)$$\n",
    "\n",
    "**Double DQN的校正**:\n",
    "由于选择和评估使用不同的噪声源，过估计偏差被大幅减少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_overestimation(n_actions=10, n_samples=10000, noise_std=1.0):\n",
    "    \"\"\"模拟过估计偏差实验\"\"\"\n",
    "    true_q = np.zeros(n_actions)  # 所有动作的真实Q值都是0\n",
    "    \n",
    "    # 标准DQN: 同一噪声源用于选择和评估\n",
    "    dqn_estimates = []\n",
    "    for _ in range(n_samples):\n",
    "        noise = np.random.randn(n_actions) * noise_std\n",
    "        estimated_q = true_q + noise\n",
    "        dqn_estimates.append(np.max(estimated_q))  # max用于选择和评估\n",
    "    \n",
    "    # Double DQN: 不同噪声源用于选择和评估\n",
    "    double_dqn_estimates = []\n",
    "    for _ in range(n_samples):\n",
    "        noise_online = np.random.randn(n_actions) * noise_std  # online network噪声\n",
    "        noise_target = np.random.randn(n_actions) * noise_std  # target network噪声\n",
    "        \n",
    "        estimated_online = true_q + noise_online\n",
    "        estimated_target = true_q + noise_target\n",
    "        \n",
    "        best_action = np.argmax(estimated_online)  # online选择\n",
    "        double_dqn_estimates.append(estimated_target[best_action])  # target评估\n",
    "    \n",
    "    return np.mean(dqn_estimates), np.mean(double_dqn_estimates), np.max(true_q)\n",
    "\n",
    "# 不同动作数量下的过估计\n",
    "action_counts = [2, 4, 8, 16, 32, 64]\n",
    "dqn_bias = []\n",
    "ddqn_bias = []\n",
    "\n",
    "for n_actions in action_counts:\n",
    "    dqn_est, ddqn_est, true_max = simulate_overestimation(n_actions=n_actions)\n",
    "    dqn_bias.append(dqn_est - true_max)\n",
    "    ddqn_bias.append(ddqn_est - true_max)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(action_counts, dqn_bias, 'ro-', linewidth=2, markersize=10, label='Standard DQN (overestimation)')\n",
    "plt.plot(action_counts, ddqn_bias, 'g^-', linewidth=2, markersize=10, label='Double DQN (corrected)')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1, label='True value')\n",
    "plt.xlabel('Number of Actions', fontsize=12)\n",
    "plt.ylabel('Estimation Bias', fontsize=12)\n",
    "plt.title('Overestimation Bias: DQN vs Double DQN\\n(动作数越多, 过估计越严重)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log', base=2)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n动作数={action_counts[-1]}时:\")\n",
    "print(f\"  DQN过估计: {dqn_bias[-1]:.3f}\")\n",
    "print(f\"  Double DQN偏差: {ddqn_bias[-1]:.3f}\")\n",
    "print(f\"  改善幅度: {(dqn_bias[-1] - ddqn_bias[-1]) / dqn_bias[-1] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dueling DQN: 价值-优势分解\n",
    "\n",
    "### 3.1 核心思想\n",
    "\n",
    "**关键洞察**: 在很多状态下，知道状态的价值比知道每个动作的精确价值更重要。\n",
    "\n",
    "将Q函数分解为：\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a')$$\n",
    "\n",
    "其中：\n",
    "- $V(s)$: 状态价值 — \"这个状态有多好？\"\n",
    "- $A(s, a)$: 优势函数 — \"动作a比平均动作好多少？\"\n",
    "\n",
    "### 3.2 可辨识性约束\n",
    "\n",
    "**问题**: $V(s) + A(s,a) = (V(s) + c) + (A(s,a) - c)$ 对任意常数c都成立。\n",
    "\n",
    "**解决方案**: 强制 $\\sum_a A(s,a) = 0$，通过减去均值实现。\n",
    "\n",
    "### 3.3 网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化Dueling架构\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 标准DQN架构\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# 画框和箭头\n",
    "boxes_dqn = [\n",
    "    {'pos': (1, 5), 'text': 'State\\n$s$', 'color': 'lightblue'},\n",
    "    {'pos': (4, 5), 'text': 'Shared\\nLayers', 'color': 'lightyellow'},\n",
    "    {'pos': (7, 5), 'text': 'Q-values\\n$Q(s,a)$', 'color': 'lightgreen'},\n",
    "]\n",
    "\n",
    "for box in boxes_dqn:\n",
    "    rect = plt.Rectangle((box['pos'][0]-0.8, box['pos'][1]-0.8), 1.6, 1.6, \n",
    "                          facecolor=box['color'], edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(box['pos'][0], box['pos'][1], box['text'], ha='center', va='center', fontsize=11)\n",
    "\n",
    "ax1.annotate('', xy=(3.2, 5), xytext=(1.8, 5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax1.annotate('', xy=(6.2, 5), xytext=(4.8, 5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax1.set_title('Standard DQN Architecture', fontsize=14)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Dueling DQN架构\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "boxes_dueling = [\n",
    "    {'pos': (1, 5), 'text': 'State\\n$s$', 'color': 'lightblue'},\n",
    "    {'pos': (3.5, 5), 'text': 'Shared\\nLayers', 'color': 'lightyellow'},\n",
    "    {'pos': (6, 7), 'text': 'Value\\n$V(s)$', 'color': 'lightcoral'},\n",
    "    {'pos': (6, 3), 'text': 'Advantage\\n$A(s,a)$', 'color': 'lightgreen'},\n",
    "    {'pos': (8.5, 5), 'text': 'Q-values\\n$Q(s,a)$', 'color': 'plum'},\n",
    "]\n",
    "\n",
    "for box in boxes_dueling:\n",
    "    rect = plt.Rectangle((box['pos'][0]-0.8, box['pos'][1]-0.8), 1.6, 1.6, \n",
    "                          facecolor=box['color'], edgecolor='black', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(box['pos'][0], box['pos'][1], box['text'], ha='center', va='center', fontsize=10)\n",
    "\n",
    "# 箭头\n",
    "ax2.annotate('', xy=(2.7, 5), xytext=(1.8, 5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax2.annotate('', xy=(5.2, 7), xytext=(4.3, 5.5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax2.annotate('', xy=(5.2, 3), xytext=(4.3, 4.5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax2.annotate('', xy=(7.7, 5.5), xytext=(6.8, 6.5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax2.annotate('', xy=(7.7, 4.5), xytext=(6.8, 3.5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# 聚合公式\n",
    "ax2.text(5, 1, r'$Q(s,a) = V(s) + A(s,a) - \\frac{1}{|A|}\\sum_{a\\'} A(s,a\\')$', \n",
    "         ha='center', fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax2.set_title('Dueling DQN Architecture', fontsize=14)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示Dueling网络的前向传播\n",
    "state_dim, action_dim, hidden_dim = 4, 2, 64\n",
    "\n",
    "# 创建Dueling网络\n",
    "dueling_net = DuelingNetwork(state_dim, action_dim, hidden_dim)\n",
    "print(\"Dueling Network Architecture:\")\n",
    "print(dueling_net)\n",
    "\n",
    "# 随机状态输入\n",
    "sample_state = torch.randn(1, state_dim)\n",
    "\n",
    "# 前向传播查看中间结果\n",
    "with torch.no_grad():\n",
    "    features = F.relu(dueling_net.fc1(sample_state))\n",
    "    features = F.relu(dueling_net.fc2(features))\n",
    "    \n",
    "    value = dueling_net.value_stream(features)\n",
    "    advantage = dueling_net.advantage_stream(features)\n",
    "    q_values = dueling_net(sample_state)\n",
    "\n",
    "print(f\"\\n中间结果:\")\n",
    "print(f\"  V(s) = {value.item():.4f}\")\n",
    "print(f\"  A(s,a) = {advantage.numpy().flatten()}\")\n",
    "print(f\"  mean A(s,a) = {advantage.mean().item():.4f}\")\n",
    "print(f\"  Q(s,a) = {q_values.numpy().flatten()}\")\n",
    "print(f\"\\n验证: V + (A - mean(A)) = Q\")\n",
    "print(f\"  {value.item():.4f} + ({advantage.numpy().flatten()} - {advantage.mean().item():.4f}) = {q_values.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Noisy Networks: 参数化探索\n",
    "\n",
    "### 4.1 核心思想\n",
    "\n",
    "**问题**: ε-greedy探索是状态无关的，对所有状态使用相同的探索概率。\n",
    "\n",
    "**解决方案**: 在网络权重中加入可学习的噪声参数，实现状态依赖的探索。\n",
    "\n",
    "**Noisy Linear Layer**:\n",
    "$$y = (\\mu^w + \\sigma^w \\odot \\varepsilon^w) x + (\\mu^b + \\sigma^b \\odot \\varepsilon^b)$$\n",
    "\n",
    "其中：\n",
    "- $\\mu$: 可学习的均值参数\n",
    "- $\\sigma$: 可学习的噪声尺度参数\n",
    "- $\\varepsilon$: 随机噪声\n",
    "\n",
    "### 4.2 因式分解噪声\n",
    "\n",
    "为减少参数量，使用因式分解高斯噪声：\n",
    "$$\\varepsilon_{ij} = f(\\varepsilon_i) \\cdot f(\\varepsilon_j), \\quad f(x) = \\text{sign}(x)\\sqrt{|x|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示NoisyLinear层\n",
    "in_features, out_features = 64, 32\n",
    "noisy_layer = NoisyLinear(in_features, out_features)\n",
    "\n",
    "print(\"NoisyLinear Layer Parameters:\")\n",
    "print(f\"  mu_weight shape: {noisy_layer.mu_weight.shape}\")\n",
    "print(f\"  sigma_weight shape: {noisy_layer.sigma_weight.shape}\")\n",
    "print(f\"  Total params: {sum(p.numel() for p in noisy_layer.parameters())}\")\n",
    "print(f\"  Standard Linear params would be: {in_features * out_features + out_features}\")\n",
    "\n",
    "# 演示噪声采样对输出的影响\n",
    "sample_input = torch.randn(1, in_features)\n",
    "\n",
    "outputs = []\n",
    "for _ in range(100):\n",
    "    noisy_layer.reset_noise()  # 重新采样噪声\n",
    "    with torch.no_grad():\n",
    "        output = noisy_layer(sample_input)\n",
    "    outputs.append(output.numpy().flatten())\n",
    "\n",
    "outputs = np.array(outputs)\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 输出分布\n",
    "ax1 = axes[0]\n",
    "for i in range(min(5, out_features)):\n",
    "    ax1.hist(outputs[:, i], bins=20, alpha=0.5, label=f'Output {i}')\n",
    "ax1.set_xlabel('Output value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Noisy Layer Output Distribution\\n(相同输入, 不同噪声采样)')\n",
    "ax1.legend()\n",
    "\n",
    "# 噪声参数可视化\n",
    "ax2 = axes[1]\n",
    "sigma_values = noisy_layer.sigma_weight.detach().numpy().flatten()\n",
    "ax2.hist(sigma_values, bins=50, alpha=0.7, color='orange')\n",
    "ax2.axvline(sigma_values.mean(), color='red', linestyle='--', label=f'Mean σ = {sigma_values.mean():.4f}')\n",
    "ax2.set_xlabel('σ value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Learned Noise Scales (σ)\\n(初始化后)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n输出统计:\")\n",
    "print(f\"  均值: {outputs.mean(axis=0)[:5]}\")\n",
    "print(f\"  标准差: {outputs.std(axis=0)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Categorical DQN (C51): 分布式强化学习\n",
    "\n",
    "### 5.1 核心思想\n",
    "\n",
    "**范式转变**: 从建模期望值转向建模完整的回报分布。\n",
    "\n",
    "**分布表示**: 使用N个固定支撑点的离散分布\n",
    "$$Z(s, a) \\sim \\text{Categorical}(z_1, ..., z_N; p_1(s,a), ..., p_N(s,a))$$\n",
    "\n",
    "其中支撑点为：\n",
    "$$z_i = V_{\\min} + i \\cdot \\Delta z, \\quad \\Delta z = \\frac{V_{\\max} - V_{\\min}}{N - 1}$$\n",
    "\n",
    "### 5.2 分布式Bellman算子\n",
    "\n",
    "$$\\mathcal{T} Z(s, a) \\stackrel{D}{=} R + \\gamma Z(S', A')$$\n",
    "\n",
    "### 5.3 投影到支撑点\n",
    "\n",
    "由于 $r + \\gamma z_j$ 可能落在支撑点之间，需要投影回离散支撑点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化Categorical DQN的分布表示\n",
    "n_atoms = 51\n",
    "v_min, v_max = -10, 10\n",
    "support = np.linspace(v_min, v_max, n_atoms)\n",
    "delta_z = (v_max - v_min) / (n_atoms - 1)\n",
    "\n",
    "# 模拟两个动作的回报分布\n",
    "def create_distribution(mean, std, support):\n",
    "    \"\"\"创建在支撑点上的高斯分布\"\"\"\n",
    "    probs = np.exp(-(support - mean)**2 / (2 * std**2))\n",
    "    probs /= probs.sum()\n",
    "    return probs\n",
    "\n",
    "dist_action1 = create_distribution(mean=5, std=2, support=support)  # 动作1: 高均值低方差\n",
    "dist_action2 = create_distribution(mean=5, std=5, support=support)  # 动作2: 相同均值高方差\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 原始分布\n",
    "ax1 = axes[0]\n",
    "ax1.bar(support, dist_action1, width=delta_z*0.8, alpha=0.7, label='Action 1 (low var)', color='blue')\n",
    "ax1.bar(support, dist_action2, width=delta_z*0.8, alpha=0.5, label='Action 2 (high var)', color='red')\n",
    "ax1.axvline(np.sum(support * dist_action1), color='blue', linestyle='--', label=f'E[Z1] = {np.sum(support * dist_action1):.1f}')\n",
    "ax1.axvline(np.sum(support * dist_action2), color='red', linestyle='--', label=f'E[Z2] = {np.sum(support * dist_action2):.1f}')\n",
    "ax1.set_xlabel('Return value')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title('Return Distributions for Two Actions\\n(期望相同, 方差不同)')\n",
    "ax1.legend()\n",
    "\n",
    "# 分布式Bellman投影演示\n",
    "ax2 = axes[1]\n",
    "reward = 1.0\n",
    "gamma = 0.99\n",
    "shifted_support = reward + gamma * support\n",
    "\n",
    "ax2.bar(support, dist_action1, width=delta_z*0.8, alpha=0.5, label='Original Z(s,a)', color='blue')\n",
    "ax2.bar(shifted_support, dist_action1, width=delta_z*0.8, alpha=0.5, label=f'r + γZ(s\\',a\\') [r={reward}]', color='green')\n",
    "ax2.axvline(v_min, color='red', linestyle=':', label=f'Support bounds: [{v_min}, {v_max}]')\n",
    "ax2.axvline(v_max, color='red', linestyle=':')\n",
    "ax2.set_xlabel('Return value')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('Distributional Bellman Shift\\n(需要投影回支撑点)')\n",
    "ax2.legend()\n",
    "\n",
    "# 投影后的分布\n",
    "ax3 = axes[2]\n",
    "# 简化的投影实现\n",
    "projected_probs = np.zeros(n_atoms)\n",
    "for i, z in enumerate(support):\n",
    "    shifted_z = np.clip(reward + gamma * z, v_min, v_max)\n",
    "    # 找到相邻的支撑点\n",
    "    lower_idx = int((shifted_z - v_min) / delta_z)\n",
    "    lower_idx = np.clip(lower_idx, 0, n_atoms - 2)\n",
    "    upper_idx = lower_idx + 1\n",
    "    # 线性插值\n",
    "    upper_weight = (shifted_z - support[lower_idx]) / delta_z\n",
    "    lower_weight = 1 - upper_weight\n",
    "    projected_probs[lower_idx] += lower_weight * dist_action1[i]\n",
    "    projected_probs[upper_idx] += upper_weight * dist_action1[i]\n",
    "\n",
    "ax3.bar(support, dist_action1, width=delta_z*0.8, alpha=0.5, label='Original', color='blue')\n",
    "ax3.bar(support, projected_probs, width=delta_z*0.4, alpha=0.7, label='After projection', color='green')\n",
    "ax3.set_xlabel('Return value')\n",
    "ax3.set_ylabel('Probability')\n",
    "ax3.set_title('Categorical Projection\\n(投影回原始支撑点)')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"C51参数:\")\n",
    "print(f\"  N_atoms = {n_atoms}\")\n",
    "print(f\"  V_min = {v_min}, V_max = {v_max}\")\n",
    "print(f\"  Δz = {delta_z:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Prioritized Experience Replay: 优先经验回放\n",
    "\n",
    "### 6.1 核心思想\n",
    "\n",
    "**问题**: 均匀采样对待所有样本一视同仁，浪费了高信息量样本的学习价值。\n",
    "\n",
    "**解决方案**: 根据TD误差大小分配采样优先级。\n",
    "\n",
    "### 6.2 优先级定义\n",
    "\n",
    "$$p_i = |\\delta_i| + \\epsilon$$\n",
    "\n",
    "其中 $\\delta_i$ 是TD误差，$\\epsilon$ 防止零优先级。\n",
    "\n",
    "### 6.3 采样概率\n",
    "\n",
    "$$P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$$\n",
    "\n",
    "- $\\alpha = 0$: 均匀采样\n",
    "- $\\alpha = 1$: 完全优先级采样\n",
    "\n",
    "### 6.4 重要性采样校正\n",
    "\n",
    "优先级采样改变了数据分布，需要IS权重校正：\n",
    "\n",
    "$$w_i = \\left( \\frac{1}{N \\cdot P(i)} \\right)^\\beta / \\max_j w_j$$\n",
    "\n",
    "$\\beta$ 从 $\\beta_0$ 逐渐退火到1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示SumTree数据结构\n",
    "capacity = 8\n",
    "tree = SumTree(capacity)\n",
    "\n",
    "# 添加样本\n",
    "priorities = [1.0, 3.0, 2.0, 4.0, 1.5, 2.5, 3.5, 0.5]\n",
    "for i, p in enumerate(priorities):\n",
    "    tree.add(p, f\"data_{i}\")\n",
    "\n",
    "print(\"SumTree Structure:\")\n",
    "print(f\"  Capacity: {capacity}\")\n",
    "print(f\"  Priorities: {priorities}\")\n",
    "print(f\"  Total priority (root): {tree.total_priority}\")\n",
    "print(f\"  Expected total: {sum(priorities)}\")\n",
    "\n",
    "# 可视化采样分布\n",
    "n_samples = 10000\n",
    "sample_counts = np.zeros(capacity)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    cumsum = np.random.uniform(0, tree.total_priority)\n",
    "    idx, priority, data = tree.get(cumsum)\n",
    "    data_idx = int(data.split('_')[1])\n",
    "    sample_counts[data_idx] += 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 优先级vs采样频率\n",
    "ax1 = axes[0]\n",
    "x = np.arange(capacity)\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, np.array(priorities) / sum(priorities), width, label='Expected (priority/total)', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, sample_counts / n_samples, width, label='Empirical sampling freq', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Data index')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title('Prioritized Sampling Distribution')\n",
    "ax1.set_xticks(x)\n",
    "ax1.legend()\n",
    "\n",
    "# β退火曲线\n",
    "ax2 = axes[1]\n",
    "beta_start = 0.4\n",
    "beta_frames = 100000\n",
    "frames = np.arange(beta_frames + 50000)\n",
    "beta = beta_start + (1 - beta_start) * np.minimum(frames / beta_frames, 1.0)\n",
    "\n",
    "ax2.plot(frames, beta, 'g-', linewidth=2)\n",
    "ax2.axhline(1.0, color='red', linestyle='--', label='β = 1 (full correction)')\n",
    "ax2.axvline(beta_frames, color='gray', linestyle=':', label=f'beta_frames = {beta_frames}')\n",
    "ax2.fill_between(frames, beta_start, beta, alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Training frames')\n",
    "ax2.set_ylabel('β (IS exponent)')\n",
    "ax2.set_title('Importance Sampling Correction Annealing')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示PrioritizedReplayBuffer\n",
    "per_buffer = PrioritizedReplayBuffer(\n",
    "    capacity=1000,\n",
    "    alpha=0.6,\n",
    "    beta_start=0.4,\n",
    "    beta_frames=10000\n",
    ")\n",
    "\n",
    "# 添加样本\n",
    "for _ in range(100):\n",
    "    state = np.random.randn(4).astype(np.float32)\n",
    "    action = np.random.randint(2)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(4).astype(np.float32)\n",
    "    done = np.random.random() < 0.1\n",
    "    per_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"PER Buffer状态:\")\n",
    "print(f\"  Size: {len(per_buffer)}\")\n",
    "print(f\"  Beta: {per_buffer.beta:.4f}\")\n",
    "\n",
    "# 采样\n",
    "states, actions, rewards, next_states, dones, indices, weights = per_buffer.sample(32)\n",
    "\n",
    "print(f\"\\n采样结果:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Weights range: [{weights.min():.4f}, {weights.max():.4f}]\")\n",
    "print(f\"  Mean weight: {weights.mean():.4f}\")\n",
    "\n",
    "# 更新优先级\n",
    "td_errors = np.random.randn(32)  # 模拟TD误差\n",
    "per_buffer.update_priorities(indices, td_errors)\n",
    "print(f\"\\n优先级更新完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. N-step Learning: 多步学习\n",
    "\n",
    "### 7.1 核心思想\n",
    "\n",
    "**TD(0)**: 仅使用即时奖励，高偏差低方差\n",
    "$$G_t^{(1)} = r_{t+1} + \\gamma V(s_{t+1})$$\n",
    "\n",
    "**Monte Carlo**: 使用完整回报，零偏差高方差\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ...$$\n",
    "\n",
    "**N-step**: 折中方案\n",
    "$$G_t^{(n)} = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n V(s_{t+n})$$\n",
    "\n",
    "### 7.2 偏差-方差权衡\n",
    "\n",
    "| n | 偏差 | 方差 | 特点 |\n",
    "|---|------|------|------|\n",
    "| 1 | 高 | 低 | 保守但稳定 |\n",
    "| 3-5 | 中 | 中 | 常用sweet spot |\n",
    "| ∞ | 零 | 高 | Monte Carlo |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示N-step returns计算\n",
    "def compute_n_step_return(rewards, gamma, n, bootstrap_value=0):\n",
    "    \"\"\"计算n-step return\"\"\"\n",
    "    n_step_return = 0.0\n",
    "    for i in range(min(n, len(rewards))):\n",
    "        n_step_return += (gamma ** i) * rewards[i]\n",
    "    if len(rewards) >= n:\n",
    "        n_step_return += (gamma ** n) * bootstrap_value\n",
    "    return n_step_return\n",
    "\n",
    "# 模拟一个episode的奖励序列\n",
    "np.random.seed(42)\n",
    "episode_rewards = np.random.randn(20) + 0.5  # 均值为正的奖励\n",
    "gamma = 0.99\n",
    "bootstrap_value = 5.0  # 最终状态价值估计\n",
    "\n",
    "# 计算不同n的return\n",
    "n_values = [1, 3, 5, 10, len(episode_rewards)]\n",
    "returns_by_n = {}\n",
    "\n",
    "for n in n_values:\n",
    "    returns = []\n",
    "    for t in range(len(episode_rewards) - n + 1):\n",
    "        rewards_slice = episode_rewards[t:t+n]\n",
    "        g = compute_n_step_return(rewards_slice, gamma, n, bootstrap_value)\n",
    "        returns.append(g)\n",
    "    returns_by_n[n] = returns\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 不同n的return比较\n",
    "ax1 = axes[0]\n",
    "for n in n_values:\n",
    "    label = 'MC' if n == len(episode_rewards) else f'n={n}'\n",
    "    ax1.plot(returns_by_n[n], 'o-', alpha=0.7, label=label, markersize=4)\n",
    "ax1.set_xlabel('Time step t')\n",
    "ax1.set_ylabel(f'G_t^(n) (γ={gamma})')\n",
    "ax1.set_title('N-step Returns at Different Time Steps')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 偏差-方差权衡可视化\n",
    "ax2 = axes[1]\n",
    "\n",
    "# 模拟多次实验\n",
    "n_experiments = 100\n",
    "n_range = range(1, 16)\n",
    "bias_estimates = []\n",
    "variance_estimates = []\n",
    "\n",
    "true_return = np.sum([gamma**i * episode_rewards[i] for i in range(len(episode_rewards))])\n",
    "\n",
    "for n in n_range:\n",
    "    experiment_returns = []\n",
    "    for _ in range(n_experiments):\n",
    "        # 添加bootstrap噪声\n",
    "        noisy_bootstrap = bootstrap_value + np.random.randn() * 2\n",
    "        g = compute_n_step_return(episode_rewards[:n], gamma, n, noisy_bootstrap)\n",
    "        experiment_returns.append(g)\n",
    "    \n",
    "    bias_estimates.append(abs(np.mean(experiment_returns) - true_return))\n",
    "    variance_estimates.append(np.var(experiment_returns))\n",
    "\n",
    "ax2.plot(list(n_range), bias_estimates, 'b-o', label='Bias', linewidth=2)\n",
    "ax2.plot(list(n_range), variance_estimates, 'r-^', label='Variance', linewidth=2)\n",
    "ax2.set_xlabel('n (number of steps)')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Bias-Variance Tradeoff in N-step Learning')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Rainbow: 集大成者\n",
    "\n",
    "### 8.1 组合策略\n",
    "\n",
    "Rainbow (Hessel et al., 2018) 组合了所有改进：\n",
    "\n",
    "1. **Double DQN** - 消除过估计\n",
    "2. **Dueling DQN** - 价值-优势分解\n",
    "3. **Noisy Networks** - 参数化探索\n",
    "4. **Categorical DQN** - 分布式RL\n",
    "5. **Prioritized Experience Replay** - 优先采样\n",
    "6. **N-step Learning** - 多步回报\n",
    "\n",
    "### 8.2 性能提升\n",
    "\n",
    "在Atari基准上的中位人类标准化分数：\n",
    "\n",
    "| 算法 | 分数 | 相对DQN提升 |\n",
    "|------|------|-------------|\n",
    "| DQN | 79% | baseline |\n",
    "| Double DQN | 117% | +48% |\n",
    "| Prioritized DQN | 141% | +78% |\n",
    "| Dueling DQN | 151% | +91% |\n",
    "| Categorical DQN | 235% | +197% |\n",
    "| **Rainbow** | **441%** | **+458%** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化Rainbow组件贡献\n",
    "components = ['DQN', '+ Double', '+ PER', '+ Dueling', '+ Noisy', '+ Categorical', '+ N-step\\n(Rainbow)']\n",
    "scores = [79, 117, 141, 151, 180, 235, 441]  # 大致的累积改进\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 累积性能\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(components)))\n",
    "bars = ax1.bar(components, scores, color=colors, edgecolor='black', linewidth=1)\n",
    "ax1.axhline(100, color='red', linestyle='--', linewidth=2, label='Human performance')\n",
    "ax1.set_ylabel('Median Human-Normalized Score (%)')\n",
    "ax1.set_title('Cumulative Performance Improvements\\n(Atari Benchmark)')\n",
    "ax1.legend()\n",
    "\n",
    "# 添加数值标签\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "             f'{score}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 消融实验结果\n",
    "ax2 = axes[1]\n",
    "ablation_components = ['Rainbow', '- PER', '- Multi-step', '- Distributional', '- Noisy', '- Dueling', '- Double']\n",
    "ablation_scores = [441, 358, 340, 315, 312, 298, 377]\n",
    "\n",
    "colors2 = ['green'] + ['red'] * (len(ablation_components) - 1)\n",
    "bars2 = ax2.barh(ablation_components, ablation_scores, color=colors2, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Median Human-Normalized Score (%)')\n",
    "ax2.set_title('Ablation Study: Removing Components\\n(哪个组件影响最大?)')\n",
    "\n",
    "for bar, score in zip(bars2, ablation_scores):\n",
    "    ax2.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,\n",
    "             f'{score}%', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"消融分析关键发现:\")\n",
    "print(\"  1. 移除PER影响最大 (-83%)\")\n",
    "print(\"  2. 移除Multi-step影响次之 (-101%)\")\n",
    "print(\"  3. 所有组件都有正向贡献\")\n",
    "print(\"  4. 组件之间存在协同效应\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 实验对比与分析\n",
    "\n",
    "下面我们在CartPole环境上对比各DQN变体的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速实验: 对比不同DQN变体\n",
    "# 注意: 为了节省时间，使用较少的episodes\n",
    "\n",
    "if HAS_GYM:\n",
    "    print(\"开始对比实验 (这可能需要几分钟...)\\n\")\n",
    "    \n",
    "    # 使用较小的参数进行快速演示\n",
    "    results = compare_variants(\n",
    "        env_name=\"CartPole-v1\",\n",
    "        variants=[\n",
    "            DQNVariant.VANILLA,\n",
    "            DQNVariant.DOUBLE,\n",
    "            DQNVariant.DUELING,\n",
    "            DQNVariant.RAINBOW,\n",
    "        ],\n",
    "        num_episodes=100,  # 实际应用中使用更多episodes\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    # 绘制对比图\n",
    "    plot_comparison(results)\n",
    "else:\n",
    "    print(\"Gymnasium未安装, 跳过实验\")\n",
    "    print(\"安装命令: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 总结与展望\n",
    "\n",
    "### 10.1 核心要点回顾\n",
    "\n",
    "| 变体 | 核心创新 | 解决的问题 |\n",
    "|------|----------|------------|\n",
    "| **Double DQN** | 解耦动作选择与评估 | 过估计偏差 |\n",
    "| **Dueling DQN** | V/A分解架构 | 泛化能力 |\n",
    "| **Noisy Networks** | 参数化噪声 | 状态无关探索 |\n",
    "| **Categorical DQN** | 分布建模 | 标量值局限 |\n",
    "| **PER** | TD误差优先级 | 样本效率 |\n",
    "| **N-step** | 多步bootstrap | 信用分配 |\n",
    "| **Rainbow** | 全部组合 | 最优性能 |\n",
    "\n",
    "### 10.2 实践建议\n",
    "\n",
    "1. **简单任务**: Vanilla DQN或Double DQN足够\n",
    "2. **中等难度**: Double + Dueling + PER是好的组合\n",
    "3. **困难任务**: 使用Rainbow获得最佳性能\n",
    "4. **计算受限**: Double DQN + N-step是性价比最高的选择\n",
    "\n",
    "### 10.3 后续学习方向\n",
    "\n",
    "- **Implicit Quantile Networks (IQN)**: 更灵活的分布建模\n",
    "- **Noisy Networks改进**: 如NoisyNet-A3C\n",
    "- **Retrace/V-trace**: 离策略校正\n",
    "- **R2D2**: 分布式+循环神经网络\n",
    "- **Agent57**: 超越人类水平的Atari智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Mnih, V. et al. (2015). Human-level control through deep reinforcement learning. *Nature*.\n",
    "2. van Hasselt, H. et al. (2016). Deep Reinforcement Learning with Double Q-learning. *AAAI*.\n",
    "3. Wang, Z. et al. (2016). Dueling Network Architectures for Deep Reinforcement Learning. *ICML*.\n",
    "4. Fortunato, M. et al. (2017). Noisy Networks for Exploration. *ICLR*.\n",
    "5. Bellemare, M. et al. (2017). A Distributional Perspective on Reinforcement Learning. *ICML*.\n",
    "6. Schaul, T. et al. (2016). Prioritized Experience Replay. *ICLR*.\n",
    "7. Hessel, M. et al. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. *AAAI*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
