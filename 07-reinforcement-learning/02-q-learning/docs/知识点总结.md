# Q-Learning 知识点总结

> 从理论到实践的完整知识框架

---

## 核心知识图谱

```
                        ┌─────────────────────────────────────────┐
                        │           强化学习 (RL)                  │
                        └─────────────────┬───────────────────────┘
                                          │
                    ┌─────────────────────┴─────────────────────┐
                    │                                           │
           ┌────────▼────────┐                        ┌────────▼────────┐
           │  基于模型        │                        │  无模型          │
           │  Model-Based    │                        │  Model-Free     │
           └─────────────────┘                        └────────┬────────┘
                                                               │
                                          ┌────────────────────┴────────────────────┐
                                          │                                         │
                                 ┌────────▼────────┐                      ┌────────▼────────┐
                                 │  基于价值        │                      │  基于策略        │
                                 │  Value-Based    │                      │  Policy-Based   │
                                 └────────┬────────┘                      └─────────────────┘
                                          │
                    ┌─────────────────────┴─────────────────────┐
                    │                                           │
           ┌────────▼────────┐                        ┌────────▼────────┐
           │  蒙特卡洛 (MC)    │                        │  时序差分 (TD)    │
           │  完整回合更新    │                        │  单步更新        │
           └─────────────────┘                        └────────┬────────┘
                                                               │
                              ┌────────────────────────────────┼────────────────────────────────┐
                              │                                │                                │
                     ┌────────▼────────┐              ┌────────▼────────┐              ┌────────▼────────┐
                     │   Q-Learning    │              │     SARSA       │              │ Expected SARSA  │
                     │   Off-Policy    │              │   On-Policy     │              │   On-Policy     │
                     └────────┬────────┘              └─────────────────┘              └─────────────────┘
                              │
                     ┌────────▼────────┐
                     │ Double Q-Learning│
                     │   消除过估计     │
                     └─────────────────┘
```

---

## 一、时序差分学习 (TD Learning)

### 1.1 核心思想

**自举 (Bootstrapping)**：用估计值更新估计值，无需等待回合结束。

### 1.2 关键公式

| 方法 | 更新公式 | 特点 |
|------|----------|------|
| **蒙特卡洛** | $V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$ | 无偏但高方差 |
| **TD(0)** | $V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$ | 有偏但低方差 |

### 1.3 TD 误差

$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$$

| $\delta_t$ | 含义 | 操作 |
|------------|------|------|
| $> 0$ | 实际比预期好 | 增大估计 |
| $< 0$ | 实际比预期差 | 减小估计 |
| $= 0$ | 预测准确 | 已收敛 |

---

## 二、Q-Learning 算法

### 2.1 核心公式

$$\boxed{Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \right]}$$

### 2.2 算法特性

| 特性 | 说明 |
|------|------|
| **类型** | 离策略 (Off-Policy) TD 控制 |
| **目标** | 直接学习最优 Q 函数 $Q^*$ |
| **关键操作** | 使用 $\max$ 选择下一状态最优动作 |
| **收敛性** | 在一定条件下保证收敛到 $Q^*$ |

### 2.3 收敛条件

1. 所有状态-动作对被无限次访问
2. 学习率满足 Robbins-Monro 条件：$\sum_t \alpha_t = \infty$, $\sum_t \alpha_t^2 < \infty$

### 2.4 算法流程

```
算法: Q-Learning

输入: S (状态空间), A (动作空间), α (学习率), γ (折扣), ε (探索率)
输出: 最优 Q 函数

1. 初始化 Q(s, a) = 0, ∀s ∈ S, a ∈ A
2. FOR each episode:
   a. S ← 初始状态
   b. REPEAT:
      i.   A ← ε-greedy(Q, S)
      ii.  执行 A, 观察 R, S'
      iii. Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]
      iv.  S ← S'
   c. UNTIL S 是终止状态
3. RETURN Q
```

---

## 三、SARSA 算法

### 3.1 核心公式

$$\boxed{Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]}$$

### 3.2 名称来源

**S**tate-**A**ction-**R**eward-**S**tate-**A**ction

更新需要五元组：$(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$

### 3.3 与 Q-Learning 对比

| 特性 | Q-Learning | SARSA |
|------|------------|-------|
| 策略类型 | Off-Policy | On-Policy |
| 更新目标 | $\max_a Q(S',a)$ | $Q(S', A')$ |
| 学习的是 | 最优策略 $\pi^*$ | 当前策略 $\pi$ |
| 探索影响 | 不影响学习 | 直接影响学习 |
| 行为特点 | 激进（最短路径） | 保守（安全路径） |

### 3.4 悬崖行走示例

```
┌───────────────────────────────────────────┐
│ .  .  .  .  .  .  .  .  .  .  .  .       │
│ .  .  .  .  .  .  .  .  .  .  .  .       │
│ .  .  .  .  .  .  .  .  .  .  .  .       │
│ S  C  C  C  C  C  C  C  C  C  C  G       │
└───────────────────────────────────────────┘

Q-Learning: 学习沿悬崖的最短路径 → 训练时常掉落
SARSA:      学习远离悬崖的安全路径 → 训练更稳定
```

---

## 四、Expected SARSA

### 4.1 核心公式

$$Q(S,A) \leftarrow Q(S,A) + \alpha \left[ R + \gamma \mathbb{E}_\pi[Q(S',A')] - Q(S,A) \right]$$

### 4.2 期望值计算

对于 ε-greedy 策略：

$$\mathbb{E}[Q(S',A')] = \frac{\epsilon}{|A|} \sum_a Q(S',a) + (1-\epsilon) \max_a Q(S',a)$$

### 4.3 特点

- 结合 SARSA 的在策略特性和 Q-Learning 的低方差
- 当 ε=0 时，退化为 Q-Learning

---

## 五、Double Q-Learning

### 5.1 过估计问题

Q-Learning 使用 $\max$ 操作会系统性高估 Q 值：

$$\mathbb{E}[\max_a \hat{Q}(s,a)] \geq \max_a Q^*(s,a)$$

### 5.2 解决方案

维护两个独立 Q 表，解耦动作选择和价值评估：

$$Q_1(S,A) \leftarrow Q_1 + \alpha[R + \gamma Q_2(S', \arg\max_a Q_1(S',a)) - Q_1]$$

### 5.3 更新策略

以 50% 概率选择更新 $Q_1$ 或 $Q_2$：
- 更新 $Q_1$：用 $Q_1$ 选择动作，用 $Q_2$ 评估
- 更新 $Q_2$：用 $Q_2$ 选择动作，用 $Q_1$ 评估

---

## 六、探索策略

### 6.1 ε-Greedy

$$\pi(a|s) = \begin{cases} 1-\epsilon+\frac{\epsilon}{|A|} & \text{if } a = \arg\max_a Q(s,a) \\ \frac{\epsilon}{|A|} & \text{otherwise} \end{cases}$$

**特点**：简单有效，探索是均匀随机的

### 6.2 Softmax (Boltzmann)

$$\pi(a|s) = \frac{\exp(Q(s,a)/\tau)}{\sum_{a'} \exp(Q(s,a')/\tau)}$$

**温度参数 τ**：
- τ → 0：趋向贪心
- τ → ∞：趋向均匀随机

### 6.3 UCB (Upper Confidence Bound)

$$A_t = \arg\max_a \left[ Q(s,a) + c\sqrt{\frac{\ln t}{N(s,a)}} \right]$$

**特点**：平衡价值估计和不确定性，有理论保证

### 6.4 策略对比

| 策略 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| ε-Greedy | 简单、稳定 | 探索均匀 | 大多数场景 |
| Softmax | 考虑 Q 值差异 | 需调参 | Q 值有意义差异 |
| UCB | 理论保证、智能探索 | 计算开销 | 多臂老虎机 |

---

## 七、超参数选择指南

### 7.1 关键参数

| 参数 | 符号 | 典型值 | 说明 |
|------|------|--------|------|
| 学习率 | $\alpha$ | 0.1 ~ 0.5 | 表格型可用较大值 |
| 折扣因子 | $\gamma$ | 0.95 ~ 0.99 | 接近1重视长期奖励 |
| 初始探索率 | $\epsilon_0$ | 1.0 | 从完全探索开始 |
| 最小探索率 | $\epsilon_{min}$ | 0.01 ~ 0.1 | 保持少量探索 |
| 衰减率 | decay | 0.99 ~ 0.999 | 控制探索下降速度 |

### 7.2 调参建议

1. **学习率**：从 0.1 开始，观察学习曲线稳定性
2. **折扣因子**：任务越长期，越接近 1
3. **探索率**：确保早期充分探索，后期以利用为主
4. **衰减速度**：与训练回合数匹配

---

## 八、算法复杂度

| 算法 | 时间复杂度 (per update) | 空间复杂度 |
|------|------------------------|------------|
| Q-Learning | $O(|A|)$ | $O(|S| \times |A|)$ |
| SARSA | $O(1)$ | $O(|S| \times |A|)$ |
| Expected SARSA | $O(|A|)$ | $O(|S| \times |A|)$ |
| Double Q-Learning | $O(|A|)$ | $O(2 \times |S| \times |A|)$ |

---

## 九、常见问题与解决方案

### 9.1 学习不稳定

**症状**：Q 值震荡，奖励曲线波动大

**解决方案**：
- 减小学习率 α
- 使用学习率调度
- 增大探索率 ε

### 9.2 收敛过慢

**症状**：需要很多回合才能看到改善

**解决方案**：
- 增大学习率 α
- 使用乐观初始化
- 检查奖励设计

### 9.3 陷入局部最优

**症状**：找到的策略明显不是最优的

**解决方案**：
- 增大探索率 ε
- 使用更长的探索阶段
- 尝试不同的探索策略

### 9.4 过估计问题

**症状**：Q 值明显高于实际回报

**解决方案**：
- 使用 Double Q-Learning
- 降低折扣因子 γ
- 使用 Expected SARSA

---

## 十、表格型方法的局限性

### 10.1 主要限制

1. **状态空间必须离散且有限**
2. **无法处理连续状态**
3. **无法泛化到未见过的状态**
4. **内存随状态数线性增长**

### 10.2 解决方案：函数近似

```
表格型 Q-Learning          →          深度 Q 网络 (DQN)
Q-Table: |S| × |A|                   神经网络: θ
Q(s,a) 直接查表                       Q(s,a;θ) 函数近似
```

**DQN 关键技术**：
- 经验回放 (Experience Replay)
- 目标网络 (Target Network)
- 双重 DQN (Double DQN)
- 优先经验回放 (Prioritized Experience Replay)

---

## 十一、实践检查清单

### 训练前
- [ ] 环境接口正确（reset, step, done）
- [ ] 奖励函数设计合理
- [ ] 状态表示完整
- [ ] 超参数初始化

### 训练中
- [ ] 监控学习曲线（奖励、步数）
- [ ] 检查探索率衰减
- [ ] 观察 Q 值分布
- [ ] 定期评估策略

### 训练后
- [ ] 验证策略正确性
- [ ] 与基线对比
- [ ] 测试鲁棒性
- [ ] 保存模型

---

## 十二、关键公式速查

### TD 更新

$$V(S) \leftarrow V(S) + \alpha[\underbrace{R + \gamma V(S')}_{\text{TD Target}} - V(S)]$$

### Q-Learning

$$Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \max_a Q(S',a) - Q(S,A)]$$

### SARSA

$$Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma Q(S',A') - Q(S,A)]$$

### Expected SARSA

$$Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \sum_a \pi(a|S') Q(S',a) - Q(S,A)]$$

### Bellman 最优方程

$$Q^*(s,a) = \mathbb{E}[R + \gamma \max_{a'} Q^*(s',a') | s, a]$$

---

## 参考文献

1. Watkins, C.J.C.H. (1989). *Learning from Delayed Rewards*. PhD Thesis.
2. Rummery, G.A. & Niranjan, M. (1994). *On-Line Q-Learning Using Connectionist Systems*.
3. Van Hasselt, H. (2010). *Double Q-learning*. NeurIPS.
4. Sutton, R.S. & Barto, A.G. (2018). *Reinforcement Learning: An Introduction*, 2nd ed.

---

[返回上级](../README.md) | [下一节：DQN](../../03-dqn/README.md)
