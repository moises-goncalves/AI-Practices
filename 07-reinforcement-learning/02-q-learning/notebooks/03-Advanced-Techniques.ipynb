{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning 高级技巧与实战\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将掌握：\n",
    "- Q-Learning 过估计问题与 Double Q-Learning\n",
    "- 学习率调度与自适应更新\n",
    "- Gymnasium 环境实战训练\n",
    "- 模型保存、加载与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第一部分：Q-Learning 过估计问题\n",
    "\n",
    "### 1.1 过估计现象\n",
    "\n",
    "Q-Learning 更新使用 $\\max$：\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\max_a Q(S',a) - Q(S,A)]$$\n",
    "\n",
    "**问题**: $\\max$ 操作会系统性地高估 Q 值\n",
    "\n",
    "**原因**: 假设所有 Q 值估计都有噪声 $\\hat{Q} = Q^* + \\epsilon$，则：\n",
    "\n",
    "$$\\mathbb{E}[\\max_a \\hat{Q}(s,a)] \\geq \\max_a \\mathbb{E}[\\hat{Q}(s,a)] = \\max_a Q^*(s,a)$$\n",
    "\n",
    "### 1.2 Double Q-Learning 解决方案\n",
    "\n",
    "**核心思想**: 解耦动作选择和价值评估\n",
    "\n",
    "维护两个 Q 表 $Q_1$ 和 $Q_2$：\n",
    "\n",
    "$$Q_1(S,A) \\leftarrow Q_1 + \\alpha[R + \\gamma Q_2(S', \\arg\\max_a Q_1(S',a)) - Q_1]$$\n",
    "\n",
    "$$Q_2(S,A) \\leftarrow Q_2 + \\alpha[R + \\gamma Q_1(S', \\arg\\max_a Q_2(S',a)) - Q_2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "print(\"库导入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "    print(f\"Gymnasium 版本: {gym.__version__}\")\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"请安装 gymnasium: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    \"\"\"训练指标记录\"\"\"\n",
    "    episode_rewards: List[float] = field(default_factory=list)\n",
    "    episode_lengths: List[int] = field(default_factory=list)\n",
    "    epsilon_history: List[float] = field(default_factory=list)\n",
    "    \n",
    "    def get_moving_average(self, window: int = 100) -> np.ndarray:\n",
    "        if len(self.episode_rewards) < window:\n",
    "            return np.array(self.episode_rewards)\n",
    "        return np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Double Q-Learning 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    \"\"\"\n",
    "    Double Q-Learning 智能体\n",
    "    \n",
    "    通过维护两个独立 Q 表，解耦动作选择与价值评估，\n",
    "    消除标准 Q-Learning 的过估计偏差。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # 两个独立的 Q 表\n",
    "        self.q_table1 = defaultdict(lambda: np.zeros(n_actions))\n",
    "        self.q_table2 = defaultdict(lambda: np.zeros(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"使用两个 Q 表的和选择动作\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        combined_q = self.q_table1[state] + self.q_table2[state]\n",
    "        return np.random.choice(np.where(np.isclose(combined_q, np.max(combined_q)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Double Q-Learning 更新\n",
    "        \n",
    "        随机选择更新 Q1 或 Q2，解耦选择和评估\n",
    "        \"\"\"\n",
    "        if np.random.random() < 0.5:\n",
    "            # 更新 Q1：用 Q1 选择，Q2 评估\n",
    "            current_q = self.q_table1[state][action]\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                best_action = np.argmax(self.q_table1[next_state])\n",
    "                target = reward + self.gamma * self.q_table2[next_state][best_action]\n",
    "            self.q_table1[state][action] += self.lr * (target - current_q)\n",
    "        else:\n",
    "            # 更新 Q2：用 Q2 选择，Q1 评估\n",
    "            current_q = self.q_table2[state][action]\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                best_action = np.argmax(self.q_table2[next_state])\n",
    "                target = reward + self.gamma * self.q_table1[next_state][best_action]\n",
    "            self.q_table2[state][action] += self.lr * (target - current_q)\n",
    "        \n",
    "        return target - current_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# 组装类\n",
    "DoubleQLearningAgent.get_action = get_action\n",
    "DoubleQLearningAgent.update = update\n",
    "DoubleQLearningAgent.decay_epsilon = decay_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 标准 Q-Learning (对比)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"标准 Q-Learning\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        return np.random.choice(np.where(np.isclose(q_values, np.max(q_values)))[0])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        current_q = self.q_table[state][action]\n",
    "        target = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.lr * (target - current_q)\n",
    "        return target - current_q\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        data = {'q_table': {str(k): v.tolist() for k, v in self.q_table.items()},\n",
    "                'epsilon': self.epsilon}\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.q_table = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        for k, v in data['q_table'].items():\n",
    "            key = eval(k) if '(' in k else int(k)\n",
    "            self.q_table[key] = np.array(v)\n",
    "        self.epsilon = data.get('epsilon', 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：Gymnasium 环境实战\n",
    "\n",
    "### 3.1 Taxi-v3 环境\n",
    "\n",
    "```\n",
    "+---------+\n",
    "|R: | : :G|    R, G, Y, B: 乘客位置/目的地\n",
    "| : | : : |    |: 墙壁\n",
    "| : : : : |\n",
    "| | : | : |\n",
    "|Y| : |B: |\n",
    "+---------+\n",
    "\n",
    "状态: 500 (位置×乘客位置×目的地)\n",
    "动作: 6 (南/北/东/西/接/放)\n",
    "奖励: 每步-1, 成功+20, 非法操作-10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "    \n",
    "    print(\"Taxi-v3 环境:\")\n",
    "    print(f\"  状态空间: {env.observation_space.n}\")\n",
    "    print(f\"  动作空间: {env.action_space.n}\")\n",
    "    \n",
    "    state, _ = env.reset(seed=42)\n",
    "    print(f\"\\n初始状态: {state}\")\n",
    "    print(env.render())\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=2000, max_steps=200, verbose=True, log_interval=200):\n",
    "    \"\"\"通用训练函数\"\"\"\n",
    "    metrics = TrainingMetrics()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        total_reward, steps = 0.0, 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            result = env.step(action)\n",
    "            \n",
    "            if len(result) == 3:\n",
    "                next_state, reward, done = result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state, total_reward, steps = next_state, total_reward + reward, steps + 1\n",
    "            if done: break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        metrics.episode_rewards.append(total_reward)\n",
    "        metrics.episode_lengths.append(steps)\n",
    "        metrics.epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        if verbose and (episode + 1) % log_interval == 0:\n",
    "            avg = np.mean(metrics.episode_rewards[-log_interval:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg:8.2f} | ε: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Taxi-v3 Q-Learning 训练\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    env = gym.make('Taxi-v3')\n",
    "    agent = QLearningAgent(n_actions=env.action_space.n, learning_rate=0.1,\n",
    "                           epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01)\n",
    "    \n",
    "    metrics = train_agent(env, agent, episodes=2000, verbose=True)\n",
    "    env.close()\n",
    "    \n",
    "    print(f\"\\n训练完成！最后100回合平均: {np.mean(metrics.episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, title=\"训练曲线\"):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    window = 50\n",
    "    \n",
    "    # 奖励\n",
    "    rewards = metrics.episode_rewards\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(rewards, alpha=0.3, color='blue')\n",
    "    axes[0].plot(range(window-1, len(rewards)), smoothed, color='blue', linewidth=2)\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Reward')\n",
    "    axes[0].set_title('回合奖励')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 步数\n",
    "    steps = metrics.episode_lengths\n",
    "    smoothed_steps = np.convolve(steps, np.ones(window)/window, mode='valid')\n",
    "    axes[1].plot(steps, alpha=0.3, color='green')\n",
    "    axes[1].plot(range(window-1, len(steps)), smoothed_steps, color='green', linewidth=2)\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('回合步数')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 探索率\n",
    "    axes[2].plot(metrics.epsilon_history, color='red')\n",
    "    axes[2].set_xlabel('Episode')\n",
    "    axes[2].set_ylabel('Epsilon')\n",
    "    axes[2].set_title('探索率衰减')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    plot_metrics(metrics, \"Taxi-v3 Q-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 评估智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, episodes=100):\n",
    "    \"\"\"评估智能体性能\"\"\"\n",
    "    rewards, successes = [], 0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        total_reward, steps = 0, 0\n",
    "        \n",
    "        while steps < 200:\n",
    "            action = agent.get_action(state, training=False)\n",
    "            result = env.step(action)\n",
    "            if len(result) == 3:\n",
    "                next_state, reward, done = result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            state, steps = next_state, steps + 1\n",
    "            \n",
    "            if done:\n",
    "                if reward == 20: successes += 1\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return {'mean': np.mean(rewards), 'std': np.std(rewards), 'success_rate': successes/episodes*100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    print(\"\\n评估训练好的智能体...\")\n",
    "    env = gym.make('Taxi-v3')\n",
    "    results = evaluate_agent(env, agent, episodes=100)\n",
    "    env.close()\n",
    "    \n",
    "    print(f\"\\n评估结果 (100回合):\")\n",
    "    print(f\"  平均奖励: {results['mean']:.2f} ± {results['std']:.2f}\")\n",
    "    print(f\"  成功率: {results['success_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 演示智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "    action_names = ['南', '北', '东', '西', '接', '放']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"演示回合\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    total_reward, steps = 0, 0\n",
    "    \n",
    "    print(\"\\n初始状态:\")\n",
    "    print(env.render())\n",
    "    \n",
    "    while steps < 15:\n",
    "        action = agent.get_action(state, training=False)\n",
    "        result = env.step(action)\n",
    "        next_state, reward, terminated, truncated, _ = result\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        print(f\"\\n步骤 {steps}: {action_names[action]}, 奖励={reward}\")\n",
    "        print(env.render())\n",
    "        \n",
    "        if done:\n",
    "            print(f\"\\n回合结束！总奖励: {total_reward}\")\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：Double Q-Learning 对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Q-Learning vs Double Q-Learning\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    env = gym.make('Taxi-v3')\n",
    "    \n",
    "    # Q-Learning\n",
    "    print(\"\\n训练 Q-Learning...\")\n",
    "    q_agent = QLearningAgent(n_actions=6, learning_rate=0.1, epsilon=1.0,\n",
    "                              epsilon_decay=0.995, epsilon_min=0.01)\n",
    "    q_metrics = train_agent(env, q_agent, episodes=1000, verbose=False)\n",
    "    \n",
    "    # Double Q-Learning\n",
    "    print(\"训练 Double Q-Learning...\")\n",
    "    double_agent = DoubleQLearningAgent(n_actions=6, learning_rate=0.1, epsilon=1.0,\n",
    "                                         epsilon_decay=0.995, epsilon_min=0.01)\n",
    "    double_metrics = train_agent(env, double_agent, episodes=1000, verbose=False)\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    window = 50\n",
    "    \n",
    "    q_smooth = np.convolve(q_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    double_smooth = np.convolve(double_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax.plot(q_smooth, label='Q-Learning', alpha=0.8)\n",
    "    ax.plot(double_smooth, label='Double Q-Learning', alpha=0.8)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.set_title('Q-Learning vs Double Q-Learning')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n最后100回合:\")\n",
    "    print(f\"  Q-Learning: {np.mean(q_metrics.episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"  Double Q-Learning: {np.mean(double_metrics.episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第五部分：学习率调度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLRQLearning(QLearningAgent):\n",
    "    \"\"\"\n",
    "    自适应学习率 Q-Learning\n",
    "    \n",
    "    α(s,a) = 1 / (1 + N(s,a))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.visit_count = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self.visit_count[state][action] += 1\n",
    "        lr = 1.0 / (1.0 + self.visit_count[state][action])\n",
    "        \n",
    "        current_q = self.q_table[state][action]\n",
    "        target = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += lr * (target - current_q)\n",
    "        return target - current_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试自适应学习率\n",
    "agent = AdaptiveLRQLearning(n_actions=4)\n",
    "state = (0, 0)\n",
    "\n",
    "print(\"自适应学习率演示:\")\n",
    "for i in range(5):\n",
    "    lr = 1.0 / (1.0 + agent.visit_count[state][0])\n",
    "    agent.update(state, 0, -1.0, (0, 1), False)\n",
    "    print(f\"  访问 {i+1}: lr = {lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **过估计问题**: Q-Learning 的 max 导致系统性高估\n",
    "2. **Double Q-Learning**: 解耦选择与评估消除偏差\n",
    "3. **学习率调度**: 基于访问次数自适应调整\n",
    "\n",
    "### 超参数建议\n",
    "\n",
    "| 参数 | 范围 | 说明 |\n",
    "|------|------|------|\n",
    "| 学习率 | 0.05-0.5 | 表格型可用较大值 |\n",
    "| 折扣因子 | 0.95-0.99 | 任务越长期越接近1 |\n",
    "| 初始探索率 | 1.0 | 从完全探索开始 |\n",
    "| 最终探索率 | 0.01-0.1 | 保持少量探索 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    \n",
    "    # 测试1: Double Q-Learning 更新\n",
    "    agent = DoubleQLearningAgent(n_actions=4, learning_rate=0.5)\n",
    "    np.random.seed(42)\n",
    "    for _ in range(10):\n",
    "        agent.update((0,0), 0, -1.0, (0,1), False)\n",
    "    assert agent.q_table1[(0,0)][0] != 0 or agent.q_table2[(0,0)][0] != 0\n",
    "    print(\"✓ 测试1: Double Q-Learning 更新\")\n",
    "    passed += 1\n",
    "    \n",
    "    # 测试2: 自适应学习率\n",
    "    agent = AdaptiveLRQLearning(n_actions=4)\n",
    "    lr1 = 1.0 / (1.0 + agent.visit_count[(0,0)][0])\n",
    "    agent.update((0,0), 0, -1.0, (0,1), False)\n",
    "    lr2 = 1.0 / (1.0 + agent.visit_count[(0,0)][0])\n",
    "    assert lr2 < lr1, \"学习率应该衰减\"\n",
    "    print(\"✓ 测试2: 自适应学习率\")\n",
    "    passed += 1\n",
    "    \n",
    "    # 测试3: 保存/加载\n",
    "    agent = QLearningAgent(n_actions=4)\n",
    "    agent.q_table[(0,0)] = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "    agent.save('_test.json')\n",
    "    \n",
    "    new_agent = QLearningAgent(n_actions=4)\n",
    "    new_agent.load('_test.json')\n",
    "    assert np.allclose(new_agent.q_table[(0,0)], agent.q_table[(0,0)])\n",
    "    \n",
    "    import os\n",
    "    os.remove('_test.json')\n",
    "    print(\"✓ 测试3: 保存/加载\")\n",
    "    passed += 1\n",
    "    \n",
    "    print(f\"\\n全部 {passed} 项测试通过!\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Van Hasselt (2010). Double Q-learning. NeurIPS.\n",
    "2. Sutton & Barto (2018). Reinforcement Learning: An Introduction.\n",
    "3. [Gymnasium Documentation](https://gymnasium.farama.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
