{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning åŸºç¡€ä¸å®ç°\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å°†æŒæ¡ï¼š\n",
    "- æ—¶åºå·®åˆ†å­¦ä¹  (TD Learning) çš„æ ¸å¿ƒæ€æƒ³\n",
    "- Q-Learning ç®—æ³•çš„æ•°å­¦åŸç†ä¸æ¨å¯¼\n",
    "- è¡¨æ ¼å‹ Q-Learning çš„å·¥ç¨‹å®ç°\n",
    "- æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ç­–ç•¥\n",
    "\n",
    "## å‰ç½®çŸ¥è¯†\n",
    "\n",
    "- é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP) åŸºç¡€\n",
    "- Python ä¸ NumPy åŸºç¡€\n",
    "- æ¦‚ç‡è®ºåŸºç¡€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šä»åŠ¨æ€è§„åˆ’åˆ°æ— æ¨¡å‹å­¦ä¹ \n",
    "\n",
    "### 1.1 åŠ¨æ€è§„åˆ’çš„å±€é™æ€§\n",
    "\n",
    "åŠ¨æ€è§„åˆ’ (DP) æ–¹æ³•éœ€è¦ï¼š\n",
    "1. **å®Œæ•´çš„ç¯å¢ƒæ¨¡å‹**ï¼š$P(s'|s,a)$ å’Œ $R(s,a,s')$\n",
    "2. **éå†æ‰€æœ‰çŠ¶æ€å’ŒåŠ¨ä½œ**\n",
    "\n",
    "**ç°å®æŒ‘æˆ˜**:\n",
    "- è½¬ç§»æ¦‚ç‡é€šå¸¸æœªçŸ¥\n",
    "- çŠ¶æ€ç©ºé—´å¯èƒ½æå…¶å·¨å¤§ï¼ˆå›´æ£‹çº¦æœ‰ $10^{170}$ ç§çŠ¶æ€ï¼‰\n",
    "\n",
    "### 1.2 æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ \n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡ä¸ç¯å¢ƒ**äº¤äº’é‡‡æ ·**å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œæ— éœ€äº‹å…ˆçŸ¥é“ç¯å¢ƒæ¨¡å‹ã€‚\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                å¼ºåŒ–å­¦ä¹ æ–¹æ³•åˆ†ç±»                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  åŸºäºæ¨¡å‹            â”‚        æ— æ¨¡å‹ (Model-Free)        â”‚\n",
    "â”‚  (Model-Based)       â”‚  åŸºäºä»·å€¼      â”‚    åŸºäºç­–ç•¥      â”‚\n",
    "â”‚  â€¢ åŠ¨æ€è§„åˆ’          â”‚  â€¢ Q-Learning  â”‚    â€¢ REINFORCE   â”‚\n",
    "â”‚  â€¢ æ¨¡å‹é¢„æµ‹æ§åˆ¶      â”‚  â€¢ SARSA       â”‚    â€¢ PPO         â”‚\n",
    "â”‚  â€¢ Dyna-Q           â”‚  â€¢ DQN         â”‚    â€¢ A3C         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šæ—¶åºå·®åˆ†å­¦ä¹  (TD Learning)\n",
    "\n",
    "### 2.1 è’™ç‰¹å¡æ´› vs æ—¶åºå·®åˆ†\n",
    "\n",
    "**è’™ç‰¹å¡æ´›æ–¹æ³•** (Monte Carlo):\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]$$\n",
    "\n",
    "- éœ€è¦ç­‰å¾…**å›åˆç»“æŸ**æ‰èƒ½æ›´æ–°\n",
    "- $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$\n",
    "\n",
    "**æ—¶åºå·®åˆ†æ–¹æ³•** (TD):\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]$$\n",
    "\n",
    "- **æ¯ä¸€æ­¥**éƒ½å¯ä»¥æ›´æ–°\n",
    "- ä½¿ç”¨**è‡ªä¸¾** (Bootstrapping)ï¼šç”¨ä¼°è®¡å€¼æ›´æ–°ä¼°è®¡å€¼\n",
    "\n",
    "### 2.2 TD è¯¯å·®\n",
    "\n",
    "**TD ç›®æ ‡**: $\\text{TD Target} = R_{t+1} + \\gamma V(S_{t+1})$\n",
    "\n",
    "**TD è¯¯å·®**: $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "| $\\delta_t$ | å«ä¹‰ | æ“ä½œ |\n",
    "|------------|------|------|\n",
    "| $> 0$ | å®é™…æ¯”é¢„æœŸå¥½ | å¢å¤§ $V(S_t)$ |\n",
    "| $< 0$ | å®é™…æ¯”é¢„æœŸå·® | å‡å° $V(S_t)$ |\n",
    "| $= 0$ | é¢„æµ‹å‡†ç¡® | å·²æ”¶æ•› |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šQ-Learning ç®—æ³•\n",
    "\n",
    "### 3.1 ç®—æ³•åŸç†\n",
    "\n",
    "Q-Learning æ˜¯**ç¦»ç­–ç•¥** (Off-Policy) TD æ§åˆ¶ç®—æ³•ã€‚\n",
    "\n",
    "**æ›´æ–°å…¬å¼**:\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "**å…³é”®ç‰¹æ€§**:\n",
    "- ä½¿ç”¨ $\\max$ æ“ä½œé€‰æ‹©ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§ Q å€¼\n",
    "- ä¸å½“å‰å®é™…é‡‡å–çš„è¡Œä¸ºç­–ç•¥æ— å…³\n",
    "- åœ¨ä¸€å®šæ¡ä»¶ä¸‹ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜ Q å‡½æ•°\n",
    "\n",
    "### 3.2 ç®—æ³•ä¼ªä»£ç \n",
    "\n",
    "```\n",
    "ç®—æ³•: Q-Learning\n",
    "\n",
    "1. åˆå§‹åŒ– Q(s, a) = 0\n",
    "2. å¯¹äºæ¯ä¸ªå›åˆ:\n",
    "   a. åˆå§‹åŒ–çŠ¶æ€ S\n",
    "   b. é‡å¤:\n",
    "      i.   ä½¿ç”¨ Îµ-greedy ä» Q é€‰æ‹©åŠ¨ä½œ A\n",
    "      ii.  æ‰§è¡Œ Aï¼Œè§‚å¯Ÿ R, S'\n",
    "      iii. Q(S, A) â† Q(S, A) + Î±[R + Î³ max_a Q(S', a) - Q(S, A)]\n",
    "      iv.  S â† S'\n",
    "   c. ç›´åˆ°ç»ˆæ­¢\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬å››éƒ¨åˆ†ï¼šä»£ç å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# å¯è§†åŒ–é…ç½®\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"ç¯å¢ƒé…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 æ‚¬å´–è¡Œèµ°ç¯å¢ƒ\n",
    "\n",
    "æ‚¬å´–è¡Œèµ° (Cliff Walking) æ˜¯ç»å…¸çš„å¼ºåŒ–å­¦ä¹ æµ‹è¯•ç¯å¢ƒï¼š\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ .  .  .  .  .  .  .  .  .  .  .  .  â”‚  row 0\n",
    "â”‚ .  .  .  .  .  .  .  .  .  .  .  .  â”‚  row 1\n",
    "â”‚ .  .  .  .  .  .  .  .  .  .  .  .  â”‚  row 2\n",
    "â”‚ S  C  C  C  C  C  C  C  C  C  C  G  â”‚  row 3\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "S: èµ·ç‚¹    G: ç›®æ ‡    C: æ‚¬å´–\n",
    "åŠ¨ä½œ: ä¸Š(0)ã€å³(1)ã€ä¸‹(2)ã€å·¦(3)\n",
    "å¥–åŠ±: æ¯æ­¥ -1ï¼Œæ‰å…¥æ‚¬å´– -100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\"æ‚¬å´–è¡Œèµ°ç¯å¢ƒ\"\"\"\n",
    "    \n",
    "    ACTIONS = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "    ACTION_NAMES = ['ä¸Š', 'å³', 'ä¸‹', 'å·¦']\n",
    "    \n",
    "    def __init__(self, height: int = 4, width: int = 12):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.start = (height - 1, 0)\n",
    "        self.goal = (height - 1, width - 1)\n",
    "        self.cliff = [(height - 1, j) for j in range(1, width - 1)]\n",
    "        self.state = self.start\n",
    "        self.n_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reset(self) -> Tuple[int, int]:\n",
    "        \"\"\"é‡ç½®ç¯å¢ƒ\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:\n",
    "        \"\"\"æ‰§è¡ŒåŠ¨ä½œ\"\"\"\n",
    "        di, dj = self.ACTIONS[action]\n",
    "        new_i = int(np.clip(self.state[0] + di, 0, self.height - 1))\n",
    "        new_j = int(np.clip(self.state[1] + dj, 0, self.width - 1))\n",
    "        next_state = (new_i, new_j)\n",
    "        \n",
    "        # æ‚¬å´–æ£€æŸ¥\n",
    "        if next_state in self.cliff:\n",
    "            self.state = self.start\n",
    "            return self.state, -100.0, False\n",
    "        \n",
    "        self.state = next_state\n",
    "        \n",
    "        # ç›®æ ‡æ£€æŸ¥\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 0.0, True\n",
    "        \n",
    "        return self.state, -1.0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def render(self, path=None):\n",
    "        \"\"\"å¯è§†åŒ–ç¯å¢ƒ\"\"\"\n",
    "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        for pos in self.cliff:\n",
    "            grid[pos[0]][pos[1]] = 'C'\n",
    "        grid[self.start[0]][self.start[1]] = 'S'\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'\n",
    "        if path:\n",
    "            for pos in path[1:-1]:\n",
    "                if pos not in self.cliff and pos != self.start and pos != self.goal:\n",
    "                    grid[pos[0]][pos[1]] = '*'\n",
    "        print(\"â”Œ\" + \"â”€\" * (self.width * 2 + 1) + \"â”\")\n",
    "        for row in grid:\n",
    "            print(\"â”‚ \" + \" \".join(row) + \" â”‚\")\n",
    "        print(\"â””\" + \"â”€\" * (self.width * 2 + 1) + \"â”˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†æ–¹æ³•æ·»åŠ åˆ°ç±»ä¸­å¹¶æµ‹è¯•\n",
    "CliffWalkingEnv.reset = reset\n",
    "CliffWalkingEnv.step = step\n",
    "CliffWalkingEnv.render = render\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "print(\"æ‚¬å´–è¡Œèµ°ç¯å¢ƒ:\")\n",
    "env.render()\n",
    "print(f\"\\nèµ·ç‚¹: {env.start}\")\n",
    "print(f\"ç»ˆç‚¹: {env.goal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Q-Learning æ™ºèƒ½ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"è¡¨æ ¼å‹ Q-Learning æ™ºèƒ½ä½“\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_action(self, state, training: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Îµ-greedy åŠ¨ä½œé€‰æ‹©\n",
    "        \n",
    "        Ï€(a|s) = Îµ/|A| + (1-Îµ)Â·ğŸ™[a = argmax Q]\n",
    "        \"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, state, action, reward, next_state, done) -> float:\n",
    "        \"\"\"\n",
    "        Q-Learning æ›´æ–°è§„åˆ™\n",
    "        \n",
    "        Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a Q(s',a) - Q(s,a)]\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        td_error = target - current_q\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_epsilon(self):\n",
    "        \"\"\"è¡°å‡æ¢ç´¢ç‡\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»„è£…å®Œæ•´ç±»\n",
    "QLearningAgent.get_action = get_action\n",
    "QLearningAgent.update = update\n",
    "QLearningAgent.decay_epsilon = decay_epsilon\n",
    "\n",
    "# åˆ›å»ºæ™ºèƒ½ä½“\n",
    "agent = QLearningAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.1,\n",
    "    epsilon_decay=1.0,\n",
    "    epsilon_min=0.1\n",
    ")\n",
    "\n",
    "print(\"Q-Learning æ™ºèƒ½ä½“é…ç½®:\")\n",
    "print(f\"  å­¦ä¹ ç‡ Î±: {agent.lr}\")\n",
    "print(f\"  æŠ˜æ‰£å› å­ Î³: {agent.gamma}\")\n",
    "print(f\"  æ¢ç´¢ç‡ Îµ: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 è®­ç»ƒå¾ªç¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, agent, episodes=500, max_steps=200, verbose=True):\n",
    "    \"\"\"è®­ç»ƒ Q-Learning æ™ºèƒ½ä½“\"\"\"\n",
    "    history = {'rewards': [], 'steps': [], 'epsilon': []}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['steps'].append(steps)\n",
    "        history['epsilon'].append(agent.epsilon)\n",
    "        \n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(history['rewards'][-100:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:8.2f} | Îµ: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ™ºèƒ½ä½“\n",
    "print(\"å¼€å§‹è®­ç»ƒ Q-Learning...\\n\")\n",
    "env = CliffWalkingEnv()\n",
    "agent = QLearningAgent(n_actions=4, learning_rate=0.5, epsilon=0.1, epsilon_decay=1.0, epsilon_min=0.1)\n",
    "history = train_q_learning(env, agent, episodes=500)\n",
    "print(f\"\\nè®­ç»ƒå®Œæˆï¼æœ€å100å›åˆå¹³å‡å¥–åŠ±: {np.mean(history['rewards'][-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history, window=10):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒæ›²çº¿\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å¥–åŠ±æ›²çº¿\n",
    "    rewards = history['rewards']\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[0].plot(rewards, alpha=0.3, color='blue', label='åŸå§‹')\n",
    "    axes[0].plot(range(window-1, len(rewards)), smoothed, color='blue', linewidth=2, label='å¹³æ»‘')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title('å­¦ä¹ æ›²çº¿')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ­¥æ•°æ›²çº¿\n",
    "    steps = history['steps']\n",
    "    smoothed_steps = np.convolve(steps, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[1].plot(steps, alpha=0.3, color='green', label='åŸå§‹')\n",
    "    axes[1].plot(range(window-1, len(steps)), smoothed_steps, color='green', linewidth=2, label='å¹³æ»‘')\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('å›åˆæ­¥æ•°')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 æå–å­¦åˆ°çš„ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(agent, env, max_steps=50):\n",
    "    \"\"\"æå–è´ªå¿ƒç­–ç•¥è·¯å¾„\"\"\"\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_action(state, training=False)\n",
    "        next_state, _, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(agent, env):\n",
    "    \"\"\"å¯è§†åŒ–ç­–ç•¥\"\"\"\n",
    "    arrow_map = {0: 'â†‘', 1: 'â†’', 2: 'â†“', 3: 'â†'}\n",
    "    \n",
    "    print(\"å­¦åˆ°çš„ç­–ç•¥:\")\n",
    "    print(\"â”Œ\" + \"â”€â”€â”€\" * env.width + \"â”\")\n",
    "    \n",
    "    for i in range(env.height):\n",
    "        row = \"â”‚\"\n",
    "        for j in range(env.width):\n",
    "            state = (i, j)\n",
    "            if state == env.start:\n",
    "                row += \" S \"\n",
    "            elif state == env.goal:\n",
    "                row += \" G \"\n",
    "            elif state in env.cliff:\n",
    "                row += \" C \"\n",
    "            elif state in agent.q_table:\n",
    "                best_action = np.argmax(agent.q_table[state])\n",
    "                row += f\" {arrow_map[best_action]} \"\n",
    "            else:\n",
    "                row += \" . \"\n",
    "        print(row + \"â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€â”€â”€\" * env.width + \"â”˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ç­–ç•¥\n",
    "visualize_policy(agent, env)\n",
    "\n",
    "# æ˜¾ç¤ºè·¯å¾„\n",
    "print(\"\\nå­¦åˆ°çš„è·¯å¾„:\")\n",
    "path = extract_path(agent, env)\n",
    "env.reset()\n",
    "env.render(path)\n",
    "print(f\"è·¯å¾„é•¿åº¦: {len(path) - 1} æ­¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Q å€¼è¡¨å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(agent, env):\n",
    "    \"\"\"å¯è§†åŒ– Q è¡¨\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ä»·å€¼å‡½æ•°\n",
    "    v_table = np.zeros((env.height, env.width))\n",
    "    for i in range(env.height):\n",
    "        for j in range(env.width):\n",
    "            state = (i, j)\n",
    "            if state in agent.q_table:\n",
    "                v_table[i, j] = np.max(agent.q_table[state])\n",
    "    \n",
    "    im = axes[0].imshow(v_table, cmap='RdYlGn', aspect='auto')\n",
    "    axes[0].set_title('V(s) = max_a Q(s,a)')\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    for pos in env.cliff:\n",
    "        axes[0].add_patch(plt.Rectangle((pos[1]-0.5, pos[0]-0.5), 1, 1, fill=True, color='black', alpha=0.5))\n",
    "    \n",
    "    # Q å€¼åˆ†å¸ƒ\n",
    "    q_values = []\n",
    "    for q_array in agent.q_table.values():\n",
    "        q_values.extend(q_array.tolist())\n",
    "    \n",
    "    axes[1].hist(q_values, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[1].set_xlabel('Q Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Q å€¼åˆ†å¸ƒ')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_q_table(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äº”éƒ¨åˆ†ï¼šæ¢ç´¢ç­–ç•¥\n",
    "\n",
    "### 5.1 æ¢ç´¢ä¸åˆ©ç”¨å›°å¢ƒ\n",
    "\n",
    "| ç­–ç•¥ | æè¿° | é£é™© |\n",
    "|------|------|------|\n",
    "| **åˆ©ç”¨** | é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œ | é™·å…¥å±€éƒ¨æœ€ä¼˜ |\n",
    "| **æ¢ç´¢** | å°è¯•æ–°åŠ¨ä½œ | æµªè´¹èµ„æº |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon):\n",
    "    \"\"\"Îµ-Greedy ç­–ç•¥\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(q_values))\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "def softmax_action(q_values, temperature):\n",
    "    \"\"\"\n",
    "    Softmax (Boltzmann) ç­–ç•¥\n",
    "    \n",
    "    P(a|s) = exp(Q(s,a)/Ï„) / Î£ exp(Q(s,a')/Ï„)\n",
    "    \"\"\"\n",
    "    q_scaled = (q_values - np.max(q_values)) / max(temperature, 1e-8)\n",
    "    exp_q = np.exp(q_scaled)\n",
    "    probs = exp_q / np.sum(exp_q)\n",
    "    return np.random.choice(len(q_values), p=probs)\n",
    "\n",
    "def ucb_action(q_values, action_counts, total_count, c=2.0):\n",
    "    \"\"\"\n",
    "    UCB ç­–ç•¥\n",
    "    \n",
    "    A = argmax[Q(s,a) + c * sqrt(ln(t) / N(s,a))]\n",
    "    \"\"\"\n",
    "    if np.any(action_counts == 0):\n",
    "        return np.random.choice(np.where(action_counts == 0)[0])\n",
    "    ucb_values = q_values + c * np.sqrt(np.log(total_count + 1) / (action_counts + 1e-8))\n",
    "    return np.argmax(ucb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºä¸åŒç­–ç•¥\n",
    "q_values = np.array([1.0, 2.0, 1.5, 0.5])\n",
    "action_counts = np.array([10, 5, 8, 2])\n",
    "n_samples = 1000\n",
    "\n",
    "print(f\"Q å€¼: {q_values}\")\n",
    "print(f\"åŠ¨ä½œè®¡æ•°: {action_counts}\\n\")\n",
    "\n",
    "# Îµ-Greedy\n",
    "eg_actions = [epsilon_greedy(q_values, 0.1) for _ in range(n_samples)]\n",
    "eg_dist = [eg_actions.count(i)/n_samples for i in range(4)]\n",
    "print(f\"Îµ-Greedy (Îµ=0.1): {[f'{d:.2f}' for d in eg_dist]}\")\n",
    "\n",
    "# Softmax\n",
    "sm_actions = [softmax_action(q_values, 0.5) for _ in range(n_samples)]\n",
    "sm_dist = [sm_actions.count(i)/n_samples for i in range(4)]\n",
    "print(f\"Softmax (Ï„=0.5):  {[f'{d:.2f}' for d in sm_dist]}\")\n",
    "\n",
    "# UCB\n",
    "ucb_actions = [ucb_action(q_values, action_counts, 100, c=2.0) for _ in range(n_samples)]\n",
    "ucb_dist = [ucb_actions.count(i)/n_samples for i in range(4)]\n",
    "print(f\"UCB (c=2.0):      {[f'{d:.2f}' for d in ucb_dist]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒè¦ç‚¹\n",
    "\n",
    "1. **Q-Learning** æ˜¯æ— æ¨¡å‹ã€ç¦»ç­–ç•¥çš„ TD æ§åˆ¶ç®—æ³•\n",
    "2. **æ›´æ–°å…¬å¼**: $Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\max_a Q(S',a) - Q(S,A)]$\n",
    "3. **ç¦»ç­–ç•¥ç‰¹æ€§**: å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œä¸å®é™…è¡Œä¸ºç­–ç•¥æ— å…³\n",
    "4. **æ¢ç´¢ç­–ç•¥**: Îµ-greedy, Softmax, UCB\n",
    "\n",
    "### è¶…å‚æ•°é€‰æ‹©\n",
    "\n",
    "| å‚æ•° | å…¸å‹å€¼ | è¯´æ˜ |\n",
    "|------|--------|------|\n",
    "| $\\alpha$ | 0.1-0.5 | å­¦ä¹ ç‡ |\n",
    "| $\\gamma$ | 0.99 | æŠ˜æ‰£å› å­ |\n",
    "| $\\epsilon$ | 1.0â†’0.01 | æ¢ç´¢ç‡ |\n",
    "\n",
    "### å±€é™æ€§\n",
    "\n",
    "- çŠ¶æ€ç©ºé—´å¿…é¡»ç¦»æ•£ä¸”æœ‰é™\n",
    "- æ— æ³•å¤„ç†è¿ç»­çŠ¶æ€\n",
    "- æ— æ³•æ³›åŒ–åˆ°æœªè§çŠ¶æ€\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**: DQN (æ·±åº¦ Q ç½‘ç»œ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## å•å…ƒæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"è¿è¡Œå•å…ƒæµ‹è¯•\"\"\"\n",
    "    print(\"å¼€å§‹å•å…ƒæµ‹è¯•...\\n\")\n",
    "    passed = 0\n",
    "    \n",
    "    # æµ‹è¯•1: ç¯å¢ƒåŸºæœ¬åŠŸèƒ½\n",
    "    env = CliffWalkingEnv()\n",
    "    assert env.reset() == (3, 0), \"èµ·å§‹çŠ¶æ€é”™è¯¯\"\n",
    "    print(\"âœ“ æµ‹è¯•1: ç¯å¢ƒåˆå§‹åŒ–\")\n",
    "    passed += 1\n",
    "    \n",
    "    # æµ‹è¯•2: æ‚¬å´–æƒ©ç½š\n",
    "    env.reset()\n",
    "    _, reward, _ = env.step(1)  # å³ç§»å…¥æ‚¬å´–\n",
    "    assert reward == -100.0, \"æ‚¬å´–æƒ©ç½šé”™è¯¯\"\n",
    "    print(\"âœ“ æµ‹è¯•2: æ‚¬å´–æƒ©ç½š\")\n",
    "    passed += 1\n",
    "    \n",
    "    # æµ‹è¯•3: Q-Learning æ›´æ–°\n",
    "    agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.9)\n",
    "    state = (0, 0)\n",
    "    agent.update(state, 0, -1.0, (0, 1), False)\n",
    "    assert agent.q_table[state][0] == -0.5, \"Qå€¼æ›´æ–°é”™è¯¯\"\n",
    "    print(\"âœ“ æµ‹è¯•3: Q-Learning æ›´æ–°\")\n",
    "    passed += 1\n",
    "    \n",
    "    # æµ‹è¯•4: è´ªå¿ƒç­–ç•¥\n",
    "    agent = QLearningAgent(n_actions=4, epsilon=0.0)\n",
    "    agent.q_table[(0,0)] = np.array([1.0, 2.0, 0.5, 0.5])\n",
    "    for _ in range(10):\n",
    "        assert agent.get_action((0,0), training=True) == 1\n",
    "    print(\"âœ“ æµ‹è¯•4: è´ªå¿ƒç­–ç•¥\")\n",
    "    passed += 1\n",
    "    \n",
    "    print(f\"\\nå…¨éƒ¨ {passed} é¡¹æµ‹è¯•é€šè¿‡!\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## å‚è€ƒèµ„æ–™\n",
    "\n",
    "1. Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD Thesis.\n",
    "2. Sutton & Barto (2018). Reinforcement Learning: An Introduction, Chapter 6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
