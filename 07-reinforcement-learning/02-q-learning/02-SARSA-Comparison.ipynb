{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA 与 Q-Learning 对比\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将学会：\n",
    "- 理解 SARSA 算法的原理\n",
    "- 掌握 On-Policy 与 Off-Policy 的区别\n",
    "- 实现 SARSA 和 Expected SARSA\n",
    "- 通过悬崖行走环境对比算法行为差异\n",
    "- 理解安全性与最优性的权衡\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- Q-Learning 算法基础\n",
    "- 时序差分学习概念\n",
    "- Python 和 NumPy 基础\n",
    "\n",
    "## 预计时间\n",
    "\n",
    "40-50 分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第1部分：On-Policy vs Off-Policy\n",
    "\n",
    "### 1.1 基本概念\n",
    "\n",
    "在强化学习中，策略可以分为两种：\n",
    "\n",
    "- **行为策略 (Behavior Policy)**：智能体实际用来与环境交互、收集数据的策略\n",
    "- **目标策略 (Target Policy)**：智能体正在学习和优化的策略\n",
    "\n",
    "### 1.2 Off-Policy (离策略)\n",
    "\n",
    "**行为策略 ≠ 目标策略**\n",
    "\n",
    "典型代表：**Q-Learning**\n",
    "\n",
    "```\n",
    "Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n",
    "                          ↑\n",
    "                    使用 max (贪心)\n",
    "                    不管实际采取什么动作\n",
    "```\n",
    "\n",
    "**特点**：\n",
    "- 学习最优策略，不受探索影响\n",
    "- 可以从其他策略（如人类演示）的经验中学习\n",
    "- 样本效率高，可重用历史数据\n",
    "\n",
    "### 1.3 On-Policy (在策略)\n",
    "\n",
    "**行为策略 = 目标策略**\n",
    "\n",
    "典型代表：**SARSA**\n",
    "\n",
    "```\n",
    "Q(S,A) ← Q(S,A) + α[R + γ Q(S',A') - Q(S,A)]\n",
    "                          ↑\n",
    "                    使用实际采取的动作 A'\n",
    "                    与行为策略一致\n",
    "```\n",
    "\n",
    "**特点**：\n",
    "- 学习当前策略的价值函数\n",
    "- 探索会影响学习结果\n",
    "- 更保守，考虑探索风险"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第2部分：SARSA 算法\n",
    "\n",
    "### 2.1 算法原理\n",
    "\n",
    "SARSA 名称来源于更新所需的五元组：**S**tate-**A**ction-**R**eward-**S**tate-**A**ction\n",
    "\n",
    "**更新公式**：\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "**与 Q-Learning 的关键区别**：使用实际采取的下一动作 $A_{t+1}$，而非 $\\max$\n",
    "\n",
    "### 2.2 算法伪代码\n",
    "\n",
    "```\n",
    "算法: SARSA\n",
    "\n",
    "1. 初始化 Q(s, a) = 0\n",
    "2. 对于每个回合:\n",
    "   a. 初始化状态 S\n",
    "   b. 使用策略选择动作 A  ← 这一步在循环外\n",
    "   c. 重复:\n",
    "      i.   执行动作 A，观察 R, S'\n",
    "      ii.  使用策略选择 A' (在 S' 下)\n",
    "      iii. Q(S, A) ← Q(S, A) + α[R + γ Q(S', A') - Q(S, A)]\n",
    "      iv.  S ← S', A ← A'\n",
    "   d. 直到 S 是终止状态\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第3部分：代码实现\n",
    "\n",
    "### 步骤1: 导入库和环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 导入必要的库\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "# 可视化配置\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"库导入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 悬崖行走环境 (复用)\n",
    "# ============================================================\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    \"\"\"悬崖行走环境\"\"\"\n",
    "    \n",
    "    ACTIONS = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "    ACTION_NAMES = ['上', '右', '下', '左']\n",
    "    \n",
    "    def __init__(self, height: int = 4, width: int = 12):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.start = (height - 1, 0)\n",
    "        self.goal = (height - 1, width - 1)\n",
    "        self.cliff = [(height - 1, j) for j in range(1, width - 1)]\n",
    "        self.state = self.start\n",
    "        self.n_actions = 4\n",
    "        \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:\n",
    "        di, dj = self.ACTIONS[action]\n",
    "        new_i = np.clip(self.state[0] + di, 0, self.height - 1)\n",
    "        new_j = np.clip(self.state[1] + dj, 0, self.width - 1)\n",
    "        next_state = (int(new_i), int(new_j))\n",
    "        \n",
    "        if next_state in self.cliff:\n",
    "            self.state = self.start\n",
    "            return self.state, -100.0, False\n",
    "        \n",
    "        self.state = next_state\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 0.0, True\n",
    "        return self.state, -1.0, False\n",
    "    \n",
    "    def render(self, path: Optional[List] = None) -> None:\n",
    "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        for pos in self.cliff:\n",
    "            grid[pos[0]][pos[1]] = 'C'\n",
    "        grid[self.start[0]][self.start[1]] = 'S'\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'\n",
    "        if path:\n",
    "            for pos in path[1:-1]:\n",
    "                if pos not in self.cliff and pos != self.start and pos != self.goal:\n",
    "                    grid[pos[0]][pos[1]] = '*'\n",
    "        print(\"┌\" + \"─\" * (self.width * 2 + 1) + \"┐\")\n",
    "        for row in grid:\n",
    "            print(\"│ \" + \" \".join(row) + \" │\")\n",
    "        print(\"└\" + \"─\" * (self.width * 2 + 1) + \"┘\")\n",
    "\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "print(\"悬崖行走环境:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2: 实现 SARSA 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"\n",
    "    SARSA 智能体 (On-Policy TD Control)\n",
    "    \n",
    "    与 Q-Learning 的关键区别:\n",
    "    - 更新使用实际采取的下一个动作 A'\n",
    "    - 学习的是当前策略的价值，而非最优策略\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        \"\"\"使用 ε-greedy 策略选择动作\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        next_action: int,  # SARSA 需要下一个动作\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        SARSA 更新规则\n",
    "        \n",
    "        Q(S,A) ← Q(S,A) + α[R + γ Q(S',A') - Q(S,A)]\n",
    "        \n",
    "        注意：需要 next_action 参数\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            # SARSA 核心：使用实际的 next_action\n",
    "            target = reward + self.gamma * self.q_table[next_state][next_action]\n",
    "        \n",
    "        td_error = target - current_q\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "print(\"SARSA 智能体类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3: 实现 Q-Learning 智能体 (对比用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning 智能体 (Off-Policy TD Control)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"Q-Learning: Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            # Q-Learning: 使用 max\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        td_error = target - current_q\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "print(\"Q-Learning 智能体类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4: 实现训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, agent, episodes=500, max_steps=200, verbose=False):\n",
    "    \"\"\"训练 Q-Learning 智能体\"\"\"\n",
    "    history = {'rewards': [], 'steps': []}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['steps'].append(steps)\n",
    "        \n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(history['rewards'][-100:])\n",
    "            print(f\"Q-Learning Episode {episode+1}: Avg Reward = {avg:.2f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def train_sarsa(env, agent, episodes=500, max_steps=200, verbose=False):\n",
    "    \"\"\"训练 SARSA 智能体\"\"\"\n",
    "    history = {'rewards': [], 'steps': []}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        # SARSA: 先选择初始动作\n",
    "        action = agent.get_action(state, training=True)\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            next_state, reward, done = env.step(action)\n",
    "            # SARSA: 在更新前选择下一个动作\n",
    "            next_action = agent.get_action(next_state, training=True)\n",
    "            \n",
    "            # SARSA 更新需要 next_action\n",
    "            agent.update(state, action, reward, next_state, next_action, done)\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action  # 关键：动作传递\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['steps'].append(steps)\n",
    "        \n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(history['rewards'][-100:])\n",
    "            print(f\"SARSA Episode {episode+1}: Avg Reward = {avg:.2f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"训练函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5: 对比实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 对比实验：Q-Learning vs SARSA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"悬崖行走环境: Q-Learning vs SARSA 对比实验\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 实验参数\n",
    "EPISODES = 500\n",
    "LEARNING_RATE = 0.5\n",
    "EPSILON = 0.1  # 固定探索率，便于观察行为差异\n",
    "\n",
    "# 创建环境和智能体\n",
    "env = CliffWalkingEnv()\n",
    "\n",
    "q_agent = QLearningAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epsilon=EPSILON,\n",
    "    epsilon_decay=1.0,  # 不衰减\n",
    "    epsilon_min=EPSILON\n",
    ")\n",
    "\n",
    "sarsa_agent = SARSAAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epsilon=EPSILON,\n",
    "    epsilon_decay=1.0,\n",
    "    epsilon_min=EPSILON\n",
    ")\n",
    "\n",
    "# 训练\n",
    "print(\"\\n训练 Q-Learning...\")\n",
    "q_history = train_q_learning(env, q_agent, episodes=EPISODES, verbose=True)\n",
    "\n",
    "print(\"\\n训练 SARSA...\")\n",
    "sarsa_history = train_sarsa(env, sarsa_agent, episodes=EPISODES, verbose=True)\n",
    "\n",
    "print(\"\\n训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤6: 可视化对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(q_history, sarsa_history, window=10):\n",
    "    \"\"\"绘制 Q-Learning 和 SARSA 的学习曲线对比\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 奖励曲线\n",
    "    q_smooth = np.convolve(q_history['rewards'], np.ones(window)/window, mode='valid')\n",
    "    sarsa_smooth = np.convolve(sarsa_history['rewards'], np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[0].plot(q_smooth, label='Q-Learning', color='blue', alpha=0.8)\n",
    "    axes[0].plot(sarsa_smooth, label='SARSA', color='red', alpha=0.8)\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title('学习曲线: 回合奖励')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加说明文本\n",
    "    axes[0].axhline(y=-13, color='green', linestyle='--', alpha=0.5, label='最优路径')\n",
    "    \n",
    "    # 步数曲线\n",
    "    q_steps_smooth = np.convolve(q_history['steps'], np.ones(window)/window, mode='valid')\n",
    "    sarsa_steps_smooth = np.convolve(sarsa_history['steps'], np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[1].plot(q_steps_smooth, label='Q-Learning', color='blue', alpha=0.8)\n",
    "    axes[1].plot(sarsa_steps_smooth, label='SARSA', color='red', alpha=0.8)\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('学习曲线: 回合步数')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印统计\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"最后100回合统计\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Q-Learning: 平均奖励 = {np.mean(q_history['rewards'][-100:]):.2f}\")\n",
    "    print(f\"SARSA:      平均奖励 = {np.mean(sarsa_history['rewards'][-100:]):.2f}\")\n",
    "\n",
    "\n",
    "plot_comparison(q_history, sarsa_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤7: 提取并对比学到的策略路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(agent, env, max_steps=50):\n",
    "    \"\"\"提取贪心策略路径\"\"\"\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_action(state, training=False)\n",
    "        next_state, _, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "# 提取路径\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"学到的策略路径对比\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nQ-Learning 学到的路径 (倾向最短路径，沿悬崖边):\")\n",
    "q_path = extract_path(q_agent, env)\n",
    "env.reset()\n",
    "env.render(q_path)\n",
    "print(f\"路径长度: {len(q_path) - 1} 步\")\n",
    "\n",
    "print(\"\\nSARSA 学到的路径 (倾向安全路径，远离悬崖):\")\n",
    "sarsa_path = extract_path(sarsa_agent, env)\n",
    "env.reset()\n",
    "env.render(sarsa_path)\n",
    "print(f\"路径长度: {len(sarsa_path) - 1} 步\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第4部分：行为差异分析\n",
    "\n",
    "### 4.1 为什么 Q-Learning 选择悬崖边路径？\n",
    "\n",
    "Q-Learning 更新使用 $\\max$，学习的是**最优策略的价值**：\n",
    "\n",
    "- 假设执行最优策略，不会掉入悬崖\n",
    "- 沿悬崖边的路径最短，奖励最高\n",
    "- 但训练时的 ε-greedy 探索会导致实际掉入悬崖\n",
    "\n",
    "**结果**：学到的策略是最优的，但训练过程中经常失败\n",
    "\n",
    "### 4.2 为什么 SARSA 选择安全路径？\n",
    "\n",
    "SARSA 使用实际采取的动作，学习的是**当前 ε-greedy 策略的价值**：\n",
    "\n",
    "- 考虑到探索时可能随机选择动作\n",
    "- 靠近悬崖时，探索可能导致掉落\n",
    "- 因此远离悬崖的路径价值更高\n",
    "\n",
    "**结果**：学到的策略更保守，但训练过程更稳定\n",
    "\n",
    "### 4.3 可视化对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_value_comparison(q_agent, sarsa_agent, env):\n",
    "    \"\"\"对比两种算法学到的价值函数\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for idx, (agent, name) in enumerate([(q_agent, 'Q-Learning'), (sarsa_agent, 'SARSA')]):\n",
    "        v_table = np.zeros((env.height, env.width))\n",
    "        for i in range(env.height):\n",
    "            for j in range(env.width):\n",
    "                state = (i, j)\n",
    "                if state in agent.q_table:\n",
    "                    v_table[i, j] = np.max(agent.q_table[state])\n",
    "        \n",
    "        im = axes[idx].imshow(v_table, cmap='RdYlGn', aspect='auto')\n",
    "        axes[idx].set_title(f'{name} 价值函数 V(s)')\n",
    "        axes[idx].set_xlabel('列')\n",
    "        axes[idx].set_ylabel('行')\n",
    "        plt.colorbar(im, ax=axes[idx])\n",
    "        \n",
    "        # 标记悬崖\n",
    "        for pos in env.cliff:\n",
    "            axes[idx].add_patch(plt.Rectangle(\n",
    "                (pos[1]-0.5, pos[0]-0.5), 1, 1,\n",
    "                fill=True, color='black', alpha=0.5\n",
    "            ))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_value_comparison(q_agent, sarsa_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第5部分：Expected SARSA\n",
    "\n",
    "### 5.1 算法原理\n",
    "\n",
    "Expected SARSA 是 SARSA 的改进版本，使用下一状态 Q 值的**期望**而非采样值：\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R + \\gamma \\mathbb{E}[Q(S',A')] - Q(S,A) \\right]$$\n",
    "\n",
    "其中期望在当前策略下计算：\n",
    "\n",
    "$$\\mathbb{E}[Q(S',A')] = \\sum_a \\pi(a|S') Q(S',a)$$\n",
    "\n",
    "**特点**：结合了 Q-Learning 的低方差和 SARSA 的在策略特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSARSAAgent:\n",
    "    \"\"\"Expected SARSA 智能体\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def _get_expected_q(self, state: Any) -> float:\n",
    "        \"\"\"计算 ε-greedy 策略下的期望 Q 值\"\"\"\n",
    "        q_values = self.q_table[state]\n",
    "        \n",
    "        # ε-greedy 策略下的动作概率\n",
    "        probs = np.ones(self.n_actions) * self.epsilon / self.n_actions\n",
    "        best_action = np.argmax(q_values)\n",
    "        probs[best_action] += 1 - self.epsilon\n",
    "        \n",
    "        return np.dot(probs, q_values)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"Expected SARSA 更新\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            # Expected SARSA: 使用期望 Q 值\n",
    "            target = reward + self.gamma * self._get_expected_q(next_state)\n",
    "        \n",
    "        td_error = target - current_q\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# 训练 Expected SARSA\n",
    "print(\"\\n训练 Expected SARSA...\")\n",
    "exp_sarsa_agent = ExpectedSARSAAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epsilon=EPSILON,\n",
    "    epsilon_decay=1.0,\n",
    "    epsilon_min=EPSILON\n",
    ")\n",
    "\n",
    "exp_sarsa_history = train_q_learning(env, exp_sarsa_agent, episodes=EPISODES, verbose=True)\n",
    "\n",
    "print(\"\\nExpected SARSA 学到的路径:\")\n",
    "exp_sarsa_path = extract_path(exp_sarsa_agent, env)\n",
    "env.reset()\n",
    "env.render(exp_sarsa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 特性 | Q-Learning | SARSA | Expected SARSA |\n",
    "|------|------------|-------|----------------|\n",
    "| 类型 | Off-Policy | On-Policy | On-Policy |\n",
    "| 更新目标 | $\\max_a Q(S',a)$ | $Q(S',A')$ | $\\mathbb{E}[Q(S',A')]$ |\n",
    "| 学习策略 | 最优策略 | 当前策略 | 当前策略 |\n",
    "| 方差 | 低 | 高 | 低 |\n",
    "| 安全性 | 激进 | 保守 | 中等 |\n",
    "\n",
    "### 选择建议\n",
    "\n",
    "- **Q-Learning**：追求最优性能，可承受训练不稳定\n",
    "- **SARSA**：需要安全探索，如机器人控制\n",
    "- **Expected SARSA**：平衡方案，实践中常用\n",
    "\n",
    "### 核心公式\n",
    "\n",
    "| 算法 | 更新公式 |\n",
    "|------|----------|\n",
    "| Q-Learning | $Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\max_a Q(S',a) - Q(S,A)]$ |\n",
    "| SARSA | $Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma Q(S',A') - Q(S,A)]$ |\n",
    "| Expected SARSA | $Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\mathbb{E}[Q(S',A')] - Q(S,A)]$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"运行单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # 测试1: SARSA 更新\n",
    "    try:\n",
    "        agent = SARSAAgent(n_actions=4, learning_rate=0.5, discount_factor=0.9)\n",
    "        state = (0, 0)\n",
    "        next_state = (0, 1)\n",
    "        \n",
    "        # 设置 next_state 的 Q 值\n",
    "        agent.q_table[next_state] = np.array([1.0, 2.0, 0.0, 0.0])\n",
    "        \n",
    "        # SARSA 更新：使用 next_action=1 (Q值为2.0)\n",
    "        agent.update(state, 0, -1.0, next_state, 1, False)\n",
    "        \n",
    "        # Q(s,a) = 0 + 0.5 * (-1 + 0.9 * 2.0 - 0) = 0.5 * 0.8 = 0.4\n",
    "        expected = 0.4\n",
    "        assert np.isclose(agent.q_table[state][0], expected), \\\n",
    "            f\"SARSA更新错误: {agent.q_table[state][0]} != {expected}\"\n",
    "        \n",
    "        print(\"测试1通过: SARSA 更新正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: Expected SARSA 期望计算\n",
    "    try:\n",
    "        agent = ExpectedSARSAAgent(n_actions=4, epsilon=0.2)\n",
    "        state = (0, 0)\n",
    "        agent.q_table[state] = np.array([1.0, 2.0, 0.5, 0.5])\n",
    "        \n",
    "        # ε=0.2 时，动作1(最优)概率 = 0.8 + 0.2/4 = 0.85\n",
    "        # 其他动作概率 = 0.2/4 = 0.05\n",
    "        # E[Q] = 0.05*1.0 + 0.85*2.0 + 0.05*0.5 + 0.05*0.5 = 1.8\n",
    "        expected_q = agent._get_expected_q(state)\n",
    "        assert np.isclose(expected_q, 1.8), f\"期望Q值计算错误: {expected_q}\"\n",
    "        \n",
    "        print(\"测试2通过: Expected SARSA 期望计算正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: Q-Learning vs SARSA 行为差异\n",
    "    try:\n",
    "        env = CliffWalkingEnv()\n",
    "        \n",
    "        # 训练两个智能体\n",
    "        q_agent = QLearningAgent(n_actions=4, learning_rate=0.5, epsilon=0.1, \n",
    "                                  epsilon_decay=1.0, epsilon_min=0.1)\n",
    "        sarsa_agent = SARSAAgent(n_actions=4, learning_rate=0.5, epsilon=0.1,\n",
    "                                  epsilon_decay=1.0, epsilon_min=0.1)\n",
    "        \n",
    "        q_hist = train_q_learning(env, q_agent, episodes=200, verbose=False)\n",
    "        sarsa_hist = train_sarsa(env, sarsa_agent, episodes=200, verbose=False)\n",
    "        \n",
    "        # 验证训练后性能\n",
    "        q_avg = np.mean(q_hist['rewards'][-50:])\n",
    "        sarsa_avg = np.mean(sarsa_hist['rewards'][-50:])\n",
    "        \n",
    "        assert q_avg > -100 and sarsa_avg > -100, \"训练未收敛\"\n",
    "        \n",
    "        print(f\"测试3通过: 算法训练正常 (Q-Learning: {q_avg:.1f}, SARSA: {sarsa_avg:.1f})\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"测试完成: {passed} 通过, {failed} 失败\")\n",
    "    if failed == 0:\n",
    "        print(\"所有测试通过！\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return failed == 0\n",
    "\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Rummery, G.A. & Niranjan, M. (1994). On-Line Q-Learning Using Connectionist Systems.\n",
    "2. Sutton, R.S. & Barto, A.G. (2018). Reinforcement Learning: An Introduction, 2nd ed. Chapter 6.\n",
    "3. Van Seijen, H., et al. (2009). A Theoretical and Empirical Analysis of Expected Sarsa.\n",
    "\n",
    "---\n",
    "\n",
    "[返回目录](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
