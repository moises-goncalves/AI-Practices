{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning 高级技巧与实战\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将学会：\n",
    "- 理解 Q-Learning 过估计问题及 Double Q-Learning 解决方案\n",
    "- 掌握学习率调度和资格迹等高级技巧\n",
    "- 使用 Gymnasium 标准环境进行训练\n",
    "- 在 Taxi-v3 环境中实现完整训练流程\n",
    "- 模型保存、加载与评估\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- Q-Learning 和 SARSA 基础\n",
    "- Python 面向对象编程\n",
    "- NumPy 和 Matplotlib\n",
    "\n",
    "## 预计时间\n",
    "\n",
    "50-70 分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第1部分：Q-Learning 的过估计问题\n",
    "\n",
    "### 1.1 过估计现象\n",
    "\n",
    "Q-Learning 更新使用 $\\max$ 操作：\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\max_a Q(S',a) - Q(S,A)]$$\n",
    "\n",
    "**问题**：$\\max$ 操作会系统性地高估 Q 值\n",
    "\n",
    "**原因分析**：\n",
    "- 假设所有 Q 值估计都有噪声：$\\hat{Q}(s,a) = Q^*(s,a) + \\epsilon_a$\n",
    "- $\\max_a \\hat{Q}(s,a) \\geq \\max_a Q^*(s,a)$（因为噪声可能使某些估计偏高）\n",
    "- 这种偏差会通过 bootstrapping 传播和累积\n",
    "\n",
    "### 1.2 Double Q-Learning\n",
    "\n",
    "**核心思想**：解耦动作选择和价值评估\n",
    "\n",
    "维护两个 Q 表 $Q_1$ 和 $Q_2$，交替更新：\n",
    "\n",
    "- 更新 $Q_1$：用 $Q_1$ 选择动作，用 $Q_2$ 评估\n",
    "  $$Q_1(S,A) \\leftarrow Q_1(S,A) + \\alpha[R + \\gamma Q_2(S', \\arg\\max_a Q_1(S',a)) - Q_1(S,A)]$$\n",
    "\n",
    "- 更新 $Q_2$：用 $Q_2$ 选择动作，用 $Q_1$ 评估\n",
    "  $$Q_2(S,A) \\leftarrow Q_2(S,A) + \\alpha[R + \\gamma Q_1(S', \\arg\\max_a Q_2(S',a)) - Q_2(S,A)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第2部分：代码实现\n",
    "\n",
    "### 步骤1: 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 导入必要的库\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 尝试导入 Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "    print(f\"Gymnasium 版本: {gym.__version__}\")\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"请安装 gymnasium: pip install gymnasium\")\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "# 可视化配置\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"\\n库导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2: 实现 Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    \"\"\"训练指标记录\"\"\"\n",
    "    episode_rewards: List[float] = field(default_factory=list)\n",
    "    episode_lengths: List[int] = field(default_factory=list)\n",
    "    epsilon_history: List[float] = field(default_factory=list)\n",
    "    \n",
    "    def get_moving_average(self, window: int = 100) -> np.ndarray:\n",
    "        \"\"\"计算移动平均\"\"\"\n",
    "        if len(self.episode_rewards) < window:\n",
    "            return np.array(self.episode_rewards)\n",
    "        return np.convolve(\n",
    "            self.episode_rewards,\n",
    "            np.ones(window) / window,\n",
    "            mode='valid'\n",
    "        )\n",
    "\n",
    "\n",
    "class DoubleQLearningAgent:\n",
    "    \"\"\"\n",
    "    Double Q-Learning 智能体\n",
    "    \n",
    "    通过维护两个 Q 表，解耦动作选择和价值评估，减少过估计偏差。\n",
    "    \n",
    "    Attributes:\n",
    "        q_table1, q_table2: 两个独立的 Q 表\n",
    "        lr: 学习率\n",
    "        gamma: 折扣因子\n",
    "        epsilon: 探索率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # 两个独立的 Q 表\n",
    "        self.q_table1: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        self.q_table2: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        \n",
    "        self.metrics = TrainingMetrics()\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        \"\"\"使用两个 Q 表的和选择动作\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        # 使用两个 Q 表的和\n",
    "        combined_q = self.q_table1[state] + self.q_table2[state]\n",
    "        max_q = np.max(combined_q)\n",
    "        max_actions = np.where(np.isclose(combined_q, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Double Q-Learning 更新\n",
    "        \n",
    "        随机选择更新 Q1 或 Q2，解耦选择和评估。\n",
    "        \"\"\"\n",
    "        if np.random.random() < 0.5:\n",
    "            # 更新 Q1：用 Q1 选择动作，Q2 评估\n",
    "            current_q = self.q_table1[state][action]\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                best_action = np.argmax(self.q_table1[next_state])\n",
    "                target = reward + self.gamma * self.q_table2[next_state][best_action]\n",
    "            td_error = target - current_q\n",
    "            self.q_table1[state][action] += self.lr * td_error\n",
    "        else:\n",
    "            # 更新 Q2：用 Q2 选择动作，Q1 评估\n",
    "            current_q = self.q_table2[state][action]\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                best_action = np.argmax(self.q_table2[next_state])\n",
    "                target = reward + self.gamma * self.q_table1[next_state][best_action]\n",
    "            td_error = target - current_q\n",
    "            self.q_table2[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        \"\"\"衰减探索率\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def get_q_values(self, state: Any) -> np.ndarray:\n",
    "        \"\"\"获取状态的平均 Q 值\"\"\"\n",
    "        return (self.q_table1[state] + self.q_table2[state]) / 2\n",
    "\n",
    "\n",
    "print(\"Double Q-Learning 智能体定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3: 实现标准 Q-Learning (对比用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"标准 Q-Learning 智能体\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        self.metrics = TrainingMetrics()\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done) -> float:\n",
    "        current_q = self.q_table[state][action]\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        td_error = target - current_q\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save(self, filepath: str) -> None:\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        data = {\n",
    "            'q_table': {str(k): v.tolist() for k, v in self.q_table.items()},\n",
    "            'epsilon': self.epsilon,\n",
    "            'config': {\n",
    "                'n_actions': self.n_actions,\n",
    "                'lr': self.lr,\n",
    "                'gamma': self.gamma\n",
    "            }\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"模型已保存到: {filepath}\")\n",
    "    \n",
    "    def load(self, filepath: str) -> None:\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.q_table = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        for k, v in data['q_table'].items():\n",
    "            try:\n",
    "                key = eval(k)\n",
    "            except:\n",
    "                key = int(k) if k.isdigit() else k\n",
    "            self.q_table[key] = np.array(v)\n",
    "        \n",
    "        self.epsilon = data.get('epsilon', 0.01)\n",
    "        print(f\"模型已从 {filepath} 加载\")\n",
    "\n",
    "\n",
    "print(\"标准 Q-Learning 智能体定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第3部分：Gymnasium 环境实战\n",
    "\n",
    "### 3.1 Taxi-v3 环境介绍\n",
    "\n",
    "Taxi-v3 是 Gymnasium 内置的经典强化学习环境：\n",
    "\n",
    "```\n",
    "+---------+\n",
    "|R: | : :G|    R, G, Y, B: 乘客位置/目的地\n",
    "| : | : : |    |: 墙壁\n",
    "| : : : : |    黄色方块: 出租车位置\n",
    "| | : | : |\n",
    "|Y| : |B: |\n",
    "+---------+\n",
    "\n",
    "状态空间: 500 种状态\n",
    "  - 出租车位置: 25 (5x5)\n",
    "  - 乘客位置: 5 (R,G,Y,B 或在车上)\n",
    "  - 目的地: 4 (R,G,Y,B)\n",
    "\n",
    "动作空间: 6 种动作\n",
    "  - 0: 向南移动\n",
    "  - 1: 向北移动\n",
    "  - 2: 向东移动\n",
    "  - 3: 向西移动\n",
    "  - 4: 接乘客\n",
    "  - 5: 放乘客\n",
    "\n",
    "奖励设计:\n",
    "  - 每步: -1\n",
    "  - 成功送达: +20\n",
    "  - 非法接/放: -10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索 Taxi 环境\n",
    "if HAS_GYM:\n",
    "    env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "    \n",
    "    print(\"Taxi-v3 环境信息:\")\n",
    "    print(f\"  状态空间大小: {env.observation_space.n}\")\n",
    "    print(f\"  动作空间大小: {env.action_space.n}\")\n",
    "    \n",
    "    # 重置环境并显示\n",
    "    state, info = env.reset(seed=42)\n",
    "    print(f\"\\n初始状态: {state}\")\n",
    "    print(f\"\\n环境渲染:\")\n",
    "    print(env.render())\n",
    "    \n",
    "    # 动作说明\n",
    "    action_names = ['南', '北', '东', '西', '接乘客', '放乘客']\n",
    "    print(\"\\n动作说明:\")\n",
    "    for i, name in enumerate(action_names):\n",
    "        print(f\"  {i}: {name}\")\n",
    "    \n",
    "    env.close()\n",
    "else:\n",
    "    print(\"跳过 Taxi 环境演示 (需要 gymnasium)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    env,\n",
    "    agent,\n",
    "    episodes: int = 2000,\n",
    "    max_steps: int = 200,\n",
    "    verbose: bool = True,\n",
    "    log_interval: int = 200\n",
    ") -> TrainingMetrics:\n",
    "    \"\"\"\n",
    "    通用训练函数\n",
    "    \n",
    "    支持 Gymnasium 环境和自定义环境。\n",
    "    \"\"\"\n",
    "    metrics = TrainingMetrics()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # 重置环境\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            \n",
    "            # 执行动作\n",
    "            result = env.step(action)\n",
    "            if len(result) == 3:\n",
    "                next_state, reward, done = result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            # 更新 Q 值\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        metrics.episode_rewards.append(total_reward)\n",
    "        metrics.episode_lengths.append(steps)\n",
    "        metrics.epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        if verbose and (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(metrics.episode_rewards[-log_interval:])\n",
    "            avg_steps = np.mean(metrics.episode_lengths[-log_interval:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
    "                  f\"Avg Steps: {avg_steps:6.1f} | \"\n",
    "                  f\"ε: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    agent.metrics = metrics\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"训练函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 在 Taxi 环境训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Taxi-v3 Q-Learning 训练\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 创建环境和智能体\n",
    "    env = gym.make('Taxi-v3')\n",
    "    \n",
    "    agent = QLearningAgent(\n",
    "        n_actions=env.action_space.n,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01\n",
    "    )\n",
    "    \n",
    "    # 训练\n",
    "    metrics = train_agent(env, agent, episodes=2000, verbose=True)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"\\n训练完成！\")\n",
    "    print(f\"最后100回合平均奖励: {np.mean(metrics.episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"Q表大小: {len(agent.q_table)} 状态\")\n",
    "else:\n",
    "    print(\"跳过 Taxi 训练 (需要 gymnasium)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(metrics: TrainingMetrics, title: str = \"训练曲线\"):\n",
    "    \"\"\"绘制训练指标\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    window = 50\n",
    "    \n",
    "    # 奖励曲线\n",
    "    rewards = metrics.episode_rewards\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(rewards, alpha=0.3, color='blue')\n",
    "    axes[0].plot(range(window-1, len(rewards)), smoothed, color='blue', linewidth=2)\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title('回合奖励')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 步数曲线\n",
    "    steps = metrics.episode_lengths\n",
    "    smoothed_steps = np.convolve(steps, np.ones(window)/window, mode='valid')\n",
    "    axes[1].plot(steps, alpha=0.3, color='green')\n",
    "    axes[1].plot(range(window-1, len(steps)), smoothed_steps, color='green', linewidth=2)\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('回合步数')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 探索率衰减\n",
    "    axes[2].plot(metrics.epsilon_history, color='red')\n",
    "    axes[2].set_xlabel('Episode')\n",
    "    axes[2].set_ylabel('Epsilon')\n",
    "    axes[2].set_title('探索率衰减')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if HAS_GYM:\n",
    "    plot_training_metrics(metrics, \"Taxi-v3 Q-Learning 训练曲线\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 评估训练好的智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    env,\n",
    "    agent,\n",
    "    episodes: int = 100,\n",
    "    render: bool = False\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    评估智能体性能\n",
    "    \n",
    "    Returns:\n",
    "        包含评估指标的字典\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    successes = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        \n",
    "        total_reward = 0\n",
    "        ep_steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.get_action(state, training=False)  # 贪心策略\n",
    "            \n",
    "            result = env.step(action)\n",
    "            if len(result) == 3:\n",
    "                next_state, reward, done = result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            ep_steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if render and ep < 3:\n",
    "                print(env.render())\n",
    "            \n",
    "            if done:\n",
    "                if reward == 20:  # Taxi 成功送达奖励\n",
    "                    successes += 1\n",
    "                break\n",
    "            \n",
    "            if ep_steps >= 200:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        steps.append(ep_steps)\n",
    "    \n",
    "    results = {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_steps': np.mean(steps),\n",
    "        'success_rate': successes / episodes * 100\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if HAS_GYM:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"评估训练好的智能体\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    env = gym.make('Taxi-v3')\n",
    "    results = evaluate_agent(env, agent, episodes=100)\n",
    "    \n",
    "    print(f\"\\n评估结果 (100回合):\")\n",
    "    print(f\"  平均奖励: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "    print(f\"  平均步数: {results['mean_steps']:.1f}\")\n",
    "    print(f\"  成功率: {results['success_rate']:.1f}%\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 演示训练好的智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_agent(env, agent, episodes: int = 2):\n",
    "    \"\"\"演示智能体行为\"\"\"\n",
    "    action_names = ['南', '北', '东', '西', '接', '放']\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"演示回合 {ep + 1}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        print(f\"\\n初始状态:\")\n",
    "        print(env.render())\n",
    "        \n",
    "        while steps < 20:  # 限制显示步数\n",
    "            action = agent.get_action(state, training=False)\n",
    "            \n",
    "            result = env.step(action)\n",
    "            if len(result) == 3:\n",
    "                next_state, reward, done = result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            print(f\"\\n步骤 {steps}: 动作={action_names[action]}, 奖励={reward}\")\n",
    "            print(env.render())\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(f\"\\n回合结束！总奖励: {total_reward}, 步数: {steps}\")\n",
    "                break\n",
    "\n",
    "\n",
    "if HAS_GYM:\n",
    "    env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "    demo_agent(env, agent, episodes=1)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第4部分：高级技巧\n",
    "\n",
    "### 4.1 学习率调度\n",
    "\n",
    "固定学习率可能导致：\n",
    "- 太大：Q值震荡，不稳定\n",
    "- 太小：收敛过慢\n",
    "\n",
    "**解决方案**：基于访问次数的衰减学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLRQLearning(QLearningAgent):\n",
    "    \"\"\"自适应学习率的 Q-Learning\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # 状态-动作访问计数\n",
    "        self.visit_count: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(self.n_actions)\n",
    "        )\n",
    "        \n",
    "    def get_learning_rate(self, state, action) -> float:\n",
    "        \"\"\"\n",
    "        基于访问次数的衰减学习率\n",
    "        \n",
    "        α(s,a) = 1 / (1 + N(s,a))\n",
    "        \"\"\"\n",
    "        count = self.visit_count[state][action]\n",
    "        return 1.0 / (1.0 + count)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done) -> float:\n",
    "        # 更新访问计数\n",
    "        self.visit_count[state][action] += 1\n",
    "        \n",
    "        # 使用自适应学习率\n",
    "        lr = self.get_learning_rate(state, action)\n",
    "        \n",
    "        current_q = self.q_table[state][action]\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        td_error = target - current_q\n",
    "        self.q_table[state][action] += lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "\n",
    "\n",
    "print(\"自适应学习率 Q-Learning 定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Double Q-Learning 对比实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Q-Learning vs Double Q-Learning 对比实验\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    env = gym.make('Taxi-v3')\n",
    "    \n",
    "    # 标准 Q-Learning\n",
    "    print(\"\\n训练标准 Q-Learning...\")\n",
    "    q_agent = QLearningAgent(\n",
    "        n_actions=env.action_space.n,\n",
    "        learning_rate=0.1,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01\n",
    "    )\n",
    "    q_metrics = train_agent(env, q_agent, episodes=1000, verbose=False)\n",
    "    \n",
    "    # Double Q-Learning\n",
    "    print(\"训练 Double Q-Learning...\")\n",
    "    double_q_agent = DoubleQLearningAgent(\n",
    "        n_actions=env.action_space.n,\n",
    "        learning_rate=0.1,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01\n",
    "    )\n",
    "    double_q_metrics = train_agent(env, double_q_agent, episodes=1000, verbose=False)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # 绘制对比\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    window = 50\n",
    "    q_smooth = np.convolve(q_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    double_smooth = np.convolve(double_q_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax.plot(q_smooth, label='Q-Learning', alpha=0.8)\n",
    "    ax.plot(double_smooth, label='Double Q-Learning', alpha=0.8)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Total Reward')\n",
    "    ax.set_title('Q-Learning vs Double Q-Learning on Taxi-v3')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n最后100回合平均奖励:\")\n",
    "    print(f\"  Q-Learning: {np.mean(q_metrics.episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"  Double Q-Learning: {np.mean(double_q_metrics.episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第5部分：模型保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GYM:\n",
    "    # 保存模型\n",
    "    model_path = \"taxi_q_learning_model.json\"\n",
    "    agent.save(model_path)\n",
    "    \n",
    "    # 创建新智能体并加载模型\n",
    "    new_agent = QLearningAgent(n_actions=6)\n",
    "    new_agent.load(model_path)\n",
    "    \n",
    "    # 验证加载的模型\n",
    "    env = gym.make('Taxi-v3')\n",
    "    results = evaluate_agent(env, new_agent, episodes=50)\n",
    "    print(f\"\\n加载模型后的评估:\")\n",
    "    print(f\"  平均奖励: {results['mean_reward']:.2f}\")\n",
    "    print(f\"  成功率: {results['success_rate']:.1f}%\")\n",
    "    env.close()\n",
    "    \n",
    "    # 清理\n",
    "    import os\n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "        print(f\"\\n已清理临时模型文件\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **过估计问题**：Q-Learning 的 max 操作导致系统性高估\n",
    "2. **Double Q-Learning**：通过解耦选择和评估减少偏差\n",
    "3. **学习率调度**：基于访问次数自适应调整\n",
    "4. **Gymnasium**：标准化的 RL 环境接口\n",
    "\n",
    "### 超参数调优建议\n",
    "\n",
    "| 参数 | 建议范围 | 说明 |\n",
    "|------|----------|------|\n",
    "| 学习率 | 0.05-0.5 | 表格型可用较大值 |\n",
    "| 折扣因子 | 0.95-0.99 | 任务越长期越接近1 |\n",
    "| 初始探索率 | 1.0 | 从完全探索开始 |\n",
    "| 最终探索率 | 0.01-0.1 | 保持少量探索 |\n",
    "| 衰减率 | 0.99-0.999 | 控制探索下降速度 |\n",
    "\n",
    "### 表格型方法局限\n",
    "\n",
    "- 状态空间必须离散且有限\n",
    "- 无法处理高维/连续状态\n",
    "- 无法泛化到未见状态\n",
    "\n",
    "**解决方案**：深度 Q 网络 (DQN) - 用神经网络近似 Q 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"运行单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # 测试1: Double Q-Learning 更新\n",
    "    try:\n",
    "        agent = DoubleQLearningAgent(n_actions=4, learning_rate=0.5)\n",
    "        state = (0, 0)\n",
    "        next_state = (0, 1)\n",
    "        \n",
    "        # 多次更新，验证两个Q表都被更新\n",
    "        np.random.seed(42)\n",
    "        for _ in range(10):\n",
    "            agent.update(state, 0, -1.0, next_state, False)\n",
    "        \n",
    "        # 验证两个Q表都有更新\n",
    "        assert agent.q_table1[state][0] != 0 or agent.q_table2[state][0] != 0\n",
    "        print(\"测试1通过: Double Q-Learning 更新正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: 自适应学习率\n",
    "    try:\n",
    "        agent = AdaptiveLRQLearning(n_actions=4)\n",
    "        state = (0, 0)\n",
    "        \n",
    "        # 初始学习率应为 1.0\n",
    "        lr1 = agent.get_learning_rate(state, 0)\n",
    "        assert np.isclose(lr1, 1.0), f\"初始学习率错误: {lr1}\"\n",
    "        \n",
    "        # 更新后学习率应衰减\n",
    "        agent.update(state, 0, -1.0, (0, 1), False)\n",
    "        lr2 = agent.get_learning_rate(state, 0)\n",
    "        assert lr2 < lr1, \"学习率应该衰减\"\n",
    "        \n",
    "        print(\"测试2通过: 自适应学习率正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: Gymnasium 环境兼容性\n",
    "    if HAS_GYM:\n",
    "        try:\n",
    "            env = gym.make('Taxi-v3')\n",
    "            agent = QLearningAgent(n_actions=env.action_space.n)\n",
    "            \n",
    "            # 运行一个回合\n",
    "            state, _ = env.reset()\n",
    "            for _ in range(10):\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                agent.update(state, action, reward, next_state, terminated or truncated)\n",
    "                state = next_state\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            env.close()\n",
    "            print(\"测试3通过: Gymnasium 兼容性正确\")\n",
    "            passed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"测试3失败: {e}\")\n",
    "            failed += 1\n",
    "    else:\n",
    "        print(\"测试3跳过: 需要 gymnasium\")\n",
    "    \n",
    "    # 测试4: 模型保存/加载\n",
    "    try:\n",
    "        agent = QLearningAgent(n_actions=4)\n",
    "        state = (0, 0)\n",
    "        agent.q_table[state] = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "        \n",
    "        # 保存\n",
    "        test_path = \"_test_model.json\"\n",
    "        agent.save(test_path)\n",
    "        \n",
    "        # 加载\n",
    "        new_agent = QLearningAgent(n_actions=4)\n",
    "        new_agent.load(test_path)\n",
    "        \n",
    "        # 验证\n",
    "        assert np.allclose(new_agent.q_table[state], agent.q_table[state])\n",
    "        \n",
    "        # 清理\n",
    "        import os\n",
    "        os.remove(test_path)\n",
    "        \n",
    "        print(\"测试4通过: 模型保存/加载正确\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"测试完成: {passed} 通过, {failed} 失败\")\n",
    "    if failed == 0:\n",
    "        print(\"所有测试通过！\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return failed == 0\n",
    "\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Van Hasselt, H. (2010). Double Q-learning. NeurIPS.\n",
    "2. Sutton, R.S. & Barto, A.G. (2018). Reinforcement Learning: An Introduction, 2nd ed.\n",
    "3. [Gymnasium Documentation](https://gymnasium.farama.org/)\n",
    "4. [OpenAI Spinning Up](https://spinningup.openai.com/)\n",
    "\n",
    "---\n",
    "\n",
    "[返回目录](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
