{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning 基础与实现\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将学会：\n",
    "- 理解时序差分学习 (TD Learning) 的核心思想\n",
    "- 掌握 Q-Learning 算法的数学原理\n",
    "- 实现表格型 Q-Learning\n",
    "- 理解探索与利用的平衡策略\n",
    "- 在悬崖行走环境中训练智能体\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- 马尔可夫决策过程 (MDP) 基本概念\n",
    "- Python 和 NumPy 基础\n",
    "- 概率论基础\n",
    "\n",
    "## 预计时间\n",
    "\n",
    "45-60 分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第1部分：从动态规划到无模型学习\n",
    "\n",
    "### 1.1 动态规划的局限性\n",
    "\n",
    "在 MDP 基础模块中，我们学习了动态规划 (DP) 方法求解最优策略。DP 方法需要：\n",
    "\n",
    "1. **完整的环境模型**：状态转移概率 $P(s'|s,a)$ 和奖励函数 $R(s,a,s')$\n",
    "2. **遍历所有状态和动作**：计算复杂度随状态空间增大而爆炸\n",
    "\n",
    "**现实问题**：\n",
    "- 转移概率通常未知（如何精确计算开车时每个操作的后果？）\n",
    "- 状态空间可能极其巨大（围棋约有 $10^{170}$ 种状态）\n",
    "\n",
    "### 1.2 无模型强化学习\n",
    "\n",
    "**核心思想**：通过与环境**交互采样**学习最优策略，无需事先知道环境模型。\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    强化学习方法分类                          │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  ┌─────────────┐           ┌─────────────────────────────┐  │\n",
    "│  │  基于模型   │           │        无模型 (Model-Free)   │  │\n",
    "│  │ (Model-Based)│          ├──────────────┬──────────────┤  │\n",
    "│  │             │           │  基于价值    │  基于策略     │  │\n",
    "│  │ • 动态规划   │           │ (Value-Based)│(Policy-Based)│  │\n",
    "│  │ • 模型预测控制│          │              │              │  │\n",
    "│  │ • Dyna-Q    │           │ • Q-Learning │ • REINFORCE  │  │\n",
    "│  └─────────────┘           │ • SARSA      │ • Actor-Critic│  │\n",
    "│                            │ • DQN        │ • PPO        │  │\n",
    "│                            └──────────────┴──────────────┘  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第2部分：时序差分学习 (TD Learning)\n",
    "\n",
    "### 2.1 蒙特卡洛 vs 时序差分\n",
    "\n",
    "**蒙特卡洛方法** (Monte Carlo, MC)：\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]$$\n",
    "\n",
    "- 需要等待**回合结束**才能更新\n",
    "- $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$ 是完整回合的实际累积回报\n",
    "- 无偏估计，但方差较大\n",
    "\n",
    "**时序差分方法** (Temporal Difference, TD)：\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]$$\n",
    "\n",
    "- **每一步**都可以更新\n",
    "- 使用**自举** (Bootstrapping)：用估计值更新估计值\n",
    "- 有偏估计，但方差较小\n",
    "\n",
    "### 2.2 TD 误差\n",
    "\n",
    "**TD 目标** (TD Target):\n",
    "\n",
    "$$\\text{TD Target} = R_{t+1} + \\gamma V(S_{t+1})$$\n",
    "\n",
    "**TD 误差** (TD Error):\n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
    "\n",
    "**直觉理解**：\n",
    "- $\\delta_t > 0$：实际比预期好，应增大 $V(S_t)$\n",
    "- $\\delta_t < 0$：实际比预期差，应减小 $V(S_t)$\n",
    "- $\\delta_t = 0$：预测准确，价值已收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第3部分：Q-Learning 算法\n",
    "\n",
    "### 3.1 算法原理\n",
    "\n",
    "Q-Learning 是一种**离策略** (Off-Policy) TD 控制算法。它直接学习最优动作价值函数 $Q^*$。\n",
    "\n",
    "**更新公式**：\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "**关键特性**：\n",
    "- 使用 $\\max$ 操作选择下一状态的最大 Q 值\n",
    "- 与当前实际采取的行为策略无关 (Off-Policy)\n",
    "- 在一定条件下保证收敛到最优 Q 函数\n",
    "\n",
    "### 3.2 算法伪代码\n",
    "\n",
    "```\n",
    "算法: Q-Learning\n",
    "\n",
    "输入: 状态空间 S, 动作空间 A, 学习率 α, 折扣因子 γ, 探索率 ε\n",
    "输出: 最优 Q 函数\n",
    "\n",
    "1. 初始化 Q(s, a) = 0，对于所有 s ∈ S, a ∈ A\n",
    "2. 对于每个回合:\n",
    "   a. 初始化状态 S\n",
    "   b. 重复 (对于回合中的每一步):\n",
    "      i.   使用 ε-greedy 从 Q 选择动作 A\n",
    "      ii.  执行动作 A，观察奖励 R 和下一状态 S'\n",
    "      iii. Q(S, A) ← Q(S, A) + α[R + γ max_a Q(S', a) - Q(S, A)]\n",
    "      iv.  S ← S'\n",
    "   c. 直到 S 是终止状态\n",
    "3. 返回 Q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第4部分：代码实现\n",
    "\n",
    "### 步骤1: 导入库和配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 导入必要的库\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# 配置参数\n",
    "# ============================================================\n",
    "\n",
    "# 设置随机种子，确保结果可重复\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# 可视化配置\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"库导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤2: 实现悬崖行走环境\n",
    "\n",
    "悬崖行走 (Cliff Walking) 是经典的强化学习测试环境：\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│ .  .  .  .  .  .  .  .  .  .  .  .  │  row 0\n",
    "│ .  .  .  .  .  .  .  .  .  .  .  .  │  row 1\n",
    "│ .  .  .  .  .  .  .  .  .  .  .  .  │  row 2\n",
    "│ S  C  C  C  C  C  C  C  C  C  C  G  │  row 3\n",
    "└─────────────────────────────────────────────┘\n",
    "  0  1  2  3  4  5  6  7  8  9 10 11\n",
    "\n",
    "S: 起点    G: 目标    C: 悬崖\n",
    "```\n",
    "\n",
    "- 动作空间：上(0)、右(1)、下(2)、左(3)\n",
    "- 奖励：每步 -1，掉入悬崖 -100 并重置到起点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\"\n",
    "    悬崖行走环境\n",
    "    \n",
    "    经典的强化学习测试环境，用于对比不同算法的行为特性。\n",
    "    智能体需要从起点 S 到达目标 G，同时避免掉入悬崖 C。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 动作定义：(行偏移, 列偏移)\n",
    "    ACTIONS = {\n",
    "        0: (-1, 0),   # 上\n",
    "        1: (0, 1),    # 右\n",
    "        2: (1, 0),    # 下\n",
    "        3: (0, -1)    # 左\n",
    "    }\n",
    "    ACTION_NAMES = ['上', '右', '下', '左']\n",
    "    \n",
    "    def __init__(self, height: int = 4, width: int = 12):\n",
    "        \"\"\"\n",
    "        初始化环境\n",
    "        \n",
    "        Args:\n",
    "            height: 网格高度\n",
    "            width: 网格宽度\n",
    "        \"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # 特殊位置定义\n",
    "        self.start = (height - 1, 0)           # 起点：左下角\n",
    "        self.goal = (height - 1, width - 1)    # 终点：右下角\n",
    "        self.cliff = [(height - 1, j) for j in range(1, width - 1)]  # 悬崖：底部中间\n",
    "        \n",
    "        # 当前状态\n",
    "        self.state = self.start\n",
    "        self.n_actions = 4\n",
    "        \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        \"\"\"重置环境到初始状态\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:\n",
    "        \"\"\"\n",
    "        执行动作\n",
    "        \n",
    "        Args:\n",
    "            action: 动作索引 (0-3)\n",
    "            \n",
    "        Returns:\n",
    "            (next_state, reward, done) 三元组\n",
    "        \"\"\"\n",
    "        # 计算下一位置（边界裁剪）\n",
    "        di, dj = self.ACTIONS[action]\n",
    "        new_i = np.clip(self.state[0] + di, 0, self.height - 1)\n",
    "        new_j = np.clip(self.state[1] + dj, 0, self.width - 1)\n",
    "        next_state = (int(new_i), int(new_j))\n",
    "        \n",
    "        # 检查是否掉入悬崖\n",
    "        if next_state in self.cliff:\n",
    "            self.state = self.start  # 重置到起点\n",
    "            return self.state, -100.0, False\n",
    "        \n",
    "        self.state = next_state\n",
    "        \n",
    "        # 检查是否到达目标\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 0.0, True\n",
    "        \n",
    "        return self.state, -1.0, False\n",
    "    \n",
    "    def render(self, path: Optional[List[Tuple[int, int]]] = None) -> None:\n",
    "        \"\"\"可视化环境\"\"\"\n",
    "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        \n",
    "        # 标记悬崖\n",
    "        for pos in self.cliff:\n",
    "            grid[pos[0]][pos[1]] = 'C'\n",
    "        \n",
    "        # 标记起点和终点\n",
    "        grid[self.start[0]][self.start[1]] = 'S'\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'\n",
    "        \n",
    "        # 标记路径\n",
    "        if path:\n",
    "            for pos in path[1:-1]:\n",
    "                if pos not in self.cliff and pos != self.start and pos != self.goal:\n",
    "                    grid[pos[0]][pos[1]] = '*'\n",
    "        \n",
    "        # 打印网格\n",
    "        print(\"┌\" + \"─\" * (self.width * 2 + 1) + \"┐\")\n",
    "        for row in grid:\n",
    "            print(\"│ \" + \" \".join(row) + \" │\")\n",
    "        print(\"└\" + \"─\" * (self.width * 2 + 1) + \"┘\")\n",
    "\n",
    "\n",
    "# 测试环境\n",
    "env = CliffWalkingEnv()\n",
    "print(\"悬崖行走环境:\")\n",
    "env.render()\n",
    "print(f\"\\n起点: {env.start}\")\n",
    "print(f\"终点: {env.goal}\")\n",
    "print(f\"悬崖位置: {env.cliff[:3]}...{env.cliff[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤3: 实现 Q-Learning 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    表格型 Q-Learning 智能体\n",
    "    \n",
    "    实现标准的 Q-Learning 算法，使用 ε-greedy 策略进行探索。\n",
    "    \n",
    "    Attributes:\n",
    "        q_table: Q 值表，字典形式 {state: [Q(s,a0), Q(s,a1), ...]}\n",
    "        lr: 学习率 α\n",
    "        gamma: 折扣因子 γ\n",
    "        epsilon: 探索率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化 Q-Learning 智能体\n",
    "        \n",
    "        Args:\n",
    "            n_actions: 动作空间大小\n",
    "            learning_rate: 学习率，控制 Q 值更新步长\n",
    "            discount_factor: 折扣因子，权衡即时与未来奖励\n",
    "            epsilon: 初始探索率\n",
    "            epsilon_decay: 探索率衰减系数\n",
    "            epsilon_min: 最小探索率\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Q 表：使用 defaultdict 自动初始化未访问状态\n",
    "        self.q_table: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        使用 ε-greedy 策略选择动作\n",
    "        \n",
    "        Args:\n",
    "            state: 当前状态\n",
    "            training: 是否处于训练模式\n",
    "            \n",
    "        Returns:\n",
    "            选择的动作索引\n",
    "        \"\"\"\n",
    "        # 训练时以 ε 概率随机探索\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        # 利用：选择 Q 值最大的动作（打破平局时随机选择）\n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Q-Learning 更新规则\n",
    "        \n",
    "        Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n",
    "        \n",
    "        Args:\n",
    "            state: 当前状态\n",
    "            action: 执行的动作\n",
    "            reward: 获得的奖励\n",
    "            next_state: 下一状态\n",
    "            done: 是否终止\n",
    "            \n",
    "        Returns:\n",
    "            TD 误差\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        # 计算 TD 目标\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            # Q-Learning 核心：使用 max 选择下一状态的最优动作\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # TD 误差\n",
    "        td_error = target - current_q\n",
    "        \n",
    "        # 更新 Q 值\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        \"\"\"衰减探索率\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# 创建智能体实例\n",
    "agent = QLearningAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=0.5,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=0.1,\n",
    "    epsilon_decay=1.0,  # 保持固定探索率\n",
    "    epsilon_min=0.1\n",
    ")\n",
    "\n",
    "print(\"Q-Learning 智能体配置:\")\n",
    "print(f\"  动作数量: {agent.n_actions}\")\n",
    "print(f\"  学习率 α: {agent.lr}\")\n",
    "print(f\"  折扣因子 γ: {agent.gamma}\")\n",
    "print(f\"  探索率 ε: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤4: 训练循环实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(\n",
    "    env: CliffWalkingEnv,\n",
    "    agent: QLearningAgent,\n",
    "    episodes: int = 500,\n",
    "    max_steps: int = 200,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    训练 Q-Learning 智能体\n",
    "    \n",
    "    Args:\n",
    "        env: 环境实例\n",
    "        agent: Q-Learning 智能体\n",
    "        episodes: 训练回合数\n",
    "        max_steps: 每回合最大步数\n",
    "        verbose: 是否打印训练进度\n",
    "        \n",
    "    Returns:\n",
    "        训练历史记录\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'rewards': [],\n",
    "        'steps': [],\n",
    "        'epsilon': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # 选择动作\n",
    "            action = agent.get_action(state, training=True)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # 更新 Q 值\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 衰减探索率\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # 记录历史\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['steps'].append(steps)\n",
    "        history['epsilon'].append(agent.epsilon)\n",
    "        \n",
    "        # 打印进度\n",
    "        if verbose and (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(history['rewards'][-100:])\n",
    "            avg_steps = np.mean(history['steps'][-100:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
    "                  f\"Avg Steps: {avg_steps:6.1f} | \"\n",
    "                  f\"ε: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# 训练智能体\n",
    "print(\"开始训练 Q-Learning...\\n\")\n",
    "env = CliffWalkingEnv()\n",
    "agent = QLearningAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=0.5,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=0.1,\n",
    "    epsilon_decay=1.0,\n",
    "    epsilon_min=0.1\n",
    ")\n",
    "\n",
    "history = train_q_learning(env, agent, episodes=500)\n",
    "\n",
    "print(f\"\\n训练完成！最后100回合平均奖励: {np.mean(history['rewards'][-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤5: 可视化学习过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: Dict[str, List[float]], window: int = 10) -> None:\n",
    "    \"\"\"绘制训练历史曲线\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 奖励曲线\n",
    "    rewards = history['rewards']\n",
    "    smoothed_rewards = np.convolve(\n",
    "        rewards, np.ones(window)/window, mode='valid'\n",
    "    )\n",
    "    \n",
    "    axes[0].plot(rewards, alpha=0.3, color='blue', label='原始奖励')\n",
    "    axes[0].plot(range(window-1, len(rewards)), smoothed_rewards, \n",
    "                 color='blue', linewidth=2, label=f'{window}回合移动平均')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title('Q-Learning 学习曲线')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 步数曲线\n",
    "    steps = history['steps']\n",
    "    smoothed_steps = np.convolve(\n",
    "        steps, np.ones(window)/window, mode='valid'\n",
    "    )\n",
    "    \n",
    "    axes[1].plot(steps, alpha=0.3, color='green', label='原始步数')\n",
    "    axes[1].plot(range(window-1, len(steps)), smoothed_steps,\n",
    "                 color='green', linewidth=2, label=f'{window}回合移动平均')\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('每回合步数')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤6: 提取并可视化学到的策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(agent: QLearningAgent, env: CliffWalkingEnv, max_steps: int = 50) -> List[Tuple[int, int]]:\n",
    "    \"\"\"从训练好的智能体提取贪心策略路径\"\"\"\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_action(state, training=False)  # 不探索\n",
    "        next_state, _, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "def visualize_policy(agent: QLearningAgent, env: CliffWalkingEnv) -> None:\n",
    "    \"\"\"可视化学到的策略\"\"\"\n",
    "    arrow_map = {0: '↑', 1: '→', 2: '↓', 3: '←'}\n",
    "    \n",
    "    print(\"学到的策略 (贪心):\")\n",
    "    print(\"┌\" + \"───\" * env.width + \"┐\")\n",
    "    \n",
    "    for i in range(env.height):\n",
    "        row = \"│\"\n",
    "        for j in range(env.width):\n",
    "            state = (i, j)\n",
    "            if state == env.start:\n",
    "                row += \" S \"\n",
    "            elif state == env.goal:\n",
    "                row += \" G \"\n",
    "            elif state in env.cliff:\n",
    "                row += \" C \"\n",
    "            elif state in agent.q_table:\n",
    "                best_action = np.argmax(agent.q_table[state])\n",
    "                row += f\" {arrow_map[best_action]} \"\n",
    "            else:\n",
    "                row += \" . \"\n",
    "        print(row + \"│\")\n",
    "    \n",
    "    print(\"└\" + \"───\" * env.width + \"┘\")\n",
    "\n",
    "\n",
    "# 可视化策略\n",
    "visualize_policy(agent, env)\n",
    "\n",
    "# 提取并显示路径\n",
    "print(\"\\n学到的路径:\")\n",
    "path = extract_path(agent, env)\n",
    "env.reset()\n",
    "env.render(path)\n",
    "print(f\"路径长度: {len(path) - 1} 步\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤7: Q 值表可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(agent: QLearningAgent, env: CliffWalkingEnv) -> None:\n",
    "    \"\"\"可视化 Q 表和价值函数\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 准备价值函数数据\n",
    "    v_table = np.zeros((env.height, env.width))\n",
    "    for i in range(env.height):\n",
    "        for j in range(env.width):\n",
    "            state = (i, j)\n",
    "            if state in agent.q_table:\n",
    "                v_table[i, j] = np.max(agent.q_table[state])\n",
    "            else:\n",
    "                v_table[i, j] = 0\n",
    "    \n",
    "    # 价值函数热力图\n",
    "    im = axes[0].imshow(v_table, cmap='RdYlGn', aspect='auto')\n",
    "    axes[0].set_title('状态价值函数 V(s) = max_a Q(s,a)')\n",
    "    axes[0].set_xlabel('列')\n",
    "    axes[0].set_ylabel('行')\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # 标记悬崖位置\n",
    "    for pos in env.cliff:\n",
    "        axes[0].add_patch(plt.Rectangle(\n",
    "            (pos[1]-0.5, pos[0]-0.5), 1, 1,\n",
    "            fill=True, color='black', alpha=0.5\n",
    "        ))\n",
    "    \n",
    "    # Q 值分布直方图\n",
    "    q_values = []\n",
    "    for state, q_array in agent.q_table.items():\n",
    "        if state not in env.cliff:\n",
    "            q_values.extend(q_array.tolist())\n",
    "    \n",
    "    axes[1].hist(q_values, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[1].set_xlabel('Q Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Q 值分布')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_q_table(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第5部分：探索策略\n",
    "\n",
    "### 5.1 探索与利用困境\n",
    "\n",
    "强化学习面临的核心挑战之一：\n",
    "\n",
    "- **利用 (Exploitation)**：选择当前已知最优的动作，最大化即时收益\n",
    "- **探索 (Exploration)**：尝试新动作，可能发现更好的策略\n",
    "\n",
    "**过度利用**：可能陷入局部最优，错过全局最优解\n",
    "**过度探索**：无法收敛，浪费计算资源\n",
    "\n",
    "### 5.2 ε-Greedy 策略\n",
    "\n",
    "最简单的探索策略：\n",
    "- 以概率 $\\epsilon$ 随机选择动作（探索）\n",
    "- 以概率 $1-\\epsilon$ 选择最优动作（利用）\n",
    "\n",
    "### 5.3 其他探索策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 不同探索策略实现\n",
    "# ============================================================\n",
    "\n",
    "def epsilon_greedy(q_values: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    ε-Greedy 策略\n",
    "    \n",
    "    以概率 ε 随机选择，否则选择最优动作\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(q_values))\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "\n",
    "def softmax_action(q_values: np.ndarray, temperature: float) -> int:\n",
    "    \"\"\"\n",
    "    Softmax (Boltzmann) 策略\n",
    "    \n",
    "    根据 Q 值的 softmax 分布选择动作:\n",
    "        P(a|s) = exp(Q(s,a)/τ) / Σ exp(Q(s,a')/τ)\n",
    "    \n",
    "    温度 τ 控制探索程度:\n",
    "        - τ→0: 趋向贪心选择\n",
    "        - τ→∞: 趋向均匀随机\n",
    "    \"\"\"\n",
    "    # 数值稳定性处理\n",
    "    q_scaled = (q_values - np.max(q_values)) / max(temperature, 1e-8)\n",
    "    exp_q = np.exp(q_scaled)\n",
    "    probs = exp_q / np.sum(exp_q)\n",
    "    return np.random.choice(len(q_values), p=probs)\n",
    "\n",
    "\n",
    "def ucb_action(q_values: np.ndarray, action_counts: np.ndarray, \n",
    "               total_count: int, c: float = 2.0) -> int:\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound (UCB) 策略\n",
    "    \n",
    "    选择置信上界最大的动作:\n",
    "        A = argmax[Q(s,a) + c * sqrt(ln(t) / N(s,a))]\n",
    "    \n",
    "    平衡价值估计和不确定性\n",
    "    \"\"\"\n",
    "    # 优先选择未访问的动作\n",
    "    if np.any(action_counts == 0):\n",
    "        return np.random.choice(np.where(action_counts == 0)[0])\n",
    "    \n",
    "    ucb_values = q_values + c * np.sqrt(np.log(total_count + 1) / (action_counts + 1e-8))\n",
    "    return np.argmax(ucb_values)\n",
    "\n",
    "\n",
    "# 演示不同策略的行为\n",
    "print(\"不同探索策略的动作选择演示\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "q_values = np.array([1.0, 2.0, 1.5, 0.5])  # 假设的 Q 值\n",
    "action_counts = np.array([10, 5, 8, 2])    # 动作选择计数\n",
    "\n",
    "print(f\"Q 值: {q_values}\")\n",
    "print(f\"动作计数: {action_counts}\")\n",
    "print()\n",
    "\n",
    "# 多次采样观察分布\n",
    "n_samples = 1000\n",
    "\n",
    "# ε-Greedy\n",
    "eg_actions = [epsilon_greedy(q_values, 0.1) for _ in range(n_samples)]\n",
    "eg_dist = [eg_actions.count(i)/n_samples for i in range(4)]\n",
    "print(f\"ε-Greedy (ε=0.1) 动作分布: {[f'{d:.2f}' for d in eg_dist]}\")\n",
    "\n",
    "# Softmax\n",
    "sm_actions = [softmax_action(q_values, 0.5) for _ in range(n_samples)]\n",
    "sm_dist = [sm_actions.count(i)/n_samples for i in range(4)]\n",
    "print(f\"Softmax (τ=0.5) 动作分布: {[f'{d:.2f}' for d in sm_dist]}\")\n",
    "\n",
    "# UCB\n",
    "ucb_actions = [ucb_action(q_values, action_counts, 100, c=2.0) for _ in range(n_samples)]\n",
    "ucb_dist = [ucb_actions.count(i)/n_samples for i in range(4)]\n",
    "print(f\"UCB (c=2.0) 动作分布: {[f'{d:.2f}' for d in ucb_dist]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **Q-Learning** 是一种无模型、离策略的时序差分控制算法\n",
    "2. **更新公式**：$Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\max_a Q(S',a) - Q(S,A)]$\n",
    "3. **离策略特性**：学习最优策略，与实际行为策略无关\n",
    "4. **探索与利用**：使用 ε-greedy 等策略平衡\n",
    "\n",
    "### 超参数选择建议\n",
    "\n",
    "| 参数 | 典型值 | 说明 |\n",
    "|------|--------|------|\n",
    "| $\\alpha$ (学习率) | 0.1 ~ 0.5 | 较大值加速学习，但可能不稳定 |\n",
    "| $\\gamma$ (折扣因子) | 0.99 | 接近 1 重视长期奖励 |\n",
    "| $\\epsilon$ (探索率) | 1.0 → 0.01 | 从探索逐渐转向利用 |\n",
    "| 衰减率 | 0.99 ~ 0.999 | 控制 ε 下降速度 |\n",
    "\n",
    "### 局限性\n",
    "\n",
    "- 状态空间必须离散且有限\n",
    "- 无法处理连续状态（如图像输入）\n",
    "- 无法泛化到未见过的状态\n",
    "\n",
    "**解决方案**：深度 Q 网络 (DQN) — 用神经网络近似 Q 函数\n",
    "\n",
    "### 下一步学习\n",
    "\n",
    "- SARSA：在策略 TD 控制\n",
    "- Double Q-Learning：减少过估计\n",
    "- DQN：深度强化学习入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 单元测试\n",
    "\n",
    "运行以下测试验证实现的正确性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 单元测试\n",
    "# ============================================================\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"运行所有单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # 测试1: 环境基本功能\n",
    "    try:\n",
    "        env = CliffWalkingEnv()\n",
    "        state = env.reset()\n",
    "        assert state == (3, 0), f\"起始状态错误: {state}\"\n",
    "        \n",
    "        # 向上移动\n",
    "        next_state, reward, done = env.step(0)\n",
    "        assert next_state == (2, 0), f\"向上移动后状态错误: {next_state}\"\n",
    "        assert reward == -1.0, f\"奖励错误: {reward}\"\n",
    "        assert not done, \"不应该结束\"\n",
    "        \n",
    "        print(\"测试1通过: 环境基本功能正常\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: 悬崖惩罚\n",
    "    try:\n",
    "        env = CliffWalkingEnv()\n",
    "        env.reset()\n",
    "        # 向右走入悬崖\n",
    "        next_state, reward, done = env.step(1)\n",
    "        assert reward == -100.0, f\"悬崖惩罚错误: {reward}\"\n",
    "        assert next_state == env.start, f\"掉入悬崖后应重置到起点\"\n",
    "        print(\"测试2通过: 悬崖惩罚正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: Q-Learning 更新\n",
    "    try:\n",
    "        agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.9)\n",
    "        state = (0, 0)\n",
    "        next_state = (0, 1)\n",
    "        \n",
    "        # 初始 Q 值应为 0\n",
    "        assert agent.q_table[state][0] == 0.0, \"初始 Q 值应为 0\"\n",
    "        \n",
    "        # 执行更新\n",
    "        td_error = agent.update(state, 0, -1.0, next_state, False)\n",
    "        \n",
    "        # 验证更新后的 Q 值\n",
    "        # Q(s,a) = 0 + 0.5 * (-1 + 0.9 * 0 - 0) = -0.5\n",
    "        expected_q = -0.5\n",
    "        assert np.isclose(agent.q_table[state][0], expected_q), \\\n",
    "            f\"Q值更新错误: {agent.q_table[state][0]} != {expected_q}\"\n",
    "        \n",
    "        print(\"测试3通过: Q-Learning 更新正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试4: ε-greedy 策略\n",
    "    try:\n",
    "        agent = QLearningAgent(n_actions=4, epsilon=0.0)  # 纯贪心\n",
    "        state = (0, 0)\n",
    "        agent.q_table[state] = np.array([1.0, 2.0, 0.5, 0.5])\n",
    "        \n",
    "        # 应该总是选择动作 1（最大 Q 值）\n",
    "        for _ in range(10):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            assert action == 1, f\"贪心策略应选择动作1，实际选择: {action}\"\n",
    "        \n",
    "        print(\"测试4通过: ε-greedy 策略正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试5: 训练收敛性\n",
    "    try:\n",
    "        env = CliffWalkingEnv()\n",
    "        agent = QLearningAgent(\n",
    "            n_actions=4,\n",
    "            learning_rate=0.5,\n",
    "            epsilon=0.1,\n",
    "            epsilon_decay=1.0,\n",
    "            epsilon_min=0.1\n",
    "        )\n",
    "        \n",
    "        history = train_q_learning(env, agent, episodes=200, verbose=False)\n",
    "        \n",
    "        # 最后50回合平均奖励应该大于 -50\n",
    "        avg_reward = np.mean(history['rewards'][-50:])\n",
    "        assert avg_reward > -100, f\"训练未收敛: avg_reward = {avg_reward}\"\n",
    "        \n",
    "        print(f\"测试5通过: 训练收敛 (最后50回合平均奖励: {avg_reward:.2f})\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试5失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 总结\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"测试完成: {passed} 通过, {failed} 失败\")\n",
    "    if failed == 0:\n",
    "        print(\"所有测试通过！\")\n",
    "    else:\n",
    "        print(\"存在失败的测试，请检查代码。\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return failed == 0\n",
    "\n",
    "\n",
    "# 运行测试\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD Thesis.\n",
    "2. Sutton, R.S. & Barto, A.G. (2018). Reinforcement Learning: An Introduction, 2nd ed. Chapter 6.\n",
    "3. [OpenAI Spinning Up - Q-Learning](https://spinningup.openai.com/)\n",
    "\n",
    "---\n",
    "\n",
    "[返回目录](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
