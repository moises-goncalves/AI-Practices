"""
åˆ†æå·¥å…·æ¨¡å— (Analysis Module)
=============================

æ ¸å¿ƒæ€æƒ³ (Core Idea):
--------------------
æä¾›ç®—æ³•åˆ†æå·¥å…·ï¼ŒåŒ…æ‹¬æ”¶æ•›æ£€æµ‹ã€è¯¯å·®è®¡ç®—ã€ç­–ç•¥æå–ç­‰åŠŸèƒ½ã€‚
å¸®åŠ©è¯„ä¼°å’Œç†è§£TDå­¦ä¹ ç®—æ³•çš„æ€§èƒ½ã€‚

æ•°å­¦åŸç† (Mathematical Theory):
------------------------------
è¯¯å·®åº¦é‡:
- RMSE = âˆš((1/n) Ã— Î£(VÌ‚(s) - V(s))Â²)
- MAE = (1/n) Ã— Î£|VÌ‚(s) - V(s)|

æ”¶æ•›åˆ¤å®š:
- åŸºäºä»·å€¼å˜åŒ–: max_s |V_{t+1}(s) - V_t(s)| < Îµ
- åŸºäºæ€§èƒ½ç¨³å®š: |E[R]_{recent} - E[R]_{previous}| / |E[R]_{previous}| < Îµ
"""

from __future__ import annotations

import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict


# ç±»å‹åˆ«å
QFunction = Dict[Tuple[Any, Any], float]
ValueFunction = Dict[Any, float]


def compute_rmse(
    estimated: Union[ValueFunction, QFunction],
    true_values: Union[ValueFunction, QFunction]
) -> float:
    """
    è®¡ç®—ä¼°è®¡å€¼ä¸çœŸå®å€¼çš„å‡æ–¹æ ¹è¯¯å·®ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    RMSEæ˜¯è¯„ä¼°ä»·å€¼å‡½æ•°ä¼°è®¡ç²¾åº¦çš„æ ‡å‡†æŒ‡æ ‡ï¼Œ
    è¡¡é‡ä¼°è®¡å€¼ä¸çœŸå®å€¼çš„å¹³å‡åå·®ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    å‡æ–¹æ ¹è¯¯å·® (Root Mean Square Error):
        RMSE = âˆš((1/n) Ã— Î£_{sâˆˆS}(VÌ‚(s) - V(s))Â²)

    ç‰¹æ€§:
    - å¯¹å¤§è¯¯å·®æ•æ„Ÿï¼ˆå¹³æ–¹æ”¾å¤§ï¼‰
    - ä¸åŸå€¼åŒé‡çº²
    - æ€»æ˜¯éè´Ÿ

    Args:
        estimated: ä¼°è®¡çš„ä»·å€¼å‡½æ•°
        true_values: çœŸå®ä»·å€¼å‡½æ•°

    Returns:
        å‡æ–¹æ ¹è¯¯å·®

    Example:
        >>> estimated = {0: 0.5, 1: 0.8, 2: 0.9}
        >>> true_values = {0: 0.5, 1: 0.7, 2: 1.0}
        >>> rmse = compute_rmse(estimated, true_values)
        >>> print(f"RMSE = {rmse:.4f}")
    """
    common_keys = set(estimated.keys()) & set(true_values.keys())

    if len(common_keys) == 0:
        return float('inf')

    squared_errors = [
        (estimated[k] - true_values[k]) ** 2
        for k in common_keys
    ]

    return float(np.sqrt(np.mean(squared_errors)))


def compute_mae(
    estimated: Union[ValueFunction, QFunction],
    true_values: Union[ValueFunction, QFunction]
) -> float:
    """
    è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·®ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    å¹³å‡ç»å¯¹è¯¯å·® (Mean Absolute Error):
        MAE = (1/n) Ã— Î£_{sâˆˆS}|VÌ‚(s) - V(s)|

    Args:
        estimated: ä¼°è®¡çš„ä»·å€¼å‡½æ•°
        true_values: çœŸå®ä»·å€¼å‡½æ•°

    Returns:
        å¹³å‡ç»å¯¹è¯¯å·®
    """
    common_keys = set(estimated.keys()) & set(true_values.keys())

    if len(common_keys) == 0:
        return float('inf')

    absolute_errors = [
        abs(estimated[k] - true_values[k])
        for k in common_keys
    ]

    return float(np.mean(absolute_errors))


def extract_greedy_policy(
    q_function: QFunction,
    n_states: int,
    n_actions: int
) -> Dict[int, int]:
    """
    ä»Qå‡½æ•°æå–è´ªå©ªç­–ç•¥ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    è´ªå©ªç­–ç•¥é€‰æ‹©æ¯ä¸ªçŠ¶æ€ä¸‹Qå€¼æœ€é«˜çš„åŠ¨ä½œã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    è´ªå©ªç­–ç•¥:
        Ï€(s) = argmax_a Q(s, a)

    è¿™æ˜¯ç¡®å®šæ€§ç­–ç•¥ï¼Œä¸åŒ…å«æ¢ç´¢ã€‚

    Args:
        q_function: Qå‡½æ•° {(state, action): value}
        n_states: çŠ¶æ€æ•°é‡
        n_actions: åŠ¨ä½œæ•°é‡

    Returns:
        ç­–ç•¥å­—å…¸ {state: action}

    Example:
        >>> q_function = {(0, 0): 1.0, (0, 1): 2.0, (1, 0): 3.0, (1, 1): 1.5}
        >>> policy = extract_greedy_policy(q_function, n_states=2, n_actions=2)
        >>> print(policy)  # {0: 1, 1: 0}
    """
    policy = {}

    for state in range(n_states):
        q_values = []
        for action in range(n_actions):
            key = (state, action)
            if key in q_function:
                q_values.append((action, q_function[key]))

        if q_values:
            best_action = max(q_values, key=lambda x: x[1])[0]
            policy[state] = best_action

    return policy


def compute_state_visitation(
    episode_trajectories: List[List[int]]
) -> Dict[int, int]:
    """
    è®¡ç®—çŠ¶æ€è®¿é—®é¢‘ç‡ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    ç»Ÿè®¡å„çŠ¶æ€è¢«è®¿é—®çš„æ¬¡æ•°ï¼Œå¸®åŠ©ç†è§£æ¢ç´¢çš„å‡åŒ€æ€§ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    çŠ¶æ€è®¿é—®é¢‘ç‡:
        N(s) = Î£_{Ï„âˆˆTrajectories} Î£_{t} ğŸ™[S_t = s]

    å‡åŒ€æ¢ç´¢åº”ä½¿å„çŠ¶æ€è®¿é—®é¢‘ç‡æ¥è¿‘ã€‚

    Args:
        episode_trajectories: å›åˆè½¨è¿¹åˆ—è¡¨ï¼Œæ¯ä¸ªè½¨è¿¹æ˜¯çŠ¶æ€åºåˆ—

    Returns:
        çŠ¶æ€è®¿é—®æ¬¡æ•°å­—å…¸ {state: count}
    """
    visitation = defaultdict(int)

    for trajectory in episode_trajectories:
        for state in trajectory:
            visitation[state] += 1

    return dict(visitation)


def detect_convergence(
    rewards: List[float],
    window: int = 100,
    threshold: float = 0.01,
    min_episodes: int = 200
) -> Tuple[bool, int]:
    """
    æ£€æµ‹è®­ç»ƒæ˜¯å¦æ”¶æ•›ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    é€šè¿‡æ£€æŸ¥æœ€è¿‘çª—å£å†…å¥–åŠ±çš„å˜åŒ–ç‡æ¥åˆ¤æ–­æ”¶æ•›ã€‚
    å½“å˜åŒ–ç‡ä½äºé˜ˆå€¼æ—¶è®¤ä¸ºå·²æ”¶æ•›ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    æ”¶æ•›åˆ¤å®šæ ‡å‡†:
        |E[R]_{recent} - E[R]_{previous}| / |E[R]_{previous}| < threshold

    è¿™æµ‹é‡ç›¸å¯¹å˜åŒ–ï¼Œå¯¹ä¸åŒé‡çº§çš„å¥–åŠ±éƒ½é€‚ç”¨ã€‚

    Args:
        rewards: å¥–åŠ±åºåˆ—
        window: æ£€æµ‹çª—å£å¤§å°
        threshold: å˜åŒ–ç‡é˜ˆå€¼
        min_episodes: æœ€å°‘éœ€è¦çš„å›åˆæ•°

    Returns:
        (æ˜¯å¦æ”¶æ•›, æ”¶æ•›å›åˆæ•°)

    Example:
        >>> rewards = list(np.random.randn(200) * 10) + [50] * 300
        >>> converged, episode = detect_convergence(rewards)
        >>> print(f"æ”¶æ•›: {converged}, å›åˆ: {episode}")
    """
    if len(rewards) < min_episodes:
        return False, -1

    for i in range(window, len(rewards) - window):
        recent = rewards[i:i+window]
        previous = rewards[i-window:i]

        mean_recent = np.mean(recent)
        mean_previous = np.mean(previous)

        # è®¡ç®—ç›¸å¯¹å˜åŒ–
        if abs(mean_previous) > 1e-8:
            change_rate = abs(mean_recent - mean_previous) / abs(mean_previous)
        else:
            change_rate = abs(mean_recent - mean_previous)

        if change_rate < threshold:
            return True, i

    return False, -1


def evaluate_policy(
    policy: Dict[int, int],
    env: Any,
    n_episodes: int = 100,
    max_steps: int = 1000,
    seed: Optional[int] = None
) -> Tuple[float, float, float]:
    """
    è¯„ä¼°ç¡®å®šæ€§ç­–ç•¥çš„æ€§èƒ½ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    é€šè¿‡å¤šæ¬¡æ¨¡æ‹Ÿæ¥ä¼°è®¡ç­–ç•¥çš„æœŸæœ›å›æŠ¥å’Œæ–¹å·®ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    ç­–ç•¥è¯„ä¼°ä¼°è®¡:
        VÌ‚(Ï€) = (1/N) Ã— Î£_{i=1}^N G_i

    å…¶ä¸­ G_i æ˜¯ç¬¬iä¸ªå›åˆçš„ç´¯ç§¯å›æŠ¥ã€‚

    Args:
        policy: ç­–ç•¥å­—å…¸ {state: action}
        env: ç¯å¢ƒå®ä¾‹
        n_episodes: è¯„ä¼°å›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
        seed: éšæœºç§å­

    Returns:
        (å¹³å‡å›æŠ¥, å›æŠ¥æ ‡å‡†å·®, å¹³å‡æ­¥æ•°)

    Example:
        >>> policy = extract_greedy_policy(q_function, n_states, n_actions)
        >>> mean_return, std_return, mean_steps = evaluate_policy(policy, env)
        >>> print(f"å¹³å‡å›æŠ¥: {mean_return:.2f} Â± {std_return:.2f}")
    """
    if seed is not None:
        np.random.seed(seed)

    episode_returns = []
    episode_lengths = []

    for _ in range(n_episodes):
        state, _ = env.reset()
        total_return = 0.0
        steps = 0

        for _ in range(max_steps):
            action = policy.get(state, 0)  # é»˜è®¤åŠ¨ä½œ0
            next_state, reward, terminated, truncated, _ = env.step(action)

            total_return += reward
            steps += 1
            state = next_state

            if terminated or truncated:
                break

        episode_returns.append(total_return)
        episode_lengths.append(steps)

    return (
        float(np.mean(episode_returns)),
        float(np.std(episode_returns)),
        float(np.mean(episode_lengths))
    )


def compute_action_value_from_state_value(
    state_value: ValueFunction,
    env: Any,
    gamma: float = 0.99
) -> QFunction:
    """
    ä»çŠ¶æ€ä»·å€¼å‡½æ•°è®¡ç®—åŠ¨ä½œä»·å€¼å‡½æ•°ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    åŠ¨ä½œä»·å€¼ä¸çŠ¶æ€ä»·å€¼çš„å…³ç³»:
        Q(s,a) = Î£_{s'} P(s'|s,a) Ã— [R(s,a,s') + Î³V(s')]

    å¯¹äºç¡®å®šæ€§ç¯å¢ƒç®€åŒ–ä¸º:
        Q(s,a) = R(s,a) + Î³V(s')

    Args:
        state_value: çŠ¶æ€ä»·å€¼å‡½æ•°
        env: ç¯å¢ƒå®ä¾‹ï¼ˆéœ€è¦æä¾›è½¬ç§»ä¿¡æ¯ï¼‰
        gamma: æŠ˜æ‰£å› å­

    Returns:
        åŠ¨ä½œä»·å€¼å‡½æ•°
    """
    q_function = {}

    # è·å–ç¯å¢ƒå‚æ•°
    n_states = env.observation_space.n
    n_actions = env.action_space.n

    for state in range(n_states):
        for action in range(n_actions):
            # æ¨¡æ‹Ÿè½¬ç§»
            env._state = env._state_to_pos(state) if hasattr(env, '_state_to_pos') else state
            original_state = env._state

            next_state, reward, _, _, _ = env.step(action)

            # è®¡ç®—Qå€¼
            next_v = state_value.get(next_state, 0.0)
            q_function[(state, action)] = reward + gamma * next_v

            # æ¢å¤çŠ¶æ€
            env._state = original_state

    return q_function


def analyze_exploration_coverage(
    q_function: QFunction,
    n_states: int,
    n_actions: int
) -> Dict[str, Any]:
    """
    åˆ†ææ¢ç´¢è¦†ç›–ç‡ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    æ£€æŸ¥Qå‡½æ•°è¦†ç›–äº†å¤šå°‘çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œ
    è¯„ä¼°æ¢ç´¢çš„å……åˆ†æ€§ã€‚

    Args:
        q_function: Qå‡½æ•°
        n_states: çŠ¶æ€æ•°é‡
        n_actions: åŠ¨ä½œæ•°é‡

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    total_pairs = n_states * n_actions
    covered_pairs = len(q_function)

    # ç»Ÿè®¡æ¯ä¸ªçŠ¶æ€è¦†ç›–çš„åŠ¨ä½œæ•°
    state_coverage = defaultdict(int)
    for (state, action) in q_function.keys():
        state_coverage[state] += 1

    fully_covered_states = sum(1 for count in state_coverage.values()
                               if count == n_actions)
    partially_covered_states = len(state_coverage) - fully_covered_states
    uncovered_states = n_states - len(state_coverage)

    return {
        'total_pairs': total_pairs,
        'covered_pairs': covered_pairs,
        'coverage_ratio': covered_pairs / total_pairs if total_pairs > 0 else 0.0,
        'fully_covered_states': fully_covered_states,
        'partially_covered_states': partially_covered_states,
        'uncovered_states': uncovered_states,
        'state_coverage': dict(state_coverage),
    }
