{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序差分学习深度教程 (Temporal Difference Learning)\n",
    "\n",
    "---\n",
    "\n",
    "## 概述\n",
    "\n",
    "本教程深入讲解时序差分(TD)学习——强化学习中最核心的概念之一。TD方法结合了蒙特卡洛方法的采样思想和动态规划的自举(Bootstrapping)思想，是现代强化学习的基石。\n",
    "\n",
    "### 学习目标\n",
    "\n",
    "完成本教程后，你将能够：\n",
    "\n",
    "1. **理解TD学习的核心思想**：掌握\"用猜测更新猜测\"的自举原理\n",
    "2. **区分各种TD算法**：TD(0), SARSA, Q-Learning, Expected SARSA, Double Q-Learning\n",
    "3. **理解资格迹**：掌握TD(λ)如何统一TD(0)和Monte Carlo\n",
    "4. **实践应用**：在经典环境中实现和比较各种TD算法\n",
    "5. **分析算法特性**：理解on-policy vs off-policy、偏差-方差权衡\n",
    "\n",
    "### 目录\n",
    "\n",
    "1. [TD学习的直觉理解](#1.-TD学习的直觉理解)\n",
    "2. [数学基础](#2.-数学基础)\n",
    "3. [TD(0)算法详解](#3.-TD(0)算法详解)\n",
    "4. [SARSA与Q-Learning对比](#4.-SARSA与Q-Learning对比)\n",
    "5. [Expected SARSA与Double Q-Learning](#5.-Expected-SARSA与Double-Q-Learning)\n",
    "6. [N-Step TD与TD(λ)](#6.-N-Step-TD与TD(λ))\n",
    "7. [实验：Cliff Walking环境](#7.-实验：Cliff-Walking环境)\n",
    "8. [实验：Random Walk与价值预测](#8.-实验：Random-Walk与价值预测)\n",
    "9. [超参数分析](#9.-超参数分析)\n",
    "10. [总结与进阶方向](#10.-总结与进阶方向)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# 导入本地模块\n",
    "from td_algorithms import (\n",
    "    TDConfig, TrainingMetrics,\n",
    "    TD0ValueLearner, SARSA, ExpectedSARSA, QLearning,\n",
    "    DoubleQLearning, NStepTD, TDLambda, SARSALambda,\n",
    "    create_td_learner, EligibilityTraceType\n",
    ")\n",
    "from environments import (\n",
    "    GridWorld, GridWorldConfig, CliffWalkingEnv,\n",
    "    WindyGridWorld, RandomWalk, Action\n",
    ")\n",
    "from utils import (\n",
    "    plot_training_curves, plot_value_heatmap,\n",
    "    plot_policy_arrows, compute_rmse,\n",
    "    visualize_cliff_walking_path\n",
    ")\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"环境准备完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. TD学习的直觉理解\n",
    "\n",
    "### 1.1 三种学习范式的对比\n",
    "\n",
    "在强化学习中，有三种主要的价值函数学习范式：\n",
    "\n",
    "| 方法 | 更新时机 | 使用信息 | 特点 |\n",
    "|------|----------|----------|------|\n",
    "| **动态规划(DP)** | 每步 | 完整环境模型 | 需要知道转移概率 |\n",
    "| **蒙特卡洛(MC)** | 回合结束 | 完整回合回报 | 无偏但高方差 |\n",
    "| **时序差分(TD)** | 每步 | 单步奖励+下一状态估计 | 有偏但低方差 |\n",
    "\n",
    "### 1.2 核心洞察：自举(Bootstrapping)\n",
    "\n",
    "TD学习的核心是**自举**——用当前的估计来更新估计。这听起来像是循环论证，但实际上非常有效：\n",
    "\n",
    "想象你在学习下棋：\n",
    "- **MC方法**：打完整盘棋，根据输赢调整每一步的评估\n",
    "- **TD方法**：走完一步，根据新局面的评估调整上一步的评估\n",
    "\n",
    "TD的优势：\n",
    "1. 不需要等到游戏结束\n",
    "2. 每一步都能学习\n",
    "3. 学习更稳定（低方差）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：TD vs MC 更新机制\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Monte Carlo 更新\n",
    "ax1 = axes[0]\n",
    "states_mc = ['S₀', 'S₁', 'S₂', 'S₃', 'S₄', 'End']\n",
    "rewards_mc = [0, -1, -1, -1, 10]\n",
    "ax1.scatter(range(6), [0]*6, s=200, c='blue', zorder=5)\n",
    "for i, (s, r) in enumerate(zip(states_mc, rewards_mc + [0])):\n",
    "    ax1.annotate(s, (i, 0.1), ha='center', fontsize=12)\n",
    "    if i < 5:\n",
    "        ax1.annotate(f'r={rewards_mc[i]}', (i+0.5, -0.1), ha='center', fontsize=10, color='gray')\n",
    "        ax1.arrow(i+0.15, 0, 0.7, 0, head_width=0.03, head_length=0.05, fc='blue', ec='blue')\n",
    "\n",
    "# 显示MC的回报计算\n",
    "g_return = sum(rewards_mc)\n",
    "ax1.annotate(f'G = {g_return}', (2.5, -0.3), ha='center', fontsize=14, \n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "ax1.annotate('等待回合结束\\n然后用G更新所有V(s)', (2.5, 0.35), ha='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "ax1.set_xlim(-0.5, 5.5)\n",
    "ax1.set_ylim(-0.5, 0.6)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Monte Carlo: 用完整回报更新', fontsize=13, fontweight='bold')\n",
    "\n",
    "# TD 更新\n",
    "ax2 = axes[1]\n",
    "states_td = ['S₀', 'S₁']\n",
    "ax2.scatter([0, 2], [0, 0], s=200, c='green', zorder=5)\n",
    "ax2.annotate('S₀\\nV=5', (0, 0.15), ha='center', fontsize=12)\n",
    "ax2.annotate('S₁\\nV=8', (2, 0.15), ha='center', fontsize=12)\n",
    "ax2.annotate('r=-1', (1, -0.08), ha='center', fontsize=10, color='gray')\n",
    "ax2.arrow(0.2, 0, 1.5, 0, head_width=0.03, head_length=0.1, fc='green', ec='green')\n",
    "\n",
    "# TD目标和更新\n",
    "ax2.annotate('TD目标 = r + γV(S₁) = -1 + 0.9×8 = 6.2', (1, -0.25), ha='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "ax2.annotate('TD误差 δ = 6.2 - 5 = 1.2', (1, -0.4), ha='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "ax2.annotate('立即更新V(S₀)\\n无需等待回合结束', (1, 0.4), ha='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "ax2.set_xlim(-0.5, 2.5)\n",
    "ax2.set_ylim(-0.6, 0.7)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('TD: 用单步奖励+估计更新', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 数学基础\n",
    "\n",
    "### 2.1 价值函数回顾\n",
    "\n",
    "**状态价值函数** $V^\\pi(s)$：在状态 $s$ 下，遵循策略 $\\pi$ 的期望回报\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$$\n",
    "\n",
    "**动作价值函数** $Q^\\pi(s, a)$：在状态 $s$ 执行动作 $a$，然后遵循策略 $\\pi$ 的期望回报\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$$\n",
    "\n",
    "### 2.2 TD(0)更新规则\n",
    "\n",
    "**TD目标(TD Target)**：\n",
    "$$G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1})$$\n",
    "\n",
    "**TD误差(TD Error)**：\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
    "\n",
    "**更新规则**：\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t$$\n",
    "\n",
    "### 2.3 为什么TD有效？\n",
    "\n",
    "关键洞察：TD目标是真实回报的**无偏估计的有偏估计**：\n",
    "\n",
    "$$\\mathbb{E}[R_{t+1} + \\gamma V^\\pi(S_{t+1})] = V^\\pi(S_t) \\quad \\text{(期望正确)}$$\n",
    "\n",
    "但由于我们用 $V(S_{t+1})$ 代替 $V^\\pi(S_{t+1})$，引入了偏差。然而，这种偏差会随着 $V$ 收敛到 $V^\\pi$ 而消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数学公式可视化\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# 绘制TD更新的流程图\n",
    "boxes = [\n",
    "    (0.1, 0.7, '当前状态\\n$S_t$', 'lightblue'),\n",
    "    (0.4, 0.7, '即时奖励\\n$R_{t+1}$', 'lightgreen'),\n",
    "    (0.7, 0.7, '下一状态\\n$S_{t+1}$', 'lightblue'),\n",
    "    (0.7, 0.4, '估计价值\\n$V(S_{t+1})$', 'lightyellow'),\n",
    "    (0.4, 0.4, 'TD目标\\n$R_{t+1} + \\gamma V(S_{t+1})$', 'lightcoral'),\n",
    "    (0.1, 0.4, '当前估计\\n$V(S_t)$', 'lightyellow'),\n",
    "    (0.25, 0.15, 'TD误差\\n$\\delta_t = $TD目标$ - V(S_t)$', 'plum'),\n",
    "]\n",
    "\n",
    "for x, y, text, color in boxes:\n",
    "    ax.add_patch(plt.Rectangle((x-0.08, y-0.08), 0.16, 0.16, \n",
    "                               facecolor=color, edgecolor='black', linewidth=2))\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=10)\n",
    "\n",
    "# 箭头\n",
    "arrows = [\n",
    "    ((0.18, 0.7), (0.32, 0.7)),   # S_t -> R\n",
    "    ((0.48, 0.7), (0.62, 0.7)),   # R -> S_{t+1}\n",
    "    ((0.7, 0.62), (0.7, 0.48)),   # S_{t+1} -> V(S_{t+1})\n",
    "    ((0.62, 0.4), (0.48, 0.4)),   # V(S_{t+1}) -> TD目标\n",
    "    ((0.4, 0.62), (0.4, 0.48)),   # R -> TD目标\n",
    "    ((0.32, 0.4), (0.18, 0.4)),   # TD目标 -> V(S_t)比较\n",
    "    ((0.1, 0.32), (0.2, 0.23)),   # V(S_t) -> δ\n",
    "    ((0.4, 0.32), (0.3, 0.23)),   # TD目标 -> δ\n",
    "]\n",
    "\n",
    "for start, end in arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "\n",
    "# 最终更新公式\n",
    "ax.text(0.5, 0.02, r'更新: $V(S_t) \\leftarrow V(S_t) + \\alpha \\cdot \\delta_t$', \n",
    "        ha='center', fontsize=14, \n",
    "        bbox=dict(boxstyle='round', facecolor='gold', alpha=0.8))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 0.9)\n",
    "ax.axis('off')\n",
    "ax.set_title('TD(0)更新流程图', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. TD(0)算法详解\n",
    "\n",
    "### 3.1 TD(0)用于策略评估\n",
    "\n",
    "给定固定策略 $\\pi$，TD(0)估计该策略的状态价值函数 $V^\\pi$。\n",
    "\n",
    "**算法流程**：\n",
    "```\n",
    "初始化: V(s) 任意初始化，V(终止状态) = 0\n",
    "\n",
    "对于每个回合:\n",
    "    初始化 S\n",
    "    对于回合中的每一步:\n",
    "        A ← 按策略π选择动作\n",
    "        执行A，观察 R, S'\n",
    "        V(S) ← V(S) + α[R + γV(S') - V(S)]\n",
    "        S ← S'\n",
    "    直到 S 是终止状态\n",
    "```\n",
    "\n",
    "### 3.2 Random Walk实验\n",
    "\n",
    "Random Walk是验证TD预测正确性的经典环境：\n",
    "- 状态：A-B-C-D-E（5个非终止状态）\n",
    "- 两端是终止状态\n",
    "- 随机向左或向右移动\n",
    "- 到达右端奖励+1，左端奖励0\n",
    "- 真实价值有解析解，便于验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Walk 环境演示\n",
    "\n",
    "env = RandomWalk(n_states=5)\n",
    "\n",
    "# 获取真实价值\n",
    "true_values = env.get_true_values(gamma=1.0)\n",
    "print(\"Random Walk 真实价值函数 (γ=1):\")\n",
    "print(\"状态:  T   A    B    C    D    E   T\")\n",
    "print(f\"价值: {true_values}\")\n",
    "\n",
    "# 可视化\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# 绘制状态\n",
    "states = ['T(0)', 'A', 'B', 'C', 'D', 'E', 'T(1)']\n",
    "x_pos = np.arange(7)\n",
    "colors = ['gray'] + ['skyblue']*5 + ['gold']\n",
    "\n",
    "for i, (x, state, color) in enumerate(zip(x_pos, states, colors)):\n",
    "    circle = plt.Circle((x, 0.5), 0.3, color=color, ec='black', lw=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, 0.5, state, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(x, 0.05, f'V={true_values[i]:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "# 绘制转移箭头\n",
    "for i in range(1, 6):\n",
    "    # 向左\n",
    "    ax.annotate('', xy=(i-0.7, 0.65), xytext=(i-0.35, 0.65),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "    ax.text(i-0.5, 0.8, '50%', ha='center', fontsize=8, color='red')\n",
    "    # 向右\n",
    "    ax.annotate('', xy=(i+0.7, 0.35), xytext=(i+0.35, 0.35),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\n",
    "    ax.text(i+0.5, 0.2, '50%', ha='center', fontsize=8, color='blue')\n",
    "\n",
    "ax.set_xlim(-0.5, 6.5)\n",
    "ax.set_ylim(-0.2, 1.1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Random Walk 环境', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD(0) vs MC 在Random Walk上的比较\n",
    "\n",
    "def td0_prediction(env, n_episodes=100, alpha=0.1, gamma=1.0):\n",
    "    \"\"\"TD(0)策略评估\"\"\"\n",
    "    V = np.zeros(env.n_total_states)\n",
    "    rmse_history = []\n",
    "    true_v = env.get_true_values(gamma)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, _, _ = env.step(0)\n",
    "            \n",
    "            # TD(0)更新\n",
    "            td_target = reward + gamma * V[next_state] * (not done)\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 计算RMSE（仅非终止状态）\n",
    "        rmse = np.sqrt(np.mean((V[1:-1] - true_v[1:-1])**2))\n",
    "        rmse_history.append(rmse)\n",
    "    \n",
    "    return V, rmse_history\n",
    "\n",
    "\n",
    "def mc_prediction(env, n_episodes=100, alpha=0.1, gamma=1.0):\n",
    "    \"\"\"Monte Carlo策略评估（每次访问MC）\"\"\"\n",
    "    V = np.zeros(env.n_total_states)\n",
    "    rmse_history = []\n",
    "    true_v = env.get_true_values(gamma)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        trajectory = [state]\n",
    "        rewards = []\n",
    "        \n",
    "        # 收集完整轨迹\n",
    "        while True:\n",
    "            next_state, reward, done, _, _ = env.step(0)\n",
    "            rewards.append(reward)\n",
    "            trajectory.append(next_state)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # MC更新（从后向前计算回报）\n",
    "        G = 0\n",
    "        for t in range(len(rewards) - 1, -1, -1):\n",
    "            G = rewards[t] + gamma * G\n",
    "            state = trajectory[t]\n",
    "            V[state] += alpha * (G - V[state])\n",
    "        \n",
    "        # 计算RMSE\n",
    "        rmse = np.sqrt(np.mean((V[1:-1] - true_v[1:-1])**2))\n",
    "        rmse_history.append(rmse)\n",
    "    \n",
    "    return V, rmse_history\n",
    "\n",
    "\n",
    "# 运行实验\n",
    "env = RandomWalk(n_states=5)\n",
    "n_episodes = 100\n",
    "n_runs = 100  # 多次运行取平均\n",
    "\n",
    "td_rmse_all = []\n",
    "mc_rmse_all = []\n",
    "\n",
    "print(\"运行TD(0) vs MC比较实验...\")\n",
    "for run in range(n_runs):\n",
    "    np.random.seed(run)\n",
    "    _, td_rmse = td0_prediction(env, n_episodes, alpha=0.1)\n",
    "    _, mc_rmse = mc_prediction(env, n_episodes, alpha=0.1)\n",
    "    td_rmse_all.append(td_rmse)\n",
    "    mc_rmse_all.append(mc_rmse)\n",
    "\n",
    "td_rmse_mean = np.mean(td_rmse_all, axis=0)\n",
    "mc_rmse_mean = np.mean(mc_rmse_all, axis=0)\n",
    "print(\"完成!\")\n",
    "\n",
    "# 绘图\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE曲线\n",
    "ax1 = axes[0]\n",
    "ax1.plot(td_rmse_mean, label='TD(0)', color='blue', linewidth=2)\n",
    "ax1.plot(mc_rmse_mean, label='Monte Carlo', color='red', linewidth=2)\n",
    "ax1.set_xlabel('回合', fontsize=12)\n",
    "ax1.set_ylabel('RMSE', fontsize=12)\n",
    "ax1.set_title('TD(0) vs MC: 价值估计误差', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 最终价值估计\n",
    "ax2 = axes[1]\n",
    "np.random.seed(42)\n",
    "V_td, _ = td0_prediction(env, 100, alpha=0.1)\n",
    "np.random.seed(42)\n",
    "V_mc, _ = mc_prediction(env, 100, alpha=0.1)\n",
    "true_v = env.get_true_values(1.0)\n",
    "\n",
    "states_label = ['A', 'B', 'C', 'D', 'E']\n",
    "x = np.arange(5)\n",
    "width = 0.25\n",
    "\n",
    "ax2.bar(x - width, true_v[1:-1], width, label='真实价值', color='green', alpha=0.8)\n",
    "ax2.bar(x, V_td[1:-1], width, label='TD(0)估计', color='blue', alpha=0.8)\n",
    "ax2.bar(x + width, V_mc[1:-1], width, label='MC估计', color='red', alpha=0.8)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(states_label)\n",
    "ax2.set_xlabel('状态', fontsize=12)\n",
    "ax2.set_ylabel('价值', fontsize=12)\n",
    "ax2.set_title('100回合后的价值估计', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 关键观察\n",
    "\n",
    "从上面的实验可以看到：\n",
    "\n",
    "1. **TD(0)收敛更快**：在相同的学习率下，TD(0)通常比MC更快收敛\n",
    "2. **低方差**：TD(0)的更新只依赖单步，方差较低\n",
    "3. **有偏但渐近无偏**：虽然TD(0)初期有偏差，但最终会收敛到正确值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. SARSA与Q-Learning对比\n",
    "\n",
    "### 4.1 从策略评估到控制\n",
    "\n",
    "TD(0)用于评估固定策略。**SARSA**和**Q-Learning**将TD思想扩展到**控制问题**——同时学习和改进策略。\n",
    "\n",
    "### 4.2 SARSA (State-Action-Reward-State-Action)\n",
    "\n",
    "**On-Policy TD控制**：学习行为策略本身的Q值\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\n",
    "\n",
    "关键：使用**实际执行的下一动作** $A_{t+1}$ 来计算TD目标\n",
    "\n",
    "### 4.3 Q-Learning\n",
    "\n",
    "**Off-Policy TD控制**：学习最优Q值，与行为策略无关\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
    "\n",
    "关键：使用**最大化的Q值** $\\max_a Q(S_{t+1}, a)$ 来计算TD目标\n",
    "\n",
    "### 4.4 核心区别\n",
    "\n",
    "| 特性 | SARSA | Q-Learning |\n",
    "|------|-------|------------|\n",
    "| 类型 | On-Policy | Off-Policy |\n",
    "| TD目标 | $R + \\gamma Q(S', A')$ | $R + \\gamma \\max_a Q(S', a)$ |\n",
    "| 学习的策略 | 行为策略(含探索) | 最优策略(不含探索) |\n",
    "| 安全性 | 更安全 | 更激进 |\n",
    "| 典型行为 | 避开风险 | 追求最优路径 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA vs Q-Learning 算法对比图示\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# SARSA\n",
    "ax1 = axes[0]\n",
    "ax1.text(0.5, 0.9, 'SARSA (On-Policy)', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 状态转移\n",
    "ax1.add_patch(plt.Rectangle((0.15, 0.5), 0.15, 0.1, facecolor='lightblue', edgecolor='black'))\n",
    "ax1.text(0.225, 0.55, '$S_t$', ha='center', fontsize=11)\n",
    "ax1.add_patch(plt.Rectangle((0.35, 0.5), 0.15, 0.1, facecolor='lightgreen', edgecolor='black'))\n",
    "ax1.text(0.425, 0.55, '$A_t$', ha='center', fontsize=11)\n",
    "ax1.add_patch(plt.Rectangle((0.55, 0.5), 0.15, 0.1, facecolor='lightyellow', edgecolor='black'))\n",
    "ax1.text(0.625, 0.55, '$R$', ha='center', fontsize=11)\n",
    "ax1.add_patch(plt.Rectangle((0.75, 0.5), 0.15, 0.1, facecolor='lightblue', edgecolor='black'))\n",
    "ax1.text(0.825, 0.55, \"$S'$\", ha='center', fontsize=11)\n",
    "\n",
    "# A' (实际选择的动作)\n",
    "ax1.add_patch(plt.Rectangle((0.75, 0.3), 0.15, 0.1, facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "ax1.text(0.825, 0.35, \"$A'$\", ha='center', fontsize=11, fontweight='bold')\n",
    "ax1.annotate('', xy=(0.825, 0.4), xytext=(0.825, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax1.text(0.88, 0.45, 'ε-greedy\\n选择', fontsize=9, color='red')\n",
    "\n",
    "# 更新公式\n",
    "ax1.text(0.5, 0.15, r\"$Q(S_t, A_t) \\leftarrow Q + \\alpha[R + \\gamma Q(S', A') - Q]$\",\n",
    "         ha='center', fontsize=12, bbox=dict(facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "ax1.text(0.5, 0.02, '使用实际执行的动作A\\'来更新', ha='center', fontsize=11, color='red')\n",
    "\n",
    "# 箭头\n",
    "for x1, x2 in [(0.3, 0.35), (0.5, 0.55), (0.7, 0.75)]:\n",
    "    ax1.annotate('', xy=(x2, 0.55), xytext=(x1, 0.55),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Q-Learning\n",
    "ax2 = axes[1]\n",
    "ax2.text(0.5, 0.9, 'Q-Learning (Off-Policy)', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 状态转移\n",
    "ax2.add_patch(plt.Rectangle((0.15, 0.5), 0.15, 0.1, facecolor='lightblue', edgecolor='black'))\n",
    "ax2.text(0.225, 0.55, '$S_t$', ha='center', fontsize=11)\n",
    "ax2.add_patch(plt.Rectangle((0.35, 0.5), 0.15, 0.1, facecolor='lightgreen', edgecolor='black'))\n",
    "ax2.text(0.425, 0.55, '$A_t$', ha='center', fontsize=11)\n",
    "ax2.add_patch(plt.Rectangle((0.55, 0.5), 0.15, 0.1, facecolor='lightyellow', edgecolor='black'))\n",
    "ax2.text(0.625, 0.55, '$R$', ha='center', fontsize=11)\n",
    "ax2.add_patch(plt.Rectangle((0.75, 0.5), 0.15, 0.1, facecolor='lightblue', edgecolor='black'))\n",
    "ax2.text(0.825, 0.55, \"$S'$\", ha='center', fontsize=11)\n",
    "\n",
    "# max Q (最大化)\n",
    "ax2.add_patch(plt.Rectangle((0.75, 0.3), 0.15, 0.1, facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "ax2.text(0.825, 0.35, r\"$\\max_a Q$\", ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.annotate('', xy=(0.825, 0.4), xytext=(0.825, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "ax2.text(0.88, 0.45, '取最大\\nQ值', fontsize=9, color='green')\n",
    "\n",
    "# 更新公式\n",
    "ax2.text(0.5, 0.15, r\"$Q(S_t, A_t) \\leftarrow Q + \\alpha[R + \\gamma \\max_a Q(S', a) - Q]$\",\n",
    "         ha='center', fontsize=12, bbox=dict(facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "ax2.text(0.5, 0.02, '使用最优动作的Q值来更新', ha='center', fontsize=11, color='green')\n",
    "\n",
    "# 箭头\n",
    "for x1, x2 in [(0.3, 0.35), (0.5, 0.55), (0.7, 0.75)]:\n",
    "    ax2.annotate('', xy=(x2, 0.55), xytext=(x1, 0.55),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Expected SARSA与Double Q-Learning\n",
    "\n",
    "### 5.1 Expected SARSA\n",
    "\n",
    "Expected SARSA使用下一状态Q值的**期望**而不是单一采样：\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha\\left[R_{t+1} + \\gamma \\mathbb{E}_\\pi[Q(S_{t+1}, A)] - Q(S_t, A_t)\\right]$$\n",
    "\n",
    "对于ε-greedy策略：\n",
    "$$\\mathbb{E}_\\pi[Q(S', A)] = \\frac{\\epsilon}{|A|} \\sum_a Q(S', a) + (1-\\epsilon) \\max_a Q(S', a)$$\n",
    "\n",
    "**优势**：消除了动作采样带来的方差\n",
    "\n",
    "### 5.2 Double Q-Learning\n",
    "\n",
    "Q-Learning的max操作会导致**最大化偏差**（过估计）。Double Q-Learning通过维护两个Q表来解决：\n",
    "\n",
    "```\n",
    "以50%概率:\n",
    "    a* = argmax_a Q_A(S', a)          # 用Q_A选择\n",
    "    Q_A(S, A) ← Q_A + α[R + γQ_B(S', a*) - Q_A(S, A)]  # 用Q_B评估\n",
    "否则:\n",
    "    a* = argmax_a Q_B(S', a)          # 用Q_B选择\n",
    "    Q_B(S, A) ← Q_B + α[R + γQ_A(S', a*) - Q_B(S, A)]  # 用Q_A评估\n",
    "```\n",
    "\n",
    "**关键洞察**：通过解耦\"选择\"和\"评估\"，避免了同一噪声来源的双重影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大化偏差演示\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 模拟场景：有多个动作，每个动作的真实价值为0，但估计有噪声\n",
    "n_actions = 10\n",
    "true_value = 0.0\n",
    "noise_std = 1.0\n",
    "n_samples = 1000\n",
    "\n",
    "max_estimates = []\n",
    "double_estimates = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    # 标准Q-Learning：估计值 = 真实值 + 噪声\n",
    "    q_estimates = true_value + np.random.randn(n_actions) * noise_std\n",
    "    max_estimates.append(np.max(q_estimates))\n",
    "    \n",
    "    # Double Q-Learning：两组独立估计\n",
    "    q_a = true_value + np.random.randn(n_actions) * noise_std\n",
    "    q_b = true_value + np.random.randn(n_actions) * noise_std\n",
    "    best_action = np.argmax(q_a)  # 用Q_A选择\n",
    "    double_estimates.append(q_b[best_action])  # 用Q_B评估\n",
    "\n",
    "# 绘图\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 直方图\n",
    "ax1 = axes[0]\n",
    "ax1.hist(max_estimates, bins=50, alpha=0.7, label=f'Q-Learning (max)', color='red', density=True)\n",
    "ax1.hist(double_estimates, bins=50, alpha=0.7, label=f'Double Q-Learning', color='blue', density=True)\n",
    "ax1.axvline(true_value, color='green', linestyle='--', linewidth=2, label=f'真实值 = {true_value}')\n",
    "ax1.axvline(np.mean(max_estimates), color='red', linestyle=':', linewidth=2)\n",
    "ax1.axvline(np.mean(double_estimates), color='blue', linestyle=':', linewidth=2)\n",
    "ax1.set_xlabel('估计值', fontsize=12)\n",
    "ax1.set_ylabel('密度', fontsize=12)\n",
    "ax1.set_title('最大化偏差演示', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# 期望值比较\n",
    "ax2 = axes[1]\n",
    "methods = ['真实值', 'Q-Learning\\n(max)', 'Double\\nQ-Learning']\n",
    "values = [true_value, np.mean(max_estimates), np.mean(double_estimates)]\n",
    "colors = ['green', 'red', 'blue']\n",
    "bars = ax2.bar(methods, values, color=colors, alpha=0.7)\n",
    "\n",
    "# 添加数值标签\n",
    "for bar, val in zip(bars, values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "             f'{val:.3f}', ha='center', fontsize=12)\n",
    "\n",
    "ax2.axhline(true_value, color='green', linestyle='--', alpha=0.5)\n",
    "ax2.set_ylabel('期望估计值', fontsize=12)\n",
    "ax2.set_title(f'期望估计值比较 (动作数={n_actions})', fontsize=13, fontweight='bold')\n",
    "\n",
    "# 添加说明\n",
    "bias = np.mean(max_estimates) - true_value\n",
    "ax2.text(1, np.mean(max_estimates) + 0.3, f'过估计偏差: {bias:.3f}', \n",
    "         ha='center', fontsize=11, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"分析结果:\")\n",
    "print(f\"  真实值: {true_value}\")\n",
    "print(f\"  Q-Learning期望估计: {np.mean(max_estimates):.4f} (过估计 {np.mean(max_estimates) - true_value:.4f})\")\n",
    "print(f\"  Double Q-Learning期望估计: {np.mean(double_estimates):.4f} (偏差 {np.mean(double_estimates) - true_value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. N-Step TD与TD(λ)\n",
    "\n",
    "### 6.1 N-Step TD\n",
    "\n",
    "N-Step TD使用n步的实际奖励，然后用第n+1步的价值估计：\n",
    "\n",
    "$$G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n})$$\n",
    "\n",
    "- n=1: TD(0)\n",
    "- n=∞: Monte Carlo\n",
    "\n",
    "### 6.2 TD(λ)与资格迹\n",
    "\n",
    "TD(λ)不是选择特定的n，而是对所有n-step回报做**几何加权平均**：\n",
    "\n",
    "$$G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)}$$\n",
    "\n",
    "**资格迹(Eligibility Traces)**提供了高效实现：\n",
    "\n",
    "$$E_t(s) = \\gamma \\lambda E_{t-1}(s) + \\mathbf{1}(S_t = s)$$\n",
    "\n",
    "资格迹追踪哪些状态对当前TD误差\"负责\"，允许单步更新实现多步效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# λ-回报的几何加权可视化\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 不同λ的权重分布\n",
    "ax1 = axes[0]\n",
    "n_steps = 15\n",
    "lambdas = [0.0, 0.5, 0.8, 0.95, 1.0]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(lambdas)))\n",
    "\n",
    "for lam, color in zip(lambdas, colors):\n",
    "    if lam < 1:\n",
    "        weights = [(1 - lam) * (lam ** (n-1)) for n in range(1, n_steps + 1)]\n",
    "    else:\n",
    "        weights = [0] * (n_steps - 1) + [1]  # MC: 只有最后一个\n",
    "    \n",
    "    ax1.plot(range(1, n_steps + 1), weights, 'o-', \n",
    "             label=f'λ={lam}', color=color, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('n-step回报', fontsize=12)\n",
    "ax1.set_ylabel('权重 (1-λ)λⁿ⁻¹', fontsize=12)\n",
    "ax1.set_title('TD(λ)中各n-step回报的权重', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 资格迹衰减示意\n",
    "ax2 = axes[1]\n",
    "gamma = 0.99\n",
    "lam = 0.9\n",
    "\n",
    "# 模拟一个访问序列\n",
    "# 状态1在t=0访问，状态2在t=5访问\n",
    "t = np.arange(20)\n",
    "\n",
    "# 状态1的资格迹\n",
    "e1 = np.zeros(20)\n",
    "e1[0] = 1\n",
    "for i in range(1, 20):\n",
    "    e1[i] = gamma * lam * e1[i-1]\n",
    "\n",
    "# 状态2的资格迹\n",
    "e2 = np.zeros(20)\n",
    "e2[5] = 1\n",
    "for i in range(6, 20):\n",
    "    e2[i] = gamma * lam * e2[i-1]\n",
    "\n",
    "ax2.plot(t, e1, 'b-o', label='状态1 (t=0访问)', markersize=5)\n",
    "ax2.plot(t, e2, 'r-s', label='状态2 (t=5访问)', markersize=5)\n",
    "ax2.axvline(0, color='blue', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(5, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax2.set_xlabel('时间步', fontsize=12)\n",
    "ax2.set_ylabel('资格迹 E(s)', fontsize=12)\n",
    "ax2.set_title(f'资格迹衰减 (γ={gamma}, λ={lam})', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 实验：Cliff Walking环境\n",
    "\n",
    "Cliff Walking是展示SARSA与Q-Learning区别的经典环境：\n",
    "\n",
    "```\n",
    ". . . . . . . . . . . .\n",
    ". . . . . . . . . . . .\n",
    ". . . . . . . . . . . .\n",
    "S C C C C C C C C C C G\n",
    "```\n",
    "\n",
    "- S: 起点\n",
    "- G: 终点\n",
    "- C: 悬崖（掉落返回起点，-100奖励）\n",
    "- 每步-1奖励\n",
    "\n",
    "### 预期行为\n",
    "- **SARSA**: 学到远离悬崖的**安全路径**（考虑探索时的风险）\n",
    "- **Q-Learning**: 学到贴着悬崖的**最优路径**（假设执行时是贪婪的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cliff Walking实验\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "\n",
    "# 配置\n",
    "config = TDConfig(\n",
    "    alpha=0.5,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.1\n",
    ")\n",
    "\n",
    "n_episodes = 500\n",
    "\n",
    "# 训练SARSA\n",
    "print(\"训练SARSA...\")\n",
    "sarsa_learner = create_td_learner('sarsa', config=config)\n",
    "sarsa_metrics = sarsa_learner.train(env, n_episodes=n_episodes, log_interval=n_episodes+1)\n",
    "\n",
    "# 训练Q-Learning\n",
    "print(\"训练Q-Learning...\")\n",
    "qlearn_learner = create_td_learner('q_learning', config=config)\n",
    "qlearn_metrics = qlearn_learner.train(env, n_episodes=n_episodes, log_interval=n_episodes+1)\n",
    "\n",
    "# 训练Expected SARSA\n",
    "print(\"训练Expected SARSA...\")\n",
    "exp_sarsa_learner = create_td_learner('expected_sarsa', config=config)\n",
    "exp_sarsa_metrics = exp_sarsa_learner.train(env, n_episodes=n_episodes, log_interval=n_episodes+1)\n",
    "\n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制学习曲线\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 20\n",
    "\n",
    "# 奖励曲线\n",
    "ax1 = axes[0]\n",
    "for metrics, name, color in [\n",
    "    (sarsa_metrics, 'SARSA', 'blue'),\n",
    "    (qlearn_metrics, 'Q-Learning', 'red'),\n",
    "    (exp_sarsa_metrics, 'Expected SARSA', 'green')\n",
    "]:\n",
    "    rewards = metrics.episode_rewards\n",
    "    if len(rewards) >= window:\n",
    "        smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        ax1.plot(smoothed, label=name, color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('回合', fontsize=12)\n",
    "ax1.set_ylabel('累积奖励', fontsize=12)\n",
    "ax1.set_title('Cliff Walking 学习曲线', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 最后100回合的统计\n",
    "ax2 = axes[1]\n",
    "last_100 = 100\n",
    "\n",
    "methods = ['SARSA', 'Q-Learning', 'Exp. SARSA']\n",
    "mean_rewards = [\n",
    "    np.mean(sarsa_metrics.episode_rewards[-last_100:]),\n",
    "    np.mean(qlearn_metrics.episode_rewards[-last_100:]),\n",
    "    np.mean(exp_sarsa_metrics.episode_rewards[-last_100:])\n",
    "]\n",
    "std_rewards = [\n",
    "    np.std(sarsa_metrics.episode_rewards[-last_100:]),\n",
    "    np.std(qlearn_metrics.episode_rewards[-last_100:]),\n",
    "    np.std(exp_sarsa_metrics.episode_rewards[-last_100:])\n",
    "]\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "bars = ax2.bar(methods, mean_rewards, yerr=std_rewards, \n",
    "               color=colors, alpha=0.7, capsize=5)\n",
    "\n",
    "for bar, val in zip(bars, mean_rewards):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             f'{val:.1f}', ha='center', fontsize=11)\n",
    "\n",
    "ax2.set_ylabel('平均奖励', fontsize=12)\n",
    "ax2.set_title(f'最后{last_100}回合平均奖励', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化学习到的路径\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "def visualize_path(ax, q_function, title):\n",
    "    \"\"\"可视化从Q函数推断的路径\"\"\"\n",
    "    height, width = 4, 12\n",
    "    \n",
    "    # 绘制网格\n",
    "    for i in range(height + 1):\n",
    "        ax.axhline(i, color='gray', linewidth=0.5)\n",
    "    for j in range(width + 1):\n",
    "        ax.axvline(j, color='gray', linewidth=0.5)\n",
    "    \n",
    "    # 绘制悬崖\n",
    "    for c in range(1, 11):\n",
    "        ax.add_patch(plt.Rectangle((c, 0), 1, 1, facecolor='red', alpha=0.5))\n",
    "    \n",
    "    # 起点和终点\n",
    "    ax.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor='green', alpha=0.5))\n",
    "    ax.text(0.5, 0.5, 'S', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.add_patch(plt.Rectangle((11, 0), 1, 1, facecolor='gold', alpha=0.5))\n",
    "    ax.text(11.5, 0.5, 'G', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 模拟贪婪路径\n",
    "    state = (3, 0)  # 起点\n",
    "    path = [state]\n",
    "    deltas = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "    \n",
    "    for _ in range(50):\n",
    "        state_idx = state[0] * width + state[1]\n",
    "        \n",
    "        q_values = []\n",
    "        for a in range(4):\n",
    "            key = (state_idx, a)\n",
    "            if key in q_function:\n",
    "                q_values.append((a, q_function[key]))\n",
    "        \n",
    "        if not q_values:\n",
    "            break\n",
    "        \n",
    "        best_action = max(q_values, key=lambda x: x[1])[0]\n",
    "        delta = deltas[best_action]\n",
    "        new_state = (\n",
    "            np.clip(state[0] + delta[0], 0, height - 1),\n",
    "            np.clip(state[1] + delta[1], 0, width - 1)\n",
    "        )\n",
    "        \n",
    "        if new_state == (3, 11):\n",
    "            path.append(new_state)\n",
    "            break\n",
    "        \n",
    "        if new_state[0] == 3 and 1 <= new_state[1] <= 10:\n",
    "            path.append(new_state)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        path.append(state)\n",
    "    \n",
    "    # 绘制路径\n",
    "    if len(path) > 1:\n",
    "        xs = [p[1] + 0.5 for p in path]\n",
    "        ys = [height - p[0] - 0.5 for p in path]\n",
    "        ax.plot(xs, ys, 'b-o', linewidth=3, markersize=8, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlim(0, width)\n",
    "    ax.set_ylim(0, height)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "visualize_path(axes[0], sarsa_learner.q_function, 'SARSA (安全路径)')\n",
    "visualize_path(axes[1], qlearn_learner.q_function, 'Q-Learning (最优路径)')\n",
    "visualize_path(axes[2], exp_sarsa_learner.q_function, 'Expected SARSA')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 结果分析\n",
    "\n",
    "从上面的实验可以观察到：\n",
    "\n",
    "1. **Q-Learning学到沿悬崖边缘的最短路径**\n",
    "   - 它认为可以完美执行贪婪策略\n",
    "   - 在训练时（有探索）会经常掉落悬崖\n",
    "   - 但评估时（纯贪婪）走的是最优路径\n",
    "\n",
    "2. **SARSA学到远离悬崖的安全路径**\n",
    "   - 它知道自己会探索，可能失误\n",
    "   - 因此选择更安全但更长的路径\n",
    "   - 训练时的平均奖励更高（更少掉落）\n",
    "\n",
    "3. **Expected SARSA介于两者之间**\n",
    "   - 消除了动作采样方差\n",
    "   - 行为更稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 实验：Random Walk与价值预测\n",
    "\n",
    "现在用更大的Random Walk环境测试不同TD算法的价值预测能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同学习率的TD(0)比较\n",
    "\n",
    "env = RandomWalk(n_states=19)  # 更大的状态空间\n",
    "true_values = env.get_true_values(gamma=1.0)\n",
    "\n",
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "n_episodes = 100\n",
    "n_runs = 50\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"测试不同学习率...\")\n",
    "for alpha in alphas:\n",
    "    rmse_all = []\n",
    "    for run in range(n_runs):\n",
    "        np.random.seed(run)\n",
    "        _, rmse = td0_prediction(env, n_episodes, alpha=alpha, gamma=1.0)\n",
    "        rmse_all.append(rmse)\n",
    "    results[alpha] = np.mean(rmse_all, axis=0)\n",
    "    print(f\"  α={alpha}: 最终RMSE = {results[alpha][-1]:.4f}\")\n",
    "\n",
    "# 绘图\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 学习曲线\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(alphas)))\n",
    "\n",
    "for alpha, color in zip(alphas, colors):\n",
    "    ax1.plot(results[alpha], label=f'α={alpha}', color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('回合', fontsize=12)\n",
    "ax1.set_ylabel('RMSE', fontsize=12)\n",
    "ax1.set_title('不同学习率的TD(0)学习曲线', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 最终RMSE vs 学习率\n",
    "ax2 = axes[1]\n",
    "final_rmse = [results[a][-1] for a in alphas]\n",
    "ax2.plot(alphas, final_rmse, 'bo-', linewidth=2, markersize=10)\n",
    "ax2.set_xlabel('学习率 α', fontsize=12)\n",
    "ax2.set_ylabel('最终RMSE', fontsize=12)\n",
    "ax2.set_title('学习率 vs 最终误差', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 标记最优\n",
    "best_idx = np.argmin(final_rmse)\n",
    "ax2.scatter([alphas[best_idx]], [final_rmse[best_idx]], \n",
    "            color='red', s=200, zorder=5, marker='*')\n",
    "ax2.annotate(f'最优 α={alphas[best_idx]}', \n",
    "             (alphas[best_idx], final_rmse[best_idx]),\n",
    "             xytext=(alphas[best_idx]+0.05, final_rmse[best_idx]+0.02),\n",
    "             fontsize=11, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD(λ) 不同λ值的比较\n",
    "\n",
    "def td_lambda_prediction(env, n_episodes=100, alpha=0.1, gamma=1.0, lam=0.0):\n",
    "    \"\"\"TD(λ)策略评估\"\"\"\n",
    "    V = np.zeros(env.n_total_states)\n",
    "    true_v = env.get_true_values(gamma)\n",
    "    rmse_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        # 资格迹\n",
    "        E = np.zeros(env.n_total_states)\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, _, _ = env.step(0)\n",
    "            \n",
    "            # TD误差\n",
    "            td_target = reward + gamma * V[next_state] * (not done)\n",
    "            td_error = td_target - V[state]\n",
    "            \n",
    "            # 更新资格迹\n",
    "            E *= gamma * lam\n",
    "            E[state] += 1\n",
    "            \n",
    "            # 更新所有状态\n",
    "            V += alpha * td_error * E\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rmse = np.sqrt(np.mean((V[1:-1] - true_v[1:-1])**2))\n",
    "        rmse_history.append(rmse)\n",
    "    \n",
    "    return V, rmse_history\n",
    "\n",
    "\n",
    "# 测试不同λ值\n",
    "env = RandomWalk(n_states=19)\n",
    "lambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 1.0]\n",
    "n_episodes = 100\n",
    "n_runs = 50\n",
    "\n",
    "lambda_results = {}\n",
    "\n",
    "print(\"测试不同λ值...\")\n",
    "for lam in lambdas:\n",
    "    rmse_all = []\n",
    "    for run in range(n_runs):\n",
    "        np.random.seed(run)\n",
    "        _, rmse = td_lambda_prediction(env, n_episodes, alpha=0.1, gamma=1.0, lam=lam)\n",
    "        rmse_all.append(rmse)\n",
    "    lambda_results[lam] = np.mean(rmse_all, axis=0)\n",
    "    print(f\"  λ={lam}: 最终RMSE = {lambda_results[lam][-1]:.4f}\")\n",
    "\n",
    "# 绘图\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 学习曲线\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(lambdas)))\n",
    "\n",
    "for lam, color in zip(lambdas, colors):\n",
    "    label = f'λ={lam}' if lam < 1 else 'λ=1 (MC)'\n",
    "    ax1.plot(lambda_results[lam], label=label, color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('回合', fontsize=12)\n",
    "ax1.set_ylabel('RMSE', fontsize=12)\n",
    "ax1.set_title('不同λ值的TD(λ)学习曲线', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# λ vs 最终RMSE\n",
    "ax2 = axes[1]\n",
    "final_rmse = [lambda_results[l][-1] for l in lambdas]\n",
    "ax2.plot(lambdas, final_rmse, 'ro-', linewidth=2, markersize=10)\n",
    "ax2.set_xlabel('λ', fontsize=12)\n",
    "ax2.set_ylabel('最终RMSE', fontsize=12)\n",
    "ax2.set_title('λ vs 最终误差', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 标记最优\n",
    "best_idx = np.argmin(final_rmse)\n",
    "ax2.scatter([lambdas[best_idx]], [final_rmse[best_idx]], \n",
    "            color='green', s=200, zorder=5, marker='*')\n",
    "ax2.annotate(f'最优 λ={lambdas[best_idx]}', \n",
    "             (lambdas[best_idx], final_rmse[best_idx]),\n",
    "             xytext=(lambdas[best_idx]+0.05, final_rmse[best_idx]+0.01),\n",
    "             fontsize=11, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 超参数分析\n",
    "\n",
    "### 9.1 学习率α的影响\n",
    "\n",
    "- **太小**：收敛慢，可能停留在次优解\n",
    "- **太大**：震荡，不稳定\n",
    "- **最优范围**：通常在0.1-0.5之间，取决于环境\n",
    "\n",
    "### 9.2 折扣因子γ的影响\n",
    "\n",
    "- **γ→0**：短视，只关心即时奖励\n",
    "- **γ→1**：远视，平等看待所有未来奖励\n",
    "- **典型值**：0.9-0.99\n",
    "\n",
    "### 9.3 探索率ε的影响\n",
    "\n",
    "- **太小**：可能陷入局部最优\n",
    "- **太大**：学习效率低，最终策略差\n",
    "- **常用策略**：从高到低递减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数敏感性分析\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "\n",
    "# 测试不同ε值\n",
    "epsilons = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "n_episodes = 300\n",
    "n_runs = 20\n",
    "\n",
    "epsilon_results = {}\n",
    "\n",
    "print(\"测试不同探索率ε...\")\n",
    "for eps in epsilons:\n",
    "    rewards_all = []\n",
    "    for run in range(n_runs):\n",
    "        np.random.seed(run)\n",
    "        config = TDConfig(alpha=0.5, gamma=0.99, epsilon=eps)\n",
    "        learner = create_td_learner('sarsa', config=config)\n",
    "        metrics = learner.train(env, n_episodes=n_episodes, log_interval=n_episodes+1)\n",
    "        rewards_all.append(metrics.episode_rewards)\n",
    "    epsilon_results[eps] = np.mean(rewards_all, axis=0)\n",
    "    print(f\"  ε={eps}: 最终平均奖励 = {np.mean(epsilon_results[eps][-50:]):.2f}\")\n",
    "\n",
    "# 绘图\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 学习曲线\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.coolwarm(np.linspace(0, 1, len(epsilons)))\n",
    "window = 20\n",
    "\n",
    "for eps, color in zip(epsilons, colors):\n",
    "    rewards = epsilon_results[eps]\n",
    "    if len(rewards) >= window:\n",
    "        smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        ax1.plot(smoothed, label=f'ε={eps}', color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('回合', fontsize=12)\n",
    "ax1.set_ylabel('累积奖励', fontsize=12)\n",
    "ax1.set_title('不同探索率ε的SARSA学习曲线', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ε vs 最终奖励\n",
    "ax2 = axes[1]\n",
    "final_rewards = [np.mean(epsilon_results[e][-50:]) for e in epsilons]\n",
    "ax2.plot(epsilons, final_rewards, 'go-', linewidth=2, markersize=10)\n",
    "ax2.set_xlabel('探索率 ε', fontsize=12)\n",
    "ax2.set_ylabel('最终平均奖励', fontsize=12)\n",
    "ax2.set_title('探索率 vs 最终性能', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 标记最优\n",
    "best_idx = np.argmax(final_rewards)\n",
    "ax2.scatter([epsilons[best_idx]], [final_rewards[best_idx]], \n",
    "            color='red', s=200, zorder=5, marker='*')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 总结与进阶方向\n",
    "\n",
    "### 10.1 核心要点回顾\n",
    "\n",
    "1. **TD学习的本质**：用\"猜测\"更新\"猜测\"，无需等待回合结束\n",
    "\n",
    "2. **关键公式**：\n",
    "   - TD目标：$R_{t+1} + \\gamma V(S_{t+1})$\n",
    "   - TD误差：$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "3. **算法选择指南**：\n",
    "   | 场景 | 推荐算法 |\n",
    "   |------|----------|\n",
    "   | 需要安全探索 | SARSA |\n",
    "   | 从历史数据学习 | Q-Learning |\n",
    "   | 需要低方差 | Expected SARSA |\n",
    "   | 噪声环境 | Double Q-Learning |\n",
    "   | 需要快速传播 | TD(λ)或n-step |\n",
    "\n",
    "4. **超参数建议**：\n",
    "   - α：从0.1开始，根据收敛速度调整\n",
    "   - γ：大多数任务用0.99\n",
    "   - ε：从0.1开始，可以随训练递减\n",
    "   - λ：从0.9开始，根据问题特性调整\n",
    "\n",
    "### 10.2 进阶方向\n",
    "\n",
    "1. **函数逼近**：\n",
    "   - 线性函数逼近 + TD学习\n",
    "   - 深度Q网络(DQN)：神经网络 + Q-Learning\n",
    "\n",
    "2. **Actor-Critic方法**：\n",
    "   - 结合策略梯度和TD学习\n",
    "   - A2C, A3C, PPO等\n",
    "\n",
    "3. **连续动作空间**：\n",
    "   - DDPG, TD3, SAC\n",
    "\n",
    "4. **模型基方法**：\n",
    "   - Dyna-Q：结合模型学习和TD学习\n",
    "\n",
    "### 10.3 参考文献\n",
    "\n",
    "- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.\n",
    "- Watkins, C. J., & Dayan, P. (1992). Q-learning. *Machine Learning*.\n",
    "- Van Hasselt, H. (2010). Double Q-learning. *NeurIPS*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结图：TD方法家族\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# 绘制层次结构\n",
    "boxes = [\n",
    "    # 顶层\n",
    "    (0.5, 0.9, '时序差分学习 (TD Learning)', 'gold', 0.25, 0.08),\n",
    "    \n",
    "    # 第二层\n",
    "    (0.25, 0.7, '策略评估\\n(Policy Evaluation)', 'lightblue', 0.18, 0.1),\n",
    "    (0.75, 0.7, '控制\\n(Control)', 'lightgreen', 0.18, 0.1),\n",
    "    \n",
    "    # 第三层 - 评估\n",
    "    (0.15, 0.5, 'TD(0)', 'lightyellow', 0.12, 0.08),\n",
    "    (0.35, 0.5, 'TD(λ)', 'lightyellow', 0.12, 0.08),\n",
    "    \n",
    "    # 第三层 - 控制\n",
    "    (0.55, 0.5, 'On-Policy', 'lightcoral', 0.12, 0.08),\n",
    "    (0.85, 0.5, 'Off-Policy', 'plum', 0.12, 0.08),\n",
    "    \n",
    "    # 第四层\n",
    "    (0.45, 0.3, 'SARSA', 'orange', 0.1, 0.08),\n",
    "    (0.6, 0.3, 'SARSA(λ)', 'orange', 0.1, 0.08),\n",
    "    (0.75, 0.3, 'Expected\\nSARSA', 'orange', 0.1, 0.08),\n",
    "    \n",
    "    (0.85, 0.3, 'Q-Learning', 'violet', 0.1, 0.08),\n",
    "    (0.95, 0.3, 'Double Q', 'violet', 0.08, 0.08),\n",
    "    \n",
    "    # 深度学习扩展\n",
    "    (0.5, 0.1, '深度强化学习', 'cyan', 0.35, 0.08),\n",
    "]\n",
    "\n",
    "for x, y, text, color, w, h in boxes:\n",
    "    ax.add_patch(plt.Rectangle((x-w/2, y-h/2), w, h, \n",
    "                               facecolor=color, edgecolor='black', linewidth=2))\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 连接线\n",
    "connections = [\n",
    "    ((0.5, 0.82), (0.25, 0.8)),\n",
    "    ((0.5, 0.82), (0.75, 0.8)),\n",
    "    ((0.25, 0.6), (0.15, 0.58)),\n",
    "    ((0.25, 0.6), (0.35, 0.58)),\n",
    "    ((0.75, 0.6), (0.55, 0.58)),\n",
    "    ((0.75, 0.6), (0.85, 0.58)),\n",
    "    ((0.55, 0.42), (0.45, 0.38)),\n",
    "    ((0.55, 0.42), (0.6, 0.38)),\n",
    "    ((0.55, 0.42), (0.75, 0.38)),\n",
    "    ((0.85, 0.42), (0.85, 0.38)),\n",
    "    ((0.85, 0.42), (0.95, 0.38)),\n",
    "    ((0.5, 0.22), (0.5, 0.18)),\n",
    "]\n",
    "\n",
    "for start, end in connections:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "\n",
    "# 深度学习扩展的子项\n",
    "deep_methods = ['DQN', 'Double DQN', 'Dueling DQN', 'Rainbow', 'A3C', 'PPO']\n",
    "for i, method in enumerate(deep_methods):\n",
    "    x = 0.25 + i * 0.1\n",
    "    ax.text(x, 0.1, method, ha='center', va='center', fontsize=8)\n",
    "\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('TD学习方法家族', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n恭喜你完成了时序差分学习教程!\")\n",
    "print(\"下一步建议: 学习Deep Q-Network (DQN)，将TD学习与深度学习结合\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
