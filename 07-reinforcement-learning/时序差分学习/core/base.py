"""
æ—¶åºå·®åˆ†å­¦ä¹ åŸºç±»æ¨¡å— (Base Classes Module)
=========================================

æ ¸å¿ƒæ€æƒ³ (Core Idea):
--------------------
å®šä¹‰TDå­¦ä¹ ç®—æ³•çš„é€šç”¨æ¥å£ã€åè®®å’ŒåŸºç±»ã€‚é‡‡ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼Œ
å°†ç®—æ³•éª¨æ¶å›ºå®šåœ¨åŸºç±»ä¸­ï¼Œå…·ä½“æ›´æ–°é€»è¾‘å»¶è¿Ÿåˆ°å­ç±»å®ç°ã€‚

æ•°å­¦åŸç† (Mathematical Theory):
------------------------------
æ‰€æœ‰TDç®—æ³•å…±äº«çš„æ ¸å¿ƒç»“æ„:
1. ä»·å€¼å‡½æ•°ä¼°è®¡: V(s)æˆ–Q(s,a)çš„è¡¨æ ¼å­˜å‚¨
2. ç­–ç•¥é€‰æ‹©: Îµ-greedyç­–ç•¥å®ç°æ¢ç´¢-åˆ©ç”¨æƒè¡¡
3. TDæ›´æ–°: Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)
4. è®­ç»ƒå¾ªç¯: é‡‡æ ·â†’æ›´æ–°â†’è®°å½•çš„æ ‡å‡†æµç¨‹

è®¾è®¡åŸåˆ™:
--------
- é¢å‘æ¥å£ç¼–ç¨‹: ä½¿ç”¨Protocolå®šä¹‰ç¯å¢ƒå’Œç­–ç•¥æ¥å£
- å¼€é—­åŸåˆ™: å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­
- æ¨¡æ¿æ–¹æ³•: å›ºå®šè®­ç»ƒæµç¨‹ï¼Œå˜åŒ–çš„æ˜¯æ›´æ–°è§„åˆ™

å¤æ‚åº¦ (Complexity):
-------------------
- åŠ¨ä½œé€‰æ‹©: O(|A|) - éœ€è¦éå†æ‰€æœ‰åŠ¨ä½œæ‰¾æœ€å¤§Qå€¼
- å•æ­¥æ›´æ–°: O(1)åˆ°O(|S|) - å–å†³äºå…·ä½“ç®—æ³•
- å­˜å‚¨ç©ºé—´: O(|S|Ã—|A|) - Qè¡¨å­˜å‚¨
"""

from __future__ import annotations

import numpy as np
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import (
    Dict, List, Optional, Tuple, Any,
    Protocol, TypeVar, Generic, runtime_checkable
)
import logging

from .config import TDConfig, TrainingMetrics

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# ç±»å‹å˜é‡
State = TypeVar('State')
Action = TypeVar('Action')


@runtime_checkable
class Environment(Protocol[State, Action]):
    """
    å¼ºåŒ–å­¦ä¹ ç¯å¢ƒåè®®ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    å®šä¹‰ç¯å¢ƒçš„æœ€å°æ¥å£ï¼Œå…¼å®¹OpenAI Gym/Gymnasiumé£æ ¼ã€‚
    ä»»ä½•å®ç°æ­¤åè®®çš„ç±»éƒ½å¯ä»¥ä½œä¸ºTDç®—æ³•çš„è®­ç»ƒç¯å¢ƒã€‚

    æ¥å£è¯´æ˜:
    --------
    - reset(): é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
    - step(action): æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›è½¬ç§»ç»“æœ
    - action_space: åŠ¨ä½œç©ºé—´ï¼Œéœ€è¦æœ‰.nå±æ€§è¡¨ç¤ºåŠ¨ä½œæ•°é‡
    - observation_space: è§‚æµ‹ç©ºé—´

    Example:
        >>> class MyEnv:
        ...     def reset(self): return state, {}
        ...     def step(self, action): return next_state, reward, done, truncated, info
        ...     @property
        ...     def action_space(self): return DiscreteSpace(4)
    """

    def reset(self) -> Tuple[State, Dict[str, Any]]:
        """
        é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€ã€‚

        Returns:
            (åˆå§‹çŠ¶æ€, ä¿¡æ¯å­—å…¸)
        """
        ...

    def step(self, action: Action) -> Tuple[State, float, bool, bool, Dict[str, Any]]:
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè§‚å¯Ÿç¯å¢ƒåé¦ˆã€‚

        Args:
            action: è¦æ‰§è¡Œçš„åŠ¨ä½œ

        Returns:
            (ä¸‹ä¸€çŠ¶æ€, å¥–åŠ±, æ˜¯å¦ç»ˆæ­¢, æ˜¯å¦æˆªæ–­, ä¿¡æ¯å­—å…¸)
        """
        ...

    @property
    def action_space(self) -> Any:
        """åŠ¨ä½œç©ºé—´ï¼Œéœ€è¦æœ‰.nå±æ€§ã€‚"""
        ...

    @property
    def observation_space(self) -> Any:
        """è§‚æµ‹ç©ºé—´ã€‚"""
        ...


@runtime_checkable
class Policy(Protocol[State, Action]):
    """
    ç­–ç•¥åè®®ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    ç­–ç•¥æ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼Œå¯ä»¥æ˜¯ç¡®å®šæ€§çš„æˆ–éšæœºæ€§çš„ã€‚
    æœ¬åè®®å®šä¹‰ç­–ç•¥çš„åŸºæœ¬æ¥å£ã€‚

    æ¥å£è¯´æ˜:
    --------
    - __call__(state): æ ¹æ®çŠ¶æ€è¿”å›åŠ¨ä½œ
    - action_probabilities(state): è¿”å›åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
    """

    def __call__(self, state: State) -> Action:
        """
        æ ¹æ®çŠ¶æ€é€‰æ‹©åŠ¨ä½œã€‚

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        ...

    def action_probabilities(self, state: State) -> Dict[Action, float]:
        """
        è¿”å›çŠ¶æ€ä¸‹å„åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            åŠ¨ä½œåˆ°æ¦‚ç‡çš„æ˜ å°„
        """
        ...


class BaseTDLearner(ABC, Generic[State, Action]):
    """
    æ—¶åºå·®åˆ†å­¦ä¹ ç®—æ³•åŸºç±»ã€‚

    æ ¸å¿ƒæ€æƒ³ (Core Idea):
    --------------------
    æä¾›TDå­¦ä¹ ç®—æ³•çš„é€šç”¨æ¡†æ¶ï¼ŒåŒ…æ‹¬:
    - ä»·å€¼å‡½æ•°ç®¡ç†ï¼ˆVå’ŒQè¡¨ï¼‰
    - Îµ-greedyç­–ç•¥å®ç°
    - æ ‡å‡†è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯
    - æŒ‡æ ‡è®°å½•å’Œæ—¥å¿—

    å­ç±»åªéœ€å®ç°update()æ–¹æ³•å³å¯è·å¾—å®Œæ•´åŠŸèƒ½ã€‚

    æ•°å­¦åŸç† (Mathematical Theory):
    ------------------------------
    Îµ-greedyç­–ç•¥:
        Ï€(a|s) = Îµ/|A| + (1-Îµ)Â·ğŸ™(a = argmax_a' Q(s,a'))

    è¯¥ç­–ç•¥ä»¥æ¦‚ç‡Îµå‡åŒ€éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰ï¼Œ
    ä»¥æ¦‚ç‡1-Îµé€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰ã€‚

    è®¾è®¡æ¨¡å¼:
    --------
    é‡‡ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼(Template Method Pattern):
    - train()å’Œtrain_episode()å®šä¹‰ç®—æ³•éª¨æ¶
    - update()æ˜¯æŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°å…·ä½“æ›´æ–°é€»è¾‘
    - é’©å­æ–¹æ³•on_episode_start/endå¯è¢«å­ç±»è¦†ç›–

    å¤æ‚åº¦ (Complexity):
    -------------------
    - epsilon_greedy_action: O(|A|)
    - greedy_action: O(|A|)
    - train_episode: O(T Ã— update_complexity)ï¼ŒTä¸ºå›åˆé•¿åº¦
    - evaluate: O(N Ã— T)ï¼ŒNä¸ºè¯„ä¼°å›åˆæ•°

    Example:
        >>> class MySARSA(BaseTDLearner):
        ...     def update(self, state, action, reward, next_state, next_action, done):
        ...         # å®ç°SARSAæ›´æ–°
        ...         return td_error
    """

    def __init__(self, config: TDConfig) -> None:
        """
        åˆå§‹åŒ–TDå­¦ä¹ å™¨ã€‚

        Args:
            config: TDå­¦ä¹ é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°
        """
        self.config = config

        # ä»·å€¼å‡½æ•°: ä½¿ç”¨defaultdictè‡ªåŠ¨åˆå§‹åŒ–æœªè§è¿‡çš„çŠ¶æ€
        self._value_function: Dict[State, float] = defaultdict(
            lambda: config.initial_value
        )
        self._q_function: Dict[Tuple[State, Action], float] = defaultdict(
            lambda: config.initial_value
        )

        # è®­ç»ƒæŒ‡æ ‡
        self.metrics = TrainingMetrics()

        # åŠ¨ä½œç©ºé—´ï¼ˆéœ€è¦é€šè¿‡set_action_spaceè®¾ç½®æˆ–è‡ªåŠ¨æ¨æ–­ï¼‰
        self._action_space: Optional[List[Action]] = None

    # =========================================================================
    # å±æ€§è®¿é—®å™¨
    # =========================================================================

    @property
    def value_function(self) -> Dict[State, float]:
        """
        è·å–çŠ¶æ€ä»·å€¼å‡½æ•°V(s)çš„å‰¯æœ¬ã€‚

        Returns:
            çŠ¶æ€åˆ°ä»·å€¼çš„æ˜ å°„å­—å…¸
        """
        return dict(self._value_function)

    @property
    def q_function(self) -> Dict[Tuple[State, Action], float]:
        """
        è·å–åŠ¨ä½œä»·å€¼å‡½æ•°Q(s,a)çš„å‰¯æœ¬ã€‚

        Returns:
            (çŠ¶æ€,åŠ¨ä½œ)å¯¹åˆ°ä»·å€¼çš„æ˜ å°„å­—å…¸
        """
        return dict(self._q_function)

    def get_value(self, state: State) -> float:
        """è·å–çŠ¶æ€ä»·å€¼V(s)ã€‚"""
        return self._value_function[state]

    def get_q_value(self, state: State, action: Action) -> float:
        """è·å–åŠ¨ä½œä»·å€¼Q(s,a)ã€‚"""
        return self._q_function[(state, action)]

    def set_action_space(self, actions: List[Action]) -> None:
        """
        è®¾ç½®åŠ¨ä½œç©ºé—´ã€‚

        Args:
            actions: æ‰€æœ‰å¯ç”¨åŠ¨ä½œçš„åˆ—è¡¨
        """
        self._action_space = actions

    # =========================================================================
    # ç­–ç•¥æ–¹æ³•
    # =========================================================================

    def epsilon_greedy_action(self, state: State) -> Action:
        """
        Îµ-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œã€‚

        æ•°å­¦åŸç† (Mathematical Theory):
        ------------------------------
        Ï€(a|s) = Îµ/|A| + (1-Îµ)Â·ğŸ™(a = argmax_a' Q(s,a'))

        ä»¥æ¦‚ç‡Îµéšæœºé€‰æ‹©åŠ¨ä½œå®ç°æ¢ç´¢ï¼Œä»¥æ¦‚ç‡1-Îµé€‰æ‹©å½“å‰ä¼°è®¡çš„
        æœ€ä¼˜åŠ¨ä½œå®ç°åˆ©ç”¨ã€‚å½“å­˜åœ¨å¤šä¸ªåŠ¨ä½œQå€¼ç›¸ç­‰æ—¶ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªã€‚

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ

        Raises:
            ValueError: æœªè®¾ç½®åŠ¨ä½œç©ºé—´

        å¤æ‚åº¦: O(|A|)
        """
        if self._action_space is None:
            raise ValueError(
                "æœªè®¾ç½®åŠ¨ä½œç©ºé—´ã€‚è¯·è°ƒç”¨set_action_space()æˆ–ä½¿ç”¨å¸¦action_space.nçš„ç¯å¢ƒã€‚"
            )

        # ä»¥Îµæ¦‚ç‡éšæœºæ¢ç´¢
        if np.random.random() < self.config.epsilon:
            return np.random.choice(self._action_space)

        # ä»¥1-Îµæ¦‚ç‡è´ªå©ªé€‰æ‹©
        q_values = [self.get_q_value(state, a) for a in self._action_space]
        max_q = max(q_values)

        # å¤„ç†å¤šä¸ªæœ€ä¼˜åŠ¨ä½œçš„æƒ…å†µï¼ˆéšæœºæ‰“ç ´å¹³å±€ï¼‰
        best_actions = [
            a for a, q in zip(self._action_space, q_values)
            if np.isclose(q, max_q)
        ]
        return np.random.choice(best_actions)

    def greedy_action(self, state: State) -> Action:
        """
        è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼ˆç”¨äºè¯„ä¼°ï¼‰ã€‚

        çº¯è´ªå©ªç­–ç•¥ï¼Œæ€»æ˜¯é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œã€‚

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            æœ€ä¼˜åŠ¨ä½œ

        å¤æ‚åº¦: O(|A|)
        """
        if self._action_space is None:
            raise ValueError("æœªè®¾ç½®åŠ¨ä½œç©ºé—´")

        q_values = [self.get_q_value(state, a) for a in self._action_space]
        max_q = max(q_values)
        best_actions = [
            a for a, q in zip(self._action_space, q_values)
            if np.isclose(q, max_q)
        ]
        return np.random.choice(best_actions)

    def get_policy(self) -> Dict[State, Action]:
        """
        ä»Qå‡½æ•°æå–è´ªå©ªç­–ç•¥ã€‚

        Returns:
            çŠ¶æ€åˆ°æœ€ä¼˜åŠ¨ä½œçš„æ˜ å°„
        """
        policy = {}
        states = set(s for (s, _) in self._q_function.keys())

        for state in states:
            policy[state] = self.greedy_action(state)

        return policy

    # =========================================================================
    # æ ¸å¿ƒæŠ½è±¡æ–¹æ³•
    # =========================================================================

    @abstractmethod
    def update(
        self,
        state: State,
        action: Action,
        reward: float,
        next_state: State,
        next_action: Optional[Action],
        done: bool
    ) -> float:
        """
        æ‰§è¡ŒTDæ›´æ–°æ­¥éª¤ã€‚

        è¿™æ˜¯æ¨¡æ¿æ–¹æ³•æ¨¡å¼ä¸­çš„æŠ½è±¡æ–¹æ³•ï¼Œå¿…é¡»ç”±å­ç±»å®ç°ã€‚
        ä¸åŒçš„TDç®—æ³•ï¼ˆSARSAã€Q-Learningç­‰ï¼‰çš„æ ¸å¿ƒåŒºåˆ«å°±åœ¨äºæ­¤æ–¹æ³•ã€‚

        Args:
            state: å½“å‰çŠ¶æ€ S_t
            action: æ‰§è¡Œçš„åŠ¨ä½œ A_t
            reward: è·å¾—çš„å³æ—¶å¥–åŠ± R_{t+1}
            next_state: ä¸‹ä¸€çŠ¶æ€ S_{t+1}
            next_action: ä¸‹ä¸€åŠ¨ä½œ A_{t+1}ï¼ˆSARSAéœ€è¦ï¼ŒQ-Learningä¸ç”¨ï¼‰
            done: æ˜¯å¦ç»ˆæ­¢

        Returns:
            TDè¯¯å·® Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)
        """
        pass

    # =========================================================================
    # é’©å­æ–¹æ³•ï¼ˆå¯è¢«å­ç±»è¦†ç›–ï¼‰
    # =========================================================================

    def on_episode_start(self, episode: int) -> None:
        """
        å›åˆå¼€å§‹æ—¶çš„é’©å­æ–¹æ³•ã€‚

        å­ç±»å¯è¦†ç›–æ­¤æ–¹æ³•æ¥æ‰§è¡Œå›åˆå¼€å§‹æ—¶çš„åˆå§‹åŒ–ï¼Œ
        ä¾‹å¦‚æ¸…ç©ºèµ„æ ¼è¿¹ã€‚

        Args:
            episode: å½“å‰å›åˆç¼–å·
        """
        pass

    def on_episode_end(self, episode: int, reward: float, length: int) -> None:
        """
        å›åˆç»“æŸæ—¶çš„é’©å­æ–¹æ³•ã€‚

        Args:
            episode: å½“å‰å›åˆç¼–å·
            reward: å›åˆç´¯ç§¯å¥–åŠ±
            length: å›åˆæ­¥æ•°
        """
        pass

    # =========================================================================
    # è®­ç»ƒæ–¹æ³•
    # =========================================================================

    def train_episode(
        self,
        env: Environment[State, Action],
        max_steps: int = 10000
    ) -> Tuple[float, int]:
        """
        è®­ç»ƒä¸€ä¸ªå®Œæ•´å›åˆã€‚

        å®ç°æ ‡å‡†çš„TDæ§åˆ¶è®­ç»ƒå¾ªç¯:
        1. é‡ç½®ç¯å¢ƒè·å–åˆå§‹çŠ¶æ€
        2. é€‰æ‹©åˆå§‹åŠ¨ä½œ
        3. å¾ªç¯: æ‰§è¡ŒåŠ¨ä½œâ†’è·å–åé¦ˆâ†’æ›´æ–°Qå€¼â†’é€‰æ‹©ä¸‹ä¸€åŠ¨ä½œ
        4. è®°å½•æŒ‡æ ‡

        Args:
            env: è®­ç»ƒç¯å¢ƒ
            max_steps: å•å›åˆæœ€å¤§æ­¥æ•°é™åˆ¶

        Returns:
            (å›åˆç´¯ç§¯å¥–åŠ±, å›åˆæ­¥æ•°)

        å¤æ‚åº¦: O(T Ã— update_complexity)ï¼ŒTä¸ºå®é™…æ­¥æ•°
        """
        state, _ = env.reset()
        action = self.epsilon_greedy_action(state)

        total_reward = 0.0
        td_errors = []

        for step in range(max_steps):
            # æ‰§è¡ŒåŠ¨ä½œï¼Œè§‚å¯Ÿç¯å¢ƒåé¦ˆ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # é€‰æ‹©ä¸‹ä¸€åŠ¨ä½œï¼ˆSARSAéœ€è¦ï¼ŒQ-Learningå¯å¿½ç•¥ï¼‰
            next_action = None if done else self.epsilon_greedy_action(next_state)

            # æ‰§è¡ŒTDæ›´æ–°ï¼ˆç”±å­ç±»å®ç°ï¼‰
            td_error = self.update(
                state, action, reward, next_state, next_action, done
            )
            td_errors.append(abs(td_error))

            total_reward += reward

            if done:
                break

            # çŠ¶æ€è½¬ç§»
            state = next_state
            action = next_action

        steps = step + 1
        avg_td_error = np.mean(td_errors) if td_errors else 0.0
        self.metrics.add_episode(total_reward, steps, avg_td_error)

        return total_reward, steps

    def train(
        self,
        env: Environment[State, Action],
        n_episodes: int = 1000,
        max_steps_per_episode: int = 10000,
        log_interval: int = 100,
        early_stop_reward: Optional[float] = None
    ) -> TrainingMetrics:
        """
        æ‰§è¡Œå®Œæ•´è®­ç»ƒè¿‡ç¨‹ã€‚

        Args:
            env: è®­ç»ƒç¯å¢ƒ
            n_episodes: è®­ç»ƒå›åˆæ•°
            max_steps_per_episode: æ¯å›åˆæœ€å¤§æ­¥æ•°
            log_interval: æ—¥å¿—è¾“å‡ºé—´éš”ï¼ˆå›åˆæ•°ï¼‰
            early_stop_reward: æ—©åœé˜ˆå€¼ï¼Œè¾¾åˆ°æ­¤å¹³å‡å¥–åŠ±ååœæ­¢

        Returns:
            è®­ç»ƒæŒ‡æ ‡å¯¹è±¡

        Example:
            >>> metrics = learner.train(env, n_episodes=500, log_interval=100)
            >>> print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(metrics.episode_rewards[-100:]):.2f}")
        """
        # è‡ªåŠ¨æ¨æ–­åŠ¨ä½œç©ºé—´
        if self._action_space is None:
            if hasattr(env.action_space, 'n'):
                self.set_action_space(list(range(env.action_space.n)))
            else:
                raise ValueError(
                    "æ— æ³•è‡ªåŠ¨æ¨æ–­åŠ¨ä½œç©ºé—´ã€‚è¯·æ‰‹åŠ¨è°ƒç”¨set_action_space()ã€‚"
                )

        for episode in range(n_episodes):
            self.on_episode_start(episode)

            reward, steps = self.train_episode(env, max_steps_per_episode)

            self.on_episode_end(episode, reward, steps)

            # æ—¥å¿—è¾“å‡º
            if (episode + 1) % log_interval == 0:
                recent_rewards = self.metrics.episode_rewards[-log_interval:]
                avg_reward = np.mean(recent_rewards)
                logger.info(
                    f"Episode {episode + 1}/{n_episodes} | "
                    f"Avg Reward: {avg_reward:.2f} | "
                    f"Last Reward: {reward:.2f} | "
                    f"Steps: {steps}"
                )

            # æ—©åœæ£€æŸ¥
            if early_stop_reward is not None:
                if len(self.metrics.episode_rewards) >= 100:
                    recent_avg = np.mean(self.metrics.episode_rewards[-100:])
                    if recent_avg >= early_stop_reward:
                        logger.info(
                            f"æ—©åœ: å¹³å‡å¥–åŠ± {recent_avg:.2f} >= {early_stop_reward}"
                        )
                        break

        return self.metrics

    def evaluate(
        self,
        env: Environment[State, Action],
        n_episodes: int = 100,
        max_steps: int = 10000
    ) -> Tuple[float, float]:
        """
        è¯„ä¼°å½“å‰ç­–ç•¥çš„æ€§èƒ½ã€‚

        ä½¿ç”¨çº¯è´ªå©ªç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰è¿è¡Œå¤šä¸ªå›åˆï¼Œç»Ÿè®¡æ€§èƒ½ã€‚

        Args:
            env: è¯„ä¼°ç¯å¢ƒ
            n_episodes: è¯„ä¼°å›åˆæ•°
            max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°

        Returns:
            (å¹³å‡å¥–åŠ±, å¥–åŠ±æ ‡å‡†å·®)

        Example:
            >>> mean_reward, std_reward = learner.evaluate(env)
            >>> print(f"è¯„ä¼°ç»“æœ: {mean_reward:.2f} Â± {std_reward:.2f}")
        """
        rewards = []

        for _ in range(n_episodes):
            state, _ = env.reset()
            total_reward = 0.0

            for _ in range(max_steps):
                action = self.greedy_action(state)
                state, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward

                if terminated or truncated:
                    break

            rewards.append(total_reward)

        return float(np.mean(rewards)), float(np.std(rewards))

    def reset(self) -> None:
        """
        é‡ç½®å­¦ä¹ å™¨çŠ¶æ€ã€‚

        æ¸…ç©ºQå‡½æ•°ã€Vå‡½æ•°å’Œè®­ç»ƒæŒ‡æ ‡ï¼Œç”¨äºé‡æ–°å¼€å§‹è®­ç»ƒã€‚
        """
        self._value_function.clear()
        self._q_function.clear()
        self.metrics.clear()
