"""
æ—¶åºå·®åˆ†å­¦ä¹ æ¨¡å— (Temporal Difference Learning Module)
=====================================================

æ ¸å¿ƒæ€æƒ³ (Core Idea):
--------------------
æ—¶åºå·®åˆ†å­¦ä¹ æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ–¹æ³•ä¹‹ä¸€ï¼Œç»“åˆäº†åŠ¨æ€è§„åˆ’çš„è‡ªä¸¾æ€æƒ³
å’Œè’™ç‰¹å¡æ´›æ–¹æ³•çš„é‡‡æ ·æ€æƒ³ã€‚TDæ–¹æ³•å¯ä»¥ä»ä¸å®Œæ•´çš„å›åˆä¸­å­¦ä¹ ï¼Œ
å¹¶ä¸”ä¸éœ€è¦ç¯å¢ƒæ¨¡å‹ã€‚

æœ¬æ¨¡å—æä¾›å®Œæ•´çš„æ—¶åºå·®åˆ†å­¦ä¹ ç®—æ³•å®ç°ã€‚

æ•°å­¦åŸç† (Mathematical Theory):
------------------------------
TDå­¦ä¹ çš„æ ¸å¿ƒæ˜¯TDè¯¯å·®:
    Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)

æ›´æ–°è§„åˆ™:
    V(S_t) â† V(S_t) + Î± Ã— Î´_t

TD(Î»)å¼•å…¥èµ„æ ¼è¿¹ï¼Œå®ç°å¤šæ­¥å›æº¯:
    e_t(s) = Î³Î»e_{t-1}(s) + ğŸ™[S_t = s]
    V(s) â† V(s) + Î± Ã— Î´_t Ã— e_t(s)

æ ¸å¿ƒç®—æ³• (Core Algorithms):
--------------------------
- TD(0): å•æ­¥TDé¢„æµ‹ï¼Œä½¿ç”¨ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼ä¼°è®¡ä½œä¸ºç›®æ ‡
- SARSA: On-Policy TDæ§åˆ¶ï¼Œä½¿ç”¨å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ›´æ–°
- Q-Learning: Off-Policy TDæ§åˆ¶ï¼Œä½¿ç”¨max Qå€¼æ›´æ–°
- Expected SARSA: ä½¿ç”¨æœŸæœ›Qå€¼æ›´æ–°ï¼Œå‡å°‘æ–¹å·®
- Double Q-Learning: ä½¿ç”¨ä¸¤ä¸ªQè¡¨æ¶ˆé™¤æœ€å¤§åŒ–åå·®
- N-Step TD: ä½¿ç”¨næ­¥å›æŠ¥ä½œä¸ºç›®æ ‡
- TD(Î»): ä½¿ç”¨èµ„æ ¼è¿¹å®ç°å¤šæ­¥å›æº¯
- SARSA(Î»): å¸¦èµ„æ ¼è¿¹çš„SARSA
- Watkins Q(Î»): å¸¦èµ„æ ¼è¿¹çš„Q-Learningï¼ˆéè´ªå©ªæ—¶åˆ‡æ–­è¿¹ï¼‰

ç¯å¢ƒ (Environments):
-------------------
- RandomWalk: TDé¢„æµ‹çš„æ ‡å‡†æµ‹è¯•åºŠ
- CliffWalking: On/Off-Policyå¯¹æ¯”çš„ç»å…¸ç¯å¢ƒ
- WindyGridWorld: æµ‹è¯•æ™ºèƒ½ä½“åº”å¯¹ç¯å¢ƒåŠ¨æ€çš„èƒ½åŠ›
- GridWorld: å¯é…ç½®çš„é€šç”¨ç½‘æ ¼ä¸–ç•Œ
- Blackjack: MCå’ŒTDæ–¹æ³•çš„æµ‹è¯•ç¯å¢ƒ

ç®—æ³•å¯¹æ¯” (Comparison):
---------------------
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ç®—æ³•       â”‚   æ›´æ–°ç±»å‹ â”‚  åå·®/æ–¹å·® â”‚   é€‚ç”¨åœºæ™¯    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   TD(0)         â”‚  On-line   â”‚  ä½æ–¹å·®    â”‚   å¿«é€Ÿå­¦ä¹     â”‚
â”‚   SARSA         â”‚  On-Policy â”‚  ç¨³å®š      â”‚   å®‰å…¨å¯¼èˆª    â”‚
â”‚   Q-Learning    â”‚  Off-Policyâ”‚  æœ€ä¼˜æ€§    â”‚   æœ€ä¼˜ç­–ç•¥    â”‚
â”‚   Expected SARSAâ”‚  On-Policy â”‚  æ›´ä½æ–¹å·®  â”‚   ä¸­é—´é€‰æ‹©    â”‚
â”‚   TD(Î»)         â”‚  å¤šæ­¥å›æº¯  â”‚  å¯è°ƒ      â”‚   ç¨€ç–å¥–åŠ±    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½¿ç”¨ç¤ºä¾‹ (Example):
-----------------
>>> from æ—¶åºå·®åˆ†å­¦ä¹  import SARSA, TDConfig, CliffWalkingEnv
>>>
>>> # åˆ›å»ºç¯å¢ƒå’Œå­¦ä¹ å™¨
>>> env = CliffWalkingEnv()
>>> config = TDConfig(alpha=0.5, gamma=1.0, epsilon=0.1)
>>> learner = SARSA(config)
>>>
>>> # è®­ç»ƒ
>>> metrics = learner.train(env, n_episodes=500)
>>>
>>> # è¯„ä¼°
>>> mean_reward, std_reward = learner.evaluate(env, n_episodes=100)
>>> print(f"å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")

æ¨¡å—ç»“æ„ (Module Structure):
---------------------------
```
æ—¶åºå·®åˆ†å­¦ä¹ /
â”œâ”€â”€ core/           # æ ¸å¿ƒç®—æ³•å®ç°
â”‚   â”œâ”€â”€ config.py       # é…ç½®ç±»
â”‚   â”œâ”€â”€ base.py         # åŸºç±»
â”‚   â”œâ”€â”€ td_prediction.py # TDé¢„æµ‹
â”‚   â”œâ”€â”€ td_control.py    # TDæ§åˆ¶
â”‚   â”œâ”€â”€ advanced.py      # é«˜çº§ç®—æ³•
â”‚   â””â”€â”€ factory.py       # å·¥å‚å‡½æ•°
â”œâ”€â”€ environments/   # å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
â”‚   â”œâ”€â”€ base.py         # åŸºç¡€ç»„ä»¶
â”‚   â”œâ”€â”€ grid_world.py   # ç½‘æ ¼ä¸–ç•Œ
â”‚   â”œâ”€â”€ cliff_walking.py # æ‚¬å´–è¡Œèµ°
â”‚   â”œâ”€â”€ windy_grid.py   # æœ‰é£ç½‘æ ¼
â”‚   â”œâ”€â”€ random_walk.py  # éšæœºæ¸¸èµ°
â”‚   â””â”€â”€ blackjack.py    # äºŒåä¸€ç‚¹
â”œâ”€â”€ utils/          # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ visualization.py # å¯è§†åŒ–
â”‚   â”œâ”€â”€ experiment.py    # å®éªŒç®¡ç†
â”‚   â”œâ”€â”€ analysis.py      # åˆ†æå·¥å…·
â”‚   â””â”€â”€ serialization.py # åºåˆ—åŒ–
â”œâ”€â”€ networks/       # ç¥ç»ç½‘ç»œç»„ä»¶
â”œâ”€â”€ tests/          # å•å…ƒæµ‹è¯•
â””â”€â”€ notebooks/      # Jupyteræ•™ç¨‹
```

å‚è€ƒæ–‡çŒ® (References):
--------------------
1. Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning:
   An Introduction (2nd ed.). MIT Press.
2. Watkins, C. J. C. H. (1989). Learning from Delayed Rewards.
   PhD thesis, Cambridge University.
3. Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning
   using connectionist systems. Technical Report CUED/F-INFENG/TR 166.
"""

from .core import (
    # é…ç½®ç±»
    TDConfig,
    TrainingMetrics,
    EligibilityTraceType,

    # åŸºç±»
    BaseTDLearner,

    # æ ¸å¿ƒç®—æ³•
    TD0ValueLearner,
    SARSA,
    ExpectedSARSA,
    QLearning,
    DoubleQLearning,
    NStepTD,
    TDLambda,
    SARSALambda,
    WatkinsQLambda,

    # å·¥å‚å‡½æ•°
    create_td_learner,
)

from environments import (
    # ç¯å¢ƒ
    GridWorld,
    GridWorldConfig,
    CliffWalkingEnv,
    WindyGridWorld,
    RandomWalk,
    Blackjack,

    # åŠ¨ä½œæšä¸¾
    Action,

    # ç©ºé—´
    DiscreteSpace,
)

from utils import (
    # å¯è§†åŒ–
    plot_training_curves,
    plot_value_heatmap,
    plot_q_value_heatmap,
    plot_policy_arrows,
    plot_td_error_analysis,
    plot_lambda_comparison,
    visualize_cliff_walking_path,

    # å®éªŒç®¡ç†
    ExperimentConfig,
    ExperimentResult,
    run_multi_seed_experiment,
    plot_multi_seed_comparison,

    # åˆ†æå·¥å…·
    compute_rmse,
    extract_greedy_policy,
    compute_state_visitation,
    detect_convergence,

    # åºåˆ—åŒ–
    save_q_function,
    load_q_function,
    save_experiment_results,
)

__version__ = "1.0.0"
__author__ = "AI-Practices"
__all__ = [
    # é…ç½®
    "TDConfig",
    "TrainingMetrics",
    "EligibilityTraceType",

    # ç®—æ³•
    "BaseTDLearner",
    "TD0ValueLearner",
    "SARSA",
    "ExpectedSARSA",
    "QLearning",
    "DoubleQLearning",
    "NStepTD",
    "TDLambda",
    "SARSALambda",
    "WatkinsQLambda",
    "create_td_learner",

    # ç¯å¢ƒ
    "GridWorld",
    "GridWorldConfig",
    "CliffWalkingEnv",
    "WindyGridWorld",
    "RandomWalk",
    "Blackjack",
    "Action",
    "DiscreteSpace",

    # å¯è§†åŒ–
    "plot_training_curves",
    "plot_value_heatmap",
    "plot_q_value_heatmap",
    "plot_policy_arrows",
    "plot_td_error_analysis",
    "plot_lambda_comparison",
    "visualize_cliff_walking_path",

    # å®éªŒç®¡ç†
    "ExperimentConfig",
    "ExperimentResult",
    "run_multi_seed_experiment",
    "plot_multi_seed_comparison",

    # åˆ†æ
    "compute_rmse",
    "extract_greedy_policy",
    "compute_state_visitation",
    "detect_convergence",

    # åºåˆ—åŒ–
    "save_q_function",
    "load_q_function",
    "save_experiment_results",
]
