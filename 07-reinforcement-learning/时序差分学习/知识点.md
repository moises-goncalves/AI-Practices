# 时序差分学习核心知识点

## 目录

1. [TD学习基础](#1-td学习基础)
2. [TD预测方法](#2-td预测方法)
3. [TD控制方法](#3-td控制方法)
4. [资格迹方法](#4-资格迹方法)
5. [算法对比分析](#5-算法对比分析)
6. [关键数学公式](#6-关键数学公式)
7. [实现细节](#7-实现细节)
8. [常见问题](#8-常见问题)

---

## 1. TD学习基础

### 1.1 什么是时序差分学习？

时序差分(Temporal Difference, TD)学习是强化学习的核心方法之一，结合了：
- **动态规划**的自举(bootstrapping)思想：使用估计值更新估计值
- **蒙特卡洛**的采样思想：从经验中学习，不需要环境模型

### 1.2 核心特点

| 特性 | TD学习 | 蒙特卡洛 | 动态规划 |
|------|--------|----------|----------|
| 需要模型 | 否 | 否 | 是 |
| 自举 | 是 | 否 | 是 |
| 在线学习 | 是 | 否 | - |
| 偏差 | 有偏 | 无偏 | - |
| 方差 | 低 | 高 | - |

### 1.3 TD误差

TD学习的核心是**TD误差**(TD Error)：

$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$$

TD误差表示"实际获得的回报"与"预期回报"之间的差异。

---

## 2. TD预测方法

### 2.1 TD(0)算法

**核心思想**：使用单步回报估计状态价值。

**更新规则**：
$$V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$$

**特点**：
- 单步更新，效率高
- 有偏但一致
- 低方差，稳定

### 2.2 N步TD

**核心思想**：使用n步回报作为目标。

**n步回报**：
$$G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})$$

**更新规则**：
$$V(S_t) \leftarrow V(S_t) + \alpha [G_t^{(n)} - V(S_t)]$$

**n的选择**：
- n=1: TD(0)，最低方差，最高偏差
- n=∞: MC，最高方差，无偏差
- 中间值通常表现最好

---

## 3. TD控制方法

### 3.1 SARSA (On-Policy)

**名称来源**：State-Action-Reward-State-Action

**更新规则**：
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$

**特点**：
- On-Policy：学习的是行为策略
- 考虑探索的影响
- 在有风险的环境中更安全

### 3.2 Q-Learning (Off-Policy)

**更新规则**：
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$

**特点**：
- Off-Policy：学习最优策略，无论行为策略
- 可能过估计Q值（最大化偏差）
- 学习最优但可能危险的策略

### 3.3 Expected SARSA

**更新规则**：
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \mathbb{E}_\pi[Q(S_{t+1}, A)] - Q(S_t, A_t)]$$

**期望计算**（ε-greedy策略）：
$$\mathbb{E}_\pi[Q(S', A)] = (1-\epsilon) \max_a Q(S', a) + \frac{\epsilon}{|A|} \sum_a Q(S', a)$$

**特点**：
- 介于SARSA和Q-Learning之间
- 方差更低
- 可以同时是On-Policy和Off-Policy

### 3.4 Double Q-Learning

**核心思想**：使用两个Q表消除最大化偏差。

**更新规则**（50%概率选择）：
$$Q_A(S, A) \leftarrow Q_A(S, A) + \alpha [R + \gamma Q_B(S', \arg\max_a Q_A(S', a)) - Q_A(S, A)]$$

**动作选择**：
$$A = \arg\max_a [Q_A(S, a) + Q_B(S, a)]$$

**特点**：
- 解决过估计问题
- 收敛更稳定
- 内存需求翻倍

---

## 4. 资格迹方法

### 4.1 TD(λ)

**核心思想**：通过资格迹实现多步回溯，统一TD(0)和MC。

**资格迹更新**：
$$e_t(s) = \gamma \lambda e_{t-1}(s) + \mathbf{1}[S_t = s]$$

**价值更新**：
$$V(s) \leftarrow V(s) + \alpha \delta_t e_t(s)$$

### 4.2 资格迹类型

| 类型 | 公式 | 特点 |
|------|------|------|
| 累积迹 | $e_t = \gamma\lambda e_{t-1} + \nabla V$ | 允许累积，可能爆炸 |
| 替换迹 | $e_t = \max(\gamma\lambda e_{t-1}, 1)$ | 上限为1，更稳定 |
| 荷兰迹 | $e_t = \gamma\lambda e_{t-1}(1-\alpha) + 1$ | 介于两者之间 |

### 4.3 λ的选择

- **λ=0**：等价于TD(0)，只用单步信息
- **λ=1**：等价于MC（在线版本）
- **最优λ**：通常在0.8-0.95之间

---

## 5. 算法对比分析

### 5.1 On-Policy vs Off-Policy

| 方面 | On-Policy (SARSA) | Off-Policy (Q-Learning) |
|------|-------------------|-------------------------|
| 学习目标 | 当前策略 | 最优策略 |
| 探索影响 | 考虑 | 不考虑 |
| 安全性 | 高 | 低 |
| 训练奖励 | 高 | 低 |
| 评估奖励 | 低 | 高 |
| 典型应用 | 安全导航 | 最优规划 |

### 5.2 CliffWalking实验结论

在悬崖行走环境中：
- **SARSA**：学习远离悬崖的安全路径
- **Q-Learning**：学习沿悬崖边缘的最短路径

这是因为：
- SARSA知道自己会探索（可能失误）
- Q-Learning假设后续都是贪婪动作

### 5.3 最大化偏差问题

**问题**：Q-Learning使用max操作，会导致过估计。

**原因**：
$$\mathbb{E}[\max_a Q(s, a)] \geq \max_a \mathbb{E}[Q(s, a)]$$

**解决方案**：Double Q-Learning，用一个表选动作，另一个表估计值。

---

## 6. 关键数学公式

### 6.1 Bellman方程

**状态价值**：
$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V^\pi(s')]$$

**动作价值**：
$$Q^\pi(s,a) = \sum_{s',r} p(s',r|s,a)[r + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')]$$

### 6.2 TD目标

| 方法 | TD目标 |
|------|--------|
| TD(0) | $R_{t+1} + \gamma V(S_{t+1})$ |
| SARSA | $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$ |
| Q-Learning | $R_{t+1} + \gamma \max_a Q(S_{t+1}, a)$ |
| Expected SARSA | $R_{t+1} + \gamma \mathbb{E}[Q(S_{t+1}, \cdot)]$ |

### 6.3 收敛条件

Robbins-Monro条件：
$$\sum_{t=1}^{\infty} \alpha_t = \infty, \quad \sum_{t=1}^{\infty} \alpha_t^2 < \infty$$

常用选择：$\alpha_t = 1/t$ 或固定小α

---

## 7. 实现细节

### 7.1 ε-greedy探索

```python
def epsilon_greedy_action(state, q_table, epsilon, n_actions):
    if np.random.random() < epsilon:
        return np.random.randint(n_actions)  # 探索
    return np.argmax([q_table[(state, a)] for a in range(n_actions)])  # 利用
```

### 7.2 学习率选择

| 场景 | 建议α值 |
|------|---------|
| 确定性环境 | 0.1 - 0.5 |
| 随机环境 | 0.01 - 0.1 |
| 资格迹方法 | 较小值 |

### 7.3 初始化策略

- **乐观初始化**：较高初始Q值，鼓励探索
- **悲观初始化**：较低初始Q值，更保守
- **零初始化**：简单但可能导致偏好问题

---

## 8. 常见问题

### Q1: 为什么TD比MC收敛更快？

**答**：TD的低方差使得更新更稳定。虽然有偏，但偏差会随着学习逐渐减小。

### Q2: 什么时候用SARSA，什么时候用Q-Learning？

**答**：
- 安全重要的场景：SARSA（考虑探索风险）
- 需要最优策略：Q-Learning（假设最优执行）

### Q3: 如何选择λ？

**答**：
- 通过交叉验证
- 通常0.8-0.95效果好
- 稀疏奖励用较大λ
- 密集奖励用较小λ

### Q4: 如何处理连续状态空间？

**答**：
1. **离散化**：分块编码(Tile Coding)
2. **函数逼近**：线性函数或神经网络
3. **傅里叶基**：适合光滑价值函数

### Q5: 资格迹会导致什么问题？

**答**：
- **迹爆炸**：使用替换迹或荷兰迹
- **方差增大**：减小λ
- **收敛变慢**：适当调整α

---

## 参考文献

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.

2. Watkins, C. J. C. H. (1989). *Learning from Delayed Rewards*. PhD thesis, Cambridge University.

3. Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems.

4. van Hasselt, H. (2010). Double Q-learning. *Advances in Neural Information Processing Systems*.

5. Singh, S., et al. (2000). Convergence results for single-step on-policy reinforcement-learning algorithms. *Machine Learning*.
