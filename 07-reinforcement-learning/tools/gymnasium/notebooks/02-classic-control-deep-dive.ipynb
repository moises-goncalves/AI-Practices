{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经典控制环境深度解析\n",
    "\n",
    "---\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "经典控制环境是一组基于**控制论经典问题**设计的低维环境。它们具有简单的状态空间和明确的物理意义，是验证强化学习算法的理想测试平台。\n",
    "\n",
    "## 本节内容\n",
    "\n",
    "1. **CartPole**: 倒立摆平衡 - 欠驱动系统控制\n",
    "2. **MountainCar**: 爬山车 - 稀疏奖励与探索\n",
    "3. **Acrobot**: 双摆 - 欠驱动摆动控制\n",
    "4. **Pendulum**: 单摆 - 连续控制入门\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle, FancyBboxPatch\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, display\n",
    "from collections import deque\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"Gymnasium 版本: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. CartPole - 倒立摆平衡\n",
    "\n",
    "### 物理系统\n",
    "\n",
    "CartPole 是一个**欠驱动系统**：通过移动小车来平衡倒立的摆杆。\n",
    "\n",
    "### 运动方程\n",
    "\n",
    "小车-摆杆系统的运动方程（拉格朗日力学推导）：\n",
    "\n",
    "$$\\ddot{\\theta} = \\frac{g\\sin\\theta + \\cos\\theta \\cdot \\frac{-F - m_p l \\dot{\\theta}^2 \\sin\\theta}{m_c + m_p}}{l\\left(\\frac{4}{3} - \\frac{m_p \\cos^2\\theta}{m_c + m_p}\\right)}$$\n",
    "\n",
    "$$\\ddot{x} = \\frac{F + m_p l (\\dot{\\theta}^2 \\sin\\theta - \\ddot{\\theta}\\cos\\theta)}{m_c + m_p}$$\n",
    "\n",
    "其中：\n",
    "- $x$: 小车位置\n",
    "- $\\theta$: 摆杆与垂直方向的夹角\n",
    "- $F$: 施加在小车上的力 (±10N)\n",
    "- $m_c = 1.0$ kg: 小车质量\n",
    "- $m_p = 0.1$ kg: 摆杆质量\n",
    "- $l = 0.5$ m: 摆杆半长\n",
    "- $g = 9.8$ m/s²: 重力加速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole 环境详解\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CartPole-v1 环境详解\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 状态空间\n",
    "print(\"\\n【状态空间】\")\n",
    "state_vars = [\n",
    "    (\"小车位置 x\", env.observation_space.low[0], env.observation_space.high[0], \"m\"),\n",
    "    (\"小车速度 ẋ\", env.observation_space.low[1], env.observation_space.high[1], \"m/s\"),\n",
    "    (\"摆杆角度 θ\", env.observation_space.low[2], env.observation_space.high[2], \"rad\"),\n",
    "    (\"摆杆角速度 θ̇\", env.observation_space.low[3], env.observation_space.high[3], \"rad/s\"),\n",
    "]\n",
    "print(f\"{'变量':<15} {'下界':>15} {'上界':>15} {'单位':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for name, low, high, unit in state_vars:\n",
    "    print(f\"{name:<15} {low:>15.2f} {high:>15.2f} {unit:>10}\")\n",
    "\n",
    "# 动作空间\n",
    "print(\"\\n【动作空间】\")\n",
    "print(f\"类型: Discrete(2)\")\n",
    "print(f\"  0: 向左施加 -10N 的力\")\n",
    "print(f\"  1: 向右施加 +10N 的力\")\n",
    "\n",
    "# 终止条件\n",
    "print(\"\\n【终止条件】\")\n",
    "print(f\"  1. |θ| > 12° (约 0.2095 rad)\")\n",
    "print(f\"  2. |x| > 2.4 m\")\n",
    "print(f\"  3. 步数 > 500 (截断)\")\n",
    "\n",
    "# 奖励\n",
    "print(\"\\n【奖励设计】\")\n",
    "print(f\"  每存活一步: +1\")\n",
    "print(f\"  目标: 累积奖励 ≥ 475\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 CartPole 系统\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 左图: 系统示意图\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(-3, 3)\n",
    "ax1.set_ylim(-0.5, 3)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# 地面\n",
    "ax1.axhline(y=0, color='brown', linewidth=4)\n",
    "ax1.fill_between([-3, 3], [-0.3], [0], color='brown', alpha=0.3)\n",
    "\n",
    "# 小车\n",
    "cart = FancyBboxPatch((-0.4, 0.05), 0.8, 0.3, boxstyle=\"round,pad=0.02\",\n",
    "                       facecolor='steelblue', edgecolor='navy', linewidth=2)\n",
    "ax1.add_patch(cart)\n",
    "\n",
    "# 轮子\n",
    "for wx in [-0.25, 0.25]:\n",
    "    wheel = Circle((wx, 0.05), 0.08, facecolor='gray', edgecolor='black')\n",
    "    ax1.add_patch(wheel)\n",
    "\n",
    "# 摆杆\n",
    "theta = 0.2  # 示例角度\n",
    "pole_length = 2.0\n",
    "pole_x = pole_length * np.sin(theta)\n",
    "pole_y = pole_length * np.cos(theta)\n",
    "ax1.plot([0, pole_x], [0.35, 0.35 + pole_y], 'r-', linewidth=10, solid_capstyle='round')\n",
    "ax1.plot(pole_x, 0.35 + pole_y, 'ro', markersize=18)\n",
    "\n",
    "# 角度标注\n",
    "arc_angles = np.linspace(np.pi/2 - theta, np.pi/2, 20)\n",
    "arc_r = 0.6\n",
    "ax1.plot(arc_r * np.cos(arc_angles), 0.35 + arc_r * np.sin(arc_angles), 'g--', linewidth=2)\n",
    "ax1.annotate('θ', (0.25, 0.8), fontsize=16, color='green', fontweight='bold')\n",
    "\n",
    "# 力箭头\n",
    "ax1.annotate('', xy=(1.2, 0.2), xytext=(0.5, 0.2),\n",
    "             arrowprops=dict(arrowstyle='->', color='orange', lw=3))\n",
    "ax1.text(0.85, 0.35, 'F', fontsize=14, color='orange', fontweight='bold')\n",
    "\n",
    "# 坐标轴标注\n",
    "ax1.annotate('', xy=(2.5, 0), xytext=(-2.5, 0),\n",
    "             arrowprops=dict(arrowstyle='->', color='black', lw=1))\n",
    "ax1.text(2.3, -0.2, 'x', fontsize=12)\n",
    "\n",
    "ax1.set_title('CartPole 系统示意图', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# 右图: 相平面\n",
    "ax2 = axes[1]\n",
    "\n",
    "# 生成相轨迹数据\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "trajectories = []\n",
    "\n",
    "for seed in range(5):\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    traj = [obs]\n",
    "    for _ in range(200):\n",
    "        # 简单策略\n",
    "        action = 1 if obs[2] > 0 else 0\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        traj.append(obs)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    trajectories.append(np.array(traj))\n",
    "env.close()\n",
    "\n",
    "# 绘制相轨迹 (角度 vs 角速度)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(trajectories)))\n",
    "for traj, color in zip(trajectories, colors):\n",
    "    ax2.plot(np.degrees(traj[:, 2]), traj[:, 3], '-', color=color, alpha=0.7, linewidth=1.5)\n",
    "    ax2.plot(np.degrees(traj[0, 2]), traj[0, 3], 'o', color=color, markersize=8)\n",
    "    ax2.plot(np.degrees(traj[-1, 2]), traj[-1, 3], 's', color=color, markersize=8)\n",
    "\n",
    "# 终止边界\n",
    "ax2.axvline(x=12, color='red', linestyle='--', alpha=0.7, label='终止边界 (±12°)')\n",
    "ax2.axvline(x=-12, color='red', linestyle='--', alpha=0.7)\n",
    "ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax2.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('摆杆角度 θ (度)', fontsize=12)\n",
    "ax2.set_ylabel('摆杆角速度 θ̇ (rad/s)', fontsize=12)\n",
    "ax2.set_title('相平面轨迹', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_xlim(-20, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole 控制策略对比\n",
    "\n",
    "def random_policy(obs):\n",
    "    return np.random.randint(2)\n",
    "\n",
    "def angle_policy(obs):\n",
    "    \"\"\"基于角度的策略\"\"\"\n",
    "    return 1 if obs[2] > 0 else 0\n",
    "\n",
    "def pd_policy(obs):\n",
    "    \"\"\"PD 控制策略\"\"\"\n",
    "    x, x_dot, theta, theta_dot = obs\n",
    "    # 角度 PD\n",
    "    u_theta = 50 * theta + 10 * theta_dot\n",
    "    # 位置 PD (小权重)\n",
    "    u_x = 0.5 * x + 1.0 * x_dot\n",
    "    return 1 if (u_theta + u_x) > 0 else 0\n",
    "\n",
    "def evaluate_policy(env_id, policy, n_episodes=50, seed=42):\n",
    "    \"\"\"评估策略\"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    rewards = []\n",
    "    for i in range(n_episodes):\n",
    "        obs, _ = env.reset(seed=seed + i)\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = policy(obs)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "# 评估\n",
    "print(\"CartPole 策略评估 (50 回合)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "policies = {\n",
    "    \"随机策略\": random_policy,\n",
    "    \"角度策略\": angle_policy,\n",
    "    \"PD控制\": pd_policy,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, policy in policies.items():\n",
    "    rewards = evaluate_policy(\"CartPole-v1\", policy)\n",
    "    results[name] = rewards\n",
    "    print(f\"{name:12s}: {np.mean(rewards):6.1f} ± {np.std(rewards):5.1f} \"\n",
    "          f\"(min: {np.min(rewards):3.0f}, max: {np.max(rewards):3.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. MountainCar - 稀疏奖励问题\n",
    "\n",
    "### 问题描述\n",
    "\n",
    "小车位于山谷底部，引擎不够强大，无法直接爬上山顶。必须**利用动量**——先向左爬坡积累势能，再向右冲刺。\n",
    "\n",
    "### 动力学方程\n",
    "\n",
    "$$v_{t+1} = v_t + 0.001 \\cdot a - 0.0025 \\cdot \\cos(3x_t)$$\n",
    "$$x_{t+1} = x_t + v_{t+1}$$\n",
    "\n",
    "其中 $a \\in \\{-1, 0, 1\\}$ 表示加速方向。\n",
    "\n",
    "### 为什么这是一个难题？\n",
    "\n",
    "- **稀疏奖励**: 除非到达目标，每步都是 -1\n",
    "- **随机策略失败**: 随机动作几乎无法到达目标\n",
    "- **需要探索**: 必须学会\"先后退再前进\"的策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCar 环境详解\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MountainCar-v0 环境详解\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n【状态空间】\")\n",
    "print(f\"  位置 x: [{env.observation_space.low[0]:.2f}, {env.observation_space.high[0]:.2f}]\")\n",
    "print(f\"  速度 v: [{env.observation_space.low[1]:.3f}, {env.observation_space.high[1]:.3f}]\")\n",
    "\n",
    "print(\"\\n【动作空间】 Discrete(3)\")\n",
    "print(f\"  0: 向左加速 (a = -1)\")\n",
    "print(f\"  1: 不加速   (a =  0)\")\n",
    "print(f\"  2: 向右加速 (a = +1)\")\n",
    "\n",
    "print(\"\\n【目标】\")\n",
    "print(f\"  到达 x ≥ 0.5 (山顶)\")\n",
    "\n",
    "print(\"\\n【奖励】\")\n",
    "print(f\"  每步: -1\")\n",
    "print(f\"  到达目标: 回合结束 (累积奖励越接近 0 越好)\")\n",
    "\n",
    "print(\"\\n【难点】\")\n",
    "print(f\"  - 稀疏奖励: 只有到达目标才有正面信号\")\n",
    "print(f\"  - 最大步数: 200 (随机策略几乎无法成功)\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 MountainCar 地形和能量\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 左图: 地形和轨迹\n",
    "ax1 = axes[0]\n",
    "\n",
    "# 地形函数\n",
    "x = np.linspace(-1.2, 0.6, 300)\n",
    "y = np.sin(3 * x) * 0.45 + 0.55\n",
    "\n",
    "ax1.plot(x, y, 'b-', linewidth=3, label='地形')\n",
    "ax1.fill_between(x, 0, y, alpha=0.2, color='green')\n",
    "\n",
    "# 关键位置\n",
    "ax1.axvline(x=-0.5, color='blue', linestyle='--', alpha=0.7, label='起点区域')\n",
    "ax1.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='目标线')\n",
    "\n",
    "# 运行一个成功轨迹\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "obs, _ = env.reset(seed=42)\n",
    "positions = [obs[0]]\n",
    "\n",
    "for _ in range(200):\n",
    "    # 动量策略\n",
    "    action = 2 if obs[1] > 0 else 0\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    positions.append(obs[0])\n",
    "    if terminated:\n",
    "        break\n",
    "env.close()\n",
    "\n",
    "# 绘制轨迹\n",
    "traj_y = np.sin(3 * np.array(positions)) * 0.45 + 0.55\n",
    "colors = np.linspace(0, 1, len(positions))\n",
    "for i in range(len(positions) - 1):\n",
    "    ax1.plot(positions[i:i+2], traj_y[i:i+2], '-', \n",
    "             color=plt.cm.plasma(colors[i]), linewidth=2, alpha=0.7)\n",
    "\n",
    "# 小车\n",
    "ax1.plot(positions[0], traj_y[0], 'go', markersize=15, label='起点')\n",
    "ax1.plot(positions[-1], traj_y[-1], 'r*', markersize=20, label='终点')\n",
    "\n",
    "ax1.set_xlabel('位置 x', fontsize=12)\n",
    "ax1.set_ylabel('高度', fontsize=12)\n",
    "ax1.set_title(f'MountainCar 轨迹 ({len(positions)} 步)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_xlim(-1.3, 0.7)\n",
    "ax1.set_ylim(0, 1.3)\n",
    "\n",
    "# 右图: 位置-速度相图\n",
    "ax2 = axes[1]\n",
    "\n",
    "# 等能量线\n",
    "pos = np.linspace(-1.2, 0.6, 100)\n",
    "vel = np.linspace(-0.07, 0.07, 100)\n",
    "P, V = np.meshgrid(pos, vel)\n",
    "\n",
    "# 势能 (与高度成正比)\n",
    "potential = np.sin(3 * P) * 0.45 + 0.55\n",
    "# 动能\n",
    "kinetic = 0.5 * V**2 * 500  # 缩放\n",
    "# 总能量\n",
    "total_energy = potential + kinetic\n",
    "\n",
    "contour = ax2.contourf(P, V, total_energy, levels=20, cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(contour, ax=ax2, label='总能量 (a.u.)')\n",
    "\n",
    "# 绘制相轨迹\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "for seed in range(3):\n",
    "    obs, _ = env.reset(seed=seed * 10)\n",
    "    traj = [obs]\n",
    "    for _ in range(200):\n",
    "        action = 2 if obs[1] > 0 else 0\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        traj.append(obs)\n",
    "        if terminated:\n",
    "            break\n",
    "    traj = np.array(traj)\n",
    "    ax2.plot(traj[:, 0], traj[:, 1], 'k-', linewidth=1.5, alpha=0.8)\n",
    "    ax2.plot(traj[0, 0], traj[0, 1], 'go', markersize=8)\n",
    "    ax2.plot(traj[-1, 0], traj[-1, 1], 'r*', markersize=12)\n",
    "env.close()\n",
    "\n",
    "ax2.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='目标')\n",
    "ax2.set_xlabel('位置 x', fontsize=12)\n",
    "ax2.set_ylabel('速度 v', fontsize=12)\n",
    "ax2.set_title('相空间与能量等高线', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCar 策略对比\n",
    "\n",
    "def random_policy_mc(obs):\n",
    "    return np.random.randint(3)\n",
    "\n",
    "def momentum_policy(obs):\n",
    "    \"\"\"跟随速度方向\"\"\"\n",
    "    return 2 if obs[1] > 0 else 0\n",
    "\n",
    "def energy_policy(obs):\n",
    "    \"\"\"基于能量的策略\"\"\"\n",
    "    position, velocity = obs\n",
    "    # 当位置较低时，通过摆动积累能量\n",
    "    if position < -0.4:\n",
    "        return 2 if velocity > 0 else 0\n",
    "    elif position > 0.4:\n",
    "        return 2  # 接近目标，全力向右\n",
    "    else:\n",
    "        return 2 if velocity > 0 else 0\n",
    "\n",
    "print(\"MountainCar 策略评估 (20 回合)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'策略':<15} {'平均奖励':>12} {'成功率':>10} {'平均步数':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, policy in [(\"随机\", random_policy_mc), (\"动量\", momentum_policy), (\"能量\", energy_policy)]:\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    rewards = []\n",
    "    successes = 0\n",
    "    steps_list = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        obs, _ = env.reset(seed=i)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            action = policy(obs)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if terminated:\n",
    "                successes += 1\n",
    "                break\n",
    "            if truncated:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "        steps_list.append(steps)\n",
    "    env.close()\n",
    "    \n",
    "    print(f\"{name:<15} {np.mean(rewards):>12.1f} {successes/20*100:>9.0f}% {np.mean(steps_list):>10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Pendulum - 连续控制\n",
    "\n",
    "### 物理系统\n",
    "\n",
    "单摆从任意位置启动，目标是通过施加扭矩将其稳定在直立位置。\n",
    "\n",
    "### 动力学方程\n",
    "\n",
    "$$\\ddot{\\theta} = -\\frac{3g}{2l}\\sin(\\theta + \\pi) + \\frac{3}{ml^2}u$$\n",
    "\n",
    "其中 $u \\in [-2, 2]$ 是施加的扭矩。\n",
    "\n",
    "### 奖励函数\n",
    "\n",
    "$$r = -(\\theta^2 + 0.1\\dot{\\theta}^2 + 0.001u^2)$$\n",
    "\n",
    "这是一个**二次型奖励**，鼓励：\n",
    "1. 角度接近 0（直立）\n",
    "2. 角速度接近 0（静止）\n",
    "3. 控制输入小（节能）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendulum 环境详解\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Pendulum-v1 环境详解\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n【状态空间】 Box(3,)\")\n",
    "print(f\"  cos(θ): [{env.observation_space.low[0]:.1f}, {env.observation_space.high[0]:.1f}]\")\n",
    "print(f\"  sin(θ): [{env.observation_space.low[1]:.1f}, {env.observation_space.high[1]:.1f}]\")\n",
    "print(f\"  θ̇:     [{env.observation_space.low[2]:.1f}, {env.observation_space.high[2]:.1f}] rad/s\")\n",
    "\n",
    "print(\"\\n【动作空间】 Box(1,) - 连续!\")\n",
    "print(f\"  扭矩 u: [{env.action_space.low[0]:.1f}, {env.action_space.high[0]:.1f}] N·m\")\n",
    "\n",
    "print(\"\\n【奖励函数】\")\n",
    "print(f\"  r = -(θ² + 0.1·θ̇² + 0.001·u²)\")\n",
    "print(f\"  范围: 约 [-16.27, 0]\")\n",
    "print(f\"  最优 (直立静止): 0\")\n",
    "\n",
    "print(\"\\n【回合设置】\")\n",
    "print(f\"  最大步数: 200\")\n",
    "print(f\"  无提前终止\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendulum 控制演示\n",
    "\n",
    "def pd_controller(obs, Kp=10.0, Kd=2.0):\n",
    "    \"\"\"PD 控制器\"\"\"\n",
    "    cos_theta, sin_theta, theta_dot = obs\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "    torque = -Kp * theta - Kd * theta_dot\n",
    "    return np.clip([torque], -2.0, 2.0)\n",
    "\n",
    "def energy_controller(obs):\n",
    "    \"\"\"能量成形控制器\"\"\"\n",
    "    cos_theta, sin_theta, theta_dot = obs\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "    \n",
    "    # 物理参数\n",
    "    g, l, m = 10.0, 1.0, 1.0\n",
    "    \n",
    "    # 当前能量\n",
    "    E = 0.5 * m * l**2 * theta_dot**2 - m * g * l * cos_theta\n",
    "    E_target = m * g * l  # 目标能量 (直立)\n",
    "    \n",
    "    # 控制策略\n",
    "    if np.abs(theta) < 0.3:  # 接近直立，用PD稳定\n",
    "        torque = -10.0 * theta - 2.0 * theta_dot\n",
    "    else:  # 能量泵浦\n",
    "        torque = -3.0 * (E - E_target) * theta_dot\n",
    "    \n",
    "    return np.clip([torque], -2.0, 2.0)\n",
    "\n",
    "# 运行并记录\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "controllers = {\n",
    "    \"PD控制\": pd_controller,\n",
    "    \"能量控制\": energy_controller\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "for idx, (name, controller) in enumerate(controllers.items()):\n",
    "    obs, _ = env.reset(seed=42)\n",
    "    thetas, theta_dots, torques, rewards_list = [], [], [], []\n",
    "    \n",
    "    for _ in range(200):\n",
    "        cos_theta, sin_theta, theta_dot = obs\n",
    "        theta = np.arctan2(sin_theta, cos_theta)\n",
    "        \n",
    "        action = controller(obs)\n",
    "        obs, reward, _, _, _ = env.step(action)\n",
    "        \n",
    "        thetas.append(np.degrees(theta))\n",
    "        theta_dots.append(theta_dot)\n",
    "        torques.append(action[0])\n",
    "        rewards_list.append(reward)\n",
    "    \n",
    "    # 绘制\n",
    "    axes[0, 0].plot(thetas, color=colors[idx], label=name, linewidth=2)\n",
    "    axes[0, 1].plot(theta_dots, color=colors[idx], label=name, linewidth=2)\n",
    "    axes[1, 0].plot(torques, color=colors[idx], label=name, linewidth=2)\n",
    "    axes[1, 1].plot(np.cumsum(rewards_list), color=colors[idx], label=name, linewidth=2)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# 设置图表\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_ylabel('角度 (度)')\n",
    "axes[0, 0].set_title('摆角变化')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_ylabel('角速度 (rad/s)')\n",
    "axes[0, 1].set_title('角速度变化')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_xlabel('步数')\n",
    "axes[1, 0].set_ylabel('扭矩')\n",
    "axes[1, 0].set_title('控制输入')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].set_xlabel('步数')\n",
    "axes[1, 1].set_ylabel('累积奖励')\n",
    "axes[1, 1].set_title('累积奖励')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 环境对比总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境对比表\n",
    "from gymnasium import spaces\n",
    "\n",
    "envs_info = [\n",
    "    (\"CartPole-v1\", \"倒立摆\", \"简单\", \"DQN, A2C\"),\n",
    "    (\"MountainCar-v0\", \"爬山车\", \"中等(探索)\", \"需要好的探索\"),\n",
    "    (\"Acrobot-v1\", \"双摆\", \"中等\", \"DQN, PPO\"),\n",
    "    (\"Pendulum-v1\", \"单摆\", \"中等(连续)\", \"DDPG, SAC, TD3\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"经典控制环境对比\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'环境ID':<25} {'描述':<10} {'状态维度':<10} {'动作空间':<15} {'难度':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for env_id, desc, difficulty, algos in envs_info:\n",
    "    try:\n",
    "        env = gym.make(env_id)\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        if isinstance(env.action_space, spaces.Discrete):\n",
    "            act_info = f\"Discrete({env.action_space.n})\"\n",
    "        else:\n",
    "            act_info = f\"Box({env.action_space.shape[0]})\"\n",
    "        env.close()\n",
    "        print(f\"{env_id:<25} {desc:<10} {obs_dim:<10} {act_info:<15} {difficulty:<15}\")\n",
    "    except:\n",
    "        print(f\"{env_id:<25} 未安装\")\n",
    "\n",
    "print(\"\\n推荐算法:\")\n",
    "for env_id, _, _, algos in envs_info:\n",
    "    print(f\"  {env_id}: {algos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 练习\n",
    "\n",
    "### 练习 1: 实现更好的 CartPole PD 控制器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_pd_policy(obs):\n",
    "    \"\"\"\n",
    "    TODO: 调整 PD 参数使平均奖励 > 450\n",
    "    \n",
    "    提示:\n",
    "    - 角度控制是主要的\n",
    "    - 位置控制是次要的，防止小车跑出边界\n",
    "    - 考虑添加角速度阻尼项\n",
    "    \"\"\"\n",
    "    x, x_dot, theta, theta_dot = obs\n",
    "    \n",
    "    # TODO: 调整这些参数\n",
    "    Kp_theta = 50.0\n",
    "    Kd_theta = 10.0\n",
    "    Kp_x = 0.5\n",
    "    Kd_x = 1.0\n",
    "    \n",
    "    u = Kp_theta * theta + Kd_theta * theta_dot + Kp_x * x + Kd_x * x_dot\n",
    "    return 1 if u > 0 else 0\n",
    "\n",
    "# 测试\n",
    "rewards = evaluate_policy(\"CartPole-v1\", optimized_pd_policy, n_episodes=50)\n",
    "print(f\"你的 PD 控制器: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")\n",
    "print(f\"目标: > 450\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 2: 分析 Pendulum 的最优奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习 2: 计算 Pendulum 的理论最优奖励\n",
    "\n",
    "\"\"\"\n",
    "Pendulum 奖励函数: r = -(θ² + 0.1·θ̇² + 0.001·u²)\n",
    "\n",
    "问题:\n",
    "1. 单步最优奖励是多少? (θ=0, θ̇=0, u=0)\n",
    "2. 单步最差奖励是多少? (θ=π, θ̇=8, u=2)\n",
    "3. 200 步回合的理论最优累积奖励是多少?\n",
    "\"\"\"\n",
    "\n",
    "# TODO: 计算\n",
    "best_single_reward = 0  # 完美状态\n",
    "worst_single_reward = -(np.pi**2 + 0.1 * 8**2 + 0.001 * 2**2)\n",
    "theoretical_best_episode = 200 * best_single_reward\n",
    "\n",
    "print(f\"单步最优奖励: {best_single_reward}\")\n",
    "print(f\"单步最差奖励: {worst_single_reward:.2f}\")\n",
    "print(f\"理论最优累积奖励: {theoretical_best_episode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "本节我们深入分析了四个经典控制环境:\n",
    "\n",
    "1. **CartPole**: 学习了欠驱动系统控制和 PD 控制器设计\n",
    "2. **MountainCar**: 理解了稀疏奖励问题和能量利用策略\n",
    "3. **Acrobot**: 认识了更复杂的欠驱动系统\n",
    "4. **Pendulum**: 入门连续控制和能量成形控制\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- 使用 Q-Learning 解决离散动作环境\n",
    "- 使用 DQN 提升性能\n",
    "- 使用 DDPG/SAC 解决连续控制问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
