{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gymnasium 强化学习环境快速入门\n",
    "\n",
    "---\n",
    "\n",
    "## 核心思想 (Core Idea)\n",
    "\n",
    "Gymnasium (原 OpenAI Gym) 提供了强化学习研究的**标准化接口**，定义了智能体与环境交互的统一 API。通过标准化的 observation-action-reward 循环，研究者可以在不同环境间无缝切换算法。\n",
    "\n",
    "## 数学原理 (Mathematical Theory)\n",
    "\n",
    "强化学习建模为**马尔可夫决策过程 (MDP)**:\n",
    "\n",
    "$$MDP = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$$\n",
    "\n",
    "其中:\n",
    "- $\\mathcal{S}$: 状态空间 (State Space)\n",
    "- $\\mathcal{A}$: 动作空间 (Action Space)  \n",
    "- $P(s'|s,a)$: 状态转移概率\n",
    "- $R(s,a,s')$: 奖励函数\n",
    "- $\\gamma \\in [0,1]$: 折扣因子\n",
    "\n",
    "智能体目标是最大化**期望累积折扣奖励**:\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境安装与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖 (如果尚未安装)\n",
    "# !pip install gymnasium[classic-control] matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(f\"Gymnasium 版本: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建第一个环境: CartPole\n",
    "\n",
    "CartPole 是经典的控制问题：通过左右移动小车来平衡竖立的杆子。\n",
    "\n",
    "### 环境说明\n",
    "\n",
    "| 属性 | 描述 |\n",
    "|------|------|\n",
    "| **状态** | [小车位置, 小车速度, 杆角度, 杆角速度] |\n",
    "| **动作** | 0 (向左推) 或 1 (向右推) |\n",
    "| **奖励** | 每步 +1 |\n",
    "| **终止** | 杆角度 > 12° 或 小车位置 > 2.4 |\n",
    "| **成功** | 平均奖励 ≥ 475 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 CartPole 环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 查看环境信息\n",
    "print(\"=\" * 50)\n",
    "print(\"环境信息\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"环境 ID: {env.spec.id}\")\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"最大步数: {env.spec.max_episode_steps}\")\n",
    "\n",
    "# 查看观测空间的具体范围\n",
    "print(\"\\n观测空间详情:\")\n",
    "print(f\"  形状: {env.observation_space.shape}\")\n",
    "print(f\"  下界: {env.observation_space.low}\")\n",
    "print(f\"  上界: {env.observation_space.high}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 环境交互基础\n",
    "\n",
    "### 核心 API\n",
    "\n",
    "```python\n",
    "observation, info = env.reset()           # 重置环境\n",
    "observation, reward, terminated, truncated, info = env.step(action)  # 执行动作\n",
    "```\n",
    "\n",
    "- `terminated`: 任务完成（成功或失败）\n",
    "- `truncated`: 回合因时间限制等原因被截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 重置环境\n",
    "observation, info = env.reset(seed=42)\n",
    "print(\"初始观测:\")\n",
    "print(f\"  小车位置: {observation[0]:.4f}\")\n",
    "print(f\"  小车速度: {observation[1]:.4f}\")\n",
    "print(f\"  杆角度:   {observation[2]:.4f} rad ({np.degrees(observation[2]):.2f}°)\")\n",
    "print(f\"  杆角速度: {observation[3]:.4f}\")\n",
    "\n",
    "# 执行一个动作\n",
    "action = 1  # 向右推\n",
    "next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"\\n执行动作: {action} (向右推)\")\n",
    "print(f\"获得奖励: {reward}\")\n",
    "print(f\"终止: {terminated}, 截断: {truncated}\")\n",
    "print(f\"新观测: {next_obs}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 运行完整回合\n",
    "\n",
    "让我们运行一个完整的回合，使用随机策略和简单的规则策略进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy_fn, seed=None, verbose=False):\n",
    "    \"\"\"\n",
    "    运行一个完整回合\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        环境实例\n",
    "    policy_fn : callable\n",
    "        策略函数，输入观测返回动作\n",
    "    seed : int, optional\n",
    "        随机种子\n",
    "    verbose : bool\n",
    "        是否打印详细信息\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    total_reward : float\n",
    "        回合总奖励\n",
    "    steps : int\n",
    "        回合步数\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action = policy_fn(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if verbose and steps % 100 == 0:\n",
    "            print(f\"步数: {steps}, 累积奖励: {total_reward}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n",
    "\n",
    "# 定义策略\n",
    "def random_policy(obs):\n",
    "    \"\"\"随机策略\"\"\"\n",
    "    return np.random.randint(2)\n",
    "\n",
    "def angle_policy(obs):\n",
    "    \"\"\"基于角度的简单策略: 杆往哪边倒就往哪边推\"\"\"\n",
    "    pole_angle = obs[2]\n",
    "    return 1 if pole_angle > 0 else 0\n",
    "\n",
    "def pid_policy(obs):\n",
    "    \"\"\"PID 控制策略\"\"\"\n",
    "    x, x_dot, theta, theta_dot = obs\n",
    "    # PD 控制\n",
    "    u = 10 * theta + 1 * theta_dot + 0.1 * x + 0.5 * x_dot\n",
    "    return 1 if u > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不同策略\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "policies = {\n",
    "    \"随机策略\": random_policy,\n",
    "    \"角度策略\": angle_policy,\n",
    "    \"PID策略\": pid_policy\n",
    "}\n",
    "\n",
    "n_episodes = 20\n",
    "results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"策略评估 ({n_episodes} 回合)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, policy in policies.items():\n",
    "    rewards = []\n",
    "    for i in range(n_episodes):\n",
    "        reward, steps = run_episode(env, policy, seed=i)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    results[name] = rewards\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  平均奖励: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")\n",
    "    print(f\"  最小/最大: {np.min(rewards):.0f} / {np.max(rewards):.0f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化策略比较\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 箱线图\n",
    "ax1 = axes[0]\n",
    "data = [results[name] for name in results]\n",
    "bp = ax1.boxplot(data, labels=list(results.keys()), patch_artist=True)\n",
    "colors = ['#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax1.set_ylabel('回合奖励')\n",
    "ax1.set_title('策略性能分布')\n",
    "ax1.axhline(y=475, color='r', linestyle='--', label='成功阈值 (475)')\n",
    "ax1.legend()\n",
    "\n",
    "# 条形图\n",
    "ax2 = axes[1]\n",
    "means = [np.mean(results[name]) for name in results]\n",
    "stds = [np.std(results[name]) for name in results]\n",
    "x = np.arange(len(results))\n",
    "bars = ax2.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(list(results.keys()))\n",
    "ax2.set_ylabel('平均奖励')\n",
    "ax2.set_title('策略平均性能')\n",
    "ax2.axhline(y=475, color='r', linestyle='--', label='成功阈值')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 探索更多环境\n",
    "\n",
    "### 5.1 MountainCar - 爬山车\n",
    "\n",
    "**挑战**: 小车引擎不够强，无法直接爬上山顶，需要利用来回摆动积累动量。\n",
    "\n",
    "$$\\text{速度更新}: v_{t+1} = v_t + 0.001 \\cdot a - 0.0025 \\cdot \\cos(3x_t)$$\n",
    "$$\\text{位置更新}: x_{t+1} = x_t + v_{t+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MountainCar-v0 环境信息\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"  位置范围: [{env.observation_space.low[0]:.2f}, {env.observation_space.high[0]:.2f}]\")\n",
    "print(f\"  速度范围: [{env.observation_space.low[1]:.3f}, {env.observation_space.high[1]:.3f}]\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"  0: 向左加速, 1: 不加速, 2: 向右加速\")\n",
    "print(f\"目标: 到达 x >= 0.5\")\n",
    "print(f\"奖励: 每步 -1，到达目标 0\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 MountainCar 地形\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# 绘制地形\n",
    "x = np.linspace(-1.2, 0.6, 200)\n",
    "y = np.sin(3 * x) * 0.45 + 0.55\n",
    "\n",
    "ax.plot(x, y, 'b-', linewidth=3, label='地形')\n",
    "ax.fill_between(x, 0, y, alpha=0.3, color='green')\n",
    "\n",
    "# 标记关键位置\n",
    "ax.axvline(x=-0.5, color='red', linestyle='--', alpha=0.7, label='起点 (x=-0.5)')\n",
    "ax.axvline(x=0.5, color='gold', linestyle='--', linewidth=2, label='目标 (x=0.5)')\n",
    "\n",
    "# 绘制小车\n",
    "car_x = -0.5\n",
    "car_y = np.sin(3 * car_x) * 0.45 + 0.55\n",
    "ax.plot(car_x, car_y + 0.05, 'ro', markersize=15, label='小车')\n",
    "\n",
    "ax.set_xlabel('位置', fontsize=12)\n",
    "ax.set_ylabel('高度', fontsize=12)\n",
    "ax.set_title('MountainCar 环境地形', fontsize=14)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-1.3, 0.7)\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCar 策略测试\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "def momentum_policy(obs):\n",
    "    \"\"\"动量策略: 跟随当前速度方向加速\"\"\"\n",
    "    position, velocity = obs\n",
    "    if velocity > 0:\n",
    "        return 2  # 向右加速\n",
    "    else:\n",
    "        return 0  # 向左加速\n",
    "\n",
    "def random_policy_mc(obs):\n",
    "    \"\"\"随机策略\"\"\"\n",
    "    return np.random.randint(3)\n",
    "\n",
    "# 运行动量策略\n",
    "print(\"测试动量策略:\")\n",
    "rewards = []\n",
    "for i in range(10):\n",
    "    reward, steps = run_episode(env, momentum_policy, seed=i)\n",
    "    rewards.append(reward)\n",
    "    print(f\"  回合 {i+1}: 奖励={reward:.0f}, 步数={steps}\")\n",
    "\n",
    "print(f\"\\n平均奖励: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Pendulum - 连续动作空间\n",
    "\n",
    "Pendulum 是**连续控制**任务，动作是连续的扭矩值。\n",
    "\n",
    "$$\\text{动力学}: \\ddot{\\theta} = -\\frac{3g}{2l}\\sin(\\theta + \\pi) + \\frac{3}{ml^2}u$$\n",
    "\n",
    "$$\\text{奖励}: r = -(\\theta^2 + 0.1\\dot{\\theta}^2 + 0.001u^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Pendulum-v1 环境信息\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"  观测: [cos(θ), sin(θ), θ̇]\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"  扭矩范围: [{env.action_space.low[0]:.1f}, {env.action_space.high[0]:.1f}]\")\n",
    "print(f\"\\n这是一个连续动作空间环境!\")\n",
    "\n",
    "# 采样动作示例\n",
    "print(f\"\\n随机采样的动作: {env.action_space.sample()}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendulum PD 控制\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "def pd_controller(obs):\n",
    "    \"\"\"PD 控制器策略\"\"\"\n",
    "    cos_theta, sin_theta, theta_dot = obs\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "    \n",
    "    # PD 控制\n",
    "    Kp, Kd = 10.0, 2.0\n",
    "    torque = -Kp * theta - Kd * theta_dot\n",
    "    \n",
    "    return np.clip([torque], -2.0, 2.0)\n",
    "\n",
    "# 运行一个回合并记录数据\n",
    "obs, _ = env.reset(seed=42)\n",
    "observations = [obs]\n",
    "actions = []\n",
    "rewards_list = []\n",
    "\n",
    "for _ in range(200):\n",
    "    action = pd_controller(obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    observations.append(obs)\n",
    "    actions.append(action[0])\n",
    "    rewards_list.append(reward)\n",
    "\n",
    "print(f\"回合总奖励: {sum(rewards_list):.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 Pendulum 控制过程\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "observations = np.array(observations)\n",
    "steps = np.arange(len(observations))\n",
    "\n",
    "# 角度\n",
    "ax1 = axes[0, 0]\n",
    "theta = np.arctan2(observations[:, 1], observations[:, 0])\n",
    "ax1.plot(steps, np.degrees(theta), 'b-', linewidth=2)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('步数')\n",
    "ax1.set_ylabel('角度 (度)')\n",
    "ax1.set_title('摆角变化')\n",
    "\n",
    "# 角速度\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(steps, observations[:, 2], 'g-', linewidth=2)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('步数')\n",
    "ax2.set_ylabel('角速度 (rad/s)')\n",
    "ax2.set_title('角速度变化')\n",
    "\n",
    "# 动作 (扭矩)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(range(len(actions)), actions, 'r-', linewidth=2)\n",
    "ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('步数')\n",
    "ax3.set_ylabel('扭矩')\n",
    "ax3.set_title('控制输入')\n",
    "\n",
    "# 奖励\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(range(len(rewards_list)), rewards_list, 'purple', linewidth=2)\n",
    "ax4.plot(range(len(rewards_list)), np.cumsum(rewards_list) / (np.arange(len(rewards_list)) + 1), \n",
    "         'orange', linewidth=2, linestyle='--', label='移动平均')\n",
    "ax4.set_xlabel('步数')\n",
    "ax4.set_ylabel('奖励')\n",
    "ax4.set_title('即时奖励')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 空间类型详解\n",
    "\n",
    "Gymnasium 定义了多种空间类型来表示观测和动作空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Gymnasium 空间类型\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Discrete - 离散空间\n",
    "discrete = spaces.Discrete(5)\n",
    "print(f\"\\n1. Discrete(5): 离散空间 {{0, 1, 2, 3, 4}}\")\n",
    "print(f\"   采样: {[discrete.sample() for _ in range(5)]}\")\n",
    "\n",
    "# 2. Box - 连续空间\n",
    "box = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "print(f\"\\n2. Box([-1,1]^3): 连续空间\")\n",
    "print(f\"   形状: {box.shape}\")\n",
    "print(f\"   采样: {box.sample()}\")\n",
    "\n",
    "# 3. MultiDiscrete - 多离散空间\n",
    "multi_discrete = spaces.MultiDiscrete([3, 2, 4])\n",
    "print(f\"\\n3. MultiDiscrete([3,2,4]): 多维离散空间\")\n",
    "print(f\"   每维范围: [0,3), [0,2), [0,4)\")\n",
    "print(f\"   采样: {multi_discrete.sample()}\")\n",
    "\n",
    "# 4. MultiBinary - 多二值空间\n",
    "multi_binary = spaces.MultiBinary(4)\n",
    "print(f\"\\n4. MultiBinary(4): 多二值空间\")\n",
    "print(f\"   采样: {multi_binary.sample()}\")\n",
    "\n",
    "# 5. Dict - 字典空间\n",
    "dict_space = spaces.Dict({\n",
    "    \"position\": spaces.Box(-10, 10, shape=(2,)),\n",
    "    \"velocity\": spaces.Box(-1, 1, shape=(2,)),\n",
    "    \"flag\": spaces.Discrete(2)\n",
    "})\n",
    "print(f\"\\n5. Dict 空间:\")\n",
    "sample = dict_space.sample()\n",
    "for key, value in sample.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 环境包装器 (Wrappers)\n",
    "\n",
    "包装器允许我们在不修改原始环境的情况下，对观测、动作和奖励进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import (\n",
    "    RecordEpisodeStatistics,\n",
    "    TimeLimit,\n",
    "    ClipAction\n",
    ")\n",
    "\n",
    "# 创建带包装器的环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = RecordEpisodeStatistics(env)  # 自动记录回合统计\n",
    "\n",
    "print(\"使用 RecordEpisodeStatistics 包装器:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "if 'episode' in info:\n",
    "    print(f\"回合奖励: {info['episode']['r']}\")\n",
    "    print(f\"回合长度: {info['episode']['l']}\")\n",
    "    print(f\"回合时间: {info['episode']['t']:.3f}s\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义包装器示例: 观测归一化\n",
    "class NormalizeObservation(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    在线观测归一化包装器\n",
    "    \n",
    "    使用 Welford 算法在线估计均值和方差\n",
    "    \"\"\"\n",
    "    def __init__(self, env, epsilon=1e-8):\n",
    "        super().__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        self.mean = np.zeros(env.observation_space.shape)\n",
    "        self.var = np.ones(env.observation_space.shape)\n",
    "        self.count = 0\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # 更新统计量\n",
    "        self.count += 1\n",
    "        delta = obs - self.mean\n",
    "        self.mean += delta / self.count\n",
    "        self.var += delta * (obs - self.mean)\n",
    "        \n",
    "        # 归一化\n",
    "        std = np.sqrt(self.var / max(1, self.count) + self.epsilon)\n",
    "        return (obs - self.mean) / std\n",
    "\n",
    "# 测试归一化包装器\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = NormalizeObservation(env)\n",
    "\n",
    "print(\"归一化前后观测对比:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "print(f\"步骤 0 - 归一化观测: {obs}\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs, _, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "print(f\"步骤 50 - 归一化观测: {obs}\")\n",
    "print(f\"\\n估计的均值: {env.mean}\")\n",
    "print(f\"估计的标准差: {np.sqrt(env.var / env.count)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 向量化环境\n",
    "\n",
    "向量化环境允许并行运行多个环境实例，大大提高采样效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.vector import SyncVectorEnv\n",
    "\n",
    "# 创建 4 个并行环境\n",
    "n_envs = 4\n",
    "\n",
    "def make_env():\n",
    "    return gym.make(\"CartPole-v1\")\n",
    "\n",
    "vec_env = SyncVectorEnv([make_env for _ in range(n_envs)])\n",
    "\n",
    "print(f\"向量化环境信息:\")\n",
    "print(f\"  环境数量: {vec_env.num_envs}\")\n",
    "print(f\"  单环境观测空间: {vec_env.single_observation_space}\")\n",
    "print(f\"  批量观测空间: {vec_env.observation_space}\")\n",
    "\n",
    "# 并行重置\n",
    "obs, info = vec_env.reset()\n",
    "print(f\"\\n批量观测形状: {obs.shape}\")\n",
    "\n",
    "# 并行执行\n",
    "actions = vec_env.action_space.sample()  # 采样 n_envs 个动作\n",
    "print(f\"批量动作: {actions}\")\n",
    "\n",
    "obs, rewards, terminateds, truncateds, infos = vec_env.step(actions)\n",
    "print(f\"批量奖励: {rewards}\")\n",
    "print(f\"批量终止: {terminateds}\")\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化环境采样效率对比\n",
    "import time\n",
    "\n",
    "n_steps = 1000\n",
    "\n",
    "# 单环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "start = time.time()\n",
    "obs, _ = env.reset()\n",
    "for _ in range(n_steps):\n",
    "    obs, _, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "single_time = time.time() - start\n",
    "env.close()\n",
    "\n",
    "# 向量化环境\n",
    "n_envs = 4\n",
    "vec_env = SyncVectorEnv([lambda: gym.make(\"CartPole-v1\") for _ in range(n_envs)])\n",
    "start = time.time()\n",
    "obs, _ = vec_env.reset()\n",
    "for _ in range(n_steps // n_envs):\n",
    "    obs, _, _, _, _ = vec_env.step(vec_env.action_space.sample())\n",
    "vec_time = time.time() - start\n",
    "vec_env.close()\n",
    "\n",
    "print(f\"采样 {n_steps} 步:\")\n",
    "print(f\"  单环境: {single_time:.3f}s\")\n",
    "print(f\"  向量化 ({n_envs} 并行): {vec_time:.3f}s\")\n",
    "print(f\"  加速比: {single_time / vec_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 练习题\n",
    "\n",
    "### 练习 1: 实现更好的 CartPole 策略\n",
    "\n",
    "尝试改进 PID 控制器的参数，使平均奖励达到 400 以上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习 1: 改进 PID 控制器\n",
    "def improved_pid_policy(obs):\n",
    "    \"\"\"\n",
    "    TODO: 调整 PID 参数以获得更好的性能\n",
    "    提示:\n",
    "    - Kp_theta: 角度比例增益\n",
    "    - Kd_theta: 角度微分增益\n",
    "    - Kp_x: 位置比例增益\n",
    "    - Kd_x: 位置微分增益\n",
    "    \"\"\"\n",
    "    x, x_dot, theta, theta_dot = obs\n",
    "    \n",
    "    # TODO: 调整这些参数\n",
    "    Kp_theta = 10.0\n",
    "    Kd_theta = 1.0\n",
    "    Kp_x = 0.1\n",
    "    Kd_x = 0.5\n",
    "    \n",
    "    u = Kp_theta * theta + Kd_theta * theta_dot + Kp_x * x + Kd_x * x_dot\n",
    "    return 1 if u > 0 else 0\n",
    "\n",
    "# 测试你的改进\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "rewards = [run_episode(env, improved_pid_policy, seed=i)[0] for i in range(20)]\n",
    "print(f\"平均奖励: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 2: 实现 MountainCar 能量策略\n",
    "\n",
    "小车的总能量 = 动能 + 势能。设计一个策略，通过调节能量来到达山顶。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习 2: 能量策略\n",
    "def energy_policy(obs):\n",
    "    \"\"\"\n",
    "    TODO: 实现基于能量的策略\n",
    "    \n",
    "    提示:\n",
    "    - 势能与高度成正比: PE ∝ sin(3*x)\n",
    "    - 动能: KE = 0.5 * m * v^2\n",
    "    - 目标: 积累足够能量到达 x = 0.5\n",
    "    \"\"\"\n",
    "    position, velocity = obs\n",
    "    \n",
    "    # TODO: 实现你的策略\n",
    "    # 基础版本: 跟随速度方向\n",
    "    if velocity > 0:\n",
    "        return 2  # 向右\n",
    "    else:\n",
    "        return 0  # 向左\n",
    "\n",
    "# 测试\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "rewards = [run_episode(env, energy_policy, seed=i)[0] for i in range(5)]\n",
    "print(f\"平均奖励: {np.mean(rewards):.1f} (越接近 0 越好)\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 3: 自定义奖励包装器\n",
    "\n",
    "实现一个奖励缩放包装器，将奖励除以一个常数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习 3: 奖励缩放包装器\n",
    "class RewardScaler(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    TODO: 实现奖励缩放包装器\n",
    "    \n",
    "    缩放后的奖励 = 原始奖励 / scale\n",
    "    \"\"\"\n",
    "    def __init__(self, env, scale=1.0):\n",
    "        super().__init__(env)\n",
    "        self.scale = scale\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # TODO: 实现奖励缩放\n",
    "        return reward / self.scale\n",
    "\n",
    "# 测试\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = RewardScaler(env, scale=10.0)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs, reward, _, _, _ = env.step(0)\n",
    "print(f\"缩放后的奖励: {reward} (原始应为 1.0)\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 总结\n",
    "\n",
    "本教程涵盖了 Gymnasium 的核心概念:\n",
    "\n",
    "1. **环境创建**: `gym.make(env_id)`\n",
    "2. **基本交互**: `reset()`, `step()`, `render()`, `close()`\n",
    "3. **空间类型**: Discrete, Box, MultiDiscrete, Dict 等\n",
    "4. **包装器**: 观测/动作/奖励预处理\n",
    "5. **向量化环境**: 并行采样提高效率\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- 学习 Q-Learning 和 SARSA 算法\n",
    "- 探索深度强化学习 (DQN, PPO)\n",
    "- 尝试更复杂的环境 (Atari, MuJoCo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出所有可用的经典控制环境\n",
    "print(\"经典控制环境列表:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "classic_envs = [\n",
    "    (\"CartPole-v1\", \"倒立摆平衡\"),\n",
    "    (\"MountainCar-v0\", \"爬山车 (离散)\"),\n",
    "    (\"MountainCarContinuous-v0\", \"爬山车 (连续)\"),\n",
    "    (\"Acrobot-v1\", \"双摆控制\"),\n",
    "    (\"Pendulum-v1\", \"单摆控制 (连续)\"),\n",
    "]\n",
    "\n",
    "for env_id, desc in classic_envs:\n",
    "    try:\n",
    "        env = gym.make(env_id)\n",
    "        obs_dim = env.observation_space.shape\n",
    "        act_type = \"离散\" if isinstance(env.action_space, spaces.Discrete) else \"连续\"\n",
    "        print(f\"  {env_id:30s} | 观测: {str(obs_dim):10s} | 动作: {act_type}\")\n",
    "        env.close()\n",
    "    except:\n",
    "        print(f\"  {env_id:30s} | 未安装\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
