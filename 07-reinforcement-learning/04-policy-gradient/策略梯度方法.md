# 策略梯度方法 (Policy Gradient Methods)

> 直接优化策略——从 REINFORCE 到 Actor-Critic

---

## 一、策略梯度 vs 价值方法

### 1.1 两种思路对比

| 方面 | 价值方法 (DQN) | 策略方法 |
|------|----------------|----------|
| **学习目标** | 学习 Q(s, a)，隐式得到策略 | 直接学习策略 π(a\|s) |
| **动作空间** | 主要用于离散 | 天然支持连续 |
| **策略类型** | 确定性 (argmax) | 随机性 (概率分布) |
| **探索机制** | 需要 ε-greedy | 策略自带随机性 |
| **收敛性** | 可能不稳定 | 理论保证收敛 |

### 1.2 策略方法的优势

1. **连续动作空间**: 直接输出连续动作
2. **随机策略**: 某些问题需要随机性（如石头剪刀布）
3. **更好的收敛性**: 策略参数变化平滑
4. **端到端学习**: 直接优化目标

### 1.3 参数化策略

**离散动作** (Softmax 策略):

$$\pi_\theta(a|s) = \frac{\exp(h(s, a; \theta))}{\sum_{a'} \exp(h(s, a'; \theta))}$$

**连续动作** (高斯策略):

$$\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)$$

---

## 二、策略梯度定理

### 2.1 目标函数

最大化期望累积回报:

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \gamma^t r_t\right]$$

### 2.2 策略梯度定理 (Sutton et al., 1999)

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi_\theta}(s, a)\right]$$

**直觉理解**:
- $\nabla_\theta \log \pi_\theta(a|s)$: 增大动作 $a$ 概率的方向
- $Q^{\pi_\theta}(s, a)$: 动作的好坏程度
- 好动作 → 增大概率，坏动作 → 减小概率

### 2.3 对数概率梯度的妙处

**为什么用 $\log \pi$ 而不是 $\pi$?**

$$\nabla_\theta \log \pi_\theta = \frac{\nabla_\theta \pi_\theta}{\pi_\theta}$$

**对数导数技巧** (Log-derivative trick):

$$\nabla_\theta \mathbb{E}_{\pi_\theta}[f(x)] = \mathbb{E}_{\pi_\theta}[f(x) \nabla_\theta \log \pi_\theta(x)]$$

**好处**: 可以用采样估计期望，无需知道环境模型

---

## 三、REINFORCE 算法

### 3.1 蒙特卡洛策略梯度 (Williams, 1992)

用完整回合的回报 $G_t$ 估计 $Q(s, a)$:

$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}$$

### 3.2 算法流程

```
算法: REINFORCE

输入: 参数化策略 π_θ
参数: 学习率 α, 折扣因子 γ

1. 初始化策略参数 θ
2. 循环:
   a. 采集一个完整轨迹 τ = (s_0, a_0, r_1, ..., s_T)
   b. 对于 t = 0, 1, ..., T-1:
      i.   计算回报 G_t = Σ_{k=t}^{T} γ^{k-t} r_{k+1}
      ii.  θ ← θ + α · G_t · ∇_θ log π_θ(a_t|s_t)
```

### 3.3 回报计算

从后向前累积计算折扣回报，时间复杂度 O(T):

```python
def compute_returns(rewards, gamma):
    """计算蒙特卡洛回报"""
    returns = []
    G = 0.0

    for reward in reversed(rewards):
        G = reward + gamma * G
        returns.insert(0, G)

    return returns
```

### 3.4 方差问题

REINFORCE 的主要问题是**高方差**:

- 回报 $G_t$ 包含很多随机性
- 导致梯度估计不稳定
- 训练收敛慢

---

## 四、Baseline 减少方差

### 4.1 引入 Baseline

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot (Q(s, a) - b(s))\right]$$

**为什么不改变期望?**

$$\mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdot b(s)] = 0$$

证明:
$$\sum_a \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) b(s) = b(s) \sum_a \nabla_\theta \pi_\theta(a|s) = b(s) \nabla_\theta 1 = 0$$

### 4.2 最佳 Baseline

状态价值函数 $V(s)$ 是接近最优的 baseline:

$$A(s, a) = Q(s, a) - V(s)$$

**优势函数** (Advantage Function): 衡量动作相对于平均的好坏

- $A > 0$: 动作优于平均
- $A < 0$: 动作劣于平均
- $A = 0$: 动作与平均持平

### 4.3 方差减少效果

使用 baseline 可以显著减少梯度估计的方差:

| 情况 | 方差 | 收敛速度 |
|------|------|----------|
| 无 Baseline | 高 | 慢 |
| 均值 Baseline | 中 | 中 |
| V(s) Baseline | 低 | 快 |

---

## 五、Actor-Critic 方法

### 5.1 从 REINFORCE 到 Actor-Critic

| 方法 | Q 估计 | 更新时机 | 方差 | 偏差 |
|------|--------|----------|------|------|
| REINFORCE | $G_t$ (MC) | 回合结束 | 高 | 无 |
| Actor-Critic | $r + \gamma V(s')$ (TD) | 每步更新 | 低 | 有 |

### 5.2 A2C 算法

**Advantage Actor-Critic**:

$$A(s, a) = r + \gamma V(s') - V(s) = \delta_t \quad \text{(TD误差)}$$

**损失函数**:

$$\mathcal{L} = \underbrace{-\log \pi(a|s) \cdot A}_{\text{Actor损失}} + \underbrace{c_1 (V(s) - G)^2}_{\text{Critic损失}} - \underbrace{c_2 H(\pi)}_{\text{熵正则化}}$$

### 5.3 共享网络架构

```
state -> [shared_net] -> features
                          |-> [actor_head] -> policy π(a|s)
                          |-> [critic_head] -> value V(s)
```

**共享特征的优点**:
1. 减少参数量
2. 通过多任务学习提升特征质量
3. 加速训练收敛

### 5.4 n-step Returns

平衡偏差和方差:

$$G_t^{(n)} = r_t + \gamma r_{t+1} + \ldots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})$$

| n 值 | 特点 |
|------|------|
| n=1 | TD(0)，低方差，高偏差 |
| n=∞ | MC，高方差，无偏差 |
| n=5~20 | 常用选择 |

---

## 六、GAE: 广义优势估计

### 6.1 GAE 公式 (Schulman et al., 2016)

$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \quad \text{(TD误差)}$$

$$A_t^{GAE} = \sum_{k=0}^{\infty} (\gamma\lambda)^k \delta_{t+k}$$

### 6.2 λ 参数的作用

| λ 值 | 效果 |
|------|------|
| λ=0 | TD(0)，$A_t = \delta_t$，高偏差低方差 |
| λ=1 | MC，$A_t = G_t - V(s_t)$，低偏差高方差 |
| λ=0.95 | 常用值，良好的偏差-方差权衡 |

### 6.3 计算方法

从后向前递归计算:

```python
def compute_gae(rewards, values, next_value, dones, gamma, gae_lambda):
    advantages = []
    gae = 0.0

    values = list(values) + [next_value]

    for t in reversed(range(len(rewards))):
        next_val = 0.0 if dones[t] else values[t + 1]
        delta = rewards[t] + gamma * next_val - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae
        advantages.insert(0, gae)

    return advantages
```

---

## 七、连续动作空间

### 7.1 高斯策略

$$\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)$$

### 7.2 Tanh 压缩

对于有界动作空间 $[-1, 1]$:

$$a = \tanh(u), \quad u \sim \mathcal{N}(\mu, \sigma^2)$$

**对数概率校正** (考虑变量变换的 Jacobian):

$$\log \pi(a|s) = \log \mathcal{N}(u|\mu,\sigma^2) - \sum_i \log(1 - \tanh^2(u_i))$$

### 7.3 动作缩放

实际动作 = tanh_action × action_scale + action_bias

```python
action = torch.tanh(u)  # [-1, 1]
action_scaled = action * action_scale + action_bias  # [low, high]
```

---

## 八、算法对比

### 8.1 方法总结

| 算法 | 特点 | 优点 | 缺点 |
|------|------|------|------|
| REINFORCE | MC 策略梯度 | 简单、无偏 | 高方差 |
| REINFORCE+Baseline | 加入 V(s) | 方差降低 | 仍需完整回合 |
| A2C | TD Actor-Critic | 每步更新，方差低 | 有偏差 |
| PPO | 限制更新幅度 | 稳定、高效 | 略复杂 |
| SAC | 最大熵 RL | 探索性强，样本效率高 | 仅限连续动作 |

### 8.2 选择建议

```
┌────────────────────────────────────────────────┐
│                  算法选择指南                   │
├────────────────────────────────────────────────┤
│                                                │
│  任务简单 + 离散动作?                           │
│    └── REINFORCE / A2C                        │
│                                                │
│  需要稳定性?                                    │
│    └── PPO (推荐首选)                          │
│                                                │
│  连续控制 + 样本效率?                           │
│    └── SAC / TD3                              │
│                                                │
│  多智能体 / 分布式?                             │
│    └── A3C / IMPALA                           │
│                                                │
└────────────────────────────────────────────────┘
```

---

## 九、实践技巧

### 9.1 超参数建议

| 参数 | 建议值 | 说明 |
|------|--------|------|
| 学习率 (Actor) | 1e-4 ~ 3e-4 | 策略变化应平稳 |
| 学习率 (Critic) | 1e-3 ~ 3e-3 | 价值可以快些收敛 |
| 折扣因子 γ | 0.99 | 短期任务用 0.95 |
| GAE λ | 0.95 | 平衡偏差-方差 |
| 熵系数 | 0.01 | 太大过度探索 |
| 梯度裁剪 | 0.5 ~ 1.0 | 防止梯度爆炸 |

### 9.2 常见问题

| 问题 | 可能原因 | 解决方案 |
|------|----------|----------|
| 策略坍塌 | 学习率太大 | 降低 Actor 学习率 |
| 探索不足 | 熵太低 | 增大熵系数 |
| 训练不稳定 | 方差太大 | 使用 GAE / 并行环境 |
| 收敛慢 | n-step 太短 | 增大 n-step |
| 价值估计不准 | Critic 欠拟合 | 增加网络容量 |

### 9.3 调试技巧

1. **监控熵**: 应该逐渐下降但不为零
2. **检查价值损失**: 应该稳定下降
3. **检查梯度范数**: 不应太大或太小
4. **可视化策略**: 查看动作分布是否合理
5. **检查优势分布**: 应该零均值

### 9.4 正交初始化

对于策略网络，正交初始化有助于稳定训练:

```python
nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))
nn.init.zeros_(layer.bias)
# 输出层使用较小的 gain
nn.init.orthogonal_(output_layer.weight, gain=0.01)
```

---

## 十、代码文件说明

### 10.1 模块结构

```
04-policy-gradient/
├── 策略梯度方法.md              # 本文档
├── policy_gradient.py           # 算法实现
└── policy_gradient_tutorial.ipynb  # 交互式教程
```

### 10.2 运行方式

```bash
# 运行单元测试
python policy_gradient.py --mode test

# 训练单个算法
python policy_gradient.py --mode train --algo a2c --episodes 500

# 比较多个算法
python policy_gradient.py --mode compare --episodes 300
```

### 10.3 主要类

| 类名 | 功能 |
|------|------|
| `DiscretePolicy` | 离散动作空间策略网络 |
| `ContinuousPolicy` | 连续动作空间高斯策略 |
| `ValueNetwork` | 状态价值网络 |
| `ActorCriticNetwork` | 共享特征的 AC 网络 |
| `REINFORCE` | 基础策略梯度算法 |
| `REINFORCEBaseline` | 带 Baseline 的 REINFORCE |
| `A2C` | 优势 Actor-Critic |

---

## 参考文献

1. Williams, R.J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine Learning*.
2. Sutton, R.S. et al. (1999). Policy gradient methods for reinforcement learning with function approximation. *NeurIPS*.
3. Schulman, J. et al. (2016). High-dimensional continuous control using generalized advantage estimation. *ICLR*.
4. Mnih, V. et al. (2016). Asynchronous methods for deep reinforcement learning. *ICML*.
5. Schulman, J. et al. (2017). Proximal policy optimization algorithms. *arXiv*.

---

[返回上级](../README.md)
