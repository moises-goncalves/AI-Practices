{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度方法教程\n",
    "\n",
    "## Policy Gradient Methods: From REINFORCE to Actor-Critic\n",
    "\n",
    "---\n",
    "\n",
    "本教程系统介绍策略梯度方法的理论基础和实现细节，包括：\n",
    "\n",
    "1. **策略梯度定理** - 直接优化策略的理论基础\n",
    "2. **REINFORCE算法** - 最基础的策略梯度方法\n",
    "3. **方差减少技术** - Baseline和优势函数\n",
    "4. **Actor-Critic方法** - 结合值函数的策略梯度\n",
    "5. **GAE** - 广义优势估计\n",
    "\n",
    "---\n",
    "\n",
    "**参考文献**:\n",
    "- Williams (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning\n",
    "- Sutton et al. (1999). Policy gradient methods for reinforcement learning with function approximation\n",
    "- Schulman et al. (2016). High-dimensional continuous control using generalized advantage estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设置\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 绘图设置\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"设备: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 策略梯度定理\n",
    "\n",
    "### 1.1 从值方法到策略方法\n",
    "\n",
    "| 方面 | 值方法 (DQN) | 策略方法 |\n",
    "|------|-------------|----------|\n",
    "| 学习目标 | Q(s,a) → 隐式策略 | 直接学习 π(a\\|s) |\n",
    "| 动作空间 | 主要用于离散 | 天然支持连续 |\n",
    "| 策略类型 | 确定性 (argmax) | 随机性 (概率分布) |\n",
    "\n",
    "### 1.2 目标函数\n",
    "\n",
    "最大化期望累积回报：\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$$\n",
    "\n",
    "### 1.3 策略梯度定理 (Sutton et al., 1999)\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi}(s, a)\\right]$$\n",
    "\n",
    "**直觉理解**:\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a|s)$: 增大动作a概率的方向\n",
    "- $Q^{\\pi}(s, a)$: 动作的好坏程度\n",
    "- 好动作 → 增大概率，坏动作 → 减小概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：对数概率梯度的作用\n",
    "\n",
    "def visualize_policy_gradient_intuition():\n",
    "    \"\"\"可视化策略梯度的直觉\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    # 初始策略\n",
    "    actions = ['左', '右', '跳']\n",
    "    initial_probs = [0.33, 0.33, 0.34]\n",
    "    \n",
    "    axes[0].bar(actions, initial_probs, color='steelblue', alpha=0.7)\n",
    "    axes[0].set_ylim(0, 0.8)\n",
    "    axes[0].set_title('初始策略 π(a|s)', fontsize=12)\n",
    "    axes[0].set_ylabel('概率')\n",
    "    \n",
    "    # 假设\"右\"获得高回报\n",
    "    rewards = [0.1, 0.9, 0.0]  # 右获得高回报\n",
    "    axes[1].bar(actions, rewards, color='green', alpha=0.7)\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    axes[1].set_title('动作回报 Q(s,a)', fontsize=12)\n",
    "    axes[1].set_ylabel('回报')\n",
    "    \n",
    "    # 更新后的策略\n",
    "    updated_probs = [0.15, 0.70, 0.15]  # 右的概率增加\n",
    "    axes[2].bar(actions, updated_probs, color='orange', alpha=0.7)\n",
    "    axes[2].set_ylim(0, 0.8)\n",
    "    axes[2].set_title('更新后策略 π\\'(a|s)', fontsize=12)\n",
    "    axes[2].set_ylabel('概率')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('策略梯度更新直觉：增加高回报动作的概率', y=1.02, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "visualize_policy_gradient_intuition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 策略网络实现\n",
    "\n",
    "### 2.1 离散动作空间：Softmax策略\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\frac{\\exp(h(s, a; \\theta))}{\\sum_{a'} \\exp(h(s, a'; \\theta))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    离散动作空间的策略网络\n",
    "    \n",
    "    使用Softmax输出动作概率分布。\n",
    "    网络输出logits，使用Categorical分布处理，保证数值稳定性。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # 正交初始化\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"输出动作logits\"\"\"\n",
    "        return self.net(state)\n",
    "    \n",
    "    def get_distribution(self, state: torch.Tensor) -> Categorical:\n",
    "        \"\"\"获取动作分布\"\"\"\n",
    "        logits = self.forward(state)\n",
    "        return Categorical(logits=logits)\n",
    "    \n",
    "    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"采样动作，返回(action, log_prob, entropy)\"\"\"\n",
    "        dist = self.get_distribution(state)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy()\n",
    "\n",
    "# 测试\n",
    "policy = DiscretePolicy(state_dim=4, action_dim=2)\n",
    "state = torch.randn(1, 4)\n",
    "\n",
    "action, log_prob, entropy = policy.sample(state)\n",
    "probs = policy.get_distribution(state).probs\n",
    "\n",
    "print(f\"状态: {state.squeeze().numpy().round(3)}\")\n",
    "print(f\"动作概率: {probs.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"采样动作: {action.item()}\")\n",
    "print(f\"log π(a|s): {log_prob.item():.4f}\")\n",
    "print(f\"熵: {entropy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 连续动作空间：高斯策略\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s)^2)$$\n",
    "\n",
    "对于有界动作空间，使用**Tanh**压缩并校正对数概率：\n",
    "\n",
    "$$a = \\tanh(u), \\quad u \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "\n",
    "$$\\log \\pi(a|s) = \\log \\mathcal{N}(u|\\mu,\\sigma^2) - \\sum_i \\log(1 - \\tanh^2(u_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    连续动作空间的高斯策略\n",
    "    \n",
    "    输出高斯分布的均值和标准差。\n",
    "    使用Tanh将动作压缩到[-1, 1]范围。\n",
    "    \"\"\"\n",
    "    \n",
    "    LOG_STD_MIN = -20.0\n",
    "    LOG_STD_MAX = 2.0\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # 可学习的对数标准差\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"输出(mean, std)\"\"\"\n",
    "        features = self.feature_net(state)\n",
    "        mean = self.mean_layer(features)\n",
    "        log_std = torch.clamp(self.log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "    \n",
    "    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"采样动作，使用重参数化技巧\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        dist = Normal(mean, std)\n",
    "        \n",
    "        # 重参数化采样\n",
    "        u = dist.rsample()\n",
    "        action = torch.tanh(u)  # 压缩到[-1, 1]\n",
    "        \n",
    "        # 校正log_prob（考虑Tanh变换的Jacobian）\n",
    "        log_prob = dist.log_prob(u).sum(dim=-1)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6).sum(dim=-1)\n",
    "        \n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "# 测试\n",
    "cont_policy = ContinuousPolicy(state_dim=3, action_dim=2)\n",
    "state = torch.randn(1, 3)\n",
    "\n",
    "action, log_prob, entropy = cont_policy.sample(state)\n",
    "mean, std = cont_policy.forward(state)\n",
    "\n",
    "print(f\"均值 μ: {mean.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"标准差 σ: {std.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"采样动作 (tanh后): {action.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"动作范围验证: [{action.min().item():.3f}, {action.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. REINFORCE算法\n",
    "\n",
    "### 3.1 算法原理\n",
    "\n",
    "REINFORCE (Williams, 1992) 使用**蒙特卡洛回报**作为Q值的无偏估计：\n",
    "\n",
    "$$G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$$\n",
    "\n",
    "梯度估计：\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}$$\n",
    "\n",
    "### 3.2 算法流程\n",
    "\n",
    "```\n",
    "1. 初始化策略参数 θ\n",
    "2. 循环:\n",
    "   a. 采集一个完整轨迹 τ = (s_0, a_0, r_1, ..., s_T)\n",
    "   b. 对于 t = 0, 1, ..., T-1:\n",
    "      - 计算回报 G_t = Σ γ^k r_{t+k}\n",
    "      - θ ← θ + α · G_t · ∇_θ log π_θ(a_t|s_t)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards: List[float], gamma: float, normalize: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算蒙特卡洛回报\n",
    "    \n",
    "    G_t = r_t + γr_{t+1} + γ²r_{t+2} + ...\n",
    "    \n",
    "    从后向前累积计算，时间复杂度O(T)\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    \n",
    "    for reward in reversed(rewards):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    if normalize and len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# 演示回报计算\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "gamma = 0.99\n",
    "\n",
    "returns = compute_returns(rewards, gamma, normalize=False)\n",
    "print(\"奖励序列:\", rewards)\n",
    "print(f\"回报序列 (γ={gamma}):\")\n",
    "for t, (r, G) in enumerate(zip(rewards, returns)):\n",
    "    print(f\"  t={t}: r={r:.1f}, G_t={G.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "    REINFORCE算法实现\n",
    "    \n",
    "    特点:\n",
    "    - 使用完整回合的回报 (Monte Carlo)\n",
    "    - 无偏估计，但方差较高\n",
    "    - 需要完整回合才能更新\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, lr: float = 1e-3, gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.policy = DiscretePolicy(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # 存储轨迹\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"根据策略采样动作\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, log_prob, _ = self.policy.sample(state_t)\n",
    "        \n",
    "        self.log_probs.append(log_prob)\n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        \"\"\"存储奖励\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self) -> float:\n",
    "        \"\"\"回合结束后更新策略\"\"\"\n",
    "        # 计算蒙特卡洛回报\n",
    "        returns = compute_returns(self.rewards, self.gamma, normalize=True)\n",
    "        \n",
    "        # 策略损失: -E[log π(a|s) * G]\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        policy_loss = -(log_probs * returns).mean()\n",
    "        \n",
    "        # 优化\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 清空\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "print(\"REINFORCE算法类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 方差减少：Baseline\n",
    "\n",
    "### 4.1 高方差问题\n",
    "\n",
    "REINFORCE的主要问题是**高方差**：\n",
    "- 回报 $G_t$ 包含很多随机性\n",
    "- 导致梯度估计不稳定\n",
    "- 训练收敛慢\n",
    "\n",
    "### 4.2 引入Baseline\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (Q(s, a) - b(s))\\right]$$\n",
    "\n",
    "**为什么不改变期望？**\n",
    "\n",
    "$$\\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot b(s)] = 0$$\n",
    "\n",
    "### 4.3 最优Baseline\n",
    "\n",
    "状态价值函数 $V(s)$ 是接近最优的baseline：\n",
    "\n",
    "$$A(s, a) = Q(s, a) - V(s)$$\n",
    "\n",
    "这称为**优势函数** (Advantage Function)，表示动作相对于平均水平的好坏程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：Baseline减少方差的效果\n",
    "\n",
    "def visualize_baseline_effect():\n",
    "    \"\"\"演示Baseline如何减少方差\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 模拟回报分布\n",
    "    n_samples = 1000\n",
    "    base_return = 100  # 平均回报较高\n",
    "    returns = np.random.normal(base_return, 20, n_samples)  # 高方差\n",
    "    \n",
    "    # 不使用baseline的梯度信号\n",
    "    gradient_no_baseline = returns\n",
    "    \n",
    "    # 使用baseline (减去均值)\n",
    "    baseline = returns.mean()\n",
    "    gradient_with_baseline = returns - baseline\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].hist(gradient_no_baseline, bins=50, alpha=0.7, color='steelblue')\n",
    "    axes[0].axvline(x=0, color='red', linestyle='--', label='零点')\n",
    "    axes[0].set_title(f'无Baseline\\n均值={np.mean(gradient_no_baseline):.1f}, 方差={np.var(gradient_no_baseline):.1f}')\n",
    "    axes[0].set_xlabel('梯度信号')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].hist(gradient_with_baseline, bins=50, alpha=0.7, color='orange')\n",
    "    axes[1].axvline(x=0, color='red', linestyle='--', label='零点')\n",
    "    axes[1].set_title(f'有Baseline\\n均值={np.mean(gradient_with_baseline):.1f}, 方差={np.var(gradient_with_baseline):.1f}')\n",
    "    axes[1].set_xlabel('梯度信号')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.suptitle('Baseline减少方差（同时保持期望不变）', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"方差减少比例: {np.var(gradient_no_baseline) / np.var(gradient_with_baseline):.2f}x\")\n",
    "\n",
    "visualize_baseline_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"状态价值网络 V(s)\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "class REINFORCEBaseline:\n",
    "    \"\"\"\n",
    "    带Baseline的REINFORCE\n",
    "    \n",
    "    使用状态价值函数V(s)作为baseline减少方差：\n",
    "    A(s,a) = G - V(s)  (优势函数)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, \n",
    "                 lr_policy: float = 1e-3, lr_value: float = 1e-3, gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.policy = DiscretePolicy(state_dim, action_dim)\n",
    "        self.value_net = ValueNetwork(state_dim)\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        action, log_prob, _ = self.policy.sample(state_t)\n",
    "        value = self.value_net(state_t)\n",
    "        \n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self) -> Tuple[float, float]:\n",
    "        # 计算回报\n",
    "        returns = compute_returns(self.rewards, self.gamma, normalize=False)\n",
    "        \n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.cat(self.values).squeeze()\n",
    "        \n",
    "        # 计算优势: A = G - V\n",
    "        advantages = returns - values.detach()  # detach很重要！\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # 策略损失\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # 价值损失 (MSE)\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # 更新\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # 清空\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "\n",
    "print(\"REINFORCEBaseline算法类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Actor-Critic方法\n",
    "\n",
    "### 5.1 从REINFORCE到Actor-Critic\n",
    "\n",
    "| 方法 | Q估计 | 更新时机 |\n",
    "|------|--------|----------|\n",
    "| REINFORCE | $G_t$ (MC) | 回合结束 |\n",
    "| Actor-Critic | $r + \\gamma V(s')$ (TD) | 每步更新 |\n",
    "\n",
    "### 5.2 TD优势估计\n",
    "\n",
    "$$A(s, a) = r + \\gamma V(s') - V(s) = \\delta_t \\quad (\\text{TD误差})$$\n",
    "\n",
    "### 5.3 A2C损失函数\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{-\\log \\pi(a|s) \\cdot A}_{\\text{Actor损失}} + \\underbrace{c_1 (V(s) - G)^2}_{\\text{Critic损失}} - \\underbrace{c_2 H(\\pi)}_{\\text{熵正则化}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    共享特征的Actor-Critic网络\n",
    "    \n",
    "    架构:\n",
    "    state -> [shared_net] -> features\n",
    "                              |-> [actor_head] -> policy\n",
    "                              |-> [critic_head] -> value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 共享特征层\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor头\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Critic头\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor):\n",
    "        features = self.shared(state)\n",
    "        logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return Categorical(logits=logits), value\n",
    "    \n",
    "    def get_action_and_value(self, state: torch.Tensor):\n",
    "        dist, value = self.forward(state)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value.squeeze(-1)\n",
    "\n",
    "# 测试\n",
    "ac_net = ActorCriticNetwork(4, 2)\n",
    "state = torch.randn(1, 4)\n",
    "action, log_prob, entropy, value = ac_net.get_action_and_value(state)\n",
    "\n",
    "print(f\"动作: {action.item()}\")\n",
    "print(f\"log π(a|s): {log_prob.item():.4f}\")\n",
    "print(f\"熵: {entropy.item():.4f}\")\n",
    "print(f\"V(s): {value.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GAE: 广义优势估计\n",
    "\n",
    "### 6.1 n-step Returns\n",
    "\n",
    "$$G_t^{(n)} = r_t + \\gamma r_{t+1} + \\ldots + \\gamma^{n-1} r_{t+n-1} + \\gamma^n V(s_{t+n})$$\n",
    "\n",
    "| n值 | 特点 |\n",
    "|-----|------|\n",
    "| n=1 | TD(0)，低方差，高偏差 |\n",
    "| n=∞ | MC，高方差，无偏差 |\n",
    "\n",
    "### 6.2 GAE公式\n",
    "\n",
    "GAE通过指数加权的多步TD误差来平衡偏差-方差：\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "$$A_t^{GAE} = \\sum_{k=0}^{\\infty} (\\gamma\\lambda)^k \\delta_{t+k}$$\n",
    "\n",
    "- $\\lambda=0$: TD(0)，高偏差低方差\n",
    "- $\\lambda=1$: Monte Carlo，低偏差高方差\n",
    "- $\\lambda=0.95$: 常用值，较好的平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    rewards: List[float],\n",
    "    values: List[float],\n",
    "    next_value: float,\n",
    "    dones: List[bool],\n",
    "    gamma: float,\n",
    "    gae_lambda: float\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    计算GAE (Generalized Advantage Estimation)\n",
    "    \n",
    "    δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "    A_t^GAE = Σ (γλ)^k δ_{t+k}\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    gae = 0.0\n",
    "    \n",
    "    values = list(values) + [next_value]\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        # 如果done，下一状态价值为0\n",
    "        next_val = 0.0 if dones[t] else values[t + 1]\n",
    "        \n",
    "        # TD误差\n",
    "        delta = rewards[t] + gamma * next_val - values[t]\n",
    "        \n",
    "        # GAE累积\n",
    "        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    returns = advantages + torch.tensor(values[:-1], dtype=torch.float32)\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "# 演示GAE计算\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "values = [0.8, 0.9, 1.0, 0.95, 0.85]\n",
    "dones = [False, False, False, False, True]\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "\n",
    "advantages, returns = compute_gae(rewards, values, 0.0, dones, gamma, gae_lambda)\n",
    "\n",
    "print(\"GAE计算示例:\")\n",
    "print(f\"奖励: {rewards}\")\n",
    "print(f\"价值估计: {values}\")\n",
    "print(f\"优势 (GAE λ={gae_lambda}): {advantages.numpy().round(3)}\")\n",
    "print(f\"回报目标: {returns.numpy().round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化：不同λ值对优势估计的影响\n",
    "\n",
    "def visualize_gae_lambda_effect():\n",
    "    \"\"\"可视化不同λ值的影响\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 模拟一个回合\n",
    "    T = 50\n",
    "    rewards = np.random.normal(1.0, 0.5, T)\n",
    "    true_values = np.cumsum(rewards[::-1])[::-1] * 0.99 ** np.arange(T)  # 近似真实价值\n",
    "    noisy_values = true_values + np.random.normal(0, 0.3, T)  # 加噪声的估计\n",
    "    dones = [False] * (T-1) + [True]\n",
    "    gamma = 0.99\n",
    "    \n",
    "    lambdas = [0.0, 0.5, 0.9, 1.0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    for ax, lam in zip(axes.flat, lambdas):\n",
    "        advantages, _ = compute_gae(rewards.tolist(), noisy_values.tolist(), 0.0, dones, gamma, lam)\n",
    "        \n",
    "        ax.plot(advantages.numpy(), label=f'GAE λ={lam}')\n",
    "        ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('时间步')\n",
    "        ax.set_ylabel('优势估计')\n",
    "        ax.set_title(f'λ={lam}: 方差={advantages.std().item():.3f}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.suptitle('GAE: 不同λ值对优势估计的影响', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gae_lambda_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 完整A2C实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C) 完整实现\n",
    "    \n",
    "    特点:\n",
    "    - 使用GAE计算优势\n",
    "    - 熵正则化促进探索\n",
    "    - 梯度裁剪防止爆炸\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.model = ActorCriticNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # 缓冲区\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, log_prob, entropy, value = self.model.get_action_and_value(state_t)\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.actions.append(action.item())\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value.item())\n",
    "        self.entropies.append(entropy)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store(self, reward: float, done: bool):\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def update(self, next_state: np.ndarray, done: bool) -> Dict[str, float]:\n",
    "        # 获取下一状态价值\n",
    "        if done:\n",
    "            next_value = 0.0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                next_state_t = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                _, next_value = self.model(next_state_t)\n",
    "                next_value = next_value.item()\n",
    "        \n",
    "        # 计算GAE\n",
    "        advantages, returns = compute_gae(\n",
    "            self.rewards, self.values, next_value, \n",
    "            self.dones, self.gamma, self.gae_lambda\n",
    "        )\n",
    "        \n",
    "        # 标准化优势\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # 准备数据\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.tensor(self.values, dtype=torch.float32)\n",
    "        entropies = torch.stack(self.entropies)\n",
    "        \n",
    "        # 计算损失\n",
    "        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        entropy_bonus = entropies.mean()\n",
    "        \n",
    "        total_loss = (\n",
    "            policy_loss \n",
    "            + self.value_coef * value_loss \n",
    "            - self.entropy_coef * entropy_bonus\n",
    "        )\n",
    "        \n",
    "        # 优化\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 清空\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        return {\n",
    "            \"policy_loss\": policy_loss.item(),\n",
    "            \"value_loss\": value_loss.item(),\n",
    "            \"entropy\": entropy_bonus.item(),\n",
    "            \"total_loss\": total_loss.item()\n",
    "        }\n",
    "\n",
    "print(\"A2C算法类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试导入gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "    print(\"Gymnasium导入成功\")\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"警告: 未安装gymnasium，跳过实际训练演示\")\n",
    "    print(\"安装命令: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env_name: str, num_episodes: int = 300, log_interval: int = 50):\n",
    "    \"\"\"训练智能体\"\"\"\n",
    "    if not HAS_GYM:\n",
    "        print(\"需要安装gymnasium才能训练\")\n",
    "        return []\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    rewards_history = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"训练 {agent.__class__.__name__} on {env_name}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if hasattr(agent, 'store'):\n",
    "                agent.store(reward, done)\n",
    "            else:\n",
    "                agent.store_reward(reward)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 更新\n",
    "        if isinstance(agent, A2C):\n",
    "            agent.update(next_state, done)\n",
    "        else:\n",
    "            agent.update()\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            print(f\"Episode {episode+1:4d} | 平均奖励: {avg_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练对比实验\n",
    "if HAS_GYM:\n",
    "    env_name = \"CartPole-v1\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    env.close()\n",
    "    \n",
    "    num_episodes = 300  # 可调整\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # REINFORCE\n",
    "    print(\"\\n[1/3] 训练REINFORCE...\")\n",
    "    agent_rf = REINFORCE(state_dim, action_dim, lr=1e-3, gamma=0.99)\n",
    "    results['REINFORCE'] = train_agent(agent_rf, env_name, num_episodes)\n",
    "    \n",
    "    # REINFORCE + Baseline\n",
    "    print(\"\\n[2/3] 训练REINFORCE+Baseline...\")\n",
    "    agent_rfb = REINFORCEBaseline(state_dim, action_dim, lr_policy=1e-3, lr_value=1e-3, gamma=0.99)\n",
    "    results['REINFORCE+Baseline'] = train_agent(agent_rfb, env_name, num_episodes)\n",
    "    \n",
    "    # A2C\n",
    "    print(\"\\n[3/3] 训练A2C...\")\n",
    "    agent_a2c = A2C(state_dim, action_dim, lr=3e-4, gamma=0.99, gae_lambda=0.95)\n",
    "    results['A2C (GAE)'] = train_agent(agent_a2c, env_name, num_episodes)\n",
    "else:\n",
    "    print(\"跳过训练（未安装gymnasium）\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制学习曲线\n",
    "def plot_learning_curves(results: Dict[str, List[float]], window: int = 50):\n",
    "    \"\"\"绘制学习曲线对比\"\"\"\n",
    "    if not results:\n",
    "        print(\"没有训练数据可以绘制\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    colors = ['steelblue', 'orange', 'green']\n",
    "    \n",
    "    for (name, rewards), color in zip(results.items(), colors):\n",
    "        # 原始曲线\n",
    "        ax.plot(rewards, alpha=0.2, color=color)\n",
    "        \n",
    "        # 滑动平均\n",
    "        if len(rewards) > window:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            ax.plot(range(window-1, len(rewards)), smoothed, label=name, color=color, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Total Reward')\n",
    "    ax.set_title('策略梯度方法对比')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 算法对比总结\n",
    "\n",
    "| 算法 | Q估计 | 更新时机 | 方差 | 偏差 | 推荐场景 |\n",
    "|------|-------|----------|------|------|----------|\n",
    "| REINFORCE | MC ($G_t$) | 回合结束 | 高 | 无 | 教学/简单环境 |\n",
    "| REINFORCE+Baseline | MC + V(s) | 回合结束 | 中 | 无 | 一般任务 |\n",
    "| A2C | TD + GAE | 每步/n-step | 低 | 有 | 生产环境 |\n",
    "| PPO | A2C + 裁剪 | 每步/n-step | 低 | 有 | **推荐首选** |\n",
    "\n",
    "### 实践建议\n",
    "\n",
    "1. **学习率**: Actor学习率应小于Critic（如1e-4 vs 1e-3）\n",
    "2. **GAE λ**: 通常使用0.95，平衡偏差-方差\n",
    "3. **熵系数**: 0.01左右，太大过度探索，太小过早收敛\n",
    "4. **梯度裁剪**: 0.5~1.0，防止不稳定\n",
    "5. **优势标准化**: 几乎总是有益的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 练习题\n",
    "\n",
    "### 练习1: 理解策略梯度\n",
    "\n",
    "为什么策略梯度公式中使用$\\log \\pi$而不是$\\pi$？请从数学和直觉两个角度解释。\n",
    "\n",
    "### 练习2: 实现改进\n",
    "\n",
    "尝试给REINFORCE添加以下改进，观察效果：\n",
    "1. 因果性：只使用未来奖励（而非整个回合）\n",
    "2. 奖励归一化\n",
    "\n",
    "### 练习3: 连续动作\n",
    "\n",
    "使用`ContinuousPolicy`类在`Pendulum-v1`环境上训练A2C。\n",
    "\n",
    "### 练习4: 超参数调优\n",
    "\n",
    "实验不同的GAE λ值(0.9, 0.95, 0.99, 1.0)，比较学习曲线的稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习代码区域\n",
    "# 在这里编写你的练习代码\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Sutton & Barto, \"Reinforcement Learning: An Introduction\", Chapter 13\n",
    "2. Schulman et al., \"High-Dimensional Continuous Control Using GAE\", 2016\n",
    "3. Mnih et al., \"Asynchronous Methods for Deep RL\" (A3C), 2016\n",
    "4. OpenAI Spinning Up: https://spinningup.openai.com/\n",
    "\n",
    "---\n",
    "\n",
    "[返回上级](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
