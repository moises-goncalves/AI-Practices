{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸åŠ¨æ€è§„åˆ’\n\n---\n\n## å­¦ä¹ ç›®æ ‡\n\né€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å°†å­¦ä¼šï¼š\n- ç†è§£é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP) çš„æ•°å­¦å®šä¹‰\n- æŒæ¡è´å°”æ›¼æ–¹ç¨‹ï¼ˆæœŸæœ›ä¸æœ€ä¼˜ï¼‰\n- å®ç°ç­–ç•¥è¯„ä¼° (Policy Evaluation)\n- å®ç°ç­–ç•¥è¿­ä»£ (Policy Iteration)\n- å®ç°å€¼è¿­ä»£ (Value Iteration)\n- åœ¨ç½‘æ ¼ä¸–ç•Œç¯å¢ƒä¸­éªŒè¯ç®—æ³•\n\n## å‰ç½®çŸ¥è¯†\n\n- Python åŸºç¡€è¯­æ³•\n- NumPy åŸºæœ¬æ“ä½œ\n- æ¦‚ç‡è®ºåŸºç¡€ï¼ˆæ¡ä»¶æ¦‚ç‡ã€æœŸæœ›ï¼‰\n- çº¿æ€§ä»£æ•°åŸºç¡€\n\n## é¢„è®¡æ—¶é—´\n\n45-60 åˆ†é’Ÿ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ç¬¬1éƒ¨åˆ†ï¼šç†è®ºèƒŒæ™¯\n\n### ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ\n\nå¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸‰å¤§èŒƒå¼ä¹‹ä¸€ï¼Œç ”ç©¶æ™ºèƒ½ä½“ (Agent) å¦‚ä½•åœ¨ä¸ç¯å¢ƒ (Environment) çš„äº¤äº’ä¸­å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚\n\n```\n              åŠ¨ä½œ a_t\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚                    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Agent  â”‚          â”‚   Env   â”‚\nâ”‚ (æ™ºèƒ½ä½“) â”‚          â”‚  (ç¯å¢ƒ)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     â–²                    â”‚\n     â”‚   çŠ¶æ€ s_{t+1}     â”‚\n     â”‚   å¥–åŠ± r_{t+1}     â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)\n\nMDP æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦æ¡†æ¶ï¼Œç”±äº”å…ƒç»„å®šä¹‰ï¼š\n\n$$\\mathcal{M} = \\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle$$\n\n| ç¬¦å· | åç§° | å®šä¹‰ |\n|------|------|------|\n| $\\mathcal{S}$ | çŠ¶æ€ç©ºé—´ | æ‰€æœ‰å¯èƒ½çŠ¶æ€çš„é›†åˆ |\n| $\\mathcal{A}$ | åŠ¨ä½œç©ºé—´ | æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„é›†åˆ |\n| $P$ | è½¬ç§»å‡½æ•° | $P(s' \\mid s, a) = \\Pr(S_{t+1}=s' \\mid S_t=s, A_t=a)$ |\n| $R$ | å¥–åŠ±å‡½æ•° | $R(s, a, s')$ æˆ– $R(s, a)$ |\n| $\\gamma$ | æŠ˜æ‰£å› å­ | $\\gamma \\in [0, 1]$ |\n\n### é©¬å°”å¯å¤«æ€§è´¨\n\næ ¸å¿ƒå‡è®¾ï¼šæœªæ¥çŠ¶æ€ä»…ä¾èµ–å½“å‰çŠ¶æ€ï¼Œä¸å†å²æ— å…³ã€‚\n\n$$P(S_{t+1} \\mid S_t, A_t, S_{t-1}, A_{t-1}, \\ldots) = P(S_{t+1} \\mid S_t, A_t)$$\n\n### ç­–ç•¥ä¸ä»·å€¼å‡½æ•°\n\nç­–ç•¥ $\\pi$: å®šä¹‰æ™ºèƒ½ä½“åœ¨å„çŠ¶æ€ä¸‹çš„è¡Œä¸ºæ–¹å¼\n- éšæœºç­–ç•¥: $\\pi(a \\mid s) = \\Pr(A_t = a \\mid S_t = s)$\n- ç¡®å®šæ€§ç­–ç•¥: $a = \\pi(s)$\n\nçŠ¶æ€ä»·å€¼å‡½æ•° $V^\\pi(s)$:\n$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$$\n\nåŠ¨ä½œä»·å€¼å‡½æ•° $Q^\\pi(s, a)$:\n$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$$\n\n### è´å°”æ›¼æ–¹ç¨‹\n\nè´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼ˆç»™å®šç­–ç•¥ï¼‰:\n$$V^\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^\\pi(s') \\right]$$\n\nè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹:\n$$V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^*(s') \\right]$$\n\nç›´è§‰ç†è§£: å½“å‰ä»·å€¼ = å³æ—¶å¥–åŠ± + æŠ˜æ‰£åçš„åç»§çŠ¶æ€ä»·å€¼"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» ç¬¬2éƒ¨åˆ†ï¼šä»£ç å®ç°\n",
    "\n",
    "### æ­¥éª¤1: å¯¼å…¥åº“å’Œé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# å¯¼å…¥å¿…è¦çš„åº“\n# ============================================================\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nfrom IPython.display import display, HTML\n\n# ============================================================\n# é…ç½®å‚æ•°\n# ============================================================\n\n# è®¾ç½®éšæœºç§å­\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n# å¯è§†åŒ–é…ç½®\nplt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\nplt.rcParams['axes.unicode_minus'] = False\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"åº“å¯¼å…¥å’Œé…ç½®å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­¥éª¤2: å®šä¹‰ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ\n",
    "\n",
    "æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª 4Ã—4 çš„ç½‘æ ¼ä¸–ç•Œï¼š\n",
    "- **S**: èµ·ç‚¹ (0,0)\n",
    "- **G**: ç›®æ ‡ (3,3)\n",
    "- **X**: éšœç¢ç‰©\n",
    "- æ¯æ­¥ç§»åŠ¨è·å¾— -1 å¥–åŠ±ï¼ˆé¼“åŠ±å¿«é€Ÿåˆ°è¾¾ç›®æ ‡ï¼‰\n",
    "- åˆ°è¾¾ç›®æ ‡è·å¾— +100 å¥–åŠ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ç½‘æ ¼ä¸–ç•Œé…ç½®\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class GridConfig:\n",
    "    \"\"\"ç½‘æ ¼ä¸–ç•Œé…ç½®\"\"\"\n",
    "    size: int = 4\n",
    "    start: Tuple[int, int] = (0, 0)\n",
    "    goal: Tuple[int, int] = (3, 3)\n",
    "    obstacles: List[Tuple[int, int]] = None\n",
    "    step_reward: float = -1.0\n",
    "    goal_reward: float = 100.0\n",
    "    slip_prob: float = 0.0  # æ»‘åŠ¨æ¦‚ç‡ï¼ˆéšæœºæ€§ï¼‰\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.obstacles is None:\n",
    "            self.obstacles = [(1, 1)]  # é»˜è®¤éšœç¢ç‰©\n",
    "\n",
    "# åˆ›å»ºé»˜è®¤é…ç½®\n",
    "config = GridConfig()\n",
    "\n",
    "print(f\"ç½‘æ ¼å¤§å°: {config.size}Ã—{config.size}\")\n",
    "print(f\"èµ·ç‚¹: {config.start}\")\n",
    "print(f\"ç›®æ ‡: {config.goal}\")\n",
    "print(f\"éšœç¢ç‰©: {config.obstacles}\")\n",
    "print(f\"æ¯æ­¥å¥–åŠ±: {config.step_reward}\")\n",
    "print(f\"ç›®æ ‡å¥–åŠ±: {config.goal_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ç½‘æ ¼ä¸–ç•Œç¯å¢ƒç±»\n# ============================================================\n\nclass GridWorld:\n    \"\"\"\n    ç½‘æ ¼ä¸–ç•Œ MDP ç¯å¢ƒ\n    \n    çŠ¶æ€: (è¡Œ, åˆ—) å…ƒç»„\n    åŠ¨ä½œ: 'ä¸Š', 'ä¸‹', 'å·¦', 'å³'\n    \"\"\"\n    \n    # åŠ¨ä½œåˆ°åæ ‡å˜åŒ–çš„æ˜ å°„\n    ACTION_DELTAS = {\n        'ä¸Š': (-1, 0),\n        'ä¸‹': (1, 0),\n        'å·¦': (0, -1),\n        'å³': (0, 1)\n    }\n    \n    def __init__(self, config: GridConfig):\n        self.config = config\n        self.size = config.size\n        self.goal = config.goal\n        self.obstacles = set(config.obstacles)\n        \n        # æ„å»ºçŠ¶æ€ç©ºé—´ï¼ˆæ’é™¤éšœç¢ç‰©ï¼‰\n        self.states = [\n            (i, j) for i in range(self.size) \n            for j in range(self.size)\n            if (i, j) not in self.obstacles\n        ]\n        \n        # åŠ¨ä½œç©ºé—´\n        self.actions = list(self.ACTION_DELTAS.keys())\n        \n    def is_terminal(self, state: Tuple[int, int]) -> bool:\n        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€\"\"\"\n        return state == self.goal\n    \n    def get_next_state(self, state: Tuple[int, int], action: str) -> Tuple[int, int]:\n        \"\"\"è®¡ç®—ä¸‹ä¸€çŠ¶æ€ï¼ˆå¤„ç†è¾¹ç•Œå’Œéšœç¢ç‰©ç¢°æ’ï¼‰\"\"\"\n        if self.is_terminal(state):\n            return state\n            \n        di, dj = self.ACTION_DELTAS[action]\n        ni = max(0, min(self.size - 1, state[0] + di))\n        nj = max(0, min(self.size - 1, state[1] + dj))\n        next_state = (ni, nj)\n        \n        # ç¢°åˆ°éšœç¢ç‰©åˆ™åœç•™åŸåœ°\n        if next_state in self.obstacles:\n            return state\n        return next_state\n    \n    def get_reward(self, state: Tuple[int, int], next_state: Tuple[int, int]) -> float:\n        \"\"\"è·å–å¥–åŠ±\"\"\"\n        if next_state == self.goal:\n            return self.config.goal_reward\n        return self.config.step_reward\n    \n    def get_transitions(self, state: Tuple[int, int], action: str) -> List[Tuple[Tuple[int, int], float, float]]:\n        \"\"\"\n        è·å–çŠ¶æ€è½¬ç§»åˆ†å¸ƒ\n        \n        Returns:\n            [(next_state, probability, reward), ...]\n        \"\"\"\n        if self.is_terminal(state):\n            return [(state, 1.0, 0.0)]\n        \n        next_state = self.get_next_state(state, action)\n        reward = self.get_reward(state, next_state)\n        return [(next_state, 1.0, reward)]  # ç¡®å®šæ€§ç¯å¢ƒ\n\n# åˆ›å»ºç¯å¢ƒ\nenv = GridWorld(config)\n\nprint(f\"\\nç¯å¢ƒåˆ›å»ºæˆåŠŸ\")\nprint(f\"çŠ¶æ€ç©ºé—´å¤§å°: {len(env.states)}\")\nprint(f\"åŠ¨ä½œç©ºé—´: {env.actions}\")\nprint(f\"çŠ¶æ€åˆ—è¡¨: {env.states}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­¥éª¤3: å¯è§†åŒ–ç½‘æ ¼ä¸–ç•Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å¯è§†åŒ–å‡½æ•°\n",
    "# ============================================================\n",
    "\n",
    "def visualize_grid(env: GridWorld, title: str = \"Grid World\"):\n",
    "    \"\"\"å¯è§†åŒ–ç½‘æ ¼ä¸–ç•Œ\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # ç»˜åˆ¶ç½‘æ ¼\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(y=i, color='black', linewidth=1)\n",
    "        ax.axvline(x=i, color='black', linewidth=1)\n",
    "    \n",
    "    # æ ‡è®°ç‰¹æ®Šä½ç½®\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) == env.config.start:\n",
    "                ax.text(j + 0.5, env.size - i - 0.5, 'S', \n",
    "                       ha='center', va='center', fontsize=20, fontweight='bold', color='blue')\n",
    "            elif (i, j) == env.goal:\n",
    "                ax.text(j + 0.5, env.size - i - 0.5, 'G', \n",
    "                       ha='center', va='center', fontsize=20, fontweight='bold', color='green')\n",
    "                ax.add_patch(plt.Rectangle((j, env.size - i - 1), 1, 1, \n",
    "                                          facecolor='lightgreen', alpha=0.5))\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size - i - 1), 1, 1, \n",
    "                                          facecolor='gray', alpha=0.8))\n",
    "                ax.text(j + 0.5, env.size - i - 0.5, 'X', \n",
    "                       ha='center', va='center', fontsize=20, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_grid(env, \"ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ç¬¬3éƒ¨åˆ†ï¼šåŠ¨æ€è§„åˆ’ç®—æ³•\n\n### 3.1 ç­–ç•¥è¯„ä¼° (Policy Evaluation)\n\nç»™å®šç­–ç•¥ $\\pi$ï¼Œè¿­ä»£è®¡ç®—å…¶ä»·å€¼å‡½æ•°ï¼š\n\n$$V_{k+1}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a) [R(s,a,s') + \\gamma V_k(s')]$$\n\nç®—æ³•èƒŒæ™¯ï¼šç­–ç•¥è¯„ä¼°ç”±Richard Bellmanæå‡ºï¼Œæ˜¯åŠ¨æ€è§„åˆ’çš„åŸºç¡€ç®—å­ï¼Œç”¨äºè¯„ä¼°ç»™å®šç­–ç•¥çš„ä»·å€¼ã€‚é€šè¿‡è¿­ä»£æ±‚è§£è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼Œå°†å½“å‰ä»·å€¼ä¼°è®¡ä»£å…¥æ–¹ç¨‹å³ä¾§äº§ç”Ÿæ–°ä¼°è®¡ï¼Œç›´åˆ°æ”¶æ•›ã€‚è¿™æ˜¯ä¸€ä¸ªä¸åŠ¨ç‚¹è¿­ä»£è¿‡ç¨‹ï¼Œä¿è¯çº¿æ€§æ”¶æ•›åˆ°ç­–ç•¥çš„çœŸå®ä»·å€¼å‡½æ•°ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ç­–ç•¥è¯„ä¼°ç®—æ³•\n# ============================================================\n\ndef policy_evaluation(\n    env: GridWorld,\n    policy: Dict[Tuple[int, int], Dict[str, float]],\n    gamma: float = 0.99,\n    theta: float = 1e-6,\n    max_iterations: int = 10000\n) -> Tuple[Dict[Tuple[int, int], float], int]:\n    \"\"\"\n    ç­–ç•¥è¯„ä¼°ï¼šè®¡ç®—ç»™å®šç­–ç•¥çš„çŠ¶æ€ä»·å€¼å‡½æ•°\n    \n    Args:\n        env: ç¯å¢ƒ\n        policy: ç­–ç•¥ Ï€(a|s)\n        gamma: æŠ˜æ‰£å› å­\n        theta: æ”¶æ•›é˜ˆå€¼\n        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°\n        \n    Returns:\n        (V, iterations): ä»·å€¼å‡½æ•°å’Œè¿­ä»£æ¬¡æ•°\n    \"\"\"\n    # åˆå§‹åŒ–ä»·å€¼å‡½æ•°\n    V = {s: 0.0 for s in env.states}\n    \n    for iteration in range(1, max_iterations + 1):\n        delta = 0.0\n        \n        for state in env.states:\n            if env.is_terminal(state):\n                continue\n                \n            old_value = V[state]\n            new_value = 0.0\n            \n            # è´å°”æ›¼æœŸæœ›æ–¹ç¨‹\n            for action in env.actions:\n                action_prob = policy.get(state, {}).get(action, 0.0)\n                if action_prob > 0:\n                    for next_state, trans_prob, reward in env.get_transitions(state, action):\n                        new_value += action_prob * trans_prob * (reward + gamma * V.get(next_state, 0.0))\n            \n            V[state] = new_value\n            delta = max(delta, abs(old_value - new_value))\n        \n        if delta < theta:\n            return V, iteration\n    \n    return V, max_iterations\n\nprint(\"ç­–ç•¥è¯„ä¼°å‡½æ•°å®šä¹‰å®Œæˆ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# æµ‹è¯•ç­–ç•¥è¯„ä¼°ï¼šå‡åŒ€éšæœºç­–ç•¥\n",
    "# ============================================================\n",
    "\n",
    "# åˆ›å»ºå‡åŒ€éšæœºç­–ç•¥\n",
    "random_policy = {\n",
    "    s: {a: 0.25 for a in env.actions}\n",
    "    for s in env.states\n",
    "}\n",
    "\n",
    "print(\"éšæœºç­–ç•¥ç¤ºä¾‹ï¼ˆæ¯ä¸ªåŠ¨ä½œæ¦‚ç‡ç›¸ç­‰ï¼‰:\")\n",
    "print(f\"  çŠ¶æ€ (0,0): {random_policy[(0, 0)]}\")\n",
    "\n",
    "# æ‰§è¡Œç­–ç•¥è¯„ä¼°\n",
    "V_random, iterations = policy_evaluation(env, random_policy, gamma=0.99)\n",
    "\n",
    "print(f\"\\nç­–ç•¥è¯„ä¼°å®Œæˆï¼Œè¿­ä»£æ¬¡æ•°: {iterations}\")\n",
    "print(\"\\nçŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€(s):\")\n",
    "for i in range(env.size):\n",
    "    row = []\n",
    "    for j in range(env.size):\n",
    "        if (i, j) in V_random:\n",
    "            row.append(f\"{V_random[(i, j)]:7.2f}\")\n",
    "        elif (i, j) in env.obstacles:\n",
    "            row.append(\"   X   \")\n",
    "        else:\n",
    "            row.append(\"   -   \")\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 ç­–ç•¥æ”¹è¿› (Policy Improvement)\n\nåŸºäºå½“å‰ä»·å€¼å‡½æ•°è´ªå©ªæ”¹è¿›ç­–ç•¥ï¼š\n\n$$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s' \\mid s, a) [R(s,a,s') + \\gamma V(s')]$$\n\nç®—æ³•èƒŒæ™¯ï¼šç­–ç•¥æ”¹è¿›å®šç†ç”±Bellmanæå‡ºï¼Œä¿è¯è´ªå©ªç­–ç•¥è‡³å°‘ä¸æ¯”åŸç­–ç•¥å·®ã€‚è¯¥å®šç†æ˜¯ç­–ç•¥è¿­ä»£æ”¶æ•›æ€§çš„ç†è®ºåŸºç¡€ã€‚ç­–ç•¥æ”¹è¿›å°†ä»·å€¼ä¿¡æ¯è½¬åŒ–ä¸ºè¡ŒåŠ¨å†³ç­–ï¼Œé€‰æ‹©ä½¿Qå€¼æœ€å¤§çš„åŠ¨ä½œæ¥æ”¹è¿›ç­–ç•¥ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ç­–ç•¥æ”¹è¿›ç®—æ³•\n# ============================================================\n\ndef policy_improvement(\n    env: GridWorld,\n    V: Dict[Tuple[int, int], float],\n    gamma: float = 0.99\n) -> Dict[Tuple[int, int], Dict[str, float]]:\n    \"\"\"\n    ç­–ç•¥æ”¹è¿›ï¼šåŸºäºä»·å€¼å‡½æ•°è´ªå©ªæ„é€ æ–°ç­–ç•¥\n    \n    Args:\n        env: ç¯å¢ƒ\n        V: çŠ¶æ€ä»·å€¼å‡½æ•°\n        gamma: æŠ˜æ‰£å› å­\n        \n    Returns:\n        æ”¹è¿›åçš„ç¡®å®šæ€§ç­–ç•¥\n    \"\"\"\n    policy = {}\n    \n    for state in env.states:\n        if env.is_terminal(state):\n            # ç»ˆæ­¢çŠ¶æ€ï¼šå‡åŒ€éšæœºç­–ç•¥\n            policy[state] = {a: 0.25 for a in env.actions}\n            continue\n        \n        # è®¡ç®—å„åŠ¨ä½œçš„ Q å€¼\n        q_values = {}\n        for action in env.actions:\n            q_val = 0.0\n            for next_state, trans_prob, reward in env.get_transitions(state, action):\n                q_val += trans_prob * (reward + gamma * V.get(next_state, 0.0))\n            q_values[action] = q_val\n        \n        # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ\n        best_value = max(q_values.values())\n        best_actions = [a for a, v in q_values.items() if abs(v - best_value) < 1e-9]\n        \n        # ç¡®å®šæ€§ç­–ç•¥\n        policy[state] = {\n            a: 1.0 if a == best_actions[0] else 0.0\n            for a in env.actions\n        }\n    \n    return policy\n\nprint(\"ç­–ç•¥æ”¹è¿›å‡½æ•°å®šä¹‰å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 ç­–ç•¥è¿­ä»£ (Policy Iteration)\n\näº¤æ›¿æ‰§è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ï¼Œç›´åˆ°ç­–ç•¥ç¨³å®šï¼š\n\n```\nÏ€â‚€ â†’ V^Ï€â‚€ â†’ Ï€â‚ â†’ V^Ï€â‚ â†’ Ï€â‚‚ â†’ ... â†’ Ï€* â†’ V*\n```\n\nç®—æ³•èƒŒæ™¯ï¼šç­–ç•¥è¿­ä»£ç”±Ronald Howardäº1960å¹´æå‡ºï¼Œæ˜¯æ±‚è§£MDPçš„ç»å…¸æ–¹æ³•ã€‚è¯¥ç®—æ³•å°†å¤æ‚çš„æœ€ä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼šè¯„ä¼°ï¼ˆç»™å®šç­–ç•¥æ±‚ä»·å€¼ï¼‰å’Œæ”¹è¿›ï¼ˆç»™å®šä»·å€¼æ±‚ç­–ç•¥ï¼‰ã€‚ç­–ç•¥æ”¹è¿›å®šç†ä¿è¯ä»·å€¼å•è°ƒé€’å¢ï¼Œæœ‰é™MDPä¸­å¿…æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ã€‚é€šå¸¸3-10æ¬¡å¤–å±‚è¿­ä»£å³å¯æ”¶æ•›ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ç­–ç•¥è¿­ä»£ç®—æ³•\n",
    "# ============================================================\n",
    "\n",
    "def policy_iteration(\n",
    "    env: GridWorld,\n",
    "    gamma: float = 0.99,\n",
    "    max_iterations: int = 100,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[Dict, Dict, int]:\n",
    "    \"\"\"\n",
    "    ç­–ç•¥è¿­ä»£ç®—æ³•\n",
    "    \n",
    "    Args:\n",
    "        env: ç¯å¢ƒ\n",
    "        gamma: æŠ˜æ‰£å› å­\n",
    "        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "        verbose: æ˜¯å¦æ‰“å°è¿›åº¦\n",
    "        \n",
    "    Returns:\n",
    "        (policy, V, iterations)\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–å‡åŒ€éšæœºç­–ç•¥\n",
    "    policy = {s: {a: 0.25 for a in env.actions} for s in env.states}\n",
    "    V = {}\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        # ç­–ç•¥è¯„ä¼°\n",
    "        V, eval_iters = policy_evaluation(env, policy, gamma)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"è¿­ä»£ {iteration}: ç­–ç•¥è¯„ä¼°ç”¨äº† {eval_iters} æ¬¡å†…å±‚è¿­ä»£\")\n",
    "        \n",
    "        # ç­–ç•¥æ”¹è¿›\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        \n",
    "        # æ£€æŸ¥æ”¶æ•›\n",
    "        if policy == new_policy:\n",
    "            if verbose:\n",
    "                print(f\"\\nâœ“ ç­–ç•¥è¿­ä»£æ”¶æ•›ï¼Œæ€»è¿­ä»£: {iteration}\")\n",
    "            return new_policy, V, iteration\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy, V, max_iterations\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"è¿è¡Œç­–ç•¥è¿­ä»£\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "policy_pi, V_pi, iters_pi = policy_iteration(env, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 å€¼è¿­ä»£ (Value Iteration)\n\nç›´æ¥è¿­ä»£è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼š\n\n$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s' \\mid s, a) [R(s,a,s') + \\gamma V_k(s')]$$\n\nç®—æ³•èƒŒæ™¯ï¼šå€¼è¿­ä»£ç”±Bellmanåœ¨1950å¹´ä»£æå‡ºï¼Œè·³è¿‡æ˜¾å¼çš„ç­–ç•¥ç»´æŠ¤ï¼Œç›´æ¥è¿­ä»£ä»·å€¼å‡½æ•°ã€‚å¯è§†ä¸ºç­–ç•¥è¿­ä»£çš„å˜ä½“ï¼Œå°†ç­–ç•¥è¯„ä¼°æˆªæ–­ä¸ºå•æ­¥æ›´æ–°ã€‚æ¯æ¬¡æ›´æ–°éšå¼åœ°è¿›è¡Œç­–ç•¥æ”¹è¿›ï¼ˆé€‰æ‹©maxï¼‰ï¼Œå› æ­¤ä¸éœ€è¦æ˜¾å¼ç»´æŠ¤ç­–ç•¥ã€‚åŸºäºBanachä¸åŠ¨ç‚¹å®šç†ï¼Œè´å°”æ›¼æœ€ä¼˜ç®—å­æ˜¯Î³-å‹ç¼©æ˜ å°„ï¼Œä¿è¯æ”¶æ•›åˆ°å”¯ä¸€ä¸åŠ¨ç‚¹ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å€¼è¿­ä»£ç®—æ³•\n",
    "# ============================================================\n",
    "\n",
    "def value_iteration(\n",
    "    env: GridWorld,\n",
    "    gamma: float = 0.99,\n",
    "    theta: float = 1e-6,\n",
    "    max_iterations: int = 10000,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[Dict, Dict, int]:\n",
    "    \"\"\"\n",
    "    å€¼è¿­ä»£ç®—æ³•\n",
    "    \n",
    "    Args:\n",
    "        env: ç¯å¢ƒ\n",
    "        gamma: æŠ˜æ‰£å› å­\n",
    "        theta: æ”¶æ•›é˜ˆå€¼\n",
    "        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "        verbose: æ˜¯å¦æ‰“å°è¿›åº¦\n",
    "        \n",
    "    Returns:\n",
    "        (policy, V, iterations)\n",
    "    \"\"\"\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        delta = 0.0\n",
    "        \n",
    "        for state in env.states:\n",
    "            if env.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            old_value = V[state]\n",
    "            \n",
    "            # è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹\n",
    "            q_values = []\n",
    "            for action in env.actions:\n",
    "                q_val = 0.0\n",
    "                for next_state, trans_prob, reward in env.get_transitions(state, action):\n",
    "                    q_val += trans_prob * (reward + gamma * V.get(next_state, 0.0))\n",
    "                q_values.append(q_val)\n",
    "            \n",
    "            V[state] = max(q_values)\n",
    "            delta = max(delta, abs(old_value - V[state]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            if verbose:\n",
    "                print(f\"âœ“ å€¼è¿­ä»£æ”¶æ•›ï¼Œè¿­ä»£æ¬¡æ•°: {iteration}\")\n",
    "            policy = policy_improvement(env, V, gamma)\n",
    "            return policy, V, iteration\n",
    "    \n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V, max_iterations\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"è¿è¡Œå€¼è¿­ä»£\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "policy_vi, V_vi, iters_vi = value_iteration(env, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ç¬¬4éƒ¨åˆ†ï¼šç»“æœå¯è§†åŒ–\n",
    "\n",
    "### å¯è§†åŒ–ç­–ç•¥å’Œä»·å€¼å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ç­–ç•¥å’Œä»·å€¼å‡½æ•°å¯è§†åŒ–\n",
    "# ============================================================\n",
    "\n",
    "def visualize_policy_and_values(\n",
    "    env: GridWorld,\n",
    "    policy: Dict,\n",
    "    V: Dict,\n",
    "    title: str = \"Policy & Value Function\"\n",
    "):\n",
    "    \"\"\"å¯è§†åŒ–ç­–ç•¥å’Œä»·å€¼å‡½æ•°\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # åŠ¨ä½œç®­å¤´ç¬¦å·\n",
    "    arrow_map = {'ä¸Š': 'â†‘', 'ä¸‹': 'â†“', 'å·¦': 'â†', 'å³': 'â†’'}\n",
    "    \n",
    "    # ===== å·¦å›¾ï¼šç­–ç•¥ =====\n",
    "    ax = axes[0]\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(y=i, color='black', linewidth=1)\n",
    "        ax.axvline(x=i, color='black', linewidth=1)\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            y = env.size - i - 0.5\n",
    "            x = j + 0.5\n",
    "            \n",
    "            if (i, j) == env.goal:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size - i - 1), 1, 1, facecolor='lightgreen', alpha=0.5))\n",
    "                ax.text(x, y, 'G', ha='center', va='center', fontsize=16, fontweight='bold', color='green')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size - i - 1), 1, 1, facecolor='gray', alpha=0.8))\n",
    "                ax.text(x, y, 'X', ha='center', va='center', fontsize=16, fontweight='bold', color='white')\n",
    "            elif (i, j) in policy:\n",
    "                best_action = max(policy[(i, j)], key=policy[(i, j)].get)\n",
    "                ax.text(x, y, arrow_map[best_action], ha='center', va='center', fontsize=20, color='blue')\n",
    "    \n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('æœ€ä¼˜ç­–ç•¥ Ï€*', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # ===== å³å›¾ï¼šä»·å€¼å‡½æ•° =====\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # åˆ›å»ºä»·å€¼çŸ©é˜µç”¨äºé¢œè‰²æ˜ å°„\n",
    "    value_matrix = np.zeros((env.size, env.size))\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) in V:\n",
    "                value_matrix[i, j] = V[(i, j)]\n",
    "            else:\n",
    "                value_matrix[i, j] = np.nan\n",
    "    \n",
    "    im = ax.imshow(value_matrix, cmap='RdYlGn', aspect='equal')\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) == env.goal:\n",
    "                ax.text(j, i, 'G', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.text(j, i, 'X', ha='center', va='center', fontsize=14, fontweight='bold', color='white')\n",
    "            elif (i, j) in V:\n",
    "                ax.text(j, i, f'{V[(i, j)]:.1f}', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    ax.set_title('çŠ¶æ€ä»·å€¼å‡½æ•° V*', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# å¯è§†åŒ–ç­–ç•¥è¿­ä»£ç»“æœ\n",
    "visualize_policy_and_values(env, policy_pi, V_pi, \"ç­–ç•¥è¿­ä»£ç»“æœ\")\n",
    "\n",
    "# å¯è§†åŒ–å€¼è¿­ä»£ç»“æœ\n",
    "visualize_policy_and_values(env, policy_vi, V_vi, \"å€¼è¿­ä»£ç»“æœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç®—æ³•å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ç®—æ³•å¯¹æ¯”\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ç­–ç•¥è¿­ä»£ vs å€¼è¿­ä»£\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'ç®—æ³•':<20} {'å¤–å±‚è¿­ä»£':>15} {'ç‰¹ç‚¹':>20}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'ç­–ç•¥è¿­ä»£':<20} {iters_pi:>15} {'ç²¾ç¡®è¯„ä¼°ï¼Œè¿­ä»£å°‘':>20}\")\n",
    "print(f\"{'å€¼è¿­ä»£':<20} {iters_vi:>15} {'æˆªæ–­è¯„ä¼°ï¼Œæ¯æ¬¡ä»£ä»·ä½':>20}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# éªŒè¯ä¸¤ç§ç®—æ³•äº§ç”Ÿç›¸åŒçš„ä»·å€¼å‡½æ•°\n",
    "value_diff = sum(abs(V_pi[s] - V_vi[s]) for s in env.states)\n",
    "print(f\"\\nä»·å€¼å‡½æ•°æ€»å·®å¼‚: {value_diff:.10f}\")\n",
    "\n",
    "if value_diff < 1e-5:\n",
    "    print(\"âœ“ ä¸¤ç§ç®—æ³•æ”¶æ•›åˆ°ç›¸åŒçš„æœ€ä¼˜ä»·å€¼å‡½æ•°ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ® ç¬¬5éƒ¨åˆ†ï¼šäº¤äº’å¼å®éªŒ\n",
    "\n",
    "### å®éªŒ1ï¼šæ”¹å˜æŠ˜æ‰£å› å­ Î³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å®éªŒï¼šä¸åŒæŠ˜æ‰£å› å­çš„å½±å“\n",
    "# ============================================================\n",
    "\n",
    "gammas = [0.5, 0.9, 0.99]\n",
    "\n",
    "print(\"ä¸åŒæŠ˜æ‰£å› å­ Î³ å¯¹ä»·å€¼å‡½æ•°çš„å½±å“:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for gamma in gammas:\n",
    "    _, V_gamma, iters = value_iteration(env, gamma=gamma, verbose=False)\n",
    "    \n",
    "    print(f\"\\nÎ³ = {gamma}, æ”¶æ•›è¿­ä»£: {iters}\")\n",
    "    print(f\"èµ·ç‚¹ V(0,0) = {V_gamma[(0, 0)]:.2f}\")\n",
    "    print(f\"ç›®æ ‡é‚»è¿‘ V(3,2) = {V_gamma[(3, 2)]:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"è§‚å¯Ÿï¼šÎ³ è¶Šå¤§ï¼Œè¿œç¦»ç›®æ ‡çš„çŠ¶æ€ä»·å€¼è¶Šé«˜ï¼ˆæ›´é‡è§†é•¿æœŸå›æŠ¥ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®éªŒ2ï¼šæ‰§è¡Œæœ€ä¼˜ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# æ‰§è¡Œæœ€ä¼˜ç­–ç•¥\n",
    "# ============================================================\n",
    "\n",
    "def execute_policy(\n",
    "    env: GridWorld,\n",
    "    policy: Dict,\n",
    "    max_steps: int = 50\n",
    ") -> Tuple[float, List[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    æ‰§è¡Œç­–ç•¥å¹¶è¿”å›è½¨è¿¹\n",
    "    \n",
    "    Returns:\n",
    "        (total_reward, trajectory)\n",
    "    \"\"\"\n",
    "    state = env.config.start\n",
    "    trajectory = [state]\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if env.is_terminal(state):\n",
    "            break\n",
    "        \n",
    "        # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ\n",
    "        best_action = max(policy[state], key=policy[state].get)\n",
    "        \n",
    "        # æ‰§è¡ŒåŠ¨ä½œ\n",
    "        transitions = env.get_transitions(state, best_action)\n",
    "        next_state, _, reward = transitions[0]\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory.append(state)\n",
    "    \n",
    "    return total_reward, trajectory\n",
    "\n",
    "# æ‰§è¡Œæœ€ä¼˜ç­–ç•¥\n",
    "total_reward, trajectory = execute_policy(env, policy_vi)\n",
    "\n",
    "print(\"æ‰§è¡Œæœ€ä¼˜ç­–ç•¥:\")\n",
    "print(f\"æ€»å¥–åŠ±: {total_reward:.1f}\")\n",
    "print(f\"æ­¥æ•°: {len(trajectory) - 1}\")\n",
    "print(f\"è½¨è¿¹: {' â†’ '.join([str(s) for s in trajectory])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å¯è§†åŒ–è½¨è¿¹\n",
    "# ============================================================\n",
    "\n",
    "def visualize_trajectory(env: GridWorld, trajectory: List[Tuple[int, int]]):\n",
    "    \"\"\"å¯è§†åŒ–æ‰§è¡Œè½¨è¿¹\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    \n",
    "    # ç»˜åˆ¶ç½‘æ ¼\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(y=i, color='black', linewidth=1)\n",
    "        ax.axvline(x=i, color='black', linewidth=1)\n",
    "    \n",
    "    # æ ‡è®°ç‰¹æ®Šä½ç½®\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            y = env.size - i - 0.5\n",
    "            x = j + 0.5\n",
    "            \n",
    "            if (i, j) == env.goal:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size - i - 1), 1, 1, facecolor='lightgreen', alpha=0.5))\n",
    "                ax.text(x, y, 'G', ha='center', va='center', fontsize=16, fontweight='bold', color='green')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size - i - 1), 1, 1, facecolor='gray', alpha=0.8))\n",
    "    \n",
    "    # ç»˜åˆ¶è½¨è¿¹\n",
    "    for idx, (i, j) in enumerate(trajectory):\n",
    "        y = env.size - i - 0.5\n",
    "        x = j + 0.5\n",
    "        \n",
    "        if idx == 0:\n",
    "            ax.plot(x, y, 'bo', markersize=15, label='èµ·ç‚¹')\n",
    "        elif idx == len(trajectory) - 1:\n",
    "            ax.plot(x, y, 'g*', markersize=20, label='ç»ˆç‚¹')\n",
    "        else:\n",
    "            ax.plot(x, y, 'r.', markersize=10)\n",
    "    \n",
    "    # è¿æ¥è½¨è¿¹ç‚¹\n",
    "    xs = [t[1] + 0.5 for t in trajectory]\n",
    "    ys = [env.size - t[0] - 0.5 for t in trajectory]\n",
    "    ax.plot(xs, ys, 'r-', linewidth=2, alpha=0.7, label='è½¨è¿¹')\n",
    "    \n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('æœ€ä¼˜ç­–ç•¥æ‰§è¡Œè½¨è¿¹', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_trajectory(env, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## æ€»ç»“\n\n### å…³é”®è¦ç‚¹\n\n1. MDP äº”å…ƒç»„: $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ å®Œæ•´æè¿°å¼ºåŒ–å­¦ä¹ é—®é¢˜\n\n2. é©¬å°”å¯å¤«æ€§è´¨: æœªæ¥ä»…ä¾èµ–å½“å‰çŠ¶æ€ï¼Œä¸å†å²æ— å…³\n\n3. è´å°”æ›¼æ–¹ç¨‹: ä»·å€¼å‡½æ•°çš„é€’å½’åˆ†è§£\n   - æœŸæœ›æ–¹ç¨‹: ç»™å®šç­–ç•¥çš„ä»·å€¼\n   - æœ€ä¼˜æ–¹ç¨‹: æœ€ä¼˜ä»·å€¼\n\n4. ç­–ç•¥è¿­ä»£: è¯„ä¼° â†’ æ”¹è¿› â†’ è¯„ä¼° â†’ ... â†’ æ”¶æ•›\n\n5. å€¼è¿­ä»£: ç›´æ¥è¿­ä»£è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹\n\n### ç®—æ³•å¯¹æ¯”\n\n| ç®—æ³• | æ¯æ¬¡è¿­ä»£ | è¿­ä»£æ¬¡æ•° | é€‚ç”¨åœºæ™¯ |\n|------|----------|----------|----------|\n| ç­–ç•¥è¿­ä»£ | å®Œæ•´ç­–ç•¥è¯„ä¼° | å°‘ï¼ˆ3-10æ¬¡ï¼‰ | å°çŠ¶æ€ç©ºé—´ |\n| å€¼è¿­ä»£ | å•æ­¥è´å°”æ›¼æ›´æ–° | å¤šï¼ˆæ•°ç™¾æ¬¡ï¼‰ | å¤§çŠ¶æ€ç©ºé—´ |\n\nç­–ç•¥è¿­ä»£ï¼šå°‘é‡å¤–å±‚è¿­ä»£ Ã— æ˜‚è´µçš„å†…å±‚è¯„ä¼°  \nå€¼è¿­ä»£ï¼šå¤§é‡å¤–å±‚è¿­ä»£ Ã— ä¾¿å®œçš„å•æ­¥æ›´æ–°\n\n### åŠ¨æ€è§„åˆ’çš„å±€é™\n\n- éœ€è¦å®Œæ•´çš„ç¯å¢ƒæ¨¡å‹ (P, R)\n- çŠ¶æ€ç©ºé—´çˆ†ç‚¸é—®é¢˜\n- æ— æ³•å¤„ç†è¿ç»­çŠ¶æ€/åŠ¨ä½œç©ºé—´\n\n### ä¸‹ä¸€æ­¥å­¦ä¹ \n\n- Q-Learning: æ— æ¨¡å‹æ–¹æ³•ï¼Œé€šè¿‡é‡‡æ ·å­¦ä¹ \n- SARSA: åœ¨ç­–ç•¥ TD å­¦ä¹ \n- DQN: æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œå‡½æ•°é€¼è¿‘\n- ç­–ç•¥æ¢¯åº¦: ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼ˆREINFORCE, PPO, SACï¼‰\n\n### ç»ƒä¹ é¢˜\n\n1. å¢åŠ æ›´å¤šéšœç¢ç‰©ï¼Œè§‚å¯Ÿç­–ç•¥å˜åŒ–\n2. æ·»åŠ éšæœºæ€§ (slip_prob > 0)ï¼Œæ¯”è¾ƒç¡®å®šæ€§å’Œéšæœºç¯å¢ƒ\n3. å®ç° Q-Learning ç®—æ³•å¹¶ä¸ DP æ–¹æ³•æ¯”è¾ƒ\n\n## å‚è€ƒèµ„æ–™\n\n- Sutton & Barto, \"Reinforcement Learning: An Introduction\" (2nd ed.), Chapter 4\n- Bellman, R. \"Dynamic Programming\", Princeton University Press, 1957\n- Howard, R. \"Dynamic Programming and Markov Processes\", MIT Press, 1960\n- David Silver, UCL RL Course, Lecture 2-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª å•å…ƒæµ‹è¯• | Unit Tests\n",
    "\n",
    "è¿è¡Œä»¥ä¸‹æµ‹è¯•éªŒè¯å®ç°çš„æ­£ç¡®æ€§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# å•å…ƒæµ‹è¯•\n# ============================================================\n\ndef run_tests():\n    \"\"\"è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•\"\"\"\n    print(\"å¼€å§‹å•å…ƒæµ‹è¯•...\\n\")\n    passed = 0\n    failed = 0\n    \n    # æµ‹è¯•1: ç¯å¢ƒçŠ¶æ€ç©ºé—´\n    try:\n        assert len(env.states) == 15, f\"çŠ¶æ€æ•°é”™è¯¯: {len(env.states)}\"\n        assert (1, 1) not in env.states, \"éšœç¢ç‰©ä¸åº”åœ¨çŠ¶æ€ç©ºé—´ä¸­\"\n        print(\"æµ‹è¯•1é€šè¿‡: çŠ¶æ€ç©ºé—´æ­£ç¡®\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•1å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æµ‹è¯•2: ç»ˆæ­¢çŠ¶æ€\n    try:\n        assert env.is_terminal((3, 3)) == True, \"ç›®æ ‡åº”ä¸ºç»ˆæ­¢çŠ¶æ€\"\n        assert env.is_terminal((0, 0)) == False, \"èµ·ç‚¹ä¸åº”ä¸ºç»ˆæ­¢çŠ¶æ€\"\n        print(\"æµ‹è¯•2é€šè¿‡: ç»ˆæ­¢çŠ¶æ€åˆ¤æ–­æ­£ç¡®\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•2å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æµ‹è¯•3: çŠ¶æ€è½¬ç§»\n    try:\n        next_s = env.get_next_state((0, 0), 'ä¸‹')\n        assert next_s == (1, 0), f\"çŠ¶æ€è½¬ç§»é”™è¯¯: {next_s}\"\n        # è¾¹ç•Œæµ‹è¯•\n        next_s = env.get_next_state((0, 0), 'ä¸Š')\n        assert next_s == (0, 0), \"è¾¹ç•Œåº”åœç•™åŸåœ°\"\n        print(\"æµ‹è¯•3é€šè¿‡: çŠ¶æ€è½¬ç§»æ­£ç¡®\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•3å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æµ‹è¯•4: ç­–ç•¥è¿­ä»£æ”¶æ•›\n    try:\n        assert iters_pi <= 10, f\"ç­–ç•¥è¿­ä»£åº”è¯¥å¿«é€Ÿæ”¶æ•›: {iters_pi}\"\n        print(f\"æµ‹è¯•4é€šè¿‡: ç­–ç•¥è¿­ä»£åœ¨ {iters_pi} æ¬¡æ”¶æ•›\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•4å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æµ‹è¯•5: å€¼è¿­ä»£æ”¶æ•›\n    try:\n        assert iters_vi <= 100, f\"å€¼è¿­ä»£åº”è¯¥åˆç†æ”¶æ•›: {iters_vi}\"\n        print(f\"æµ‹è¯•5é€šè¿‡: å€¼è¿­ä»£åœ¨ {iters_vi} æ¬¡æ”¶æ•›\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•5å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æµ‹è¯•6: ä¸¤ç§ç®—æ³•ç»“æœä¸€è‡´\n    try:\n        diff = sum(abs(V_pi[s] - V_vi[s]) for s in env.states)\n        assert diff < 1e-5, f\"ä»·å€¼å‡½æ•°å·®å¼‚è¿‡å¤§: {diff}\"\n        print(\"æµ‹è¯•6é€šè¿‡: ç­–ç•¥è¿­ä»£ä¸å€¼è¿­ä»£ç»“æœä¸€è‡´\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•6å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æµ‹è¯•7: æœ€ä¼˜ç­–ç•¥èƒ½åˆ°è¾¾ç›®æ ‡\n    try:\n        reward, traj = execute_policy(env, policy_vi)\n        assert traj[-1] == env.goal, \"æœ€ä¼˜ç­–ç•¥åº”è¯¥åˆ°è¾¾ç›®æ ‡\"\n        assert len(traj) <= 10, f\"è·¯å¾„åº”è¯¥è¾ƒçŸ­: {len(traj)}\"\n        print(f\"æµ‹è¯•7é€šè¿‡: æœ€ä¼˜ç­–ç•¥åœ¨ {len(traj)-1} æ­¥åˆ°è¾¾ç›®æ ‡\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•7å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æµ‹è¯•8: ä»·å€¼å‡½æ•°å•è°ƒæ€§\n    try:\n        # ç¦»ç›®æ ‡è¶Šè¿‘ï¼Œä»·å€¼è¶Šé«˜\n        assert V_vi[(3, 2)] > V_vi[(0, 0)], \"é è¿‘ç›®æ ‡çš„ä»·å€¼åº”è¯¥æ›´é«˜\"\n        assert V_vi[(2, 3)] > V_vi[(0, 0)], \"é è¿‘ç›®æ ‡çš„ä»·å€¼åº”è¯¥æ›´é«˜\"\n        print(\"æµ‹è¯•8é€šè¿‡: ä»·å€¼å‡½æ•°å•è°ƒæ€§æ­£ç¡®\")\n        passed += 1\n    except AssertionError as e:\n        print(f\"æµ‹è¯•8å¤±è´¥: {e}\")\n        failed += 1\n    \n    # æ€»ç»“\n    print(f\"\\n{'='*50}\")\n    print(f\"æµ‹è¯•å®Œæˆ: {passed} é€šè¿‡, {failed} å¤±è´¥\")\n    if failed == 0:\n        print(\"æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å®ç°æ­£ç¡®ã€‚\")\n    else:\n        print(\"å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚\")\n    print(f\"{'='*50}\")\n    \n    return failed == 0\n\n# è¿è¡Œæµ‹è¯•\nrun_tests()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_as_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}