{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies and Value Functions\n",
    "\n",
    "## Overview\n",
    "This notebook explores policies and value functions, the two key concepts for solving MDPs.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand deterministic and stochastic policies\n",
    "2. Learn state value functions V(s) and action value functions Q(s,a)\n",
    "3. Implement policy evaluation\n",
    "4. Visualize policies and value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/dingziming/PycharmProjects/AI-Practices/07-reinforcement-learning/马尔科夫决策过程')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.core import (\n",
    "    State, Action, MarkovDecisionProcess,\n",
    "    DeterministicPolicy, StochasticPolicy,\n",
    "    StateValueFunction, ActionValueFunction\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Deterministic Policies\n",
    "\n",
    "A deterministic policy π(s) → a maps each state to exactly one action.\n",
    "\n",
    "**Advantages**:\n",
    "- Simple representation: O(|S|) storage\n",
    "- Often optimal for finite MDPs\n",
    "- Easy to interpret and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple MDP\n",
    "mdp = MarkovDecisionProcess(discount_factor=0.99)\n",
    "\n",
    "# Add states\n",
    "states = [State(i) for i in range(4)]\n",
    "for s in states:\n",
    "    mdp.add_state(s)\n",
    "\n",
    "# Add actions\n",
    "actions = [Action(i, f\"a{i}\") for i in range(2)]\n",
    "for a in actions:\n",
    "    mdp.add_action(a)\n",
    "\n",
    "mdp.set_initial_state(states[0])\n",
    "\n",
    "print(f\"Created MDP with {mdp.get_state_space_size()} states and {mdp.get_action_space_size()} actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deterministic policy\n",
    "policy = DeterministicPolicy(mdp)\n",
    "\n",
    "# Set action for each state\n",
    "policy.set_action(states[0], actions[0])\n",
    "policy.set_action(states[1], actions[1])\n",
    "policy.set_action(states[2], actions[0])\n",
    "policy.set_action(states[3], actions[1])\n",
    "\n",
    "print(\"Deterministic policy created\")\n",
    "print(f\"Policy is complete: {policy.is_complete()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query policy\n",
    "for state in states:\n",
    "    action = policy.get_action(state)\n",
    "    prob = policy.get_action_probability(state, action)\n",
    "    print(f\"π(s={state.state_id}) = a{action.action_id}, P(a|s) = {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert policy to array\n",
    "policy_array = policy.to_array()\n",
    "print(f\"Policy array shape: {policy_array.shape}\")\n",
    "print(f\"Policy array: {policy_array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stochastic Policies\n",
    "\n",
    "A stochastic policy π(a|s) defines a probability distribution over actions for each state.\n",
    "\n",
    "**Advantages**:\n",
    "- Enables exploration\n",
    "- Necessary for convergence in some algorithms\n",
    "- Useful for theoretical analysis\n",
    "\n",
    "**Disadvantages**:\n",
    "- Larger storage: O(|S| × |A|)\n",
    "- More complex to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stochastic policy\n",
    "stoch_policy = StochasticPolicy(mdp)\n",
    "\n",
    "# Set uniform policy: π(a|s) = 1/|A| for all s,a\n",
    "stoch_policy.set_uniform_policy()\n",
    "\n",
    "print(\"Uniform stochastic policy created\")\n",
    "print(f\"Policy is complete: {stoch_policy.is_complete()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query stochastic policy\n",
    "state = states[0]\n",
    "probs = stoch_policy.get_all_action_probabilities(state)\n",
    "print(f\"Action probabilities for state {state.state_id}:\")\n",
    "for action, prob in probs.items():\n",
    "    print(f\"  π(a={action.action_id}|s={state.state_id}) = {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate probabilities\n",
    "try:\n",
    "    stoch_policy.validate_probabilities()\n",
    "    print(\"Policy probabilities are valid (sum to 1)\")\nexcept ValueError as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert stochastic policy to array\n",
    "stoch_array = stoch_policy.to_array()\n",
    "print(f\"Stochastic policy array shape: {stoch_array.shape}\")\n",
    "print(f\"Policy array (first 2 states):\")\n",
    "print(stoch_array[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: State Value Functions\n",
    "\n",
    "The state value function V(s) estimates the expected long-term reward from state s.\n",
    "\n",
    "$$V^\\pi(s) = E[\\sum_{t=0}^{\\infty} \\gamma^t R_t | s_0=s, \\pi]$$\n",
    "\n",
    "**Key Properties**:\n",
    "- Storage: O(|S|)\n",
    "- Sufficient for policy extraction (with transition model)\n",
    "- Computed via policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state value function\n",
    "value_fn = StateValueFunction(mdp, initial_value=0.0)\n",
    "\n",
    "print(\"State value function created\")\n",
    "print(f\"Initial values: {value_fn.to_array()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values\n",
    "value_fn.set_value(states[0], 10.0)\n",
    "value_fn.set_value(states[1], 5.0)\n",
    "value_fn.set_value(states[2], 3.0)\n",
    "value_fn.set_value(states[3], 1.0)\n",
    "\n",
    "print(\"Values set\")\n",
    "print(f\"Values: {value_fn.to_array()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query value statistics\n",
    "print(f\"Max value: {value_fn.get_max_value():.3f}\")\n",
    "print(f\"Min value: {value_fn.get_min_value():.3f}\")\n",
    "print(f\"Mean value: {value_fn.get_mean_value():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update values\n",
    "value_fn.update_value(states[0], -2.0)  # Decrease by 2\n",
    "print(f\"After update: {value_fn.to_array()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Action Value Functions (Q-functions)\n",
    "\n",
    "The action value function Q(s,a) estimates the expected long-term reward from taking action a in state s.\n",
    "\n",
    "$$Q^\\pi(s,a) = E[\\sum_{t=0}^{\\infty} \\gamma^t R_t | s_0=s, a_0=a, \\pi]$$\n",
    "\n",
    "**Key Properties**:\n",
    "- Storage: O(|S| × |A|)\n",
    "- Enables direct policy extraction (no transition model needed)\n",
    "- Foundation for Q-learning and other model-free algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create action value function\n",
    "q_fn = ActionValueFunction(mdp, initial_value=0.0)\n",
    "\n",
    "print(\"Action value function created\")\n",
    "print(f\"Q-function shape: {q_fn.to_array().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Q-values\n",
    "q_fn.set_value(states[0], actions[0], 10.0)\n",
    "q_fn.set_value(states[0], actions[1], 5.0)\n",
    "q_fn.set_value(states[1], actions[0], 3.0)\n",
    "q_fn.set_value(states[1], actions[1], 8.0)\n",
    "\n",
    "print(\"Q-values set\")\n",
    "print(f\"Q-function array:\")\n",
    "print(q_fn.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Q-values for a state\n",
    "state = states[0]\n",
    "q_values = q_fn.get_action_values(state)\n",
    "print(f\"Q-values for state {state.state_id}:\")\n",
    "for action, q_val in q_values.items():\n",
    "    print(f\"  Q(s={state.state_id}, a={action.action_id}) = {q_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract greedy policy from Q-function\n",
    "best_action = q_fn.get_best_action(states[0])\n",
    "max_q = q_fn.get_max_action_value(states[0])\n",
    "\n",
    "print(f\"Best action for state 0: a{best_action.action_id}\")\n",
    "print(f\"Max Q-value: {max_q:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Relationship Between V and Q\n",
    "\n",
    "The state value and action value functions are related:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$$\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate relationship\n",
    "state = states[0]\n",
    "\n",
    "# Get Q-values\n",
    "q_vals = q_fn.get_action_values(state)\n",
    "print(f\"Q-values for state {state.state_id}: {list(q_vals.values())}\")\n",
    "\n",
    "# Get policy probabilities\n",
    "probs = stoch_policy.get_all_action_probabilities(state)\n",
    "print(f\"Policy probabilities: {list(probs.values())}\")\n",
    "\n",
    "# Compute V(s) = Σ_a π(a|s) Q(s,a)\n",
    "v_computed = sum(probs[a] * q_vals[a] for a in actions)\n",
    "print(f\"\\nComputed V(s) from Q-values: {v_computed:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts:\n",
    "1. **Deterministic policies** map states to single actions\n",
    "2. **Stochastic policies** define action probability distributions\n",
    "3. **State value functions** V(s) estimate long-term rewards from states\n",
    "4. **Action value functions** Q(s,a) estimate rewards from state-action pairs\n",
    "5. **V and Q are related** through the policy and transition model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
