{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Environments and Applications\n",
    "\n",
    "## Overview\n",
    "This notebook explores three classic benchmark environments for testing MDP algorithms.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand different environment characteristics\n",
    "2. Solve each environment with different algorithms\n",
    "3. Analyze optimal policies\n",
    "4. Compare algorithm performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/dingziming/PycharmProjects/AI-Practices/07-reinforcement-learning/马尔科夫决策过程')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environments import GridWorld, FrozenLake, CliffWalking\n",
    "from src.solvers import ValueIterationSolver, PolicyIterationSolver\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: GridWorld Environment\n",
    "\n",
    "**Characteristics**:\n",
    "- Deterministic transitions\n",
    "- Grid-based navigation\n",
    "- Obstacles and goals\n",
    "- Intuitive visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridWorld\n",
    "gridworld = GridWorld(height=5, width=5, discount_factor=0.99)\n",
    "\n",
    "# Configure environment\n",
    "gridworld.set_start(0, 0)\n",
    "gridworld.set_goal(4, 4)\n",
    "gridworld.set_obstacle(2, 2)\n",
    "gridworld.set_obstacle(2, 3)\n",
    "\n",
    "# Build transitions\n",
    "gridworld.build_transitions()\n",
    "\n",
    "print(\"GridWorld created\")\n",
    "print(f\"States: {gridworld.get_state_space_size()}\")\n",
    "print(f\"Actions: {gridworld.get_action_space_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve GridWorld\n",
    "gw_solver = ValueIterationSolver(gridworld, theta=1e-6)\n",
    "gw_value_fn, gw_policy = gw_solver.solve(verbose=False)\n",
    "\n",
    "print(f\"GridWorld solved in {gw_solver.get_iteration_count()} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GridWorld policy\n",
    "print(\"\\nGridWorld Optimal Policy:\")\n",
    "print(gridworld.render(policy=gw_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value function\n",
    "print(\"\\nGridWorld Value Function:\")\n",
    "values = gw_value_fn.to_array().reshape((gridworld.height, gridworld.width))\n",
    "for row in range(gridworld.height):\n",
    "    for col in range(gridworld.width):\n",
    "        print(f\"{values[row, col]:7.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: FrozenLake Environment\n",
    "\n",
    "**Characteristics**:\n",
    "- Stochastic transitions (slippery ice)\n",
    "- 4x4 grid\n",
    "- Holes (terminal states with 0 reward)\n",
    "- Goal (terminal state with +1 reward)\n",
    "- Demonstrates importance of handling uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake\n",
    "frozenlake = FrozenLake(discount_factor=0.99)\n",
    "\n",
    "# Build transitions\n",
    "frozenlake.build_transitions()\n",
    "\n",
    "print(\"FrozenLake created\")\n",
    "print(f\"States: {frozenlake.get_state_space_size()}\")\n",
    "print(f\"Actions: {frozenlake.get_action_space_size()}\")\n",
    "print(f\"\\nLayout:\")\n",
    "print(frozenlake.get_layout_description())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve FrozenLake\n",
    "fl_solver = ValueIterationSolver(frozenlake, theta=1e-6)\n",
    "fl_value_fn, fl_policy = fl_solver.solve(verbose=False)\n",
    "\n",
    "print(f\"FrozenLake solved in {fl_solver.get_iteration_count()} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FrozenLake policy\n",
    "print(\"\\nFrozenLake Optimal Policy:\")\n",
    "print(frozenlake.render(policy=fl_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value function\n",
    "print(\"\\nFrozenLake Value Function:\")\n",
    "values = fl_value_fn.to_array().reshape((frozenlake.height, frozenlake.width))\n",
    "for row in range(frozenlake.height):\n",
    "    for col in range(frozenlake.width):\n",
    "        print(f\"{values[row, col]:7.3f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CliffWalking Environment\n",
    "\n",
    "**Characteristics**:\n",
    "- Risk-reward trade-off\n",
    "- 4x12 grid\n",
    "- Cliff (large negative reward)\n",
    "- Optimal policy hugs cliff (risky but efficient)\n",
    "- Demonstrates importance of robust policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CliffWalking\n",
    "cliffwalking = CliffWalking(height=4, width=12, discount_factor=0.99)\n",
    "\n",
    "# Build transitions\n",
    "cliffwalking.build_transitions()\n",
    "\n",
    "print(\"CliffWalking created\")\n",
    "print(f\"States: {cliffwalking.get_state_space_size()}\")\n",
    "print(f\"Actions: {cliffwalking.get_action_space_size()}\")\n",
    "print(f\"\\n{cliffwalking.get_grid_info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve CliffWalking\n",
    "cw_solver = ValueIterationSolver(cliffwalking, theta=1e-6)\n",
    "cw_value_fn, cw_policy = cw_solver.solve(verbose=False)\n",
    "\n",
    "print(f\"CliffWalking solved in {cw_solver.get_iteration_count()} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CliffWalking policy\n",
    "print(\"\\nCliffWalking Optimal Policy:\")\n",
    "print(cliffwalking.render(policy=cw_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value function (first few rows)\n",
    "print(\"\\nCliffWalking Value Function (first 2 rows):\")\n",
    "values = cw_value_fn.to_array().reshape((cliffwalking.height, cliffwalking.width))\n",
    "for row in range(2):\n",
    "    for col in range(cliffwalking.width):\n",
    "        print(f\"{values[row, col]:7.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Environment Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare environments\n",
    "print(\"Environment Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Environment':<15} {'States':<10} {'Actions':<10} {'Iterations':<15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'GridWorld':<15} {gridworld.get_state_space_size():<10} {gridworld.get_action_space_size():<10} {gw_solver.get_iteration_count():<15}\")\n",
    "print(f\"{'FrozenLake':<15} {frozenlake.get_state_space_size():<10} {frozenlake.get_action_space_size():<10} {fl_solver.get_iteration_count():<15}\")\n",
    "print(f\"{'CliffWalking':<15} {cliffwalking.get_state_space_size():<10} {cliffwalking.get_action_space_size():<10} {cw_solver.get_iteration_count():<15}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare convergence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# GridWorld convergence\n",
    "gw_conv = gw_solver.get_convergence_history()\n",
    "axes[0].semilogy(range(1, len(gw_conv)+1), gw_conv, 'b-o', linewidth=2, markersize=4)\n",
    "axes[0].set_title('GridWorld Convergence')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Max Value Change')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# FrozenLake convergence\n",
    "fl_conv = fl_solver.get_convergence_history()\n",
    "axes[1].semilogy(range(1, len(fl_conv)+1), fl_conv, 'g-o', linewidth=2, markersize=4)\n",
    "axes[1].set_title('FrozenLake Convergence')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Max Value Change')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# CliffWalking convergence\n",
    "cw_conv = cw_solver.get_convergence_history()\n",
    "axes[2].semilogy(range(1, len(cw_conv)+1), cw_conv, 'r-o', linewidth=2, markersize=4)\n",
    "axes[2].set_title('CliffWalking Convergence')\n",
    "axes[2].set_xlabel('Iteration')\n",
    "axes[2].set_ylabel('Max Value Change')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Policy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze GridWorld policy\n",
    "print(\"GridWorld Policy Analysis:\")\n",
    "print(f\"Max value: {gw_value_fn.get_max_value():.3f}\")\n",
    "print(f\"Min value: {gw_value_fn.get_min_value():.3f}\")\n",
    "print(f\"Mean value: {gw_value_fn.get_mean_value():.3f}\")\n",
    "print(f\"Policy is complete: {gw_policy.is_complete()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze FrozenLake policy\n",
    "print(\"\\nFrozenLake Policy Analysis:\")\n",
    "print(f\"Max value: {fl_value_fn.get_max_value():.3f}\")\n",
    "print(f\"Min value: {fl_value_fn.get_min_value():.3f}\")\n",
    "print(f\"Mean value: {fl_value_fn.get_mean_value():.3f}\")\n",
    "print(f\"Policy is complete: {fl_policy.is_complete()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CliffWalking policy\n",
    "print(\"\\nCliffWalking Policy Analysis:\")\n",
    "print(f\"Max value: {cw_value_fn.get_max_value():.3f}\")\n",
    "print(f\"Min value: {cw_value_fn.get_min_value():.3f}\")\n",
    "print(f\"Mean value: {cw_value_fn.get_mean_value():.3f}\")\n",
    "print(f\"Policy is complete: {cw_policy.is_complete()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key insights:\n",
    "1. **GridWorld**: Deterministic, straightforward optimal path\n",
    "2. **FrozenLake**: Stochastic, requires robust policy\n",
    "3. **CliffWalking**: Risk-reward trade-off, optimal policy is risky\n",
    "4. **All environments solved successfully** with Value Iteration\n",
    "5. **Convergence speed varies** with environment complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
