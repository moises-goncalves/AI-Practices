{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Solving Algorithms\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates three fundamental algorithms for solving MDPs:\n",
    "1. Value Iteration\n",
    "2. Policy Iteration\n",
    "3. Linear Programming\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand how each algorithm works\n",
    "2. Compare convergence behavior\n",
    "3. Analyze computational complexity\n",
    "4. Visualize convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Create Simple MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/dingziming/PycharmProjects/AI-Practices/07-reinforcement-learning/马尔科夫决策过程')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environments import GridWorld\n",
    "from src.solvers import ValueIterationSolver, PolicyIterationSolver, LinearProgrammingSolver\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 3x3 GridWorld\n",
    "env = GridWorld(height=3, width=3, discount_factor=0.99)\n",
    "\n",
    "# Set start and goal\n",
    "env.set_start(0, 0)\n",
    "env.set_goal(2, 2)\n",
    "\n",
    "# Add obstacle\n",
    "env.set_obstacle(1, 1)\n",
    "\n",
    "# Build transitions\n",
    "env.build_transitions()\n",
    "\n",
    "print(f\"GridWorld created: {env.height}x{env.width}\")\n",
    "print(f\"States: {env.get_state_space_size()}\")\n",
    "print(f\"Actions: {env.get_action_space_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the environment\n",
    "print(\"GridWorld Layout:\")\n",
    "print(\"S = Start, G = Goal, # = Obstacle, . = Empty\")\n",
    "print()\n",
    "for row in range(env.height):\n",
    "    for col in range(env.width):\n",
    "        if (row, col) == (0, 0):\n",
    "            print(\"S\", end=\" \")\n",
    "        elif (row, col) == (2, 2):\n",
    "            print(\"G\", end=\" \")\n",
    "        elif (row, col) == (1, 1):\n",
    "            print(\"#\", end=\" \")\n",
    "        else:\n",
    "            print(\".\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Value Iteration\n",
    "\n",
    "**Algorithm**: Repeatedly apply Bellman optimality operator until convergence\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V_k(s')]$$\n",
    "\n",
    "**Advantages**:\n",
    "- Simple to implement\n",
    "- Guaranteed convergence\n",
    "- No explicit policy maintenance\n",
    "\n",
    "**Disadvantages**:\n",
    "- May require many iterations\n",
    "- Each iteration is O(|S|²|A|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run Value Iteration solver\n",
    "vi_solver = ValueIterationSolver(env, theta=1e-6)\n",
    "\n",
    "print(\"Running Value Iteration...\")\n",
    "vi_value_fn, vi_policy = vi_solver.solve(max_iterations=100, verbose=True)\n",
    "\n",
    "print(f\"\\nConverged in {vi_solver.get_iteration_count()} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value function\n",
    "print(\"\\nValue Function (Value Iteration):\")\n",
    "values = vi_value_fn.to_array().reshape((env.height, env.width))\n",
    "for row in range(env.height):\n",
    "    for col in range(env.width):\n",
    "        print(f\"{values[row, col]:7.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display policy\n",
    "print(\"\\nOptimal Policy (Value Iteration):\")\n",
    "print(env.render(policy=vi_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "convergence = vi_solver.get_convergence_history()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(range(1, len(convergence)+1), convergence, 'b-o', linewidth=2, markersize=4)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Max Value Change', fontsize=12)\n",
    "plt.title('Value Iteration Convergence', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final max change: {convergence[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Policy Iteration\n",
    "\n",
    "**Algorithm**: Alternate between policy evaluation and policy improvement\n",
    "\n",
    "1. **Policy Evaluation**: Compute V^π(s) for current policy\n",
    "2. **Policy Improvement**: π'(s) = argmax_a Σ_{s'} P(s'|s,a)[R(s,a,s') + γV^π(s')]\n",
    "\n",
    "**Advantages**:\n",
    "- Often converges in fewer iterations than VI\n",
    "- Maintains explicit policy\n",
    "\n",
    "**Disadvantages**:\n",
    "- Each iteration more expensive (policy evaluation)\n",
    "- More complex to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run Policy Iteration solver\n",
    "pi_solver = PolicyIterationSolver(env, theta=1e-6)\n",
    "\n",
    "print(\"Running Policy Iteration...\")\n",
    "pi_value_fn, pi_policy = pi_solver.solve(max_iterations=100, verbose=True)\n",
    "\n",
    "print(f\"\\nConverged in {pi_solver.get_iteration_count()} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value function\n",
    "print(\"\\nValue Function (Policy Iteration):\")\n",
    "values = pi_value_fn.to_array().reshape((env.height, env.width))\n",
    "for row in range(env.height):\n",
    "    for col in range(env.width):\n",
    "        print(f\"{values[row, col]:7.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display policy\n",
    "print(\"\\nOptimal Policy (Policy Iteration):\")\n",
    "print(env.render(policy=pi_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Linear Programming\n",
    "\n",
    "**Algorithm**: Formulate optimal value computation as LP\n",
    "\n",
    "$$\\min \\sum_s V(s)$$\n",
    "$$\\text{s.t. } V(s) \\geq R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V(s') \\quad \\forall s,a$$\n",
    "\n",
    "**Advantages**:\n",
    "- Guaranteed optimal solution\n",
    "- Single solve (no iterations)\n",
    "\n",
    "**Disadvantages**:\n",
    "- Computationally expensive for large MDPs\n",
    "- Requires LP solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run Linear Programming solver\n",
    "lp_solver = LinearProgrammingSolver(env)\n",
    "\n",
    "print(\"Running Linear Programming solver...\")\n",
    "lp_value_fn, lp_policy = lp_solver.solve(verbose=True)\n",
    "\n",
    "print(\"\\nLP solver completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value function\n",
    "print(\"\\nValue Function (Linear Programming):\")\n",
    "values = lp_value_fn.to_array().reshape((env.height, env.width))\n",
    "for row in range(env.height):\n",
    "    for col in range(env.width):\n",
    "        print(f\"{values[row, col]:7.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display policy\n",
    "print(\"\\nOptimal Policy (Linear Programming):\")\n",
    "print(env.render(policy=lp_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": ,
   "source": [
    "## Part 5: Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare value functions\n",
    "vi_vals = vi_value_fn.to_array()\n",
    "pi_vals = pi_value_fn.to_array()\n",
    "lp_vals = lp_value_fn.to_array()\n",
    "\n",
    "print(\"Value Function Comparison:\")\n",
    "print(f\"VI vs PI max difference: {np.max(np.abs(vi_vals - pi_vals)):.2e}\")\n",
    "print(f\"VI vs LP max difference: {np.max(np.abs(vi_vals - lp_vals)):.2e}\")\n",
    "print(f\"PI vs LP max difference: {np.max(np.abs(pi_vals - lp_vals)):.2e}\")\n",
    "\n",
    "print(\"\\nAll algorithms converged to same solution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\nAlgorithm Comparison Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Algorithm':<20} {'Iterations':<15} {'Complexity':<25}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Value Iteration':<20} {vi_solver.get_iteration_count():<15} {'O(k|S|²|A|)':<25}\")\n",
    "print(f\"{'Policy Iteration':<20} {pi_solver.get_iteration_count():<15} {'O(k(|S|³+|S|²|A|))':<25}\")\n",
    "print(f\"{'Linear Programming':<20} {'1':<15} {'O(|S|³)':<25}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "1. **Value Iteration**: Simple, iterative, good for small MDPs\n",
    "2. **Policy Iteration**: Fewer iterations, more complex per iteration\n",
    "3. **Linear Programming**: Direct solution, expensive for large MDPs\n",
    "4. **All converge to same optimal solution**\n",
    "5. **Choice depends on problem size and structure**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
