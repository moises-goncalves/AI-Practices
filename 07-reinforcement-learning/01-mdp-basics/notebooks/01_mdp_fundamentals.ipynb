{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP) Fundamentals\n",
    "\n",
    "## Overview\n",
    "This notebook introduces the core concepts of Markov Decision Processes, the mathematical framework for sequential decision-making under uncertainty.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand MDP components: states, actions, transitions, rewards\n",
    "2. Learn the Bellman equations\n",
    "3. Implement basic MDP structures\n",
    "4. Visualize MDP components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/dingziming/PycharmProjects/AI-Practices/07-reinforcement-learning/马尔科夫决策过程')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.core import State, Action, TransitionModel, RewardFunction, MarkovDecisionProcess\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding States and Actions\n",
    "\n",
    "### States\n",
    "A state represents a configuration of the environment. States must be:\n",
    "- **Markovian**: The future depends only on the current state, not history\n",
    "- **Sufficient**: Contains all relevant information for decision-making\n",
    "\n",
    "### Actions\n",
    "Actions are decisions available to the agent in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create states\n",
    "state_0 = State(state_id=0, features=np.array([0, 0]))\n",
    "state_1 = State(state_id=1, features=np.array([0, 1]))\n",
    "state_2 = State(state_id=2, features=np.array([1, 0]))\n",
    "\n",
    "print(f\"State 0: {state_0}\")\n",
    "print(f\"State 1: {state_1}\")\n",
    "print(f\"State 2: {state_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create actions\n",
    "action_up = Action(action_id=0, name=\"UP\")\n",
    "action_down = Action(action_id=1, name=\"DOWN\")\n",
    "action_left = Action(action_id=2, name=\"LEFT\")\n",
    "action_right = Action(action_id=3, name=\"RIGHT\")\n",
    "\n",
    "print(f\"Action UP: {action_up}\")\n",
    "print(f\"Action DOWN: {action_down}\")\n",
    "print(f\"\\nActions are hashable (can be used as dict keys):\")\n",
    "action_dict = {action_up: \"move up\", action_down: \"move down\"}\n",
    "print(action_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Transition Model\n",
    "\n",
    "The transition model P(s'|s,a) defines the probability of reaching state s' when taking action a in state s.\n",
    "\n",
    "**Key Property**: Probabilities must sum to 1 for each (s,a) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transition model\n",
    "transition_model = TransitionModel()\n",
    "\n",
    "# Define deterministic transitions\n",
    "# From state 0, action UP leads to state 1 with probability 1.0\n",
    "transition_model.set_transition(state_0, action_up, state_1, probability=1.0)\n",
    "\n",
    "# From state 0, action DOWN leads to state 2 with probability 1.0\n",
    "transition_model.set_transition(state_0, action_down, state_2, probability=1.0)\n",
    "\n",
    "print(\"Transitions set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query transition distribution\n",
    "dist = transition_model.get_transition_distribution(state_0, action_up)\n",
    "print(f\"P(s'|s=0, a=UP): {dist}\")\n",
    "\n",
    "# Verify probabilities sum to 1\n",
    "prob_sum = sum(dist.values())\n",
    "print(f\"Probability sum: {prob_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Reward Function\n",
    "\n",
    "The reward function R(s,a,s') provides immediate feedback for transitions.\n",
    "\n",
    "**Design Principle**: Rewards should encode the agent's objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reward function\n",
    "reward_fn = RewardFunction(reward_type=\"state_action_next_state\")\n",
    "\n",
    "# Set rewards\n",
    "reward_fn.set_reward(state_0, action_up, state_1, reward=1.0)  # Positive reward\n",
    "reward_fn.set_reward(state_0, action_down, state_2, reward=-1.0)  # Negative reward\n",
    "\n",
    "print(\"Rewards set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query rewards\n",
    "r1 = reward_fn.get_reward(state_0, action_up, state_1)\n",
    "r2 = reward_fn.get_reward(state_0, action_down, state_2)\n",
    "\n",
    "print(f\"R(s=0, a=UP, s'=1) = {r1}\")\n",
    "print(f\"R(s=0, a=DOWN, s'=2) = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete MDP\n",
    "\n",
    "An MDP combines all components: M = (S, A, P, R, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MDP\n",
    "mdp = MarkovDecisionProcess(discount_factor=0.99)\n",
    "\n",
    "# Add states\n",
    "mdp.add_state(state_0)\n",
    "mdp.add_state(state_1)\n",
    "mdp.add_state(state_2)\n",
    "\n",
    "# Add actions\n",
    "mdp.add_action(action_up)\n",
    "mdp.add_action(action_down)\n",
    "\n",
    "# Set transition and reward models\n",
    "mdp.transition_model = transition_model\n",
    "mdp.reward_function = reward_fn\n",
    "\n",
    "# Set initial state\n",
    "mdp.set_initial_state(state_0)\n",
    "\n",
    "# Mark terminal state\n",
    "mdp.add_terminal_state(state_1)\n",
    "\n",
    "print(f\"MDP created successfully\")\n",
    "print(f\"State space size: {mdp.get_state_space_size()}\")\n",
    "print(f\"Action space size: {mdp.get_action_space_size()}\")\n",
    "print(f\"Discount factor: {mdp.gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate MDP\n",
    "try:\n",
    "    mdp.validate()\n",
    "    print(\"MDP validation passed!\")\nexcept ValueError as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": ,
   "source": [
    "## Part 6: The Bellman Equation\n",
    "\n",
    "The Bellman equation is the foundation of dynamic programming:\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "This equation states that the optimal value of a state equals the maximum expected reward from taking an action plus the discounted value of the resulting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of Bellman equation\n",
    "print(\"Bellman Equation Components:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"State s: {state_0}\")\n",
    "print(f\"Action a: {action_up}\")\n",
    "print(f\"\\nTransition: P(s'|s,a) = {transition_model.get_transition_distribution(state_0, action_up)}\")\n",
    "print(f\"Reward: R(s,a,s') = {reward_fn.get_reward(state_0, action_up, state_1)}\")\n",
    "print(f\"Discount factor: γ = {mdp.gamma}\")\n",
    "print(f\"\\nBellman backup: R + γV(s')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "1. **States** represent environment configurations\n",
    "2. **Actions** are decisions available to the agent\n",
    "3. **Transitions** define environment dynamics\n",
    "4. **Rewards** encode objectives\n",
    "5. **Bellman equation** connects values across states\n",
    "6. **Discount factor** balances immediate vs future rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
