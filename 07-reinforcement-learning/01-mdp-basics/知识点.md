# 马尔可夫决策过程 (Markov Decision Process)

本文档系统介绍强化学习的数学基础——马尔可夫决策过程 (MDP)，涵盖状态、动作、奖励、策略、价值函数与贝尔曼方程等核心概念。

---

## 1. 强化学习概述

### 1.1 三大机器学习范式

| 范式 | 数据特点 | 学习目标 | 典型问题 |
|------|----------|----------|----------|
| 监督学习 | $(x_i, y_i)$ 标签数据 | 学习映射 $f: X \to Y$ | 分类、回归 |
| 无监督学习 | $\{x_i\}$ 无标签数据 | 发现数据结构 | 聚类、降维 |
| **强化学习** | 交互序列、延迟奖励 | 最大化累积回报 | 决策、控制 |

### 1.2 强化学习框架

强化学习研究**智能体 (Agent)** 如何在与**环境 (Environment)** 的交互中学习最优行为策略。

```
                    动作 a_t
         ┌────────────────────┐
         │                    ▼
    ┌─────────┐          ┌─────────┐
    │  Agent  │          │   Env   │
    │ (智能体) │          │  (环境)  │
    └─────────┘          └─────────┘
         ▲                    │
         │   状态 s_{t+1}     │
         │   奖励 r_{t+1}     │
         └────────────────────┘
```

**交互循环**：
1. 智能体观测当前状态 $s_t$
2. 根据策略选择动作 $a_t$
3. 环境转移至新状态 $s_{t+1}$，反馈奖励 $r_{t+1}$
4. 智能体利用反馈更新策略

### 1.3 典型应用

- **游戏 AI**: AlphaGo、Atari 游戏、星际争霸 II
- **机器人控制**: 机械臂操作、四足机器人、自动驾驶
- **资源调度**: 数据中心能耗优化、通信网络调度
- **金融交易**: 量化策略、投资组合管理
- **推荐系统**: 长期用户满意度优化

---

## 2. MDP 形式化定义

### 2.1 五元组表示

马尔可夫决策过程由五元组定义：

$$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$$

| 符号 | 名称 | 定义 |
|------|------|------|
| $\mathcal{S}$ | 状态空间 | 所有可能状态的集合 |
| $\mathcal{A}$ | 动作空间 | 所有可能动作的集合 |
| $P$ | 转移函数 | $P(s' \mid s, a) = \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)$ |
| $R$ | 奖励函数 | $R(s, a, s')$ 或 $R(s, a)$ |
| $\gamma$ | 折扣因子 | $\gamma \in [0, 1]$ |

### 2.2 马尔可夫性质

**马尔可夫性 (Markov Property)**: 未来状态仅依赖当前状态，与历史无关。

$$P(S_{t+1} \mid S_t, A_t, S_{t-1}, A_{t-1}, \ldots, S_0, A_0) = P(S_{t+1} \mid S_t, A_t)$$

**含义**: 当前状态 $S_t$ 包含了预测未来所需的全部信息。

### 2.3 各组件详解

#### 状态空间 $\mathcal{S}$

状态是对环境的完整描述。根据问题特性可分为：

- **离散状态**: 如棋盘格局、网格位置
- **连续状态**: 如机器人关节角度、车辆位置速度

```python
# 离散状态示例：网格世界
State = Tuple[int, int]  # (行, 列)
states = [(i, j) for i in range(4) for j in range(4)]

# 连续状态示例：倒立摆
# state = [cart_position, cart_velocity, pole_angle, pole_angular_velocity]
```

**完全可观测 vs 部分可观测**:
- MDP: 智能体能完全观测状态
- POMDP: 智能体仅能获得部分观测

#### 动作空间 $\mathcal{A}$

动作是智能体可采取的行为。

- **离散动作**: $\mathcal{A} = \{\text{上}, \text{下}, \text{左}, \text{右}\}$
- **连续动作**: $\mathcal{A} \subseteq \mathbb{R}^n$ (如关节扭矩)

#### 转移函数 $P$

描述环境动力学：

$$P(s' \mid s, a) = \Pr(S_{t+1} = s' \mid S_t = s, A_t = a)$$

- **确定性环境**: $P(s' \mid s, a) \in \{0, 1\}$
- **随机性环境**: $\sum_{s'} P(s' \mid s, a) = 1$

#### 奖励函数 $R$

奖励是环境对智能体行为的即时反馈信号。常见形式：

- $R(s, a, s')$: 依赖转移三元组
- $R(s, a)$: 仅依赖状态-动作对
- $R(s)$: 仅依赖状态

**奖励设计原则**:
1. **稀疏 vs 密集**: 稀疏奖励学习困难但更自然；密集奖励加速学习但需仔细设计
2. **奖励塑形 (Reward Shaping)**: 添加中间奖励引导学习
3. **避免奖励黑客 (Reward Hacking)**: 防止智能体利用奖励漏洞

#### 折扣因子 $\gamma$

折扣因子控制对未来奖励的重视程度。累积回报定义为：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

| $\gamma$ 取值 | 效果 |
|---------------|------|
| $\gamma = 0$ | 贪婪策略，只看即时奖励 |
| $\gamma = 1$ | 无折扣，所有未来奖励等价 |
| $\gamma \approx 0.99$ | 常用值，平衡短期与长期 |

---

## 3. 策略与价值函数

### 3.1 策略 (Policy)

策略 $\pi$ 定义了智能体在各状态下的行为方式。

**随机策略**: $\pi(a \mid s) = \Pr(A_t = a \mid S_t = s)$

**确定性策略**: $a = \pi(s)$

```python
# 随机策略示例：ε-贪婪
def epsilon_greedy(Q, state, epsilon=0.1):
    if random.random() < epsilon:
        return random.choice(actions)  # 探索
    return argmax(Q[state])            # 利用
```

### 3.2 状态价值函数 $V^\pi(s)$

从状态 $s$ 出发，遵循策略 $\pi$ 的期望累积回报：

$$V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s\right]$$

### 3.3 动作价值函数 $Q^\pi(s, a)$

从状态 $s$ 执行动作 $a$，然后遵循策略 $\pi$ 的期望累积回报：

$$Q^\pi(s, a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a]$$

### 3.4 V 与 Q 的关系

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) Q^\pi(s, a)$$

$$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^\pi(s')$$

### 3.5 最优价值函数

**最优状态价值**:

$$V^*(s) = \max_\pi V^\pi(s)$$

**最优动作价值**:

$$Q^*(s, a) = \max_\pi Q^\pi(s, a)$$

**最优策略**: 选择最大化 $Q^*$ 的动作

$$\pi^*(s) = \arg\max_a Q^*(s, a)$$

---

## 4. 贝尔曼方程

### 4.1 贝尔曼期望方程

价值函数的递归分解形式：

**状态价值**:

$$V^\pi(s) = \sum_{a} \pi(a \mid s) \left[ R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^\pi(s') \right]$$

**动作价值**:

$$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \sum_{a'} \pi(a' \mid s') Q^\pi(s', a')$$

**直觉**: 当前价值 = 即时奖励 + 折扣后的后继状态价值

### 4.2 贝尔曼最优方程

最优价值函数满足的递归方程：

$$V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^*(s') \right]$$

$$Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^*(s', a')$$

### 4.3 备份图 (Backup Diagram)

贝尔曼方程可用备份图直观理解：

```
      (s)                 (s,a)
       │                    │
       ├─a₁─○──s'₁         ├──s'₁──○─a'₁
       │    ○──s'₂         │       ○─a'₂
       └─a₂─○──s'₃         └──s'₂──○─a'₃

   V(s) 备份图          Q(s,a) 备份图
```

---

## 5. 动态规划求解

当环境模型（$P$ 和 $R$）已知时，可使用动态规划精确求解 MDP。动态规划方法由Richard Bellman在1950年代提出，是求解序贯决策问题的系统化方法。

### 5.1 策略评估 (Policy Evaluation)

给定策略 $\pi$，迭代计算其价值函数。

**提出背景**: 策略评估是动态规划的基础算子，用于回答"给定策略有多好"这一问题。在策略改进前，需要先评估当前策略的价值。

**算法**:

```
初始化 V(s) = 0, ∀s ∈ S
重复:
    Δ ← 0
    对每个 s ∈ S:
        v ← V(s)
        V(s) ← Σ_a π(a|s) Σ_{s'} P(s'|s,a) [R(s,a,s') + γV(s')]
        Δ ← max(Δ, |v - V(s)|)
直到 Δ < θ (收敛阈值)
```

**算法原理**: 通过迭代求解贝尔曼期望方程，将当前价值估计代入方程右侧产生新估计。这是一个不动点迭代过程，保证线性收敛。

**复杂度**: $O(|\mathcal{S}|^2 |\mathcal{A}|)$ 每次迭代

**优点**:
- 理论简单，易于实现
- 收敛性有理论保证
- 可并行化计算

**缺点**:
- 需要完整环境模型
- 收敛速度受 $\gamma$ 影响
- 状态空间大时效率低

### 5.2 策略改进 (Policy Improvement)

基于当前价值函数贪婪改进策略：

$$\pi'(s) = \arg\max_a \sum_{s'} P(s' \mid s, a) [R(s, a, s') + \gamma V^\pi(s')]$$

**提出动机**: 策略评估告诉我们当前策略的价值，但不告诉我们如何改进。策略改进提供了系统化的改进方法。

**策略改进定理**: $Q^{\pi}(s, \pi'(s)) \geq V^{\pi}(s) \Rightarrow V^{\pi'}(s) \geq V^{\pi}(s)$

这一定理保证贪婪策略至少不比原策略差，是策略迭代收敛性的理论基础。

### 5.3 策略迭代 (Policy Iteration)

交替执行策略评估和策略改进：

```
初始化策略 π (如随机策略)
重复:
    1. 策略评估: 计算 V^π
    2. 策略改进: π' ← greedy(V^π)
    3. 若 π' = π 则停止
    4. π ← π'
返回 π, V^π
```

**提出背景**: 策略迭代由Ronald Howard于1960年提出，将MDP求解问题分解为两个子问题：评估（给定策略求价值）和改进（给定价值求策略）。

**为什么这样做**: 直接求解最优策略和价值函数很困难，但分别求解评估和改进问题相对简单。通过交替求解，最终收敛到最优解。

**收敛性保证**:
- 策略改进定理保证价值单调递增
- 有限MDP中策略数量有限，单调序列必收敛
- 收敛到全局最优（无局部最优问题）

**特点**: 保证收敛到最优策略；外层迭代次数少（通常3-10次）但每次评估代价高

**适用场景**: 小到中等规模状态空间，需要中间策略可执行

### 5.4 值迭代 (Value Iteration)

直接迭代贝尔曼最优方程：

```
初始化 V(s) = 0, ∀s ∈ S
重复:
    Δ ← 0
    对每个 s ∈ S:
        v ← V(s)
        V(s) ← max_a Σ_{s'} P(s'|s,a) [R(s,a,s') + γV(s')]
        Δ ← max(Δ, |v - V(s)|)
直到 Δ < θ
从 V 提取策略 π
```

**提出背景**: 值迭代由Bellman提出，跳过显式的策略维护，直接迭代价值函数。可视为策略迭代的变体，将策略评估截断为单步更新。

**核心思想**: 每次更新隐式地进行策略改进（选择max），因此不需要显式维护策略。这种"截断评估"在大状态空间中更高效。

**收敛性**: 基于Banach不动点定理，贝尔曼最优算子是 $\gamma$-压缩映射，保证收敛到唯一不动点。

**特点**: 每次迭代代价低但迭代次数多

**适用场景**: 大规模状态空间，内存受限环境

### 5.5 方法对比

| 维度 | 策略迭代 | 值迭代 |
|------|----------|--------|
| **每次迭代复杂度** | $O(\|\mathcal{S}\|^2 \|\mathcal{A}\|)$ + 评估 | $O(\|\mathcal{S}\|^2 \|\mathcal{A}\|)$ |
| **迭代次数** | 少（3-10次） | 多（数百次） |
| **特点** | 完全策略评估 | 单步截断评估 |
| **中间结果** | 每步有可执行策略 | 收敛前无策略 |
| **内存需求** | 需存储策略和价值 | 仅存储价值 |
| **适用场景** | 小规模精确求解 | 大规模近似求解 |
| **收敛速度** | 外层快，内层慢 | 均匀但总体慢 |

**实践建议**:
- **小规模问题** ($|\mathcal{S}| < 10^4$): 策略迭代通常更快
- **大规模问题**: 值迭代内存效率更高
- **需要中间策略**: 必须使用策略迭代
- **$\gamma$ 接近1**: 值迭代相对更稳定

**统一视角**: 值迭代可视为策略迭代的特例，将策略评估的"收敛直到 $\Delta < \theta$"替换为"仅1次迭代"。这种截断在实践中往往不影响最终收敛到最优解。

---

## 6. 代码实现

本目录包含完整的网格世界环境与动态规划求解器实现。

### 6.1 文件结构

```
01-mdp-basics/
├── 01-MDP与动态规划.ipynb  # 交互式学习 Notebook
├── grid_world_dp.py        # 网格世界环境与 DP 算法
└── 马尔可夫决策过程.md     # 本文档
```

### 6.2 核心类

```python
# 环境配置
config = GridWorldConfig(
    size=4,
    start=(0, 0),
    goal=(3, 3),
    obstacles=[(1, 1)],
    slip_probability=0.0  # 确定性环境
)

# 创建环境
env = GridWorld(config)

# 创建求解器
solver = DynamicProgrammingSolver(env, gamma=0.99)

# 策略迭代
result = solver.policy_iteration()
env.render_policy(result.policy)
env.render_values(result.value_function)

# 值迭代
result = solver.value_iteration()
```

### 6.3 运行示例

```bash
python grid_world_dp.py
```

输出包括：
- 15 个单元测试验证
- 策略迭代与值迭代结果
- 策略可视化（箭头表示）
- 价值函数可视化
- 策略执行统计

---

## 7. 从 DP 到 RL：局限与突破

### 7.1 动态规划的局限

动态规划方法虽然理论完备，但在实际应用中面临三大根本性挑战：

1. **需要完整环境模型**
   - 转移概率 $P(s'|s,a)$ 通常未知或难以准确建模
   - 现实世界：机器人与物理环境的交互、股票市场动态等
   - 获取精确模型的成本往往高于直接学习策略

2. **维度灾难 (Curse of Dimensionality)**
   - 状态空间随维度指数增长
   - 围棋: $\approx 10^{170}$ 种状态
   - 连续控制: 无限维状态空间
   - 单次迭代需遍历所有状态，计算和存储开销难以承受

3. **连续空间处理**
   - 动态规划基于表格表示，无法直接处理连续状态/动作
   - 离散化导致维度爆炸和近似误差
   - 需要函数逼近技术（神经网络等）

**历史意义**: 尽管存在这些局限，动态规划为后续方法提供了理论基础。贝尔曼方程是所有强化学习算法的核心，动态规划的思想（自举、策略改进）贯穿整个强化学习发展史。

### 7.2 突破方向与解决方案

| 挑战 | 传统DP的困境 | 突破方向 | 代表方法 |
|------|-------------|---------|---------|
| **模型未知** | 需要 P(s'\|s,a) | 无模型学习 | Q-Learning, SARSA |
| **状态空间大** | 表格存储爆炸 | 函数逼近 | DQN, Actor-Critic |
| **连续空间** | 无法表格化 | 策略梯度 | REINFORCE, PPO, SAC |
| **样本效率** | 需要完整遍历 | 经验回放 | DQN, Rainbow |
| **探索利用** | 假设已知最优 | 探索策略 | ε-greedy, UCB |

### 7.3 学习路径演进图

```
                    动态规划 (1950s)
                    需要模型 P,R
                         │
         ┌───────────────┴───────────────┐
         │                               │
    时序差分学习 (1980s)              蒙特卡洛方法 (1990s)
    无需模型，自举更新                无需模型，完整轨迹
         │                               │
    ┌────┴────┐                     ┌────┴────┐
    │         │                     │         │
Q-Learning  SARSA              REINFORCE   Actor-Critic
离策略      在策略               策略梯度     结合价值与策略
    │         │                     │         │
    └────┬────┘                     └────┬────┘
         │                               │
    深度Q网络 (2013)                 深度策略梯度 (2015)
    DQN, Rainbow                    TRPO, PPO, SAC
    函数逼近+经验回放                 稳定训练+连续控制
         │                               │
         └───────────────┬───────────────┘
                         │
                  现代强化学习 (2020s)
                  多智能体、元学习、离线RL
```

### 7.4 关键思想的延续

**从DP到现代RL的核心继承**:

1. **贝尔曼方程**
   - DP: 精确求解贝尔曼方程
   - TD学习: 近似求解贝尔曼方程
   - DQN: 用神经网络逼近Q函数的贝尔曼方程

2. **自举 (Bootstrapping)**
   - DP: 用旧估计更新新估计 $V(s) \leftarrow \text{期望}[R + \gamma V(s')]$
   - TD(0): $V(s) \leftarrow V(s) + \alpha[R + \gamma V(s') - V(s)]$
   - Q-Learning: $Q(s,a) \leftarrow Q(s,a) + \alpha[R + \gamma \max_{a'} Q(s',a') - Q(s,a)]$

3. **策略改进**
   - DP: 贪婪策略改进
   - Q-Learning: ε-贪婪探索
   - Actor-Critic: 显式策略改进

4. **迭代优化**
   - DP: 同步遍历所有状态
   - TD: 异步更新访问状态
   - 经验回放: 随机批次更新

**关键区别**:

| 方面 | 动态规划 | 现代RL |
|------|---------|--------|
| 环境知识 | 完全已知 | 通过交互学习 |
| 更新方式 | 同步遍历 | 异步采样 |
| 价值表示 | 表格 | 函数逼近 |
| 探索策略 | 不需要 | 必需 |
| 样本来源 | 模型计算 | 环境交互 |

### 7.5 实际应用建议

**何时使用动态规划**:
- 环境模型完全已知且精确
- 状态空间较小（< $10^6$ 状态）
- 需要理论最优解
- 离线规划问题（如路径规划）

**何时转向RL**:
- 环境模型未知或难以建模
- 大规模或连续状态空间
- 在线学习场景
- 需要从数据中学习

**混合方法**:
- Dyna-Q: 结合模型学习和无模型RL
- MCTS: 蒙特卡洛树搜索（AlphaGo的核心）
- 基于模型的RL: 学习环境模型后用DP规划

---

## 参考文献

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
2. Bellman, R. (1957). *Dynamic Programming*. Princeton University Press.
3. Puterman, M. L. (1994). *Markov Decision Processes: Discrete Stochastic Dynamic Programming*. Wiley.
4. Silver, D. (2015). UCL Course on Reinforcement Learning. [Lecture Notes](https://www.davidsilver.uk/teaching/)

---

[返回上级](../README.md)
