# 马尔可夫决策过程 (Markov Decision Process)

本文档系统介绍强化学习的数学基础——马尔可夫决策过程 (MDP)，涵盖状态、动作、奖励、策略、价值函数与贝尔曼方程等核心概念。

---

## 1. 强化学习概述

### 1.1 三大机器学习范式

| 范式 | 数据特点 | 学习目标 | 典型问题 |
|------|----------|----------|----------|
| 监督学习 | $(x_i, y_i)$ 标签数据 | 学习映射 $f: X \to Y$ | 分类、回归 |
| 无监督学习 | $\{x_i\}$ 无标签数据 | 发现数据结构 | 聚类、降维 |
| **强化学习** | 交互序列、延迟奖励 | 最大化累积回报 | 决策、控制 |

### 1.2 强化学习框架

强化学习研究**智能体 (Agent)** 如何在与**环境 (Environment)** 的交互中学习最优行为策略。

```
                    动作 a_t
         ┌────────────────────┐
         │                    ▼
    ┌─────────┐          ┌─────────┐
    │  Agent  │          │   Env   │
    │ (智能体) │          │  (环境)  │
    └─────────┘          └─────────┘
         ▲                    │
         │   状态 s_{t+1}     │
         │   奖励 r_{t+1}     │
         └────────────────────┘
```

**交互循环**：
1. 智能体观测当前状态 $s_t$
2. 根据策略选择动作 $a_t$
3. 环境转移至新状态 $s_{t+1}$，反馈奖励 $r_{t+1}$
4. 智能体利用反馈更新策略

### 1.3 典型应用

- **游戏 AI**: AlphaGo、Atari 游戏、星际争霸 II
- **机器人控制**: 机械臂操作、四足机器人、自动驾驶
- **资源调度**: 数据中心能耗优化、通信网络调度
- **金融交易**: 量化策略、投资组合管理
- **推荐系统**: 长期用户满意度优化

---

## 2. MDP 形式化定义

### 2.1 五元组表示

马尔可夫决策过程由五元组定义：

$$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$$

| 符号 | 名称 | 定义 |
|------|------|------|
| $\mathcal{S}$ | 状态空间 | 所有可能状态的集合 |
| $\mathcal{A}$ | 动作空间 | 所有可能动作的集合 |
| $P$ | 转移函数 | $P(s' \mid s, a) = \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)$ |
| $R$ | 奖励函数 | $R(s, a, s')$ 或 $R(s, a)$ |
| $\gamma$ | 折扣因子 | $\gamma \in [0, 1]$ |

### 2.2 马尔可夫性质

**马尔可夫性 (Markov Property)**: 未来状态仅依赖当前状态，与历史无关。

$$P(S_{t+1} \mid S_t, A_t, S_{t-1}, A_{t-1}, \ldots, S_0, A_0) = P(S_{t+1} \mid S_t, A_t)$$

**含义**: 当前状态 $S_t$ 包含了预测未来所需的全部信息。

### 2.3 各组件详解

#### 状态空间 $\mathcal{S}$

状态是对环境的完整描述。根据问题特性可分为：

- **离散状态**: 如棋盘格局、网格位置
- **连续状态**: 如机器人关节角度、车辆位置速度

```python
# 离散状态示例：网格世界
State = Tuple[int, int]  # (行, 列)
states = [(i, j) for i in range(4) for j in range(4)]

# 连续状态示例：倒立摆
# state = [cart_position, cart_velocity, pole_angle, pole_angular_velocity]
```

**完全可观测 vs 部分可观测**:
- MDP: 智能体能完全观测状态
- POMDP: 智能体仅能获得部分观测

#### 动作空间 $\mathcal{A}$

动作是智能体可采取的行为。

- **离散动作**: $\mathcal{A} = \{\text{上}, \text{下}, \text{左}, \text{右}\}$
- **连续动作**: $\mathcal{A} \subseteq \mathbb{R}^n$ (如关节扭矩)

#### 转移函数 $P$

描述环境动力学：

$$P(s' \mid s, a) = \Pr(S_{t+1} = s' \mid S_t = s, A_t = a)$$

- **确定性环境**: $P(s' \mid s, a) \in \{0, 1\}$
- **随机性环境**: $\sum_{s'} P(s' \mid s, a) = 1$

#### 奖励函数 $R$

奖励是环境对智能体行为的即时反馈信号。常见形式：

- $R(s, a, s')$: 依赖转移三元组
- $R(s, a)$: 仅依赖状态-动作对
- $R(s)$: 仅依赖状态

**奖励设计原则**:
1. **稀疏 vs 密集**: 稀疏奖励学习困难但更自然；密集奖励加速学习但需仔细设计
2. **奖励塑形 (Reward Shaping)**: 添加中间奖励引导学习
3. **避免奖励黑客 (Reward Hacking)**: 防止智能体利用奖励漏洞

#### 折扣因子 $\gamma$

折扣因子控制对未来奖励的重视程度。累积回报定义为：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

| $\gamma$ 取值 | 效果 |
|---------------|------|
| $\gamma = 0$ | 贪婪策略，只看即时奖励 |
| $\gamma = 1$ | 无折扣，所有未来奖励等价 |
| $\gamma \approx 0.99$ | 常用值，平衡短期与长期 |

---

## 3. 策略与价值函数

### 3.1 策略 (Policy)

策略 $\pi$ 定义了智能体在各状态下的行为方式。

**随机策略**: $\pi(a \mid s) = \Pr(A_t = a \mid S_t = s)$

**确定性策略**: $a = \pi(s)$

```python
# 随机策略示例：ε-贪婪
def epsilon_greedy(Q, state, epsilon=0.1):
    if random.random() < epsilon:
        return random.choice(actions)  # 探索
    return argmax(Q[state])            # 利用
```

### 3.2 状态价值函数 $V^\pi(s)$

从状态 $s$ 出发，遵循策略 $\pi$ 的期望累积回报：

$$V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s\right]$$

### 3.3 动作价值函数 $Q^\pi(s, a)$

从状态 $s$ 执行动作 $a$，然后遵循策略 $\pi$ 的期望累积回报：

$$Q^\pi(s, a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a]$$

### 3.4 V 与 Q 的关系

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) Q^\pi(s, a)$$

$$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^\pi(s')$$

### 3.5 最优价值函数

**最优状态价值**:

$$V^*(s) = \max_\pi V^\pi(s)$$

**最优动作价值**:

$$Q^*(s, a) = \max_\pi Q^\pi(s, a)$$

**最优策略**: 选择最大化 $Q^*$ 的动作

$$\pi^*(s) = \arg\max_a Q^*(s, a)$$

---

## 4. 贝尔曼方程

### 4.1 贝尔曼期望方程

价值函数的递归分解形式：

**状态价值**:

$$V^\pi(s) = \sum_{a} \pi(a \mid s) \left[ R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^\pi(s') \right]$$

**动作价值**:

$$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \sum_{a'} \pi(a' \mid s') Q^\pi(s', a')$$

**直觉**: 当前价值 = 即时奖励 + 折扣后的后继状态价值

### 4.2 贝尔曼最优方程

最优价值函数满足的递归方程：

$$V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^*(s') \right]$$

$$Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^*(s', a')$$

### 4.3 备份图 (Backup Diagram)

贝尔曼方程可用备份图直观理解：

```
      (s)                 (s,a)
       │                    │
       ├─a₁─○──s'₁         ├──s'₁──○─a'₁
       │    ○──s'₂         │       ○─a'₂
       └─a₂─○──s'₃         └──s'₂──○─a'₃

   V(s) 备份图          Q(s,a) 备份图
```

---

## 5. 动态规划求解

当环境模型（$P$ 和 $R$）已知时，可使用动态规划精确求解 MDP。

### 5.1 策略评估 (Policy Evaluation)

给定策略 $\pi$，迭代计算其价值函数。

**算法**:

```
初始化 V(s) = 0, ∀s ∈ S
重复:
    Δ ← 0
    对每个 s ∈ S:
        v ← V(s)
        V(s) ← Σ_a π(a|s) Σ_{s'} P(s'|s,a) [R(s,a,s') + γV(s')]
        Δ ← max(Δ, |v - V(s)|)
直到 Δ < θ (收敛阈值)
```

**复杂度**: $O(|\mathcal{S}|^2 |\mathcal{A}|)$ 每次迭代

### 5.2 策略改进 (Policy Improvement)

基于当前价值函数贪婪改进策略：

$$\pi'(s) = \arg\max_a \sum_{s'} P(s' \mid s, a) [R(s, a, s') + \gamma V^\pi(s')]$$

**策略改进定理**: $Q^{\pi}(s, \pi'(s)) \geq V^{\pi}(s) \Rightarrow V^{\pi'}(s) \geq V^{\pi}(s)$

### 5.3 策略迭代 (Policy Iteration)

交替执行策略评估和策略改进：

```
初始化策略 π (如随机策略)
重复:
    1. 策略评估: 计算 V^π
    2. 策略改进: π' ← greedy(V^π)
    3. 若 π' = π 则停止
    4. π ← π'
返回 π, V^π
```

**特点**: 保证收敛到最优策略；外层迭代次数少但每次评估代价高

### 5.4 值迭代 (Value Iteration)

直接迭代贝尔曼最优方程：

```
初始化 V(s) = 0, ∀s ∈ S
重复:
    Δ ← 0
    对每个 s ∈ S:
        v ← V(s)
        V(s) ← max_a Σ_{s'} P(s'|s,a) [R(s,a,s') + γV(s')]
        Δ ← max(Δ, |v - V(s)|)
直到 Δ < θ
从 V 提取策略 π
```

**特点**: 每次迭代代价低但迭代次数多

### 5.5 方法对比

| 方法 | 每次迭代复杂度 | 迭代次数 | 特点 |
|------|----------------|----------|------|
| 策略迭代 | $O(\|\mathcal{S}\|^2 \|\mathcal{A}\|)$ + 评估 | 少 | 精确评估 |
| 值迭代 | $O(\|\mathcal{S}\| \|\mathcal{A}\|)$ | 多 | 截断评估 |

---

## 6. 代码实现

本目录包含完整的网格世界环境与动态规划求解器实现。

### 6.1 文件结构

```
01-mdp-basics/
├── 01-MDP与动态规划.ipynb  # 交互式学习 Notebook
├── grid_world_dp.py        # 网格世界环境与 DP 算法
└── 马尔可夫决策过程.md     # 本文档
```

### 6.2 核心类

```python
# 环境配置
config = GridWorldConfig(
    size=4,
    start=(0, 0),
    goal=(3, 3),
    obstacles=[(1, 1)],
    slip_probability=0.0  # 确定性环境
)

# 创建环境
env = GridWorld(config)

# 创建求解器
solver = DynamicProgrammingSolver(env, gamma=0.99)

# 策略迭代
result = solver.policy_iteration()
env.render_policy(result.policy)
env.render_values(result.value_function)

# 值迭代
result = solver.value_iteration()
```

### 6.3 运行示例

```bash
python grid_world_dp.py
```

输出包括：
- 15 个单元测试验证
- 策略迭代与值迭代结果
- 策略可视化（箭头表示）
- 价值函数可视化
- 策略执行统计

---

## 7. 从 DP 到 RL

### 7.1 动态规划的局限

1. **需要完整环境模型**: 转移概率 $P$ 和奖励 $R$ 通常未知
2. **状态空间爆炸**: 围棋有 $10^{170}$ 种状态，无法遍历
3. **连续状态/动作**: 无法直接表格化

### 7.2 解决方案

| 问题 | 解决方案 |
|------|----------|
| 模型未知 | 无模型学习 (Model-Free RL) |
| 状态空间大 | 函数逼近 (神经网络) |
| 连续空间 | 策略梯度方法 |

**后续学习路径**:

```
MDP 理论 (本节)
    │
    ├── 无模型方法
    │   ├── 时序差分学习 (TD)
    │   └── Q-Learning
    │
    └── 深度强化学习
        ├── DQN
        └── 策略梯度 (A2C, PPO)
```

---

## 参考文献

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
2. Bellman, R. (1957). *Dynamic Programming*. Princeton University Press.
3. Puterman, M. L. (1994). *Markov Decision Processes: Discrete Stochastic Dynamic Programming*. Wiley.
4. Silver, D. (2015). UCL Course on Reinforcement Learning. [Lecture Notes](https://www.davidsilver.uk/teaching/)

---

[返回上级](../README.md)
