{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 流行强化学习算法详解\n",
    "# Popular Reinforcement Learning Algorithms: A Comprehensive Guide\n",
    "\n",
    "---\n",
    "\n",
    "## 目录 (Table of Contents)\n",
    "\n",
    "1. [引言与理论基础](#1-引言与理论基础)\n",
    "2. [DDPG: 深度确定性策略梯度](#2-ddpg-深度确定性策略梯度)\n",
    "3. [TD3: 双延迟深度确定性策略梯度](#3-td3-双延迟深度确定性策略梯度)\n",
    "4. [SAC: 软演员-评论家](#4-sac-软演员-评论家)\n",
    "5. [算法对比与实验](#5-算法对比与实验)\n",
    "6. [最佳实践与调参指南](#6-最佳实践与调参指南)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置 (Environment Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子确保可复现性\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 检查GPU可用性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入我们的实现模块\n",
    "from popular_rl_algorithms import (\n",
    "    DDPGAgent, DDPGConfig,\n",
    "    TD3Agent, TD3Config,\n",
    "    SACAgent, SACConfig,\n",
    "    ReplayBuffer,\n",
    "    DeterministicActor, GaussianActor,\n",
    "    QNetwork, TwinQNetwork,\n",
    "    train_continuous_agent,\n",
    "    evaluate_agent,\n",
    "    compare_algorithms,\n",
    ")\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "    print(\"Gymnasium loaded successfully!\")\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"Warning: gymnasium not installed. Some features will be unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 引言与理论基础\n",
    "\n",
    "### 1.1 强化学习问题形式化\n",
    "\n",
    "强化学习问题通常被形式化为**马尔可夫决策过程 (MDP)**，由五元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ 定义：\n",
    "\n",
    "| 符号 | 含义 | 说明 |\n",
    "|------|------|------|\n",
    "| $\\mathcal{S}$ | 状态空间 | 环境可能处于的所有状态集合 |\n",
    "| $\\mathcal{A}$ | 动作空间 | 智能体可执行的所有动作集合 |\n",
    "| $P(s'\\|s, a)$ | 状态转移概率 | 在状态$s$执行动作$a$后转移到$s'$的概率 |\n",
    "| $R(s, a, s')$ | 奖励函数 | 转移时获得的即时奖励 |\n",
    "| $\\gamma \\in [0, 1]$ | 折扣因子 | 未来奖励的衰减系数 |\n",
    "\n",
    "### 1.2 核心目标\n",
    "\n",
    "**目标**：找到最优策略 $\\pi^*$，使得期望累积折扣奖励最大化：\n",
    "\n",
    "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t\\right]$$\n",
    "\n",
    "其中 $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots)$ 是遵循策略 $\\pi$ 产生的轨迹。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 价值函数\n",
    "\n",
    "#### 状态价值函数 (State Value Function)\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s\\right]$$\n",
    "\n",
    "**直觉**：从状态$s$开始，遵循策略$\\pi$，期望能获得多少累积奖励？\n",
    "\n",
    "#### 动作价值函数 (Action Value Function / Q-Function)\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0 = a\\right]$$\n",
    "\n",
    "**直觉**：在状态$s$执行动作$a$，然后遵循策略$\\pi$，期望能获得多少累积奖励？\n",
    "\n",
    "#### 优势函数 (Advantage Function)\n",
    "$$A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)$$\n",
    "\n",
    "**直觉**：动作$a$比平均水平好多少？正值表示优于平均，负值表示劣于平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化价值函数的概念\n",
    "def visualize_value_concepts():\n",
    "    \"\"\"可视化V(s)、Q(s,a)和A(s,a)的关系\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # V(s) - 状态价值\n",
    "    states = np.linspace(-2, 2, 100)\n",
    "    v_values = -states**2 + 4  # 模拟的V(s)\n",
    "    axes[0].plot(states, v_values, 'b-', linewidth=2)\n",
    "    axes[0].fill_between(states, v_values, alpha=0.3)\n",
    "    axes[0].set_xlabel('State s')\n",
    "    axes[0].set_ylabel('V(s)')\n",
    "    axes[0].set_title('State Value Function V(s)\\n\"这个状态有多好?\"')\n",
    "    axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Q(s,a) - 动作价值\n",
    "    actions = ['左', '不动', '右']\n",
    "    q_values = [2.5, 3.0, 3.5]  # 模拟的Q(s,a)\n",
    "    colors = ['#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "    bars = axes[1].bar(actions, q_values, color=colors, edgecolor='black')\n",
    "    axes[1].set_xlabel('Action a')\n",
    "    axes[1].set_ylabel('Q(s, a)')\n",
    "    axes[1].set_title('Action Value Function Q(s,a)\\n\"在状态s执行动作a有多好?\"')\n",
    "    axes[1].axhline(y=np.mean(q_values), color='r', linestyle='--', label=f'V(s) = {np.mean(q_values):.1f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # A(s,a) - 优势函数\n",
    "    v_s = np.mean(q_values)\n",
    "    a_values = [q - v_s for q in q_values]\n",
    "    colors_a = ['red' if a < 0 else 'green' for a in a_values]\n",
    "    axes[2].bar(actions, a_values, color=colors_a, edgecolor='black', alpha=0.7)\n",
    "    axes[2].set_xlabel('Action a')\n",
    "    axes[2].set_ylabel('A(s, a)')\n",
    "    axes[2].set_title('Advantage Function A(s,a) = Q(s,a) - V(s)\\n\"这个动作比平均好多少?\"')\n",
    "    axes[2].axhline(y=0, color='k', linestyle='-', linewidth=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_value_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Bellman方程\n",
    "\n",
    "价值函数满足递归关系，即**Bellman方程**：\n",
    "\n",
    "#### Bellman期望方程\n",
    "$$V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi}\\left[R(s, a) + \\gamma \\mathbb{E}_{s' \\sim P}[V^\\pi(s')]\\right]$$\n",
    "\n",
    "$$Q^\\pi(s, a) = R(s, a) + \\gamma \\mathbb{E}_{s' \\sim P}\\left[\\mathbb{E}_{a' \\sim \\pi}[Q^\\pi(s', a')]\\right]$$\n",
    "\n",
    "#### Bellman最优方程\n",
    "$$V^*(s) = \\max_a \\left[R(s, a) + \\gamma \\mathbb{E}_{s' \\sim P}[V^*(s')]\\right]$$\n",
    "\n",
    "$$Q^*(s, a) = R(s, a) + \\gamma \\mathbb{E}_{s' \\sim P}\\left[\\max_{a'} Q^*(s', a')\\right]$$\n",
    "\n",
    "**核心思想**：当前状态的价值 = 即时奖励 + 折扣后的未来价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 策略梯度定理\n",
    "\n",
    "对于参数化策略 $\\pi_\\theta$，目标函数对参数的梯度为：\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) A^{\\pi_\\theta}(s_t, a_t)\\right]$$\n",
    "\n",
    "**直觉解释**：\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$：增加动作$a_t$概率的方向\n",
    "- $A^{\\pi_\\theta}(s_t, a_t)$：动作$a_t$的\"好坏程度\"\n",
    "- 乘积含义：好的动作（正优势）增加概率，坏的动作（负优势）减少概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 算法分类\n",
    "\n",
    "```\n",
    "强化学习算法\n",
    "├── 基于价值 (Value-Based)\n",
    "│   ├── Q-Learning\n",
    "│   ├── DQN (及变体: Double, Dueling, Rainbow)\n",
    "│   └── 适用于离散动作空间\n",
    "│\n",
    "├── 基于策略 (Policy-Based)\n",
    "│   ├── REINFORCE\n",
    "│   ├── PPO, TRPO\n",
    "│   └── 适用于连续/离散动作空间\n",
    "│\n",
    "└── Actor-Critic (结合两者)\n",
    "    ├── A2C/A3C\n",
    "    ├── DDPG → TD3 (确定性策略)\n",
    "    └── SAC (随机策略 + 最大熵)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化算法分类\n",
    "def visualize_algorithm_taxonomy():\n",
    "    \"\"\"创建算法分类的可视化图表\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 绘制分类框\n",
    "    boxes = {\n",
    "        'Value-Based': (1, 7, 2.5, 2, '#3498db'),\n",
    "        'Policy-Based': (4, 7, 2.5, 2, '#e74c3c'),\n",
    "        'Actor-Critic': (7, 7, 2.5, 2, '#2ecc71'),\n",
    "    }\n",
    "    \n",
    "    algorithms = {\n",
    "        'Value-Based': ['DQN', 'Double DQN', 'Dueling DQN', 'Rainbow'],\n",
    "        'Policy-Based': ['REINFORCE', 'PPO', 'TRPO'],\n",
    "        'Actor-Critic': ['A2C/A3C', 'DDPG', 'TD3', 'SAC'],\n",
    "    }\n",
    "    \n",
    "    for name, (x, y, w, h, color) in boxes.items():\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=True, facecolor=color, \n",
    "                             edgecolor='black', linewidth=2, alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + w/2, y + h - 0.3, name, ha='center', va='top', \n",
    "               fontsize=12, fontweight='bold', color='white')\n",
    "        \n",
    "        # 添加算法列表\n",
    "        for i, algo in enumerate(algorithms[name]):\n",
    "            ax.text(x + w/2, y + h - 0.8 - i*0.4, algo, ha='center', \n",
    "                   fontsize=10, color='white')\n",
    "    \n",
    "    # 添加特性标注\n",
    "    ax.text(2.25, 4.5, '离散动作\\n高样本效率\\n确定性策略', ha='center', \n",
    "           fontsize=9, style='italic', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    ax.text(5.25, 4.5, '连续/离散动作\\nOn-Policy\\n高方差', ha='center', \n",
    "           fontsize=9, style='italic', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    ax.text(8.25, 4.5, '连续动作\\nOff-Policy\\n低方差', ha='center', \n",
    "           fontsize=9, style='italic', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    ax.set_title('深度强化学习算法分类', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # 添加箭头连接\n",
    "    ax.annotate('', xy=(2.25, 5.2), xytext=(2.25, 7),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "    ax.annotate('', xy=(5.25, 5.2), xytext=(5.25, 7),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "    ax.annotate('', xy=(8.25, 5.2), xytext=(8.25, 7),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_algorithm_taxonomy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DDPG: 深度确定性策略梯度\n",
    "\n",
    "### 2.1 核心思想\n",
    "\n",
    "**DDPG (Deep Deterministic Policy Gradient)** 将DQN扩展到连续动作空间：\n",
    "\n",
    "- **确定性策略**: $a = \\mu_\\theta(s)$，直接输出动作值\n",
    "- **经验回放**: 打破样本相关性\n",
    "- **目标网络**: 稳定训练目标\n",
    "- **探索噪声**: 通过添加噪声实现探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数学原理\n",
    "\n",
    "#### 确定性策略梯度定理 (Silver et al., 2014)\n",
    "\n",
    "对于确定性策略 $\\mu_\\theta: \\mathcal{S} \\rightarrow \\mathcal{A}$，梯度为：\n",
    "\n",
    "$$\\nabla_\\theta J = \\mathbb{E}_{s \\sim \\mathcal{D}}\\left[\\nabla_a Q(s, a)|_{a=\\mu_\\theta(s)} \\cdot \\nabla_\\theta \\mu_\\theta(s)\\right]$$\n",
    "\n",
    "**直觉**：沿着Q值增大的方向调整策略\n",
    "\n",
    "#### Critic更新 (TD学习)\n",
    "\n",
    "$$L(\\phi) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}}\\left[(r + \\gamma Q_{\\phi'}(s', \\mu_{\\theta'}(s')) - Q_\\phi(s, a))^2\\right]$$\n",
    "\n",
    "#### 目标网络软更新 (Polyak Averaging)\n",
    "\n",
    "$$\\theta' \\leftarrow \\tau \\theta + (1 - \\tau) \\theta'$$\n",
    "$$\\phi' \\leftarrow \\tau \\phi + (1 - \\tau) \\phi'$$\n",
    "\n",
    "其中 $\\tau \\ll 1$ (典型值: 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG架构可视化\n",
    "def visualize_ddpg_architecture():\n",
    "    \"\"\"可视化DDPG的网络架构\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Actor网络\n",
    "    actor_box = plt.Rectangle((1, 6), 3, 3, fill=True, facecolor='#3498db', \n",
    "                               edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax.add_patch(actor_box)\n",
    "    ax.text(2.5, 8.5, 'Actor μ_θ(s)', ha='center', va='center', \n",
    "           fontsize=12, fontweight='bold', color='white')\n",
    "    ax.text(2.5, 7.5, 's → a', ha='center', va='center', fontsize=11, color='white')\n",
    "    ax.text(2.5, 6.7, '确定性策略', ha='center', va='center', fontsize=10, color='white')\n",
    "    \n",
    "    # Critic网络\n",
    "    critic_box = plt.Rectangle((6, 6), 3, 3, fill=True, facecolor='#e74c3c', \n",
    "                                edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax.add_patch(critic_box)\n",
    "    ax.text(7.5, 8.5, 'Critic Q_φ(s,a)', ha='center', va='center', \n",
    "           fontsize=12, fontweight='bold', color='white')\n",
    "    ax.text(7.5, 7.5, '(s,a) → Q', ha='center', va='center', fontsize=11, color='white')\n",
    "    ax.text(7.5, 6.7, '价值估计', ha='center', va='center', fontsize=10, color='white')\n",
    "    \n",
    "    # 目标网络\n",
    "    target_actor = plt.Rectangle((1, 1), 3, 2, fill=True, facecolor='#3498db', \n",
    "                                  edgecolor='black', linewidth=2, alpha=0.4, linestyle='--')\n",
    "    ax.add_patch(target_actor)\n",
    "    ax.text(2.5, 2, \"Actor' μ_θ'(s)\", ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    target_critic = plt.Rectangle((6, 1), 3, 2, fill=True, facecolor='#e74c3c', \n",
    "                                   edgecolor='black', linewidth=2, alpha=0.4, linestyle='--')\n",
    "    ax.add_patch(target_critic)\n",
    "    ax.text(7.5, 2, \"Critic' Q_φ'(s,a)\", ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # 经验回放缓冲区\n",
    "    buffer_box = plt.Rectangle((11, 4), 2.5, 4, fill=True, facecolor='#2ecc71', \n",
    "                                edgecolor='black', linewidth=2, alpha=0.8)\n",
    "    ax.add_patch(buffer_box)\n",
    "    ax.text(12.25, 7.5, 'Replay', ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
    "    ax.text(12.25, 6.8, 'Buffer', ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
    "    ax.text(12.25, 5.8, '(s,a,r,s\\',d)', ha='center', va='center', fontsize=9, color='white')\n",
    "    ax.text(12.25, 5.0, '均匀采样', ha='center', va='center', fontsize=9, color='white')\n",
    "    \n",
    "    # 箭头\n",
    "    ax.annotate('', xy=(6, 7.5), xytext=(4, 7.5),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    ax.text(5, 7.8, 'a', ha='center', fontsize=10)\n",
    "    \n",
    "    ax.annotate('', xy=(2.5, 6), xytext=(2.5, 3),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1.5, ls='--'))\n",
    "    ax.text(3.2, 4.5, 'τ-soft update', ha='left', fontsize=9, color='gray')\n",
    "    \n",
    "    ax.annotate('', xy=(7.5, 6), xytext=(7.5, 3),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1.5, ls='--'))\n",
    "    \n",
    "    ax.annotate('', xy=(11, 6), xytext=(9, 7),\n",
    "               arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "    \n",
    "    ax.set_title('DDPG Architecture', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ddpg_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DDPG算法伪代码\n",
    "\n",
    "```\n",
    "算法: DDPG\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "初始化:\n",
    "    Actor网络 μ_θ, Critic网络 Q_φ\n",
    "    目标网络 μ_θ' ← μ_θ, Q_φ' ← Q_φ\n",
    "    经验回放缓冲区 D\n",
    "\n",
    "For each episode:\n",
    "    观察初始状态 s\n",
    "    \n",
    "    For each step:\n",
    "        1. 选择动作: a = μ_θ(s) + ε,  ε ~ N(0, σ²)  # 添加探索噪声\n",
    "        2. 执行动作，观察 (r, s', done)\n",
    "        3. 存储转移: D ← D ∪ {(s, a, r, s', done)}\n",
    "        4. 从D中采样批次 B\n",
    "        \n",
    "        5. 计算TD目标:\n",
    "           y = r + γ(1-done) · Q_φ'(s', μ_θ'(s'))\n",
    "        \n",
    "        6. 更新Critic:\n",
    "           φ ← φ - α_Q · ∇_φ (1/|B|) Σ(y - Q_φ(s,a))²\n",
    "        \n",
    "        7. 更新Actor:\n",
    "           θ ← θ + α_μ · ∇_θ (1/|B|) Σ Q_φ(s, μ_θ(s))\n",
    "        \n",
    "        8. 软更新目标网络:\n",
    "           θ' ← τθ + (1-τ)θ'\n",
    "           φ' ← τφ + (1-τ)φ'\n",
    "        \n",
    "        s ← s'\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG核心代码演示\n",
    "class DDPGDemo:\n",
    "    \"\"\"DDPG核心实现演示（简化版）\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # Actor: s -> a\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # 输出[-1, 1]\n",
    "        )\n",
    "        \n",
    "        # Critic: (s, a) -> Q\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        print(\"DDPG网络结构:\")\n",
    "        print(f\"  Actor: {state_dim} -> [256] -> [256] -> {action_dim}\")\n",
    "        print(f\"  Critic: {state_dim + action_dim} -> [256] -> [256] -> 1\")\n",
    "    \n",
    "    def select_action(self, state, noise_std=0.1):\n",
    "        \"\"\"确定性策略 + 探索噪声\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state) * self.max_action\n",
    "        \n",
    "        # 添加高斯噪声进行探索\n",
    "        noise = torch.randn_like(action) * noise_std * self.max_action\n",
    "        action = (action + noise).clamp(-self.max_action, self.max_action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def compute_critic_loss(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"Critic TD损失\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 目标动作（来自目标Actor）\n",
    "            next_actions = self.actor(next_states) * self.max_action\n",
    "            # 目标Q值\n",
    "            target_q = rewards + gamma * (1 - dones) * self.critic(\n",
    "                torch.cat([next_states, next_actions], dim=-1)\n",
    "            )\n",
    "        \n",
    "        # 当前Q值\n",
    "        current_q = self.critic(torch.cat([states, actions], dim=-1))\n",
    "        \n",
    "        # MSE损失\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        return loss\n",
    "    \n",
    "    def compute_actor_loss(self, states):\n",
    "        \"\"\"Actor策略梯度损失\"\"\"\n",
    "        # 最大化Q(s, μ(s))，即最小化-Q\n",
    "        actions = self.actor(states) * self.max_action\n",
    "        q_values = self.critic(torch.cat([states, actions], dim=-1))\n",
    "        loss = -q_values.mean()\n",
    "        return loss\n",
    "\n",
    "# 演示\n",
    "demo = DDPGDemo(state_dim=4, action_dim=2, max_action=1.0)\n",
    "\n",
    "# 模拟输入\n",
    "batch_size = 32\n",
    "states = torch.randn(batch_size, 4)\n",
    "actions = torch.randn(batch_size, 2).clamp(-1, 1)\n",
    "rewards = torch.randn(batch_size, 1)\n",
    "next_states = torch.randn(batch_size, 4)\n",
    "dones = torch.zeros(batch_size, 1)\n",
    "\n",
    "# 计算损失\n",
    "critic_loss = demo.compute_critic_loss(states, actions, rewards, next_states, dones)\n",
    "actor_loss = demo.compute_actor_loss(states)\n",
    "\n",
    "print(f\"\\nCritic Loss: {critic_loss.item():.4f}\")\n",
    "print(f\"Actor Loss: {actor_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 DDPG的问题与局限\n",
    "\n",
    "| 问题 | 原因 | 影响 |\n",
    "|------|------|------|\n",
    "| **Q值过估计** | $\\max$ 操作引入正向偏差 | 策略次优 |\n",
    "| **训练不稳定** | Critic误差传递给Actor | 性能震荡 |\n",
    "| **超参数敏感** | 噪声、学习率耦合 | 调参困难 |\n",
    "| **探索不足** | 确定性策略 + 简单噪声 | 局部最优 |\n",
    "\n",
    "这些问题促使了TD3和SAC的发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. TD3: 双延迟深度确定性策略梯度\n",
    "\n",
    "### 3.1 核心思想\n",
    "\n",
    "**TD3 (Twin Delayed DDPG)** 通过三个关键技术解决DDPG的问题：\n",
    "\n",
    "1. **Clipped Double Q-learning**: 使用双Q网络，取最小值减少过估计\n",
    "2. **Delayed Policy Updates**: 延迟Actor更新，让Critic先稳定\n",
    "3. **Target Policy Smoothing**: 为目标动作添加噪声，正则化Q函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 数学原理\n",
    "\n",
    "#### 1. Clipped Double Q-learning\n",
    "\n",
    "维护两个独立的Q网络 $Q_{\\phi_1}$ 和 $Q_{\\phi_2}$：\n",
    "\n",
    "$$y = r + \\gamma \\min_{i=1,2} Q_{\\phi_i'}(s', \\tilde{a}')$$\n",
    "\n",
    "**为什么取最小值？**\n",
    "- 过估计源于 $\\mathbb{E}[\\max] \\geq \\max \\mathbb{E}$\n",
    "- 取最小值提供悲观估计，抵消过估计偏差\n",
    "\n",
    "#### 2. Delayed Policy Updates\n",
    "\n",
    "每更新D次Critic，才更新一次Actor：\n",
    "\n",
    "```python\n",
    "if update_step % policy_delay == 0:\n",
    "    update_actor()\n",
    "    soft_update_targets()\n",
    "```\n",
    "\n",
    "**为什么延迟？**\n",
    "- Critic误差会传递给Actor\n",
    "- 让Critic先收敛到较准确的估计\n",
    "- 典型值: D = 2\n",
    "\n",
    "#### 3. Target Policy Smoothing\n",
    "\n",
    "$$\\tilde{a}' = \\mu_{\\theta'}(s') + \\text{clip}(\\epsilon, -c, c), \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "**为什么添加噪声？**\n",
    "- Q函数可能有尖锐的峰值（过拟合到某些动作）\n",
    "- 噪声使Q学习平滑的价值函数\n",
    "- 类似正则化效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化TD3的三个改进\n",
    "def visualize_td3_improvements():\n",
    "    \"\"\"可视化TD3相比DDPG的三个关键改进\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 1. Clipped Double Q\n",
    "    ax1 = axes[0]\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    q1 = 5 + np.sin(x) + np.random.randn(100) * 0.3\n",
    "    q2 = 5 + np.sin(x) + np.random.randn(100) * 0.3 + 0.5\n",
    "    q_min = np.minimum(q1, q2)\n",
    "    \n",
    "    ax1.plot(x, q1, 'b-', alpha=0.5, label='Q₁')\n",
    "    ax1.plot(x, q2, 'r-', alpha=0.5, label='Q₂')\n",
    "    ax1.plot(x, q_min, 'g-', linewidth=2, label='min(Q₁, Q₂)')\n",
    "    ax1.axhline(y=5, color='k', linestyle='--', alpha=0.3, label='True Q')\n",
    "    ax1.set_xlabel('State-Action')\n",
    "    ax1.set_ylabel('Q Value')\n",
    "    ax1.set_title('1. Clipped Double Q-learning\\n减少过估计偏差')\n",
    "    ax1.legend(fontsize=8)\n",
    "    \n",
    "    # 2. Delayed Policy Updates\n",
    "    ax2 = axes[1]\n",
    "    steps = np.arange(0, 20)\n",
    "    critic_updates = np.ones(20)\n",
    "    actor_updates = np.zeros(20)\n",
    "    actor_updates[::2] = 1  # 每2步更新一次\n",
    "    \n",
    "    width = 0.35\n",
    "    ax2.bar(steps - width/2, critic_updates, width, label='Critic Updates', color='#e74c3c', alpha=0.7)\n",
    "    ax2.bar(steps + width/2, actor_updates, width, label='Actor Updates', color='#3498db', alpha=0.7)\n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Update')\n",
    "    ax2.set_title('2. Delayed Policy Updates\\nActor每D步更新一次')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(-1, 20)\n",
    "    \n",
    "    # 3. Target Policy Smoothing\n",
    "    ax3 = axes[2]\n",
    "    a_mean = 0.5\n",
    "    a_samples = np.random.randn(500) * 0.2 + a_mean\n",
    "    a_clipped = np.clip(a_samples, a_mean - 0.5, a_mean + 0.5)\n",
    "    \n",
    "    ax3.hist(a_clipped, bins=30, density=True, alpha=0.7, color='#2ecc71', edgecolor='black')\n",
    "    ax3.axvline(x=a_mean, color='r', linestyle='--', linewidth=2, label=f'μ(s\\') = {a_mean}')\n",
    "    ax3.axvline(x=a_mean - 0.5, color='k', linestyle=':', label='Clip bounds')\n",
    "    ax3.axvline(x=a_mean + 0.5, color='k', linestyle=':')\n",
    "    ax3.set_xlabel('Target Action')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('3. Target Policy Smoothing\\nã\\' = μ\\'(s\\') + clip(ε, -c, c)')\n",
    "    ax3.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_td3_improvements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TD3算法伪代码\n",
    "\n",
    "```\n",
    "算法: TD3\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "初始化:\n",
    "    Actor μ_θ, Twin Critics Q_φ₁, Q_φ₂\n",
    "    目标网络 θ' ← θ, φ'₁ ← φ₁, φ'₂ ← φ₂\n",
    "    回放缓冲区 D\n",
    "\n",
    "For each timestep t:\n",
    "    # 探索\n",
    "    a = μ_θ(s) + ε,  ε ~ N(0, σ²)\n",
    "    执行a，观察(r, s', done)\n",
    "    D ← D ∪ {(s, a, r, s', done)}\n",
    "    \n",
    "    # 学习\n",
    "    采样批次 B ~ D\n",
    "    \n",
    "    # Target Policy Smoothing\n",
    "    ã' = μ_θ'(s') + clip(ε, -c, c),  ε ~ N(0, σ_target²)\n",
    "    \n",
    "    # Clipped Double Q-learning\n",
    "    y = r + γ(1-done) · min(Q_φ'₁(s', ã'), Q_φ'₂(s', ã'))\n",
    "    \n",
    "    # 更新Critics\n",
    "    φᵢ ← minimize (1/|B|) Σ(y - Q_φᵢ(s, a))²,  i=1,2\n",
    "    \n",
    "    # Delayed Policy Updates\n",
    "    if t mod d == 0:\n",
    "        # 更新Actor (只用Q₁)\n",
    "        θ ← maximize (1/|B|) Σ Q_φ₁(s, μ_θ(s))\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        θ' ← τθ + (1-τ)θ'\n",
    "        φ'ᵢ ← τφᵢ + (1-τ)φ'ᵢ,  i=1,2\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD3核心代码演示\n",
    "class TD3Demo:\n",
    "    \"\"\"TD3核心实现演示\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.max_action = max_action\n",
    "        self.policy_delay = 2\n",
    "        self.policy_noise = 0.2\n",
    "        self.noise_clip = 0.5\n",
    "        self.update_count = 0\n",
    "        \n",
    "        # Actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, action_dim), nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Twin Critics\n",
    "        self.critic1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.critic2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        print(\"TD3网络结构:\")\n",
    "        print(f\"  Actor: {state_dim} -> [256] -> [256] -> {action_dim}\")\n",
    "        print(f\"  Critic1 & Critic2: {state_dim + action_dim} -> [256] -> [256] -> 1\")\n",
    "        print(f\"  Policy Delay: {self.policy_delay}\")\n",
    "    \n",
    "    def compute_target_actions(self, next_states):\n",
    "        \"\"\"带噪声的目标动作（Target Policy Smoothing）\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 目标动作\n",
    "            next_actions = self.actor(next_states) * self.max_action\n",
    "            \n",
    "            # 添加裁剪噪声\n",
    "            noise = torch.randn_like(next_actions) * self.policy_noise\n",
    "            noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            # 裁剪到动作空间\n",
    "            next_actions = (next_actions + noise).clamp(-self.max_action, self.max_action)\n",
    "        \n",
    "        return next_actions\n",
    "    \n",
    "    def compute_critic_loss(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"Clipped Double Q-learning损失\"\"\"\n",
    "        # 目标动作（带噪声）\n",
    "        next_actions = self.compute_target_actions(next_states)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 两个目标Q值\n",
    "            target_q1 = self.critic1(torch.cat([next_states, next_actions], dim=-1))\n",
    "            target_q2 = self.critic2(torch.cat([next_states, next_actions], dim=-1))\n",
    "            \n",
    "            # 取最小值 (Clipped Double Q)\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "            target_q = rewards + gamma * (1 - dones) * target_q\n",
    "        \n",
    "        # 当前Q值\n",
    "        current_q1 = self.critic1(torch.cat([states, actions], dim=-1))\n",
    "        current_q2 = self.critic2(torch.cat([states, actions], dim=-1))\n",
    "        \n",
    "        # 两个Critic的损失\n",
    "        loss1 = F.mse_loss(current_q1, target_q)\n",
    "        loss2 = F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        return loss1 + loss2, current_q1.mean(), current_q2.mean()\n",
    "    \n",
    "    def should_update_actor(self):\n",
    "        \"\"\"检查是否应该更新Actor（Delayed Update）\"\"\"\n",
    "        self.update_count += 1\n",
    "        return self.update_count % self.policy_delay == 0\n",
    "\n",
    "# 演示\n",
    "td3_demo = TD3Demo(state_dim=4, action_dim=2, max_action=1.0)\n",
    "\n",
    "# 模拟训练\n",
    "batch_size = 32\n",
    "states = torch.randn(batch_size, 4)\n",
    "actions = torch.randn(batch_size, 2).clamp(-1, 1)\n",
    "rewards = torch.randn(batch_size, 1)\n",
    "next_states = torch.randn(batch_size, 4)\n",
    "dones = torch.zeros(batch_size, 1)\n",
    "\n",
    "critic_loss, q1_mean, q2_mean = td3_demo.compute_critic_loss(\n",
    "    states, actions, rewards, next_states, dones\n",
    ")\n",
    "\n",
    "print(f\"\\nCritic Loss: {critic_loss.item():.4f}\")\n",
    "print(f\"Q1 Mean: {q1_mean.item():.4f}, Q2 Mean: {q2_mean.item():.4f}\")\n",
    "print(f\"Should update actor: {td3_demo.should_update_actor()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. SAC: 软演员-评论家\n",
    "\n",
    "### 4.1 核心思想\n",
    "\n",
    "**SAC (Soft Actor-Critic)** 引入最大熵框架：\n",
    "\n",
    "$$J(\\pi) = \\sum_t \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi}\\left[r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))\\right]$$\n",
    "\n",
    "**关键创新**：\n",
    "- 最大化奖励的同时最大化策略熵\n",
    "- 随机策略提供内在探索\n",
    "- 自动调节温度参数 $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 最大熵强化学习\n",
    "\n",
    "#### 为什么要最大化熵？\n",
    "\n",
    "1. **探索**: 高熵策略自然探索更多动作\n",
    "2. **鲁棒性**: 不过度依赖单一动作序列\n",
    "3. **多模态**: 可以学习多种解决方案\n",
    "\n",
    "#### 软价值函数\n",
    "\n",
    "**软V函数**:\n",
    "$$V(s) = \\mathbb{E}_{a \\sim \\pi}\\left[Q(s, a) - \\alpha \\log \\pi(a|s)\\right]$$\n",
    "\n",
    "**软Q函数**:\n",
    "$$Q(s, a) = r + \\gamma \\mathbb{E}_{s'}\\left[V(s')\\right]$$\n",
    "\n",
    "#### 自动温度调节\n",
    "\n",
    "$$J(\\alpha) = \\mathbb{E}_{a \\sim \\pi}\\left[-\\alpha(\\log \\pi(a|s) + \\bar{\\mathcal{H}})\\right]$$\n",
    "\n",
    "其中 $\\bar{\\mathcal{H}}$ 是目标熵（通常设为 $-\\dim(\\mathcal{A})$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化最大熵的效果\n",
    "def visualize_entropy_effect():\n",
    "    \"\"\"可视化熵正则化对策略的影响\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    # 低熵策略（确定性）\n",
    "    ax1 = axes[0]\n",
    "    low_entropy = np.exp(-(x - 0.5)**2 / 0.1)\n",
    "    low_entropy /= low_entropy.sum() * (x[1] - x[0])\n",
    "    ax1.fill_between(x, low_entropy, alpha=0.7, color='#e74c3c')\n",
    "    ax1.set_xlabel('Action')\n",
    "    ax1.set_ylabel('π(a|s)')\n",
    "    ax1.set_title('低熵策略 (α → 0)\\n确定性，集中在单一动作')\n",
    "    ax1.set_ylim(0, 3)\n",
    "    \n",
    "    # 中等熵策略\n",
    "    ax2 = axes[1]\n",
    "    mid_entropy = np.exp(-(x - 0.5)**2 / 1.0)\n",
    "    mid_entropy /= mid_entropy.sum() * (x[1] - x[0])\n",
    "    ax2.fill_between(x, mid_entropy, alpha=0.7, color='#2ecc71')\n",
    "    ax2.set_xlabel('Action')\n",
    "    ax2.set_ylabel('π(a|s)')\n",
    "    ax2.set_title('中等熵策略 (合适的α)\\n平衡探索与利用')\n",
    "    ax2.set_ylim(0, 0.8)\n",
    "    \n",
    "    # 高熵策略（均匀）\n",
    "    ax3 = axes[2]\n",
    "    high_entropy = np.ones_like(x) / 6  # 近似均匀分布\n",
    "    ax3.fill_between(x, high_entropy, alpha=0.7, color='#3498db')\n",
    "    ax3.set_xlabel('Action')\n",
    "    ax3.set_ylabel('π(a|s)')\n",
    "    ax3.set_title('高熵策略 (α → ∞)\\n随机，接近均匀分布')\n",
    "    ax3.set_ylim(0, 0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_entropy_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SAC的高斯策略\n",
    "\n",
    "SAC使用**可学习的高斯策略**：\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s)^2)$$\n",
    "\n",
    "对于有界动作空间，使用**tanh压缩**：\n",
    "\n",
    "$$u \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad a = \\tanh(u)$$\n",
    "\n",
    "**对数概率（考虑变量变换）**：\n",
    "\n",
    "$$\\log \\pi(a|s) = \\log \\mathcal{N}(u|\\mu, \\sigma^2) - \\sum_i \\log(1 - \\tanh^2(u_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC高斯策略演示\n",
    "class SACPolicyDemo(nn.Module):\n",
    "    \"\"\"SAC的可重参数化高斯策略\"\"\"\n",
    "    \n",
    "    LOG_STD_MIN = -20\n",
    "    LOG_STD_MAX = 2\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 共享特征提取\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 均值和对数标准差\n",
    "        self.mean_layer = nn.Linear(256, action_dim)\n",
    "        self.log_std_layer = nn.Linear(256, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"输出高斯分布参数\"\"\"\n",
    "        features = self.shared(state)\n",
    "        mean = self.mean_layer(features)\n",
    "        log_std = self.log_std_layer(features)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        return mean, log_std.exp()\n",
    "    \n",
    "    def sample(self, state):\n",
    "        \"\"\"使用重参数化技巧采样\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        # 重参数化: u = mean + std * epsilon\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        u = normal.rsample()  # 可微分的采样\n",
    "        \n",
    "        # Tanh压缩到[-1, 1]\n",
    "        action = torch.tanh(u)\n",
    "        \n",
    "        # 计算对数概率（带Jacobian校正）\n",
    "        log_prob = normal.log_prob(u).sum(dim=-1)\n",
    "        # Jacobian: d(tanh(u))/du = 1 - tanh²(u)\n",
    "        log_prob -= (2 * (np.log(2) - u - F.softplus(-2 * u))).sum(dim=-1)\n",
    "        \n",
    "        return action, log_prob, mean\n",
    "\n",
    "# 演示\n",
    "policy = SACPolicyDemo(state_dim=4, action_dim=2)\n",
    "state = torch.randn(5, 4)  # 5个样本\n",
    "\n",
    "action, log_prob, mean = policy.sample(state)\n",
    "\n",
    "print(\"SAC高斯策略采样:\")\n",
    "print(f\"  State shape: {state.shape}\")\n",
    "print(f\"  Action shape: {action.shape}\")\n",
    "print(f\"  Log prob shape: {log_prob.shape}\")\n",
    "print(f\"\\n  Sample actions:\\n{action}\")\n",
    "print(f\"\\n  Log probabilities: {log_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 SAC算法伪代码\n",
    "\n",
    "```\n",
    "算法: SAC (带自动温度调节)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "初始化:\n",
    "    Actor π_θ (高斯策略)\n",
    "    Twin Critics Q_φ₁, Q_φ₂\n",
    "    目标Critics Q_φ'₁, Q_φ'₂\n",
    "    温度 α (可学习)\n",
    "    目标熵 H̄ = -dim(A)\n",
    "\n",
    "For each timestep:\n",
    "    # 从策略采样动作\n",
    "    a ~ π_θ(·|s)\n",
    "    执行a，观察(r, s', done)\n",
    "    D ← D ∪ {(s, a, r, s', done)}\n",
    "    \n",
    "    # 采样批次\n",
    "    B ~ D\n",
    "    \n",
    "    # 计算目标 (Soft Bellman)\n",
    "    a' ~ π_θ(·|s')\n",
    "    y = r + γ(1-done) · (min(Q_φ'₁(s',a'), Q_φ'₂(s',a')) - α log π_θ(a'|s'))\n",
    "    \n",
    "    # 更新Critics\n",
    "    φᵢ ← minimize (1/|B|) Σ(y - Q_φᵢ(s,a))²,  i=1,2\n",
    "    \n",
    "    # 更新Actor\n",
    "    a_new ~ π_θ(·|s)\n",
    "    θ ← maximize (1/|B|) Σ[min(Q_φ₁(s,a_new), Q_φ₂(s,a_new)) - α log π_θ(a_new|s)]\n",
    "    \n",
    "    # 更新温度 (自动调节)\n",
    "    α ← minimize (1/|B|) Σ[-α(log π_θ(a_new|s) + H̄)]\n",
    "    \n",
    "    # 软更新目标Critics\n",
    "    φ'ᵢ ← τφᵢ + (1-τ)φ'ᵢ,  i=1,2\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 算法对比与实验\n",
    "\n",
    "### 5.1 算法特性对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建算法对比表格\n",
    "comparison_data = {\n",
    "    '特性': ['策略类型', '动作空间', 'On/Off Policy', '样本效率', \n",
    "             '训练稳定性', '探索机制', '超参敏感度', '核心贡献'],\n",
    "    'DDPG': ['确定性', '连续', 'Off-policy', '高', \n",
    "             '低', '添加噪声', '高', 'DPG + 神经网络'],\n",
    "    'TD3': ['确定性', '连续', 'Off-policy', '高', \n",
    "            '中', '添加噪声', '中', 'Twin Q + Delayed + Smoothing'],\n",
    "    'SAC': ['随机', '连续', 'Off-policy', '高', \n",
    "            '高', '熵正则化', '低', '最大熵 + 自动温度']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df.set_index('特性', inplace=True)\n",
    "\n",
    "# 显示表格\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟训练曲线对比\n",
    "def simulate_training_curves():\n",
    "    \"\"\"模拟三种算法的训练曲线\"\"\"\n",
    "    np.random.seed(42)\n",
    "    timesteps = np.arange(0, 100000, 1000)\n",
    "    \n",
    "    # DDPG: 不稳定，震荡大\n",
    "    ddpg_base = -200 + 180 * (1 - np.exp(-timesteps / 30000))\n",
    "    ddpg_noise = np.random.randn(len(timesteps)) * 30\n",
    "    ddpg_returns = ddpg_base + ddpg_noise\n",
    "    ddpg_returns[50:60] -= 80  # 模拟崩溃\n",
    "    \n",
    "    # TD3: 更稳定，但可能收敛较慢\n",
    "    td3_base = -200 + 190 * (1 - np.exp(-timesteps / 35000))\n",
    "    td3_noise = np.random.randn(len(timesteps)) * 15\n",
    "    td3_returns = td3_base + td3_noise\n",
    "    \n",
    "    # SAC: 最稳定，收敛最快\n",
    "    sac_base = -200 + 200 * (1 - np.exp(-timesteps / 25000))\n",
    "    sac_noise = np.random.randn(len(timesteps)) * 10\n",
    "    sac_returns = sac_base + sac_noise\n",
    "    \n",
    "    # 绘图\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(timesteps, ddpg_returns, 'b-', alpha=0.3)\n",
    "    ax.plot(timesteps, pd.Series(ddpg_returns).rolling(10).mean(), 'b-', \n",
    "           linewidth=2, label='DDPG')\n",
    "    \n",
    "    ax.plot(timesteps, td3_returns, 'orange', alpha=0.3)\n",
    "    ax.plot(timesteps, pd.Series(td3_returns).rolling(10).mean(), 'orange', \n",
    "           linewidth=2, label='TD3')\n",
    "    \n",
    "    ax.plot(timesteps, sac_returns, 'g-', alpha=0.3)\n",
    "    ax.plot(timesteps, pd.Series(sac_returns).rolling(10).mean(), 'g-', \n",
    "           linewidth=2, label='SAC')\n",
    "    \n",
    "    ax.set_xlabel('Timesteps', fontsize=12)\n",
    "    ax.set_ylabel('Episode Return', fontsize=12)\n",
    "    ax.set_title('算法训练曲线对比 (Pendulum-v1 模拟)', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=-200, color='k', linestyle='--', alpha=0.3, label='Random')\n",
    "    \n",
    "    # 标注\n",
    "    ax.annotate('DDPG崩溃', xy=(55000, -100), xytext=(60000, -50),\n",
    "               arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "               fontsize=10, color='blue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "simulate_training_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 实际训练演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际训练演示（如果gymnasium可用）\n",
    "if HAS_GYM:\n",
    "    print(\"开始实际训练演示...\")\n",
    "    print(\"注意: 完整训练需要较长时间，这里只演示少量步数\")\n",
    "    \n",
    "    # 创建环境\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    print(f\"\\n环境: Pendulum-v1\")\n",
    "    print(f\"状态维度: {state_dim}\")\n",
    "    print(f\"动作维度: {action_dim}\")\n",
    "    print(f\"动作范围: [-{max_action}, {max_action}]\")\n",
    "    \n",
    "    # 创建SAC智能体\n",
    "    config = SACConfig(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        max_action=max_action,\n",
    "        batch_size=64,\n",
    "    )\n",
    "    agent = SACAgent(config)\n",
    "    \n",
    "    # 快速训练演示\n",
    "    num_episodes = 5\n",
    "    episode_returns = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset(seed=42 + ep)\n",
    "        episode_return = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 200:\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_returns.append(episode_return)\n",
    "        print(f\"Episode {ep + 1}: Return = {episode_return:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"\\n平均回报: {np.mean(episode_returns):.2f}\")\n",
    "else:\n",
    "    print(\"gymnasium未安装，跳过实际训练演示\")\n",
    "    print(\"安装: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 最佳实践与调参指南\n",
    "\n",
    "### 6.1 超参数推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数推荐表\n",
    "hyperparams = {\n",
    "    '超参数': ['学习率 (Actor)', '学习率 (Critic)', 'Batch Size', \n",
    "              'Buffer Size', 'γ (Discount)', 'τ (Soft Update)',\n",
    "              '探索噪声', 'Hidden Dims'],\n",
    "    'DDPG': ['1e-4', '1e-3', '64-256', '1M', '0.99', '0.005',\n",
    "             '0.1 (Gaussian)', '[256, 256]'],\n",
    "    'TD3': ['3e-4', '3e-4', '256', '1M', '0.99', '0.005',\n",
    "            '0.1 (Gaussian)', '[256, 256]'],\n",
    "    'SAC': ['3e-4', '3e-4', '256', '1M', '0.99', '0.005',\n",
    "            '自动 (熵)', '[256, 256]']\n",
    "}\n",
    "\n",
    "df_hyper = pd.DataFrame(hyperparams)\n",
    "df_hyper.set_index('超参数', inplace=True)\n",
    "display(HTML(df_hyper.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 调参建议\n",
    "\n",
    "#### 训练不稳定时\n",
    "1. **降低学习率**: 特别是Actor的学习率\n",
    "2. **增加τ**: 让目标网络更新更慢\n",
    "3. **增加Batch Size**: 减少梯度方差\n",
    "4. **梯度裁剪**: 防止梯度爆炸\n",
    "\n",
    "#### 收敛太慢时\n",
    "1. **增加学习率**: 但要注意稳定性\n",
    "2. **增加网络容量**: 更宽/更深的网络\n",
    "3. **检查奖励设计**: 是否稀疏？是否scale合适？\n",
    "\n",
    "#### 探索不足时\n",
    "1. **DDPG/TD3**: 增加探索噪声σ\n",
    "2. **SAC**: 检查α是否太小，或增加target_entropy\n",
    "3. **增加初始随机步数**: 收集更多样化的经验\n",
    "\n",
    "### 6.3 算法选择指南\n",
    "\n",
    "```\n",
    "选择哪个算法?\n",
    "│\n",
    "├─ 需要确定性策略吗?\n",
    "│  ├─ 是 → TD3 (比DDPG更稳定)\n",
    "│  └─ 否 → SAC (通常首选)\n",
    "│\n",
    "├─ 样本效率重要吗?\n",
    "│  ├─ 是 → SAC 或 TD3 (Off-policy)\n",
    "│  └─ 否 → 可考虑PPO (更简单)\n",
    "│\n",
    "└─ 需要多模态策略吗?\n",
    "   ├─ 是 → SAC (随机策略)\n",
    "   └─ 否 → TD3 也可以\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "本notebook详细介绍了三种流行的深度强化学习算法：\n",
    "\n",
    "| 算法 | 核心创新 | 最佳场景 |\n",
    "|------|----------|----------|\n",
    "| **DDPG** | DPG + 神经网络 | 历史意义，现已被超越 |\n",
    "| **TD3** | Twin Q + Delayed + Smoothing | 需要确定性策略时 |\n",
    "| **SAC** | 最大熵 + 自动温度 | 通用首选，最稳定 |\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **理解问题**: 所有算法都在解决连续动作空间的策略学习\n",
    "2. **稳定性很重要**: 从DDPG到TD3到SAC，稳定性逐步提升\n",
    "3. **探索是关键**: SAC的熵正则化提供了原则性的探索机制\n",
    "4. **实践出真知**: 理论之外，大量实验调参是必要的\n",
    "\n",
    "### 进一步学习\n",
    "\n",
    "- **论文**: 阅读原始论文理解更多细节\n",
    "- **代码**: 运行`popular_rl_algorithms.py`进行实验\n",
    "- **扩展**: 探索更高级的算法如PPO、D4PG、MPO等"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
