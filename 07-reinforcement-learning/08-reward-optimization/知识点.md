# 学习优化奖励 (Reward Optimization) 核心知识点

> 本文档是强化学习奖励优化技术的系统性总结，涵盖理论基础、核心算法、数学推导与工程实践。

---

## 目录

1. [概述：稀疏奖励问题](#1-概述稀疏奖励问题)
2. [势能基奖励塑形 (PBRS)](#2-势能基奖励塑形-pbrs)
3. [逆强化学习 (IRL)](#3-逆强化学习-irl)
4. [好奇心驱动探索](#4-好奇心驱动探索)
5. [后见经验回放 (HER)](#5-后见经验回放-her)
6. [方法对比与选择指南](#6-方法对比与选择指南)
7. [面试高频问题](#7-面试高频问题)

---

## 1. 概述：稀疏奖励问题

### 1.1 问题定义

**稀疏奖励环境**的特点：
- 大部分状态转移的奖励为0
- 只有达成目标时才有非零奖励
- 例如：机器人抓取（抓到=+1，否则=0）

### 1.2 为什么稀疏奖励困难？

```
信用分配问题 (Credit Assignment Problem)
├── 时间信用分配：哪个时间步的动作导致了成功？
│   └── 例：100步后到达目标，哪一步最关键？
└── 结构信用分配：哪个状态特征与成功相关？
    └── 例：位置、速度、角度中哪个最重要？
```

**数学直觉**：
- 随机策略成功概率：$P(\text{success}) \approx (\frac{1}{|A|})^T$
- 状态空间越大，随机探索越低效

### 1.3 解决方案分类

| 方法 | 核心思想 | 是否改变最优策略 |
|------|----------|------------------|
| 奖励塑形 | 添加辅助奖励信号 | PBRS不改变 |
| 逆强化学习 | 从演示学习奖励函数 | 学习真实奖励 |
| 好奇心探索 | 生成内在动机奖励 | 可能改变 |
| HER | 重标注目标生成成功经验 | 不改变 |

---

## 2. 势能基奖励塑形 (PBRS)

### 2.1 核心定理

**Ng-Harada-Russell定理 (1999)**：

给定MDP $M = (S, A, P, R, \gamma)$，塑形后的MDP $M'$ 使用：

$$R'(s, a, s') = R(s, a, s') + F(s, a, s')$$

**定理**：$M$ 和 $M'$ 具有相同最优策略 **当且仅当**：

$$\boxed{F(s, a, s') = \gamma \Phi(s') - \Phi(s)}$$

其中 $\Phi: S \rightarrow \mathbb{R}$ 是有界势能函数。

### 2.2 证明直觉

考虑轨迹 $\tau = (s_0, a_0, s_1, a_1, ..., s_T)$：

$$\sum_{t=0}^{T-1} \gamma^t F(s_t, a_t, s_{t+1}) = \sum_{t=0}^{T-1} \gamma^t [\gamma\Phi(s_{t+1}) - \Phi(s_t)]$$

**望远镜求和**（相邻项相消）：

$$= \gamma^T \Phi(s_T) - \Phi(s_0)$$

由于 $|\Phi|$ 有界，当 $T \to \infty$ 时，$\gamma^T \Phi(s_T) \to 0$。

因此，塑形奖励的累积有界，不影响最优策略。

### 2.3 常用势能函数

#### 距离基势能
$$\Phi(s) = -\alpha \cdot \|s_{pos} - g\|_p$$

- 直觉：越靠近目标，势能越高
- 优点：简单，可解释
- 缺点：迷宫环境可能导致局部最优

#### 子目标势能
$$\Phi(s) = \sum_{i} w_i \cdot \mathbb{1}[\text{reached}(s, g_i)] - d(s, g_{next})$$

- 适用：长horizon任务
- 需要：领域知识定义子目标序列

#### 学习势能
$$\Phi_\theta(s) \approx V^*(s)$$

- 理想情况：势能 = 最优值函数
- 可从专家演示或预训练学习

### 2.4 自适应权重衰减

初期强塑形，后期弱塑形：

$$\lambda_t = \max(\lambda_{min}, \lambda_0 \cdot \alpha^t)$$

**动机**：
- 初期：提供强引导，加速学习
- 后期：减弱影响，确保收敛到真实最优

### 2.5 实践要点

```python
# 正确使用PBRS
class PBRSAgent:
    def compute_shaped_reward(self, s, a, s_next, r, done):
        if done:
            phi_next = 0  # 终止状态势能为0
        else:
            phi_next = self.potential(s_next)

        phi_s = self.potential(s)
        shaping = self.gamma * phi_next - phi_s

        return r + self.shaping_weight * shaping
```

⚠️ **常见错误**：
- 忘记在终止状态设置 $\Phi(s_{terminal}) = 0$
- 使用非PBRS形式的塑形函数（会改变最优策略）

---

## 3. 逆强化学习 (IRL)

### 3.1 问题定义

**输入**：MDP\R（不含奖励）+ 专家演示 $D = \{\tau_1, ..., \tau_n\}$

**输出**：奖励函数 $R$ 使专家行为近似最优

### 3.2 IRL的模糊性

**关键洞察**：存在无穷多个奖励函数与同一策略一致！

- $R(s) = 0$：所有策略都最优
- $R(s) = c$：同上
- 任何使专家最优的R

**解决方案**：添加额外约束

### 3.3 最大边际IRL

**目标**：找到使专家与其他策略差距最大的奖励

$$\max_\theta \min_\pi [\theta^T (\mu_E - \mu_\pi)]$$

约束：$\|\theta\|_2 \leq 1$

**几何解释**：在特征期望空间找最优分离超平面

### 3.4 最大熵IRL

**建模假设**：专家行为服从Boltzmann分布

$$P(\tau | \theta) = \frac{1}{Z(\theta)} \exp\left(\sum_t R_\theta(s_t, a_t)\right)$$

**优化目标**：最大化演示数据的对数似然

$$\max_\theta \sum_{\tau \in D} \log P(\tau | \theta)$$

**梯度**（简洁形式）：

$$\nabla_\theta \mathcal{L} = \mu_E - \mu_\theta$$

- $\mu_E$：专家特征期望
- $\mu_\theta$：当前奖励下策略的特征期望

### 3.5 GAIL (生成对抗模仿学习)

**核心思想**：不显式恢复奖励，直接匹配行为分布

$$\min_\pi \max_D \mathbb{E}_{\pi_E}[\log D(s,a)] + \mathbb{E}_\pi[\log(1-D(s,a))]$$

**隐式奖励**：
$$r(s,a) = -\log(1 - D(s,a))$$

**优势**：
- 不需要解决MDP内循环
- 可处理高维连续动作

### 3.6 AIRL (可迁移奖励)

**判别器结构**：

$$D(s,a,s') = \frac{\exp(f(s,a,s'))}{\exp(f(s,a,s')) + \pi(a|s)}$$

其中：
$$f(s,a,s') = g(s,a) + \gamma h(s') - h(s)$$

- $g(s,a)$：可迁移的奖励函数
- $h(s)$：吸收动力学的塑形项

---

## 4. 好奇心驱动探索

### 4.1 核心思想

$$r_{total} = r_{extrinsic} + \beta \cdot r_{intrinsic}$$

**内在奖励**：预测误差 → 新颖性 → 探索动机

### 4.2 ICM (内在好奇心模块)

**架构**：
```
观测 s → [特征编码器] → f(s)
         ↓
[前向模型] g(f(s), a) → f̂(s')
         ↓
内在奖励 = η · ||f(s') - f̂(s')||²
```

**逆向模型**（辅助任务）：
$$\hat{a} = h(f(s), f(s'))$$

**作用**：强制编码器学习与动作相关的特征

### 4.3 RND (随机网络蒸馏)

**架构**：
- 目标网络 $f_{rand}$：固定随机初始化
- 预测网络 $\hat{f}$：训练匹配目标

$$r_i(s) = \|f_{rand}(s) - \hat{f}(s)\|^2$$

**优势**：
- 更简单，无需逆向模型
- 对随机噪声更鲁棒

**缺点**：
- 可能奖励无意义的随机性（如电视雪花）

### 4.4 基于计数的探索

$$r_i(s) = \frac{\beta}{\sqrt{N(s)}}$$

- $N(s)$：状态访问次数
- 对连续空间需要离散化或伪计数

### 4.5 方法对比

| 方法 | 内在信号 | 计算开销 | 随机环境鲁棒性 |
|------|----------|----------|----------------|
| ICM | 前向预测误差 | O(网络) | 中等 |
| RND | 随机目标预测 | O(网络) | 低 |
| 计数 | 访问次数倒数 | O(1) | 高 |
| 集成 | 模型不一致性 | O(K×网络) | 高 |

---

## 5. 后见经验回放 (HER)

### 5.1 核心思想

**问题**：目标条件任务中，失败轨迹包含有用信息

**解决**：将失败轨迹重标注为成功（换个目标看）

### 5.2 数学形式

原始转移：$(s, a, g, r, s')$，其中 $r = 0$（失败）

重标注转移：$(s, a, g', r', s')$，其中：
- $g' = \text{achieved\_goal}(s')$
- $r' = R(s', g') = 1$（成功！）

### 5.3 目标选择策略

| 策略 | 描述 | 适用场景 |
|------|------|----------|
| final | 使用轨迹最终达成的目标 | 简单任务 |
| future | 从未来达成的目标中采样 | **最常用** |
| episode | 从整个轨迹的达成目标采样 | 覆盖更广 |
| random | 从所有历史达成目标采样 | 多样性最高 |

### 5.4 算法流程

```python
def her_store_episode(episode, k=4):
    for t, transition in enumerate(episode):
        # 1. 存储原始转移
        buffer.add(transition)

        # 2. 采样k个后见目标
        hindsight_goals = sample_goals(episode, t, strategy='future')

        for g_new in hindsight_goals:
            # 3. 重计算奖励
            r_new = reward_fn(transition.achieved_goal, g_new)

            # 4. 存储重标注转移
            buffer.add(transition.with_goal(g_new, r_new))
```

### 5.5 为什么HER有效？

**样本效率分析**：

原始：$P(r > 0) \approx 0$（几乎全是失败）

HER后：$P(r > 0) \approx \frac{k}{k+1}$（大部分是"成功"）

**信息论视角**：失败轨迹告诉我们"如何到达这些状态"，这是有价值的知识。

---

## 6. 方法对比与选择指南

### 6.1 方法特性对比

| 维度 | PBRS | IRL | Curiosity | HER |
|------|------|-----|-----------|-----|
| 需要专家演示 | ✗ | ✓ | ✗ | ✗ |
| 需要领域知识 | 中 | 低 | 低 | 低 |
| 策略不变性 | ✓ | N/A | ✗ | ✓ |
| 计算开销 | 低 | 中-高 | 中 | 中 |
| 适用环境 | 目标明确 | 有演示 | 探索困难 | 目标条件 |

### 6.2 选择决策树

```
是否有专家演示？
├── 是 → 逆强化学习/模仿学习
│         ├── 需要可迁移奖励？ → AIRL
│         └── 只需模仿行为？ → GAIL/BC
└── 否 → 是目标条件任务？
          ├── 是 → HER
          └── 否 → 探索是主要困难？
                    ├── 是 → Curiosity (ICM/RND)
                    └── 否 → PBRS（如果有好的势能函数）
```

### 6.3 组合使用

**HER + PBRS**：
- HER提供成功样本
- PBRS提供中间引导

**Curiosity + HER**：
- Curiosity驱动探索新状态
- HER从探索中学习

---

## 7. 面试高频问题

### Q1: PBRS为什么保证策略不变性？

**答案要点**：
1. 望远镜求和使累积塑形奖励有界
2. 有界项在无穷horizon下不影响最优策略
3. 数学证明：$\sum \gamma^t F = \gamma^T\Phi(s_T) - \Phi(s_0) \to -\Phi(s_0)$

### Q2: IRL为什么有模糊性？如何解决？

**答案要点**：
1. 模糊性来源：多个奖励函数可以使同一策略最优
2. 解决方法：
   - 最大边际：选择区分度最大的奖励
   - 最大熵：选择使行为最随机的奖励
   - 正则化：偏好简单的奖励函数

### Q3: ICM中逆向模型的作用？

**答案要点**：
1. 强制编码器学习动作相关的特征
2. 过滤掉不可控的环境变化（如背景运动）
3. 提高前向预测的信噪比

### Q4: HER的局限性？

**答案要点**：
1. 需要能定义"达成的目标"映射
2. 目标空间需要与状态空间对齐
3. 不适用于非目标条件任务
4. 可能引入分布偏移

### Q5: 何时选择curiosity vs HER？

**答案要点**：
- **Curiosity**：通用探索，无需目标结构，计算开销较高
- **HER**：目标条件任务，样本效率高，需要目标定义

---

## 参考文献

1. Ng, A.Y. et al. (1999). Policy invariance under reward transformations. *ICML*.
2. Ziebart, B.D. et al. (2008). Maximum entropy inverse reinforcement learning. *AAAI*.
3. Pathak, D. et al. (2017). Curiosity-driven exploration by self-supervised prediction. *ICML*.
4. Andrychowicz, M. et al. (2017). Hindsight experience replay. *NeurIPS*.
5. Ho, J. & Ermon, S. (2016). Generative adversarial imitation learning. *NeurIPS*.
6. Fu, J. et al. (2018). Learning robust rewards with adversarial inverse RL. *ICLR*.
7. Burda, Y. et al. (2019). Exploration by random network distillation. *ICLR*.

---

*本文档最后更新：2024年*
