{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络超参数调整\n",
    "\n",
    "本教程介绍神经网络超参数调整的系统方法，包括手动搜索、随机搜索和学习率查找器。\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解超参数对模型性能的影响\n",
    "2. 掌握学习率查找器(LR Finder)的使用\n",
    "3. 学会使用随机搜索进行超参数优化\n",
    "4. 了解1-Cycle学习率策略\n",
    "\n",
    "## 主要超参数\n",
    "\n",
    "| 超参数 | 影响 | 典型范围 |\n",
    "|--------|------|----------|\n",
    "| 学习率 | 收敛速度和稳定性 | 1e-4 ~ 1e-1 |\n",
    "| 隐藏层数 | 模型复杂度 | 1 ~ 5 |\n",
    "| 神经元数量 | 层容量 | 32 ~ 512 |\n",
    "| 批量大小 | 训练稳定性 | 16 ~ 256 |\n",
    "| 激活函数 | 非线性特性 | relu, tanh, elu |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境配置与数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"TensorFlow版本: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# 划分数据集\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"训练集: {X_train.shape}\")\n",
    "print(f\"验证集: {X_valid.shape}\")\n",
    "print(f\"测试集: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型构建函数\n",
    "\n",
    "定义一个参数化的模型构建函数，便于超参数搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=0.01, \n",
    "                activation='relu', input_shape=[8]):\n",
    "    \"\"\"\n",
    "    构建可配置的回归模型\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_hidden : int\n",
    "        隐藏层数量\n",
    "    n_neurons : int\n",
    "        每层神经元数量\n",
    "    learning_rate : float\n",
    "        学习率\n",
    "    activation : str\n",
    "        激活函数类型\n",
    "    input_shape : list\n",
    "        输入形状\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Model : 编译好的模型\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # 添加隐藏层\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=activation))\n",
    "    \n",
    "    # 输出层（回归任务不需要激活函数）\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    # 编译模型\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 测试模型构建\n",
    "test_model = build_model(n_hidden=2, n_neurons=30)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学习率查找器 (LR Finder)\n",
    "\n",
    "学习率是最重要的超参数。LR Finder通过指数增长学习率来找到最佳范围。\n",
    "\n",
    "### 原理\n",
    "\n",
    "1. 从极小学习率开始（如1e-10）\n",
    "2. 每个batch后指数增加学习率\n",
    "3. 记录学习率和损失的对应关系\n",
    "4. 选择损失快速下降区域的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    学习率查找器回调\n",
    "    \n",
    "    通过指数增长学习率，找到最佳学习率范围\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_lr=1e-10, max_lr=10.0, steps=100):\n",
    "        \"\"\"\n",
    "        初始化LR Finder\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        min_lr : float\n",
    "            最小学习率\n",
    "        max_lr : float\n",
    "            最大学习率\n",
    "        steps : int\n",
    "            总步数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        # 计算每步的乘法因子\n",
    "        self.factor = np.exp(np.log(max_lr / min_lr) / steps)\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"训练开始时设置初始学习率\"\"\"\n",
    "        self.model.optimizer.learning_rate.assign(self.min_lr)\n",
    "    \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        \"\"\"每个batch后记录并更新学习率\"\"\"\n",
    "        # 获取当前学习率和损失\n",
    "        lr = float(self.model.optimizer.learning_rate)\n",
    "        loss = logs.get('loss')\n",
    "        \n",
    "        # 记录\n",
    "        self.lrs.append(lr)\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # 检查是否应该停止（损失爆炸）\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "        if loss > self.best_loss * 4:  # 损失增长超过4倍\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "        \n",
    "        # 增加学习率\n",
    "        new_lr = lr * self.factor\n",
    "        if new_lr > self.max_lr:\n",
    "            self.model.stop_training = True\n",
    "        else:\n",
    "            self.model.optimizer.learning_rate.assign(new_lr)\n",
    "    \n",
    "    def plot(self, skip_start=10, skip_end=5):\n",
    "        \"\"\"\n",
    "        绘制学习率-损失曲线\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        skip_start : int\n",
    "            跳过开头的不稳定点\n",
    "        skip_end : int\n",
    "            跳过结尾的爆炸点\n",
    "        \"\"\"\n",
    "        lrs = self.lrs[skip_start:-skip_end] if skip_end > 0 else self.lrs[skip_start:]\n",
    "        losses = self.losses[skip_start:-skip_end] if skip_end > 0 else self.losses[skip_start:]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate (log scale)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 标记最小损失点\n",
    "        min_idx = np.argmin(losses)\n",
    "        plt.axvline(x=lrs[min_idx], color='r', linestyle='--', \n",
    "                    label=f'Min Loss at LR={lrs[min_idx]:.2e}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # 推荐学习率\n",
    "        suggested_lr = lrs[min_idx] / 10\n",
    "        print(f\"最小损失对应学习率: {lrs[min_idx]:.2e}\")\n",
    "        print(f\"建议使用学习率: {suggested_lr:.2e}\")\n",
    "        return suggested_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用LR Finder\n",
    "model = build_model(n_hidden=1, n_neurons=30, learning_rate=1e-10)\n",
    "\n",
    "# 计算步数（约1个epoch）\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "\n",
    "lr_finder = LRFinder(min_lr=1e-10, max_lr=10.0, steps=n_steps)\n",
    "\n",
    "# 运行LR Finder\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[lr_finder],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 绘制结果并获取建议学习率\n",
    "suggested_lr = lr_finder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 手动超参数搜索\n",
    "\n",
    "使用找到的学习率，系统地测试不同的超参数组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperparameters(n_hidden, n_neurons, learning_rate, \n",
    "                            activation='relu', epochs=30):\n",
    "    \"\"\"\n",
    "    评估一组超参数的性能\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : 包含训练历史和最终损失的字典\n",
    "    \"\"\"\n",
    "    model = build_model(\n",
    "        n_hidden=n_hidden,\n",
    "        n_neurons=n_neurons,\n",
    "        learning_rate=learning_rate,\n",
    "        activation=activation\n",
    "    )\n",
    "    \n",
    "    # 使用早停防止过拟合\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_loss = model.evaluate(X_valid, y_valid, verbose=0)[0]\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'val_loss': val_loss,\n",
    "        'params': {\n",
    "            'n_hidden': n_hidden,\n",
    "            'n_neurons': n_neurons,\n",
    "            'learning_rate': learning_rate,\n",
    "            'activation': activation\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义搜索空间（简化版本用于演示）\n",
    "param_grid = {\n",
    "    'n_hidden': [1, 2, 3],\n",
    "    'n_neurons': [30, 50, 100],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# 随机选择一些组合进行测试（随机搜索）\n",
    "n_trials = 5  # 为了演示只测试5组\n",
    "results = []\n",
    "\n",
    "print(\"开始超参数搜索...\\n\")\n",
    "\n",
    "for i in range(n_trials):\n",
    "    # 随机选择超参数\n",
    "    params = {\n",
    "        'n_hidden': np.random.choice(param_grid['n_hidden']),\n",
    "        'n_neurons': np.random.choice(param_grid['n_neurons']),\n",
    "        'learning_rate': np.random.choice(param_grid['learning_rate']),\n",
    "        'activation': np.random.choice(param_grid['activation'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Trial {i+1}/{n_trials}: {params}\")\n",
    "    \n",
    "    result = evaluate_hyperparameters(**params, epochs=20)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"  验证损失: {result['val_loss']:.4f}\\n\")\n",
    "\n",
    "# 找出最佳结果\n",
    "best_result = min(results, key=lambda x: x['val_loss'])\n",
    "print(\"=\"*50)\n",
    "print(f\"最佳超参数: {best_result['params']}\")\n",
    "print(f\"最佳验证损失: {best_result['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化搜索结果\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "val_losses = [r['val_loss'] for r in results]\n",
    "labels = [f\"Trial {i+1}\" for i in range(len(results))]\n",
    "colors = ['green' if loss == min(val_losses) else 'steelblue' for loss in val_losses]\n",
    "\n",
    "bars = ax.bar(labels, val_losses, color=colors)\n",
    "ax.set_ylabel('Validation Loss (MSE)')\n",
    "ax.set_title('Hyperparameter Search Results')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 添加数值标签\n",
    "for bar, loss in zip(bars, val_losses):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{loss:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 1-Cycle学习率策略\n",
    "\n",
    "1-Cycle策略是一种高效的训练方法：\n",
    "1. 学习率先从低值上升到最大值（热身阶段）\n",
    "2. 然后从最大值下降到极低值（冷却阶段）\n",
    "\n",
    "### 优势\n",
    "- 更快收敛\n",
    "- 更好的泛化性能\n",
    "- 不需要早停"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    1-Cycle学习率调度器\n",
    "    \n",
    "    实现Leslie Smith的1-Cycle学习率策略\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_lr, total_steps, warmup_pct=0.3, \n",
    "                 min_lr_ratio=25, final_lr_ratio=1000):\n",
    "        \"\"\"\n",
    "        初始化1-Cycle调度器\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_lr : float\n",
    "            最大学习率\n",
    "        total_steps : int\n",
    "            总训练步数\n",
    "        warmup_pct : float\n",
    "            热身阶段占总步数的比例\n",
    "        min_lr_ratio : float\n",
    "            初始学习率 = max_lr / min_lr_ratio\n",
    "        final_lr_ratio : float\n",
    "            最终学习率 = max_lr / final_lr_ratio\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_lr = max_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = int(total_steps * warmup_pct)\n",
    "        self.min_lr = max_lr / min_lr_ratio\n",
    "        self.final_lr = max_lr / final_lr_ratio\n",
    "        self.step = 0\n",
    "        self.lrs = []\n",
    "    \n",
    "    def _get_lr(self):\n",
    "        \"\"\"根据当前步数计算学习率\"\"\"\n",
    "        if self.step < self.warmup_steps:\n",
    "            # 热身阶段：余弦上升\n",
    "            progress = self.step / self.warmup_steps\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * \\\n",
    "                 (1 - np.cos(progress * np.pi)) / 2\n",
    "        else:\n",
    "            # 冷却阶段：余弦下降\n",
    "            progress = (self.step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = self.final_lr + (self.max_lr - self.final_lr) * \\\n",
    "                 (1 + np.cos(progress * np.pi)) / 2\n",
    "        return lr\n",
    "    \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"每个batch开始时更新学习率\"\"\"\n",
    "        lr = self._get_lr()\n",
    "        self.model.optimizer.learning_rate.assign(lr)\n",
    "        self.lrs.append(lr)\n",
    "        self.step += 1\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"绘制学习率曲线\"\"\"\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.lrs)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('1-Cycle Learning Rate Schedule')\n",
    "        plt.axvline(x=self.warmup_steps, color='r', linestyle='--', \n",
    "                    label=f'Warmup End ({self.warmup_steps} steps)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用1-Cycle策略训练模型\n",
    "model = build_model(n_hidden=2, n_neurons=50, learning_rate=0.01)\n",
    "\n",
    "# 计算总步数\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "# 创建1-Cycle调度器\n",
    "one_cycle = OneCycleScheduler(\n",
    "    max_lr=0.01,\n",
    "    total_steps=total_steps,\n",
    "    warmup_pct=0.3\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[one_cycle],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 绘制学习率曲线\n",
    "one_cycle.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 损失曲线\n",
    "axes[0].plot(history.history['loss'], label='Training')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Loss Curves (1-Cycle)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE曲线\n",
    "axes[1].plot(history.history['mae'], label='Training')\n",
    "axes[1].plot(history.history['val_mae'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE Curves (1-Cycle)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 最终评估\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n测试集 - MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 对比实验：固定学习率 vs 1-Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用固定学习率训练\n",
    "model_fixed = build_model(n_hidden=2, n_neurons=50, learning_rate=0.01)\n",
    "\n",
    "history_fixed = model_fixed.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 使用1-Cycle策略训练\n",
    "model_1cycle = build_model(n_hidden=2, n_neurons=50, learning_rate=0.01)\n",
    "\n",
    "one_cycle = OneCycleScheduler(\n",
    "    max_lr=0.01,\n",
    "    total_steps=steps_per_epoch * 30,\n",
    "    warmup_pct=0.3\n",
    ")\n",
    "\n",
    "history_1cycle = model_1cycle.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[one_cycle],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 对比结果\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_fixed.history['val_loss'], label='Fixed LR', linestyle='--')\n",
    "plt.plot(history_1cycle.history['val_loss'], label='1-Cycle')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Fixed LR vs 1-Cycle Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 最终对比\n",
    "fixed_loss = model_fixed.evaluate(X_test, y_test, verbose=0)[0]\n",
    "cycle_loss = model_1cycle.evaluate(X_test, y_test, verbose=0)[0]\n",
    "\n",
    "print(f\"固定学习率测试MSE: {fixed_loss:.4f}\")\n",
    "print(f\"1-Cycle测试MSE: {cycle_loss:.4f}\")\n",
    "print(f\"改进: {(fixed_loss - cycle_loss) / fixed_loss * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 使用最佳超参数训练最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用找到的最佳超参数\n",
    "best_params = best_result['params']\n",
    "print(f\"最佳超参数: {best_params}\")\n",
    "\n",
    "# 构建最终模型\n",
    "final_model = build_model(\n",
    "    n_hidden=best_params['n_hidden'],\n",
    "    n_neurons=best_params['n_neurons'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    activation=best_params['activation']\n",
    ")\n",
    "\n",
    "# 计算1-Cycle参数\n",
    "epochs = 50\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "# 使用1-Cycle策略训练\n",
    "one_cycle = OneCycleScheduler(\n",
    "    max_lr=best_params['learning_rate'],\n",
    "    total_steps=total_steps,\n",
    "    warmup_pct=0.3\n",
    ")\n",
    "\n",
    "# 添加模型检查点\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# 训练\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[one_cycle, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 加载最佳模型\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "# 最终评估\n",
    "test_loss, test_mae = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n最终模型测试集 - MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "### 超参数调整流程\n",
    "\n",
    "1. **使用LR Finder**: 确定学习率的合适范围\n",
    "2. **随机搜索**: 在参数空间中随机采样，比网格搜索更高效\n",
    "3. **1-Cycle策略**: 使用学习率调度提升训练效果\n",
    "4. **早停和检查点**: 防止过拟合并保存最佳模型\n",
    "\n",
    "### 关键超参数优先级\n",
    "\n",
    "1. **学习率**: 最重要，使用LR Finder确定\n",
    "2. **网络深度**: 从浅层开始，逐步增加\n",
    "3. **神经元数量**: 宁可多一些，用正则化控制\n",
    "4. **批量大小**: 32-64通常是好的起点\n",
    "\n",
    "### 进阶工具\n",
    "\n",
    "- **Keras Tuner**: 更系统的超参数搜索\n",
    "- **Optuna**: 贝叶斯优化\n",
    "- **Ray Tune**: 分布式超参数搜索"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
