{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Subclass API构建动态模型\n",
    "\n",
    "本教程介绍Keras的Model Subclassing API，这是最灵活但也最复杂的模型构建方式。\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解Model子类化的基本语法\n",
    "2. 掌握自定义模型的构建方法\n",
    "3. 学会在call方法中实现动态计算逻辑\n",
    "4. 了解三种API的适用场景和权衡\n",
    "\n",
    "## 三种API对比\n",
    "\n",
    "| API类型 | 灵活性 | 调试难度 | 适用场景 |\n",
    "|---------|--------|----------|----------|\n",
    "| Sequential | 低 | 简单 | 线性堆叠模型 |\n",
    "| Functional | 中 | 中等 | 静态多输入/输出 |\n",
    "| Subclass | 高 | 复杂 | 动态计算图、研究 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境配置与数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"TensorFlow版本: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并预处理数据\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 准备多输入数据\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "\n",
    "print(f\"训练集: {X_train.shape}\")\n",
    "print(f\"Wide输入: {X_train_A.shape}, Deep输入: {X_train_B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Subclassing基础\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "Model子类化需要：\n",
    "1. 继承`keras.Model`类\n",
    "2. 在`__init__`中定义层\n",
    "3. 在`call`方法中实现前向传播逻辑\n",
    "\n",
    "### 基本模板\n",
    "\n",
    "```python\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 在这里定义层\n",
    "        self.dense1 = keras.layers.Dense(30, activation='relu')\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # 在这里定义前向传播逻辑\n",
    "        x = self.dense1(inputs)\n",
    "        return self.output_layer(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的自定义模型示例\n",
    "class SimpleRegressor(keras.Model):\n",
    "    \"\"\"\n",
    "    简单的回归模型\n",
    "    \n",
    "    架构: Input -> Dense(30, ReLU) -> Dense(30, ReLU) -> Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        \"\"\"\n",
    "        初始化模型层\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        units : int\n",
    "            隐藏层神经元数量\n",
    "        activation : str\n",
    "            激活函数\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : tensor\n",
    "            输入张量\n",
    "        training : bool, optional\n",
    "            是否处于训练模式（用于Dropout等）\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tensor : 模型输出\n",
    "        \"\"\"\n",
    "        x = self.hidden1(inputs)\n",
    "        x = self.hidden2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# 创建模型实例\n",
    "simple_model = SimpleRegressor(units=30, name='simple_regressor')\n",
    "\n",
    "# 编译模型\n",
    "simple_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer='sgd',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 构建模型（指定输入形状）\n",
    "simple_model.build(input_shape=(None, 8))\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练简单模型\n",
    "history_simple = simple_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估\n",
    "test_loss, test_mae = simple_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n测试集 MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wide & Deep模型（子类化版本）\n",
    "\n",
    "使用子类化API实现多输入多输出的Wide & Deep模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Wide & Deep模型的子类化实现\n",
    "    \n",
    "    架构:\n",
    "    - Wide路径: 直接连接到输出\n",
    "    - Deep路径: 两层隐藏层 + 辅助输出\n",
    "    - 合并后产生主输出\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        \"\"\"\n",
    "        初始化Wide & Deep模型\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        units : int\n",
    "            隐藏层神经元数量\n",
    "        activation : str\n",
    "            激活函数类型\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Deep路径的隐藏层\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation, name='deep_hidden1')\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation, name='deep_hidden2')\n",
    "        \n",
    "        # 输出层\n",
    "        self.main_output = keras.layers.Dense(1, name='main_output')\n",
    "        self.aux_output = keras.layers.Dense(1, name='aux_output')\n",
    "        \n",
    "        # 合并层\n",
    "        self.concat = keras.layers.Concatenate(name='concat')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : tuple of tensors\n",
    "            (wide_input, deep_input)\n",
    "        training : bool, optional\n",
    "            训练模式标志\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (main_output, aux_output)\n",
    "        \"\"\"\n",
    "        # 解包输入\n",
    "        input_wide, input_deep = inputs\n",
    "        \n",
    "        # Deep路径\n",
    "        hidden1_out = self.hidden1(input_deep)\n",
    "        hidden2_out = self.hidden2(hidden1_out)\n",
    "        \n",
    "        # 合并Wide和Deep路径\n",
    "        concat_out = self.concat([input_wide, hidden2_out])\n",
    "        \n",
    "        # 计算输出\n",
    "        main_out = self.main_output(concat_out)\n",
    "        aux_out = self.aux_output(hidden2_out)\n",
    "        \n",
    "        return main_out, aux_out\n",
    "\n",
    "# 创建模型\n",
    "wide_deep_model = WideAndDeepModel(units=30, name='wide_and_deep')\n",
    "\n",
    "# 编译模型\n",
    "wide_deep_model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练Wide & Deep模型\n",
    "history_wd = wide_deep_model.fit(\n",
    "    (X_train_A, X_train_B), (y_train, y_train),\n",
    "    epochs=30,\n",
    "    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "results = wide_deep_model.evaluate(\n",
    "    (X_test_A, X_test_B), (y_test, y_test), verbose=0\n",
    ")\n",
    "\n",
    "print(\"评估结果:\")\n",
    "print(f\"总损失: {results[0]:.4f}\")\n",
    "print(f\"主输出损失: {results[1]:.4f}\")\n",
    "print(f\"辅助输出损失: {results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 动态行为示例\n",
    "\n",
    "子类化API的最大优势是可以在`call`方法中实现动态计算逻辑，\n",
    "例如条件分支、循环等Python控制流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicModel(keras.Model):\n",
    "    \"\"\"\n",
    "    带有动态行为的模型示例\n",
    "    \n",
    "    特点:\n",
    "    - 训练时使用Dropout\n",
    "    - 可以根据输入动态选择计算路径\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units=30, dropout_rate=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation='relu')\n",
    "        self.hidden2 = keras.layers.Dense(units, activation='relu')\n",
    "        self.dropout = keras.layers.Dropout(dropout_rate)\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        前向传播，带有动态Dropout\n",
    "        \n",
    "        training参数在fit()时自动设为True，\n",
    "        在evaluate()和predict()时自动设为False\n",
    "        \"\"\"\n",
    "        x = self.hidden1(inputs)\n",
    "        \n",
    "        # Dropout只在训练时生效\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        x = self.hidden2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "# 创建并训练动态模型\n",
    "dynamic_model = DynamicModel(units=30, dropout_rate=0.2, name='dynamic_model')\n",
    "dynamic_model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "history_dynamic = dynamic_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估\n",
    "test_loss, test_mae = dynamic_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n测试集 MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 自定义层\n",
    "\n",
    "除了自定义模型，还可以创建自定义层，实现更细粒度的控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    残差块（Residual Block）自定义层\n",
    "    \n",
    "    实现: output = activation(input + Dense(Dense(input)))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"根据输入形状构建层的权重\"\"\"\n",
    "        self.dense1 = keras.layers.Dense(self.units, activation='relu')\n",
    "        self.dense2 = keras.layers.Dense(input_shape[-1])  # 输出维度与输入相同\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"前向传播：残差连接\"\"\"\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.activation(inputs + x)  # 残差连接\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"返回层配置，用于模型序列化\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'activation': keras.activations.serialize(self.activation)\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义层构建模型\n",
    "class ResNet(keras.Model):\n",
    "    \"\"\"使用残差块的模型\"\"\"\n",
    "    \n",
    "    def __init__(self, n_blocks=3, units=30, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dense = keras.layers.Dense(units, activation='relu')\n",
    "        self.res_blocks = [ResidualBlock(units) for _ in range(n_blocks)]\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.input_dense(inputs)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# 创建并训练ResNet\n",
    "resnet = ResNet(n_blocks=3, units=30, name='resnet')\n",
    "resnet.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "history_resnet = resnet.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_mae = resnet.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n测试集 MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比不同模型的训练曲线\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 损失曲线\n",
    "axes[0].plot(history_simple.history['loss'], label='Simple')\n",
    "axes[0].plot(history_dynamic.history['loss'], label='Dynamic (Dropout)')\n",
    "axes[0].plot(history_resnet.history['loss'], label='ResNet')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 验证损失\n",
    "axes[1].plot(history_simple.history['val_loss'], label='Simple')\n",
    "axes[1].plot(history_dynamic.history['val_loss'], label='Dynamic (Dropout)')\n",
    "axes[1].plot(history_resnet.history['val_loss'], label='ResNet')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 注意事项与最佳实践\n",
    "\n",
    "### 子类化API的优势\n",
    "\n",
    "1. **完全的灵活性**: 可以实现任意复杂的前向传播逻辑\n",
    "2. **动态计算图**: 支持条件分支、循环等控制流\n",
    "3. **易于调试**: 可以在call方法中使用print或断点\n",
    "4. **研究友好**: 适合实现新的模型架构\n",
    "\n",
    "### 子类化API的限制\n",
    "\n",
    "1. **无法自动推断形状**: 需要显式调用build()或传入数据\n",
    "2. **序列化复杂**: 需要实现get_config()方法\n",
    "3. **不支持某些Keras功能**: 如plot_model可能无法显示完整结构\n",
    "\n",
    "### 选择建议\n",
    "\n",
    "- **日常使用**: 优先使用Sequential或Functional API\n",
    "- **复杂架构**: 静态图用Functional，动态图用Subclass\n",
    "- **研究实验**: Subclass API最适合快速原型开发"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **继承正确的基类**: `keras.Model`用于模型，`keras.layers.Layer`用于自定义层\n",
    "2. **__init__定义组件**: 所有可训练层都应在初始化时创建\n",
    "3. **call实现逻辑**: 前向传播的完整计算流程\n",
    "4. **training参数**: 用于区分训练和推理模式\n",
    "\n",
    "### 何时使用子类化\n",
    "\n",
    "- 需要动态计算图（如循环神经网络的动态展开）\n",
    "- 实现复杂的注意力机制\n",
    "- 研究新的网络架构\n",
    "- 需要在前向传播中使用Python控制流"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
