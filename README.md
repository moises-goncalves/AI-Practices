<div align="center">

<!-- åŠ¨æ€æ¸å˜å¤´å›¾ - iOSé£æ ¼ -->
<img src="https://capsule-render.vercel.app/api?type=waving&color=0:007AFF,50:5856D6,100:AF52DE&height=180&section=header&text=AI-Practices&fontSize=52&fontColor=ffffff&fontAlignY=35&desc=A%20Systematic%20Approach%20to%20AI%20Research%20%26%20Engineering&descSize=16&descAlignY=55&animation=twinkling" width="100%"/>

<br/>

<!-- å¤šè¡ŒåŠ¨æ€æ‰“å­—æ•ˆæœ -->
<a href="https://github.com/zimingttkx/AI-Practices">
  <img src="https://readme-typing-svg.demolab.com?font=SF+Pro+Display&weight=600&size=28&duration=3000&pause=1000&color=007AFF&center=true&vCenter=true&multiline=true&repeat=false&width=600&height=50&lines=Full-Stack+AI+Learning+Laboratory" alt="Title" />
</a>

<br/>

<a href="https://github.com/zimingttkx/AI-Practices">
  <img src="https://readme-typing-svg.demolab.com?font=SF+Pro+Text&weight=400&size=14&duration=2500&pause=500&color=8E8E93&center=true&vCenter=true&width=700&height=25&lines=Machine+Learning+%E2%9C%A6+Deep+Learning+%E2%9C%A6+Computer+Vision+%E2%9C%A6+NLP+%E2%9C%A6+Generative+AI+%E2%9C%A6+Reinforcement+Learning" alt="Topics" />
</a>

<br/><br/>

<!-- iOSé£æ ¼å¯¼èˆªæŒ‰é’® -->
<a href="./README_EN.md">
  <img src="https://img.shields.io/badge/English-README-007AFF?style=for-the-badge&logo=readme&logoColor=white&labelColor=000000" alt="English">
</a>
&nbsp;
<a href="https://zimingttkx.github.io/AI-Practices/">
  <img src="https://img.shields.io/badge/Documentation-Online-34C759?style=for-the-badge&logo=gitbook&logoColor=white&labelColor=000000" alt="Docs">
</a>
&nbsp;
<a href="#-quick-start">
  <img src="https://img.shields.io/badge/Quick-Start-FF9500?style=for-the-badge&logo=rocket&logoColor=white&labelColor=000000" alt="Start">
</a>

<br/><br/>

<!-- æ ¸å¿ƒç»Ÿè®¡å¾½ç«  - iOSé…è‰² -->
<a href="https://github.com/zimingttkx/AI-Practices/stargazers">
  <img src="https://img.shields.io/github/stars/zimingttkx/AI-Practices?style=for-the-badge&logo=github&logoColor=white&labelColor=1c1c1e&color=FFD60A&label=Stars" alt="Stars">
</a>
&nbsp;
<a href="https://github.com/zimingttkx/AI-Practices/network/members">
  <img src="https://img.shields.io/github/forks/zimingttkx/AI-Practices?style=for-the-badge&logo=git&logoColor=white&labelColor=1c1c1e&color=30D158&label=Forks" alt="Forks">
</a>
&nbsp;
<a href="https://github.com/zimingttkx/AI-Practices/issues">
  <img src="https://img.shields.io/github/issues/zimingttkx/AI-Practices?style=for-the-badge&logo=github&logoColor=white&labelColor=1c1c1e&color=FF453A&label=Issues" alt="Issues">
</a>
&nbsp;
<a href="./LICENSE">
  <img src="https://img.shields.io/github/license/zimingttkx/AI-Practices?style=for-the-badge&logo=opensourceinitiative&logoColor=white&labelColor=1c1c1e&color=BF5AF2&label=License" alt="License">
</a>

<br/><br/>

<!-- æ´»è·ƒåº¦æŒ‡æ ‡ -->
<a href="https://github.com/zimingttkx/AI-Practices/commits/main">
  <img src="https://img.shields.io/github/last-commit/zimingttkx/AI-Practices?style=flat-square&logo=github&logoColor=white&labelColor=2c2c2e&color=64D2FF&label=Last%20Commit" alt="Last Commit">
</a>
&nbsp;
<a href="https://github.com/zimingttkx/AI-Practices/graphs/commit-activity">
  <img src="https://img.shields.io/github/commit-activity/m/zimingttkx/AI-Practices?style=flat-square&logo=github&logoColor=white&labelColor=2c2c2e&color=FF9F0A&label=Monthly%20Commits" alt="Commit Activity">
</a>
&nbsp;
<a href="https://github.com/zimingttkx/AI-Practices">
  <img src="https://img.shields.io/github/repo-size/zimingttkx/AI-Practices?style=flat-square&logo=github&logoColor=white&labelColor=2c2c2e&color=8E8E93&label=Size" alt="Repo Size">
</a>
&nbsp;
<img src="https://komarev.com/ghpvc/?username=zimingttkx-AI-Practices&style=flat-square&color=007AFF&label=Views" alt="Views">

<br/><br/>

<!-- åŠ¨æ€GitHubç»Ÿè®¡ -->
<img src="https://github-readme-stats.vercel.app/api?username=zimingttkx&show_icons=true&theme=transparent&hide_border=true&title_color=007AFF&icon_color=34C759&text_color=8E8E93&bg_color=00000000" height="150" alt="GitHub Stats"/>
&nbsp;
<img src="https://github-readme-streak-stats.herokuapp.com?user=zimingttkx&theme=transparent&hide_border=true&ring=007AFF&fire=FF9500&currStreakLabel=007AFF&sideLabels=8E8E93&currStreakNum=FFFFFF&sideNums=FFFFFF&dates=8E8E93" height="150" alt="GitHub Streak"/>

</div>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## ğŸ“‹ Table of Contents

<details open>
<summary><b>Quick Navigation</b></summary>
<br/>

| Section | Description | Link |
|:-------:|:------------|:----:|
| ğŸ¯ | **Overview** - é¡¹ç›®æ¦‚è¿°ä¸ç ”ç©¶èƒŒæ™¯ | [Jump](#-overview) |
| ğŸ—ï¸ | **Architecture** - ç³»ç»Ÿæ¶æ„ä¸æ¨¡å—è®¾è®¡ | [Jump](#ï¸-architecture) |
| ğŸ“š | **Curriculum** - ä¹å¤§æ ¸å¿ƒå­¦ä¹ æ¨¡å— | [Jump](#-curriculum) |
| ğŸ› ï¸ | **Tech Stack** - æŠ€æœ¯æ ˆä¸å·¥å…·é“¾ | [Jump](#ï¸-tech-stack) |
| ğŸš€ | **Quick Start** - ç¯å¢ƒé…ç½®ä¸å¿«é€Ÿå¯åŠ¨ | [Jump](#-quick-start) |
| ğŸ“Š | **Results** - å®éªŒç»“æœä¸ç«èµ›æˆç»© | [Jump](#-results) |
| ğŸ“„ | **Citation** - å¼•ç”¨æœ¬é¡¹ç›® | [Jump](#-citation) |

</details>

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## ğŸ¯ Overview

<div align="center">

<!-- é¡¹ç›®æ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡ - iOS é£æ ¼åŒè¡Œå¸ƒå±€ -->
<table>
<tr>
<td align="center" width="160">
<img src="https://img.shields.io/badge/113+-Notebooks-007AFF?style=for-the-badge&labelColor=1c1c1e" alt="Notebooks"/><br/>
<b>å¯å¤ç°å®éªŒ</b><br/>
<sub>Jupyter Notebooks</sub>
</td>
<td align="center" width="160">
<img src="https://img.shields.io/badge/9-Modules-34C759?style=for-the-badge&labelColor=1c1c1e" alt="Modules"/><br/>
<b>æ ¸å¿ƒæ¨¡å—</b><br/>
<sub>Learning Modules</sub>
</td>
<td align="center" width="160">
<img src="https://img.shields.io/badge/19-Projects-FF9500?style=for-the-badge&labelColor=1c1c1e" alt="Projects"/><br/>
<b>å®æˆ˜é¡¹ç›®</b><br/>
<sub>Real-world Projects</sub>
</td>
<td align="center" width="160">
<img src="https://img.shields.io/badge/149k+-LOC-5856D6?style=for-the-badge&labelColor=1c1c1e" alt="LOC"/><br/>
<b>ä»£ç è¡Œæ•°</b><br/>
<sub>Lines of Code</sub>
</td>
<td align="center" width="160">
<img src="https://img.shields.io/badge/2x_Gold-Kaggle-FF2D55?style=for-the-badge&labelColor=1c1c1e" alt="Kaggle"/><br/>
<b>ç«èµ›é‡‘ç‰Œ</b><br/>
<sub>Competition Medals</sub>
</td>
</tr>
</table>

<br/>

<!-- æŠ€æœ¯è¦†ç›–èŒƒå›´ -->
<img src="https://img.shields.io/badge/ML-Supervised_|_Unsupervised_|_Ensemble-007AFF?style=flat-square&labelColor=1c1c1e" alt="ML"/>
&nbsp;
<img src="https://img.shields.io/badge/DL-CNN_|_RNN_|_Transformer_|_GAN-5856D6?style=flat-square&labelColor=1c1c1e" alt="DL"/>
&nbsp;
<img src="https://img.shields.io/badge/RL-DQN_|_PPO_|_SAC-AF52DE?style=flat-square&labelColor=1c1c1e" alt="RL"/>

</div>

<br/>

### Research Background

> **AI-Practices** æ˜¯ä¸€ä¸ªç³»ç»ŸåŒ–ã€å·¥ç¨‹åŒ–çš„äººå·¥æ™ºèƒ½å­¦ä¹ ä¸ç ”ç©¶å¹³å°ï¼Œé‡‡ç”¨ **æ¸è¿›å¼å­¦ä¹ æ¡†æ¶ (Progressive Learning Framework)** æ–¹æ³•è®ºï¼Œä¸ºç ”ç©¶äººå‘˜ã€å·¥ç¨‹å¸ˆå’Œå­¦ä¹ è€…æä¾›å®Œæ•´çš„ AI æŠ€æœ¯æ ˆå­¦ä¹ è·¯å¾„ã€‚

<br/>

### Methodology

æœ¬é¡¹ç›®éµå¾ª **"ç†è®ºé©±åŠ¨ã€å®è·µä¸ºæœ¬ã€å·¥ç¨‹å¯¼å‘"** çš„ä¸‰ä½ä¸€ä½“è®¾è®¡ç†å¿µï¼Œæ„å»ºä»ç†è®ºåˆ°å®æˆ˜çš„å®Œæ•´å­¦ä¹ é—­ç¯ï¼š

<div align="center">

<!-- å­¦ä¹ é˜¶æ®µæµç¨‹ -->
<img src="https://img.shields.io/badge/Phase_I-Theory_First-007AFF?style=for-the-badge" alt="Phase I"/>
<img src="https://img.shields.io/badge/â†’-white?style=for-the-badge" alt="arrow"/>
<img src="https://img.shields.io/badge/Phase_II-From_Scratch-5856D6?style=for-the-badge" alt="Phase II"/>
<img src="https://img.shields.io/badge/â†’-white?style=for-the-badge" alt="arrow"/>
<img src="https://img.shields.io/badge/Phase_III-Framework-AF52DE?style=for-the-badge" alt="Phase III"/>
<img src="https://img.shields.io/badge/â†’-white?style=for-the-badge" alt="arrow"/>
<img src="https://img.shields.io/badge/Phase_IV-Practice-FF2D55?style=for-the-badge" alt="Phase IV"/>

</div>

<br/>

<div align="center">

| Phase | Principle | Method | Output | Goal |
|:-----:|:----------|:-------|:-------|:----:|
| **I** | **Theory First** | Math derivation + Algorithm analysis | Theory Notes | ğŸ¯ |
| **II** | **From Scratch** | NumPy implementation from scratch | Core Code | ğŸ”§ |
| **III** | **Framework** | PyTorch / TensorFlow engineering | Production Code | âš¡ |
| **IV** | **Practice** | Kaggle + Real-world Projects | Complete Solutions | ğŸ† |

</div>

<br/>

**Core Advantages:**

- ğŸ”„ **Progressive Learning** â€” Each phase builds upon the previous, ensuring knowledge continuity
- ğŸ§  **Theory + Practice** â€” Not just "How", but "Why" â€” develop independent problem-solving skills
- ğŸ­ **Engineering Mindset** â€” From academic research to industrial deployment

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## ğŸ—ï¸ Architecture

### Module Dependencies

> æ¨¡å—é—´çš„ä¾èµ–å…³ç³»éµå¾ª**æ¸è¿›å¼å­¦ä¹ è·¯å¾„**ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½å»ºç«‹åœ¨å‰ä¸€é˜¶æ®µçš„åŸºç¡€ä¹‹ä¸Šã€‚

<div align="center">

| Phase | Module | Prerequisites | Core Skills Acquired |
|:-----:|:-------|:--------------|:--------------------|
| **â… ** | ğŸ“š **01-Foundations** | Python, NumPy | ç»å…¸ ML ç®—æ³•ã€æ•°å­¦åŸºç¡€ã€æ¨¡å‹è¯„ä¼° |
| **â…¡** | ğŸ§  **02-Neural Networks** | 01-Foundations | åå‘ä¼ æ’­ã€ä¼˜åŒ–å™¨ã€æ­£åˆ™åŒ–æŠ€æœ¯ |
| **â…¡** | ğŸ‘ï¸ **03-Computer Vision** | 02-Neural Networks | CNN æ¶æ„ã€è¿ç§»å­¦ä¹ ã€å›¾åƒå¤„ç† |
| **â…¡** | ğŸ“ **04-Sequence Models** | 02-Neural Networks | RNN/LSTMã€Attentionã€Transformer |
| **â…¢** | âš¡ **05-Advanced Topics** | 03-CV, 04-Seq | åˆ†å¸ƒå¼è®­ç»ƒã€è¶…å‚ä¼˜åŒ–ã€æ¨¡å‹éƒ¨ç½² |
| **â…¢** | ğŸ¨ **06-Generative Models** | 05-Advanced | VAEã€GANã€Diffusion Models |
| **â…¢** | ğŸ® **07-Reinforcement Learning** | 05-Advanced | å€¼å‡½æ•°ã€ç­–ç•¥æ¢¯åº¦ã€Actor-Critic |
| **â…£** | ğŸ† **09-Practical Projects** | 03-CV, 04-Seq, 06-Gen | ç«¯åˆ°ç«¯é¡¹ç›®ã€Kaggle ç«èµ›å®æˆ˜ |
| **â€”** | ğŸ“– **08-Theory Notes** | â€” | æ•°å­¦æ¨å¯¼å‚è€ƒï¼ˆå¯éšæ—¶æŸ¥é˜…ï¼‰ |

</div>

### Directory Structure

```
AI-Practices/
â”œâ”€â”€ ğŸ“š 01-foundations/          # æœºå™¨å­¦ä¹ åŸºç¡€ (ML Fundamentals)
â”œâ”€â”€ ğŸ§  02-neural-networks/      # ç¥ç»ç½‘ç»œ (Deep Learning Core)
â”œâ”€â”€ ğŸ‘ï¸ 03-computer-vision/      # è®¡ç®—æœºè§†è§‰ (CNN, ViT, Detection)
â”œâ”€â”€ ğŸ“ 04-sequence-models/      # åºåˆ—æ¨¡å‹ (RNN, Transformer, LLM)
â”œâ”€â”€ âš¡ 05-advanced-topics/      # é«˜çº§ä¸“é¢˜ (Optimization, Deployment)
â”œâ”€â”€ ğŸ¨ 06-generative-models/    # ç”Ÿæˆæ¨¡å‹ (VAE, GAN, Diffusion)
â”œâ”€â”€ ğŸ® 07-reinforcement-learning/ # å¼ºåŒ–å­¦ä¹  (DQN, PPO, SAC)
â”œâ”€â”€ ğŸ“– 08-theory-notes/         # ç†è®ºç¬”è®° (Math, Optimization)
â”œâ”€â”€ ğŸ† 09-practical-projects/   # å®æˆ˜é¡¹ç›® (Kaggle, Industry)
â””â”€â”€ ğŸ”§ utils/                   # å·¥å…·åº“ (Data, Viz, Metrics)
```

<details>
<summary><b>ğŸ“‚ å±•å¼€å®Œæ•´ç›®å½•ç»“æ„</b></summary>

```
AI-Practices/
â”‚
â”œâ”€â”€ ğŸ“š 01-foundations/                 # æœºå™¨å­¦ä¹ åŸºç¡€
â”‚   â”œâ”€â”€ training-models/               #   ä¼˜åŒ–æ–¹æ³•: SGD, Adam, L-BFGS
â”‚   â”œâ”€â”€ classification/                #   åˆ†ç±»ç®—æ³•: LR, SVM, Decision Tree
â”‚   â”œâ”€â”€ ensemble-learning/             #   é›†æˆæ–¹æ³•: Bagging, Boosting, Stacking
â”‚   â””â”€â”€ unsupervised-learning/         #   æ— ç›‘ç£: Clustering, Dimensionality Reduction
â”‚
â”œâ”€â”€ ğŸ§  02-neural-networks/             # ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ 
â”‚   â”œâ”€â”€ keras-introduction/            #   æ¡†æ¶å…¥é—¨: Sequential, Functional API
â”‚   â”œâ”€â”€ training-deep-networks/        #   è®­ç»ƒæŠ€å·§: BatchNorm, Dropout, Residual
â”‚   â””â”€â”€ custom-models/                 #   è‡ªå®šä¹‰: Layer, Loss, Training Loop
â”‚
â”œâ”€â”€ ğŸ‘ï¸ 03-computer-vision/             # è®¡ç®—æœºè§†è§‰
â”‚   â”œâ”€â”€ cnn-architectures/             #   æ¶æ„æ¼”è¿›: LeNet â†’ ResNet â†’ EfficientNet â†’ ViT
â”‚   â”œâ”€â”€ transfer-learning/             #   è¿ç§»å­¦ä¹ : Feature Extraction, Fine-tuning
â”‚   â””â”€â”€ model-interpretability/        #   å¯è§£é‡Šæ€§: Grad-CAM, SHAP
â”‚
â”œâ”€â”€ ğŸ“ 04-sequence-models/             # åºåˆ—æ¨¡å‹ä¸NLP
â”‚   â”œâ”€â”€ rnn-lstm-gru/                  #   å¾ªç¯ç½‘ç»œ: Vanishing Gradient, Gating
â”‚   â”œâ”€â”€ attention-transformer/         #   æ³¨æ„åŠ›æœºåˆ¶: Self-Attention, Multi-Head
â”‚   â””â”€â”€ pretrained-models/             #   é¢„è®­ç»ƒ: BERT, GPT, T5
â”‚
â”œâ”€â”€ âš¡ 05-advanced-topics/             # é«˜çº§ä¸“é¢˜
â”‚   â”œâ”€â”€ hyperparameter-tuning/         #   è¶…å‚ä¼˜åŒ–: Optuna, Ray Tune
â”‚   â”œâ”€â”€ distributed-training/          #   åˆ†å¸ƒå¼: Data Parallel, Model Parallel
â”‚   â””â”€â”€ model-deployment/              #   éƒ¨ç½²: TensorRT, ONNX, TFLite
â”‚
â”œâ”€â”€ ğŸ¨ 06-generative-models/           # ç”Ÿæˆå¼æ¨¡å‹
â”‚   â”œâ”€â”€ variational-autoencoders/      #   VAE: Latent Space, Reparameterization
â”‚   â”œâ”€â”€ generative-adversarial/        #   GAN: DCGAN, WGAN, StyleGAN
â”‚   â””â”€â”€ diffusion-models/              #   æ‰©æ•£: DDPM, Stable Diffusion
â”‚
â”œâ”€â”€ ğŸ® 07-reinforcement-learning/      # å¼ºåŒ–å­¦ä¹ 
â”‚   â”œâ”€â”€ value-based/                   #   å€¼æ–¹æ³•: Q-Learning, DQN, Double DQN
â”‚   â”œâ”€â”€ policy-based/                  #   ç­–ç•¥æ–¹æ³•: REINFORCE, PPO, SAC
â”‚   â””â”€â”€ model-based/                   #   æ¨¡å‹æ–¹æ³•: World Models, MuZero
â”‚
â”œâ”€â”€ ğŸ“– 08-theory-notes/                # ç†è®ºå‚è€ƒæ‰‹å†Œ
â”‚   â”œâ”€â”€ mathematical-foundations/      #   æ•°å­¦åŸºç¡€: Linear Algebra, Probability
â”‚   â”œâ”€â”€ optimization-theory/           #   ä¼˜åŒ–ç†è®º: Convex, Non-convex
â”‚   â””â”€â”€ information-theory/            #   ä¿¡æ¯è®º: Entropy, KL-Divergence
â”‚
â”œâ”€â”€ ğŸ† 09-practical-projects/          # å®æˆ˜é¡¹ç›®
â”‚   â”œâ”€â”€ kaggle-competitions/           #   ç«èµ›æ–¹æ¡ˆ: Gold Medal Solutions
â”‚   â””â”€â”€ industry-applications/         #   å·¥ä¸šåº”ç”¨: End-to-End Pipelines
â”‚
â””â”€â”€ ğŸ”§ utils/                          # å·¥å…·åº“
    â”œâ”€â”€ data/                          #   æ•°æ®å¤„ç†
    â”œâ”€â”€ visualization/                 #   å¯è§†åŒ–
    â””â”€â”€ metrics/                       #   è¯„ä¼°æŒ‡æ ‡
```

</details>

<br/>

### ğŸ’» Core Implementation

> ä»¥ä¸‹ä»£ç å±•ç¤ºäº†æœ¬é¡¹ç›®çš„å·¥ç¨‹è´¨é‡æ ‡å‡†ï¼Œä½“ç°äº† **ç±»å‹å®‰å…¨**ã€**æ¨¡å—åŒ–è®¾è®¡** å’Œ **ç”Ÿäº§çº§å®è·µ**ã€‚

<details open>
<summary><b>ğŸ”¥ Multi-Head Self-Attention (PyTorch)</b></summary>

```python
"""
Multi-Head Self-Attention Implementation
Reference: Vaswani et al., "Attention Is All You Need" (NeurIPS 2017)
"""
from __future__ import annotations

import math
from typing import Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor


class MultiHeadSelfAttention(nn.Module):
    """
    Scaled Dot-Product Multi-Head Self-Attention mechanism.

    Implements the attention formula:
        Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

    Args:
        d_model: Model embedding dimension
        n_heads: Number of parallel attention heads
        dropout: Dropout probability for attention weights
        bias: Whether to include bias in linear projections
    """

    def __init__(
        self,
        d_model: int = 512,
        n_heads: int = 8,
        dropout: float = 0.1,
        bias: bool = True,
    ) -> None:
        super().__init__()

        assert d_model % n_heads == 0, \
            f"d_model ({d_model}) must be divisible by n_heads ({n_heads})"

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # Dimension per head
        self.scale = math.sqrt(self.d_k)

        # Fused QKV projection for efficiency
        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=bias)
        self.out_proj = nn.Linear(d_model, d_model, bias=bias)
        self.dropout = nn.Dropout(dropout)

        self._init_weights()

    def _init_weights(self) -> None:
        """Xavier uniform initialization for stable training."""
        nn.init.xavier_uniform_(self.qkv_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        if self.qkv_proj.bias is not None:
            nn.init.zeros_(self.qkv_proj.bias)
            nn.init.zeros_(self.out_proj.bias)

    def forward(
        self,
        x: Tensor,
        mask: Optional[Tensor] = None,
        return_attention: bool = False,
    ) -> Tuple[Tensor, Optional[Tensor]]:
        """
        Forward pass of multi-head self-attention.

        Args:
            x: Input tensor of shape (batch, seq_len, d_model)
            mask: Optional attention mask (batch, 1, 1, seq_len) or (batch, 1, seq_len, seq_len)
            return_attention: Whether to return attention weights

        Returns:
            output: Attended output of shape (batch, seq_len, d_model)
            attn_weights: Attention weights if return_attention=True, else None
        """
        B, L, _ = x.shape

        # Fused QKV projection: (B, L, 3*d_model) -> 3 x (B, n_heads, L, d_k)
        qkv = self.qkv_proj(x).reshape(B, L, 3, self.n_heads, self.d_k)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, n_heads, L, d_k)
        q, k, v = qkv.unbind(0)

        # Scaled dot-product attention: softmax(QK^T / âˆšd_k) V
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale

        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, float("-inf"))

        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)

        # Reshape and project: (B, n_heads, L, d_k) -> (B, L, d_model)
        attn_output = attn_output.transpose(1, 2).reshape(B, L, self.d_model)
        output = self.out_proj(attn_output)

        return output, attn_weights if return_attention else None
```

</details>

<details>
<summary><b>ğŸ® PPO Trainer (Reinforcement Learning)</b></summary>

```python
"""
Proximal Policy Optimization (PPO) Trainer
Reference: Schulman et al., "Proximal Policy Optimization Algorithms" (2017)
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Iterator, Tuple

import torch
import torch.nn as nn
from torch import Tensor
from torch.distributions import Categorical


@dataclass
class PPOConfig:
    """PPO hyperparameters with sensible defaults."""
    gamma: float = 0.99           # Discount factor
    gae_lambda: float = 0.95      # GAE parameter
    clip_epsilon: float = 0.2     # PPO clipping range
    entropy_coef: float = 0.01    # Entropy bonus coefficient
    value_coef: float = 0.5       # Value loss coefficient
    max_grad_norm: float = 0.5    # Gradient clipping threshold
    n_epochs: int = 4             # PPO update epochs
    batch_size: int = 64          # Mini-batch size


class PPOTrainer:
    """
    Production-ready PPO trainer with GAE and gradient clipping.

    Implements the clipped surrogate objective:
        L^CLIP(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]

    where r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)
    """

    def __init__(
        self,
        policy: nn.Module,
        optimizer: torch.optim.Optimizer,
        config: PPOConfig = PPOConfig(),
    ) -> None:
        self.policy = policy
        self.optimizer = optimizer
        self.config = config

    def compute_gae(
        self,
        rewards: Tensor,
        values: Tensor,
        dones: Tensor,
        next_value: Tensor,
    ) -> Tuple[Tensor, Tensor]:
        """
        Compute Generalized Advantage Estimation (GAE).

        GAE(Î³,Î») = Î£_{l=0}^{âˆ} (Î³Î»)^l Î´_{t+l}
        where Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
        """
        T = len(rewards)
        advantages = torch.zeros_like(rewards)
        last_gae = 0.0

        for t in reversed(range(T)):
            next_val = next_value if t == T - 1 else values[t + 1]
            delta = rewards[t] + self.config.gamma * next_val * (1 - dones[t]) - values[t]
            advantages[t] = last_gae = delta + \
                self.config.gamma * self.config.gae_lambda * (1 - dones[t]) * last_gae

        returns = advantages + values
        return advantages, returns

    def update(self, rollout_buffer: Dict[str, Tensor]) -> Dict[str, float]:
        """
        Perform PPO update with clipped objective.

        Returns:
            Dictionary of training metrics
        """
        states = rollout_buffer["states"]
        actions = rollout_buffer["actions"]
        old_log_probs = rollout_buffer["log_probs"]
        advantages = rollout_buffer["advantages"]
        returns = rollout_buffer["returns"]

        # Normalize advantages for stable training
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        metrics = {"policy_loss": 0.0, "value_loss": 0.0, "entropy": 0.0}

        for _ in range(self.config.n_epochs):
            for batch in self._get_batches(len(states)):
                # Forward pass
                logits, values = self.policy(states[batch])
                dist = Categorical(logits=logits)

                new_log_probs = dist.log_prob(actions[batch])
                entropy = dist.entropy().mean()

                # PPO clipped objective
                ratio = torch.exp(new_log_probs - old_log_probs[batch])
                surr1 = ratio * advantages[batch]
                surr2 = torch.clamp(
                    ratio,
                    1 - self.config.clip_epsilon,
                    1 + self.config.clip_epsilon
                ) * advantages[batch]

                policy_loss = -torch.min(surr1, surr2).mean()
                value_loss = nn.functional.mse_loss(values.squeeze(), returns[batch])

                # Combined loss with entropy bonus
                loss = (
                    policy_loss
                    + self.config.value_coef * value_loss
                    - self.config.entropy_coef * entropy
                )

                # Optimization step with gradient clipping
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), self.config.max_grad_norm)
                self.optimizer.step()

                # Accumulate metrics
                metrics["policy_loss"] += policy_loss.item()
                metrics["value_loss"] += value_loss.item()
                metrics["entropy"] += entropy.item()

        # Average metrics
        n_updates = self.config.n_epochs * (len(states) // self.config.batch_size)
        return {k: v / n_updates for k, v in metrics.items()}

    def _get_batches(self, dataset_size: int) -> Iterator[Tensor]:
        """Generate random mini-batch indices."""
        indices = torch.randperm(dataset_size)
        for start in range(0, dataset_size, self.config.batch_size):
            yield indices[start:start + self.config.batch_size]
```

</details>

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%" alt="divider">

<br/>

## ğŸ“š Curriculum

<details open>
<summary><b>ğŸ“˜ 01 - Foundations | æœºå™¨å­¦ä¹ åŸºç¡€</b></summary>
<br/>

> å»ºç«‹åšå®çš„æœºå™¨å­¦ä¹ ç†è®ºåŸºç¡€ï¼ŒæŒæ¡ç»å…¸ç®—æ³•çš„åŸç†ä¸å®ç°

| Topic | Algorithm | Complexity | Key Concepts |
|:------|:----------|:-----------|:-------------|
| Linear Models | OLS, Ridge, Lasso | O(ndÂ²) | Regularization, Bias-Variance |
| Classification | Logistic, SVM | O(nÂ²) ~ O(nÂ³) | Maximum Margin, Kernel Trick |
| Tree Methods | CART, RF, GBDT | O(n log n) | Information Gain, Ensemble |
| Dimensionality | PCA, t-SNE, UMAP | O(nÂ²) ~ O(nÂ³) | Manifold Learning |

**Tech**: ![scikit-learn](https://img.shields.io/badge/Scikit--Learn-F7931E?style=flat-square&logo=scikit-learn&logoColor=white) ![XGBoost](https://img.shields.io/badge/XGBoost-189FDD?style=flat-square&logoColor=white) ![LightGBM](https://img.shields.io/badge/LightGBM-2980B9?style=flat-square&logoColor=white)

</details>

<details>
<summary><b>ğŸ§  02 - Neural Networks | ç¥ç»ç½‘ç»œ</b></summary>
<br/>

> æŒæ¡æ·±åº¦å­¦ä¹ æ ¸å¿ƒæŠ€æœ¯ä¸è®­ç»ƒæ–¹æ³•

| Topic | Techniques | Description |
|:------|:-----------|:------------|
| Initialization | Xavier, He, Orthogonal | æƒé‡åˆå§‹åŒ–ç­–ç•¥ |
| Normalization | BatchNorm, LayerNorm, GroupNorm | å½’ä¸€åŒ–æŠ€æœ¯ |
| Regularization | Dropout, DropConnect, Stochastic Depth | æ­£åˆ™åŒ–æ–¹æ³• |
| Optimization | SGD+Momentum, Adam, AdamW, LAMB | ä¼˜åŒ–ç®—æ³• |

**Tech**: ![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=flat-square&logo=tensorflow&logoColor=white) ![Keras](https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=keras&logoColor=white)

</details>

<details>
<summary><b>ğŸ‘ï¸ 03 - Computer Vision | è®¡ç®—æœºè§†è§‰</b></summary>
<br/>

> ç³»ç»Ÿå­¦ä¹  CNN æ¶æ„æ¼”è¿›ä¸è§†è§‰ä»»åŠ¡

**æ¶æ„æ¼”è¿›**:
```
LeNet (1998) â†’ AlexNet (2012) â†’ VGG (2014) â†’ GoogLeNet (2014)
                                    â†“
ResNet (2015) â†’ DenseNet (2016) â†’ EfficientNet (2019) â†’ ViT (2020)
```

**Tech**: ![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=flat-square&logo=tensorflow&logoColor=white) ![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=flat-square&logo=opencv&logoColor=white)

</details>

<details>
<summary><b>ğŸ“ 04 - Sequence Models | åºåˆ—æ¨¡å‹</b></summary>
<br/>

> æŒæ¡åºåˆ—å»ºæ¨¡ä» RNN åˆ° Transformerï¼Œæ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦æœ¬è´¨

**Scaled Dot-Product Attention** *(Vaswani et al., NeurIPS 2017)*:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

å…¶ä¸­ $Q \in \mathbb{R}^{n \times d_k}$, $K \in \mathbb{R}^{m \times d_k}$, $V \in \mathbb{R}^{m \times d_v}$ã€‚ç¼©æ”¾å› å­ $\sqrt{d_k}$ é˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax æ¢¯åº¦æ¶ˆå¤±ã€‚

**Multi-Head Attention** é€šè¿‡å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´æ•è·ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ï¼š

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

$$\text{where } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

**Tech**: ![HuggingFace](https://img.shields.io/badge/Transformers-FFD21E?style=flat-square&logo=huggingface&logoColor=black) ![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)

</details>

<details>
<summary><b>âš¡ 05~07 - Advanced Modules | è¿›é˜¶æ¨¡å—</b></summary>
<br/>

| Module | Focus | Key Methods |
|:-------|:------|:------------|
| **05 Advanced** | å·¥ç¨‹ä¼˜åŒ– | Mixed Precision, Gradient Checkpointing, Optuna |
| **06 Generative** | ç”Ÿæˆæ¨¡å‹ | VAE, GAN (DCGAN/WGAN/StyleGAN), Diffusion |
| **07 RL** | å¼ºåŒ–å­¦ä¹  | DQN, A2C, PPO, SAC, World Models |

<br/>

**ğŸ® Reinforcement Learning æ ¸å¿ƒå…¬å¼**

**Bellman Optimality Equation** â€” å¼ºåŒ–å­¦ä¹ çš„ç†è®ºåŸºçŸ³ï¼š

$$Q^*(s, a) = \mathbb{E}\left[r + \gamma \max_{a'} Q^*(s', a') \mid s, a\right]$$

**PPO Clipped Objective** *(Schulman et al., 2017)* â€” ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼š

$$L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

å…¶ä¸­ $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ ä¸ºé‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼Œ$\hat{A}_t$ ä¸ºä¼˜åŠ¿å‡½æ•°ä¼°è®¡ã€‚

</details>

<details>
<summary><b>ğŸ† 09 - Practical Projects | å®æˆ˜é¡¹ç›®</b></summary>
<br/>

```
09-practical-projects/
â”œâ”€â”€ ğŸ“Š 01-ml-basics/                    # MLåŸºç¡€é¡¹ç›®
â”‚   â”œâ”€â”€ titanic-survival-xgboost/       #   Titanic ç”Ÿå­˜é¢„æµ‹
â”‚   â””â”€â”€ otto-classification/            #   Otto äº§å“å¤šåˆ†ç±»
â”‚
â”œâ”€â”€ ğŸ‘ï¸ 02-computer-vision/              # CVé¡¹ç›®
â”‚   â””â”€â”€ mnist-cnn/                      #   MNIST æ‰‹å†™æ•°å­—è¯†åˆ«
â”‚
â”œâ”€â”€ ğŸ“ 03-nlp/                          # NLPé¡¹ç›®
â”‚   â”œâ”€â”€ sentiment-analysis-lstm/        #   LSTM æƒ…æ„Ÿåˆ†æ
â”‚   â”œâ”€â”€ transformer-text-classification/#   Transformer æ–‡æœ¬åˆ†ç±»
â”‚   â””â”€â”€ transformer-ner/                #   å‘½åå®ä½“è¯†åˆ«
â”‚
â”œâ”€â”€ ğŸ“ˆ 04-time-series/                  # æ—¶åºé¡¹ç›®
â”‚   â””â”€â”€ temperature-prediction-lstm/    #   æ¸©åº¦é¢„æµ‹
â”‚
â””â”€â”€ ğŸ† 05-kaggle-competitions/          # Kaggle ç«èµ›
    â”œâ”€â”€ Feedback-ELL-1st-Place/         #   ğŸ¥‡ é‡‘ç‰Œæ–¹æ¡ˆ
    â””â”€â”€ RSNA-2023-1st-Place/            #   ğŸ¥‡ é‡‘ç‰Œæ–¹æ¡ˆ
```

</details>

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## ğŸ› ï¸ Tech Stack

<div align="center">

<!-- Skill Icons åŠ¨æ€å±•ç¤º -->
<a href="https://skillicons.dev">
  <img src="https://skillicons.dev/icons?i=python,pytorch,tensorflow,sklearn,opencv,anaconda,jupyter,vscode,git,github,docker,linux&theme=dark&perline=6" alt="Tech Stack" />
</a>

<br/><br/>

<!-- ç‰ˆæœ¬å¾½ç« çŸ©é˜µ -->
<table>
<tr>
<th align="center">ğŸ¤– Deep Learning</th>
<th align="center">ğŸ“Š Data Science</th>
<th align="center">ğŸ”§ Development</th>
</tr>
<tr>
<td align="center">

![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13+-FF6F00?style=flat-square&logo=tensorflow&logoColor=white)
![Keras](https://img.shields.io/badge/Keras-3.x-D00000?style=flat-square&logo=keras&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)
![Transformers](https://img.shields.io/badge/Transformers-4.30+-FFD21E?style=flat-square&logo=huggingface&logoColor=black)

</td>
<td align="center">

![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-1.3+-F7931E?style=flat-square&logo=scikit-learn&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-2.0+-189FDD?style=flat-square&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-2.0+-150458?style=flat-square&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-1.24+-013243?style=flat-square&logo=numpy&logoColor=white)

</td>
<td align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-Lab_4+-F37626?style=flat-square&logo=jupyter&logoColor=white)
![Git](https://img.shields.io/badge/Git-2.40+-F05032?style=flat-square&logo=git&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-24+-2496ED?style=flat-square&logo=docker&logoColor=white)

</td>
</tr>
</table>

</div>

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/zimingttkx/AI-Practices.git
cd AI-Practices

# Create Conda environment
conda create -n ai-practices python=3.10 -y
conda activate ai-practices

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}')"
python -c "import torch; print(f'PyTorch: {torch.__version__}')"

# Launch Jupyter Lab
jupyter lab
```

### Hardware Requirements

| Component | Minimum | Recommended |
|:----------|:--------|:------------|
| **CPU** | 4 cores | 8+ cores |
| **RAM** | 8 GB | 32 GB |
| **GPU** | GTX 1060 | RTX 3080+ |
| **Storage** | 50 GB | 200 GB SSD |

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## ğŸ“Š Results

### Kaggle Competitions

<div align="center">

| Competition | Rank | Medal | Year |
|:------------|:----:|:-----:|:----:|
| Feedback Prize - ELL | **Top 1%** | ğŸ¥‡ Gold | 2023 |
| RSNA Abdominal Trauma | **Top 1%** | ğŸ¥‡ Gold | 2023 |
| American Express Default | Top 5% | ğŸ¥ˆ Silver | 2022 |
| RSNA Lumbar Spine | Top 10% | ğŸ¥‰ Bronze | 2024 |

</div>

### Model Benchmarks

<div align="center">

| Model | Dataset | Top-1 Acc | Params | FLOPs |
|:------|:--------|:---------:|:------:|:-----:|
| ResNet-50 | ImageNet | 76.1% | 25.6M | 4.1G |
| EfficientNet-B0 | ImageNet | 77.1% | 5.3M | 0.4G |
| ViT-B/16 | ImageNet | 77.9% | 86M | 17.6G |
| BERT-base | SST-2 | 93.2% | 110M | - |

</div>

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## ğŸ“„ Citation

If this project helps your research, please cite:

```bibtex
@misc{ai-practices2024,
  author       = {zimingttkx},
  title        = {AI-Practices: A Systematic Approach to AI Research and Engineering},
  year         = {2024},
  publisher    = {GitHub},
  howpublished = {\url{https://github.com/zimingttkx/AI-Practices}}
}
```

<br/>

## ğŸ“œ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) for details.

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

## â­ Star History

<div align="center">

<a href="https://star-history.com/#zimingttkx/AI-Practices&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=zimingttkx/AI-Practices&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=zimingttkx/AI-Practices&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=zimingttkx/AI-Practices&type=Date" width="700" />
  </picture>
</a>

<br/><br/>

<!-- åŠ¨æ€Starè¶‹åŠ¿ -->
<a href="https://github.com/zimingttkx/AI-Practices/stargazers">
  <img src="https://reporoster.com/stars/dark/zimingttkx/AI-Practices" alt="Stargazers" width="600"/>
</a>

</div>

<br/>

<!-- åŠ¨æ€åˆ†éš”çº¿ -->
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

<br/>

<div align="center">

<!-- åŠ¨æ€é¡µè„š -->
<a href="https://github.com/zimingttkx/AI-Practices">
  <img src="https://readme-typing-svg.demolab.com?font=SF+Pro+Text&weight=300&size=12&duration=4000&pause=2000&color=8E8E93&center=true&vCenter=true&repeat=true&width=300&height=20&lines=Made+with+%E2%9D%A4%EF%B8%8F+for+the+AI+Community" alt="Footer" />
</a>

<br/><br/>

<!-- åŠ¨æ€æ¸å˜é¡µè„š -->
<img src="https://capsule-render.vercel.app/api?type=waving&color=0:007AFF,50:5856D6,100:AF52DE&height=100&section=footer" width="100%"/>

</div>
