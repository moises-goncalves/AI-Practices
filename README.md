<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=0:1a1a2e,50:16213e,100:0f3460&height=200&section=header&text=AI-Practices&fontSize=72&fontColor=e94560&fontAlignY=35&desc=A%20Systematic%20Approach%20to%20Artificial%20Intelligence%20Research%20%26%20Engineering&descSize=18&descAlignY=55&descAlign=50&animation=fadeIn" width="100%"/>

<br/>

<!-- Technical Badges -->
<p>
<img src="https://img.shields.io/badge/Python-3.10+-blue?style=flat&logo=python&logoColor=white" alt="Python"/>
<img src="https://img.shields.io/badge/TensorFlow-2.13+-orange?style=flat&logo=tensorflow&logoColor=white" alt="TensorFlow"/>
<img src="https://img.shields.io/badge/PyTorch-2.0+-red?style=flat&logo=pytorch&logoColor=white" alt="PyTorch"/>
<img src="https://img.shields.io/badge/License-MIT-green?style=flat" alt="License"/>
<img src="https://img.shields.io/github/last-commit/zimingttkx/AI-Practices?style=flat&color=purple" alt="Last Commit"/>
</p>

<p>
<a href="https://zimingttkx.github.io/AI-Practices/"><img src="https://img.shields.io/badge/Documentation-Online-blue?style=for-the-badge" alt="Docs"/></a>
<a href="./README_EN.md"><img src="https://img.shields.io/badge/English-README-gray?style=for-the-badge" alt="English"/></a>
</p>

</div>

---

## Abstract

**AI-Practices** æ˜¯ä¸€ä¸ªç³»ç»ŸåŒ–çš„äººå·¥æ™ºèƒ½å­¦ä¹ ä¸ç ”ç©¶å¹³å°ï¼Œé‡‡ç”¨ **æ¸è¿›å¼å­¦ä¹ æ¡†æ¶ (Progressive Learning Framework, PLF)** æ–¹æ³•è®ºï¼Œæ„å»ºäº†ä»ç»å…¸æœºå™¨å­¦ä¹ åˆ°å‰æ²¿æ·±åº¦å­¦ä¹ çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚æœ¬é¡¹ç›®æ¶µç›– **113+ å¯å¤ç°å®éªŒ**ã€**9 ä¸ªæ ¸å¿ƒæ¨¡å—** å’Œ **19 ä¸ªç«¯åˆ°ç«¯é¡¹ç›®**ï¼ŒåŒ…æ‹¬å¤šä¸ª **Kaggle ç«èµ›é‡‘ç‰Œæ–¹æ¡ˆ**ã€‚é¡¹ç›®éµå¾ªè½¯ä»¶å·¥ç¨‹æœ€ä½³å®è·µï¼Œä»£ç æ€»é‡è¶…è¿‡ **149,000 è¡Œ**ï¼Œå…¨éƒ¨ç¬¦åˆ PEP8 è§„èŒƒã€‚

**å…³é”®è¯**: æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , è®¡ç®—æœºè§†è§‰, è‡ªç„¶è¯­è¨€å¤„ç†, å¼ºåŒ–å­¦ä¹ , ç”Ÿæˆå¼æ¨¡å‹

---

## Table of Contents

- [1. Introduction](#1-introduction)
- [2. Methodology](#2-methodology)
- [3. System Architecture](#3-system-architecture)
- [4. Modules Overview](#4-modules-overview)
- [5. Experimental Setup](#5-experimental-setup)
- [6. Results & Benchmarks](#6-results--benchmarks)
- [7. Quick Start](#7-quick-start)
- [8. Citation](#8-citation)
- [9. License](#9-license)

---

## 1. Introduction

### 1.1 Research Background

éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ„å»ºä¸€ä¸ªç³»ç»ŸåŒ–ã€å¯å¤ç°çš„å­¦ä¹ å¹³å°å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰å­¦ä¹ èµ„æºå¾€å¾€å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

| é—®é¢˜ | æè¿° |
|:-----|:-----|
| **ç¢ç‰‡åŒ–** | çŸ¥è¯†ç‚¹åˆ†æ•£ï¼Œç¼ºä¹ç³»ç»Ÿæ€§ |
| **ç†è®ºå®è·µè„±èŠ‚** | ç†è®ºè®²è§£ä¸ä»£ç å®ç°åˆ†ç¦» |
| **å¯å¤ç°æ€§å·®** | ç¼ºä¹å®Œæ•´çš„å®éªŒç¯å¢ƒé…ç½® |
| **å·¥ç¨‹åŒ–ä¸è¶³** | å¿½è§†è½¯ä»¶å·¥ç¨‹æœ€ä½³å®è·µ |

### 1.2 Objectives

æœ¬é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ª **ç†è®ºé©±åŠ¨ã€å®è·µä¸ºæœ¬ã€å·¥ç¨‹å¯¼å‘** çš„ AI å­¦ä¹ å¹³å°ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AI-Practices                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â—† 113+ Reproducible Experiments                                â”‚
â”‚  â—† 9 Core Learning Modules                                      â”‚
â”‚  â—† 19 End-to-End Projects                                       â”‚
â”‚  â—† 149,000+ Lines of Production-Quality Code                    â”‚
â”‚  â—† Multiple Kaggle Gold Medal Solutions                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Contributions

æœ¬é¡¹ç›®çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š

1. **æ¸è¿›å¼å­¦ä¹ æ¡†æ¶ (PLF)**: æå‡ºå››é˜¶æ®µå­¦ä¹ æ–¹æ³•è®ºï¼Œå®ç°ä»ç†è®ºåˆ°å®è·µçš„å¹³æ»‘è¿‡æ¸¡
2. **æ¨¡å—åŒ–è¯¾ç¨‹ä½“ç³»**: è®¾è®¡ 9 ä¸ªç›¸äº’å…³è”çš„å­¦ä¹ æ¨¡å—ï¼Œè¦†ç›– AI æ ¸å¿ƒæŠ€æœ¯æ ˆ
3. **å¯å¤ç°å®éªŒé›†**: æä¾› 113+ ä¸ªå®Œæ•´çš„ Jupyter å®éªŒï¼Œå«è¯¦ç»†æ³¨é‡Š
4. **å·¥ç¨‹åŒ–é¡¹ç›®æ¨¡æ¿**: å»ºç«‹æ ‡å‡†åŒ–çš„é¡¹ç›®ç»“æ„å’Œä»£ç è§„èŒƒ
5. **ç«èµ›çº§è§£å†³æ–¹æ¡ˆ**: åŒ…å«å¤šä¸ª Kaggle Top 1% é‡‘ç‰Œæ–¹æ¡ˆ

---

## 2. Methodology

### 2.1 Progressive Learning Framework (PLF)

æœ¬é¡¹ç›®é‡‡ç”¨å››é˜¶æ®µæ¸è¿›å¼å­¦ä¹ æ¡†æ¶ï¼š

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Progressive Learning Framework   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼               â–¼               â–¼             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Theory  â”‚  â”‚  Impl   â”‚    â”‚Frameworkâ”‚    â”‚Practice â”‚    â”‚  Eval   â”‚
   â”‚  First  â”‚â”€â–¶â”‚  From   â”‚â”€â”€â”€â–¶â”‚ Master  â”‚â”€â”€â”€â–¶â”‚ Project â”‚â”€â”€â”€â–¶â”‚  Kaggle â”‚
   â”‚         â”‚  â”‚ Scratch â”‚    â”‚         â”‚    â”‚         â”‚    â”‚         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚              â”‚              â”‚              â”‚
       â–¼             â–¼              â–¼              â–¼              â–¼
   æ•°å­¦åŸç†      æ ¸å¿ƒç®—æ³•       TensorFlow      ç«¯åˆ°ç«¯é¡¹ç›®      ç«èµ›éªŒè¯
   ç®—æ³•ç†è®º      åº•å±‚å®ç°       PyTorch         çœŸå®åœºæ™¯        æ’åæŒ‡æ ‡
```

### 2.2 Learning Principles

| Phase | åŸåˆ™ | æ–¹æ³• | äº§å‡º |
|:-----:|:-----|:-----|:-----|
| **â… ** | Theory First | æ•°å­¦æ¨å¯¼ + ç®—æ³•åˆ†æ | ç†è®ºç¬”è®° |
| **â…¡** | Implementation | NumPy ä»é›¶å®ç° | æ ¸å¿ƒä»£ç  |
| **â…¢** | Framework | TensorFlow/PyTorch | å·¥ç¨‹ä»£ç  |
| **â…£** | Practice | çœŸå®é¡¹ç›® + ç«èµ› | å®Œæ•´æ–¹æ¡ˆ |

---

## 3. System Architecture

### 3.1 Module Dependencies

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#1a1a2e', 'primaryTextColor': '#eee', 'primaryBorderColor': '#e94560', 'lineColor': '#e94560', 'secondaryColor': '#16213e'}}}%%
graph TB
    subgraph Phase1["Phase 1: Foundation"]
        A[01-Foundations<br/>æœºå™¨å­¦ä¹ ç†è®ºåŸºç¡€]
    end

    subgraph Phase2["Phase 2: Deep Learning Core"]
        B[02-Neural Networks<br/>æ·±åº¦å­¦ä¹ åŸºçŸ³]
        C[03-Computer Vision<br/>è§†è§‰æ„ŸçŸ¥]
        D[04-Sequence Models<br/>åºåˆ—å»ºæ¨¡]
    end

    subgraph Phase3["Phase 3: Advanced"]
        E[05-Advanced Topics<br/>å·¥ç¨‹ä¼˜åŒ–]
        F[06-Generative<br/>ç”Ÿæˆæ¨¡å‹]
        G[07-RL<br/>å¼ºåŒ–å­¦ä¹ ]
    end

    subgraph Phase4["Phase 4: Application"]
        H[09-Projects<br/>å®æˆ˜é¡¹ç›®]
    end

    A --> B
    B --> C & D
    C & D --> E
    E --> F & G
    F & G --> H
    C & D --> H
```

### 3.2 Directory Structure

```
AI-Practices/
â”‚
â”œâ”€â”€ 01-foundations/                 # æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º
â”‚   â”œâ”€â”€ training-models/            #   ä¼˜åŒ–æ–¹æ³•: SGD, Adam, L-BFGS
â”‚   â”œâ”€â”€ classification/             #   åˆ†ç±»ç®—æ³•: LR, SVM, Decision Tree
â”‚   â”œâ”€â”€ ensemble-learning/          #   é›†æˆæ–¹æ³•: Bagging, Boosting, Stacking
â”‚   â””â”€â”€ unsupervised-learning/      #   æ— ç›‘ç£: Clustering, Dimensionality Reduction
â”‚
â”œâ”€â”€ 02-neural-networks/             # ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ 
â”‚   â”œâ”€â”€ keras-introduction/         #   æ¡†æ¶å…¥é—¨: Sequential, Functional API
â”‚   â”œâ”€â”€ training-deep-networks/     #   è®­ç»ƒæŠ€å·§: BatchNorm, Dropout, Residual
â”‚   â””â”€â”€ custom-models/              #   è‡ªå®šä¹‰: Layer, Loss, Training Loop
â”‚
â”œâ”€â”€ 03-computer-vision/             # è®¡ç®—æœºè§†è§‰
â”‚   â”œâ”€â”€ cnn-architectures/          #   æ¶æ„æ¼”è¿›: LeNet â†’ ResNet â†’ EfficientNet
â”‚   â”œâ”€â”€ transfer-learning/          #   è¿ç§»å­¦ä¹ : Feature Extraction, Fine-tuning
â”‚   â””â”€â”€ model-interpretability/     #   å¯è§£é‡Šæ€§: Grad-CAM, SHAP
â”‚
â”œâ”€â”€ 04-sequence-models/             # åºåˆ—æ¨¡å‹ä¸NLP
â”‚   â”œâ”€â”€ rnn-lstm-gru/               #   å¾ªç¯ç½‘ç»œ: Vanishing Gradient, Gating
â”‚   â”œâ”€â”€ attention-transformer/      #   æ³¨æ„åŠ›æœºåˆ¶: Self-Attention, Multi-Head
â”‚   â””â”€â”€ pretrained-models/          #   é¢„è®­ç»ƒ: BERT, GPT, T5
â”‚
â”œâ”€â”€ 05-advanced-topics/             # é«˜çº§ä¸“é¢˜
â”‚   â”œâ”€â”€ hyperparameter-tuning/      #   è¶…å‚ä¼˜åŒ–: Optuna, Ray Tune
â”‚   â”œâ”€â”€ distributed-training/       #   åˆ†å¸ƒå¼: Data Parallel, Model Parallel
â”‚   â””â”€â”€ model-deployment/           #   éƒ¨ç½²: TensorRT, ONNX, TFLite
â”‚
â”œâ”€â”€ 06-generative-models/           # ç”Ÿæˆå¼æ¨¡å‹
â”‚   â”œâ”€â”€ variational-autoencoders/   #   VAE: Latent Space, Reparameterization
â”‚   â”œâ”€â”€ generative-adversarial/     #   GAN: DCGAN, WGAN, StyleGAN
â”‚   â””â”€â”€ diffusion-models/           #   æ‰©æ•£: DDPM, Stable Diffusion
â”‚
â”œâ”€â”€ 07-reinforcement-learning/      # å¼ºåŒ–å­¦ä¹ 
â”‚   â”œâ”€â”€ value-based/                #   å€¼æ–¹æ³•: Q-Learning, DQN, Double DQN
â”‚   â”œâ”€â”€ policy-based/               #   ç­–ç•¥æ–¹æ³•: REINFORCE, PPO, SAC
â”‚   â””â”€â”€ model-based/                #   æ¨¡å‹æ–¹æ³•: World Models, MuZero
â”‚
â”œâ”€â”€ 08-theory-notes/                # ç†è®ºå‚è€ƒæ‰‹å†Œ
â”‚   â”œâ”€â”€ mathematical-foundations/   #   æ•°å­¦åŸºç¡€: Linear Algebra, Probability
â”‚   â”œâ”€â”€ optimization-theory/        #   ä¼˜åŒ–ç†è®º: Convex, Non-convex
â”‚   â””â”€â”€ information-theory/         #   ä¿¡æ¯è®º: Entropy, KL-Divergence
â”‚
â”œâ”€â”€ 09-practical-projects/          # å®æˆ˜é¡¹ç›®
â”‚   â”œâ”€â”€ kaggle-competitions/        #   ç«èµ›æ–¹æ¡ˆ: Gold Medal Solutions
â”‚   â””â”€â”€ industry-applications/      #   å·¥ä¸šåº”ç”¨: End-to-End Pipelines
â”‚
â””â”€â”€ utils/                          # å·¥å…·åº“
    â”œâ”€â”€ data/                       #   æ•°æ®å¤„ç†
    â”œâ”€â”€ visualization/              #   å¯è§†åŒ–
    â””â”€â”€ metrics/                    #   è¯„ä¼°æŒ‡æ ‡
```

---

## 4. Modules Overview

### 4.1 Foundation Module (01)

> **ç›®æ ‡**: å»ºç«‹æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€ä¸ç»å…¸ç®—æ³•ç†è®º

| Topic | Algorithm | Complexity | Key Concepts |
|:------|:----------|:-----------|:-------------|
| Linear Models | OLS, Ridge, Lasso | O(ndÂ²) | Regularization, Bias-Variance |
| Classification | Logistic, SVM | O(nÂ²) ~ O(nÂ³) | Maximum Margin, Kernel Trick |
| Tree Methods | CART, RF, GBDT | O(n log n) | Information Gain, Ensemble |
| Dimensionality | PCA, t-SNE, UMAP | O(nÂ²) ~ O(nÂ³) | Manifold Learning |

### 4.2 Neural Networks Module (02)

> **ç›®æ ‡**: æŒæ¡æ·±åº¦å­¦ä¹ æ ¸å¿ƒæŠ€æœ¯ä¸è®­ç»ƒæ–¹æ³•

**æ ¸å¿ƒå†…å®¹**:

$$\text{Forward: } \mathbf{h}^{(l)} = \sigma(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})$$

$$\text{Backward: } \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(l)}} \cdot \frac{\partial \mathbf{h}^{(l)}}{\partial \mathbf{W}^{(l)}}$$

| Topic | Techniques |
|:------|:-----------|
| Initialization | Xavier, He, Orthogonal |
| Normalization | BatchNorm, LayerNorm, GroupNorm |
| Regularization | Dropout, DropConnect, Stochastic Depth |
| Optimization | SGD+Momentum, Adam, AdamW, LAMB |

### 4.3 Computer Vision Module (03)

> **ç›®æ ‡**: ç³»ç»Ÿå­¦ä¹  CNN æ¶æ„æ¼”è¿›ä¸è§†è§‰ä»»åŠ¡

**æ¶æ„æ¼”è¿›**:

```
LeNet (1998) â†’ AlexNet (2012) â†’ VGG (2014) â†’ GoogLeNet (2014)
                                    â†“
ResNet (2015) â†’ DenseNet (2016) â†’ EfficientNet (2019) â†’ ViT (2020)
```

### 4.4 Sequence Models Module (04)

> **ç›®æ ‡**: æŒæ¡åºåˆ—å»ºæ¨¡ä» RNN åˆ° Transformer

**Attention Mechanism**:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### 4.5 Advanced Topics Module (05-07)

| Module | Focus | Key Methods |
|:-------|:------|:------------|
| **05-Advanced** | å·¥ç¨‹ä¼˜åŒ– | Mixed Precision, Gradient Checkpointing |
| **06-Generative** | ç”Ÿæˆæ¨¡å‹ | VAE, GAN, Diffusion |
| **07-RL** | å¼ºåŒ–å­¦ä¹  | DQN, PPO, SAC |

---

## 5. Experimental Setup

### 5.1 Environment Configuration

```bash
# Clone repository
git clone https://github.com/zimingttkx/AI-Practices.git
cd AI-Practices

# Create environment
conda create -n ai-practices python=3.10 -y
conda activate ai-practices

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "import tensorflow as tf; print(f'TF: {tf.__version__}')"
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
```

### 5.2 Hardware Requirements

| Component | Minimum | Recommended |
|:----------|:--------|:------------|
| CPU | 4 cores | 8+ cores |
| RAM | 8 GB | 32 GB |
| GPU | GTX 1060 | RTX 3080+ |
| Storage | 50 GB | 200 GB SSD |

### 5.3 Software Stack

<table>
<tr>
<th>Category</th>
<th>Package</th>
<th>Version</th>
</tr>
<tr>
<td rowspan="4"><b>Deep Learning</b></td>
<td>TensorFlow</td><td>â‰¥2.13.0</td>
</tr>
<tr><td>PyTorch</td><td>â‰¥2.0.0</td></tr>
<tr><td>Keras</td><td>â‰¥3.0.0</td></tr>
<tr><td>Transformers</td><td>â‰¥4.30.0</td></tr>
<tr>
<td rowspan="3"><b>Machine Learning</b></td>
<td>Scikit-learn</td><td>â‰¥1.3.0</td>
</tr>
<tr><td>XGBoost</td><td>â‰¥2.0.0</td></tr>
<tr><td>LightGBM</td><td>â‰¥4.0.0</td></tr>
<tr>
<td rowspan="3"><b>Data Processing</b></td>
<td>NumPy</td><td>â‰¥1.24.0</td>
</tr>
<tr><td>Pandas</td><td>â‰¥2.0.0</td></tr>
<tr><td>OpenCV</td><td>â‰¥4.8.0</td></tr>
</table>

---

## 6. Results & Benchmarks

### 6.1 Kaggle Competition Results

| Competition | Rank | Medal | Year |
|:------------|:----:|:-----:|:----:|
| Feedback Prize - ELL | **Top 1%** | ğŸ¥‡ Gold | 2023 |
| RSNA Abdominal Trauma | **Top 1%** | ğŸ¥‡ Gold | 2023 |
| American Express Default | Top 5% | ğŸ¥ˆ Silver | 2022 |
| RSNA Lumbar Spine | Top 10% | ğŸ¥‰ Bronze | 2024 |

### 6.2 Model Benchmarks

#### Computer Vision

| Model | Dataset | Top-1 Acc | Params | FLOPs |
|:------|:--------|:---------:|:------:|:-----:|
| ResNet-50 | ImageNet | 76.1% | 25.6M | 4.1G |
| EfficientNet-B0 | ImageNet | 77.1% | 5.3M | 0.4G |
| ViT-B/16 | ImageNet | 77.9% | 86M | 17.6G |

#### Natural Language Processing

| Model | Task | Metric | Score |
|:------|:-----|:------:|:-----:|
| BERT-base | SST-2 | Accuracy | 93.2% |
| RoBERTa | MNLI | Accuracy | 87.6% |
| T5-base | SQuAD | F1 | 88.9 |

---

## 7. Quick Start

### 7.1 Run Your First Experiment

```bash
# Navigate to project
cd 09-practical-projects/02-computer-vision/01-mnist-cnn

# Train model
python src/train.py --epochs 20 --batch_size 64

# Evaluate
python src/evaluate.py --checkpoint best_model.pt
```

**Expected Output**:
```
Epoch 20/20 - loss: 0.0234 - accuracy: 99.21%
Test Accuracy: 99.12% | F1-Score: 0.991
```

### 7.2 Launch Jupyter Lab

```bash
jupyter lab --port=8888
# Open browser: http://localhost:8888
```

---

## 8. Citation

If this project contributes to your research, please cite:

```bibtex
@misc{ai-practices2024,
  author       = {zimingttkx},
  title        = {{AI-Practices}: A Systematic Approach to Artificial Intelligence Research and Engineering},
  year         = {2024},
  publisher    = {GitHub},
  howpublished = {\url{https://github.com/zimingttkx/AI-Practices}},
  note         = {Accessed: 2024}
}
```

---

## 9. License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=0:1a1a2e,50:16213e,100:0f3460&height=100&section=footer" width="100%"/>

<sub>
<b>AI-Practices</b> â€” A Systematic Approach to AI Research & Engineering<br/>
Copyright Â© 2024 | <a href="https://github.com/zimingttkx/AI-Practices">GitHub Repository</a>
</sub>

</div>
