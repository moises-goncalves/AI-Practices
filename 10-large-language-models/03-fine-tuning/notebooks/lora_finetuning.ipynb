{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA 微调实战\n",
    "\n",
    "**SOTA 教育标准实现** | 参数高效微调技术\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解LoRA低秩分解的数学原理\n",
    "2. 掌握LoRA层的实现方法\n",
    "3. 学会将LoRA应用到预训练模型\n",
    "4. 实现权重合并与保存\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [环境配置](#1-环境配置)\n",
    "2. [LoRA原理](#2-lora原理)\n",
    "3. [LoRA实现](#3-lora实现)\n",
    "4. [应用到模型](#4-应用到模型)\n",
    "5. [权重合并](#5-权重合并)\n",
    "6. [验证测试](#6-验证测试)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import math\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. LoRA原理\n",
    "\n",
    "### 2.1 核心思想\n",
    "\n",
    "LoRA用低秩分解近似权重更新：\n",
    "\n",
    "$$W' = W + \\Delta W = W + BA$$\n",
    "\n",
    "其中：\n",
    "- $W \\in \\mathbb{R}^{d \\times k}$: 原始权重（冻结）\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$: 低秩矩阵\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$: 低秩矩阵\n",
    "- $r \\ll \\min(d, k)$: 秩\n",
    "\n",
    "### 2.2 参数量对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数量对比分析\n",
    "d, k = 4096, 4096\n",
    "r_values = [4, 8, 16, 32, 64]\n",
    "\n",
    "print(f'原始参数量: {d * k:,}')\n",
    "print('\\nLoRA参数量对比:')\n",
    "print('-' * 40)\n",
    "for r in r_values:\n",
    "    lora_params = d * r + r * k\n",
    "    ratio = lora_params / (d * k) * 100\n",
    "    print(f'r={r:2d}: {lora_params:>8,} ({ratio:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LoRA实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"LoRA线性层。\n",
    "    \n",
    "    将原始线性层包装，添加低秩适配器。\n",
    "    \n",
    "    数学原理:\n",
    "        h = Wx + BAx * (alpha/r)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, original_layer: nn.Linear, r: int = 8, \n",
    "                 alpha: int = 16, dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # 冻结原始权重\n",
    "        original_layer.weight.requires_grad = False\n",
    "        if original_layer.bias is not None:\n",
    "            original_layer.bias.requires_grad = False\n",
    "        \n",
    "        # LoRA矩阵\n",
    "        self.lora_A = nn.Parameter(torch.zeros(r, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # 初始化: A用kaiming，B用零\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        self.merged = False\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.merged:\n",
    "            return self.original_layer(x)\n",
    "        # 原始输出 + LoRA增量\n",
    "        result = self.original_layer(x)\n",
    "        lora_out = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return result + lora_out * self.scaling\n",
    "    \n",
    "    def merge(self) -> None:\n",
    "        \"\"\"将LoRA权重合并到原始权重。\"\"\"\n",
    "        if not self.merged:\n",
    "            self.original_layer.weight.data += (self.lora_B @ self.lora_A * self.scaling)\n",
    "            self.merged = True\n",
    "    \n",
    "    def unmerge(self) -> None:\n",
    "        \"\"\"从原始权重中移除LoRA权重。\"\"\"\n",
    "        if self.merged:\n",
    "            self.original_layer.weight.data -= (self.lora_B @ self.lora_A * self.scaling)\n",
    "            self.merged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证LoRA层\n",
    "linear = nn.Linear(768, 768)\n",
    "lora_linear = LoRALinear(linear, r=8, alpha=16)\n",
    "\n",
    "x = torch.randn(2, 10, 768)\n",
    "out = lora_linear(x)\n",
    "\n",
    "trainable = sum(p.numel() for p in lora_linear.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in lora_linear.parameters())\n",
    "\n",
    "print(f'输出形状: {out.shape}')\n",
    "print(f'可训练参数: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)')\n",
    "print(f'✓ LoRA层测试通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 应用到模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model: nn.Module, target_modules: list, \n",
    "               r: int = 8, alpha: int = 16) -> nn.Module:\n",
    "    \"\"\"将LoRA应用到模型的指定模块。\n",
    "    \n",
    "    Args:\n",
    "        model: 原始模型\n",
    "        target_modules: 目标模块名称列表\n",
    "        r: LoRA秩\n",
    "        alpha: 缩放因子\n",
    "    Returns:\n",
    "        应用了LoRA的模型\n",
    "    \"\"\"\n",
    "    modules_to_modify = {}\n",
    "    \n",
    "    # 查找目标模块\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            for target in target_modules:\n",
    "                if re.search(target, name):\n",
    "                    modules_to_modify[name] = module\n",
    "                    break\n",
    "    \n",
    "    # 替换为LoRA层\n",
    "    for name, module in modules_to_modify.items():\n",
    "        parts = name.rsplit('.', 1)\n",
    "        if len(parts) == 2:\n",
    "            parent = model.get_submodule(parts[0])\n",
    "            attr = parts[1]\n",
    "        else:\n",
    "            parent = model\n",
    "            attr = name\n",
    "        setattr(parent, attr, LoRALinear(module, r=r, alpha=alpha))\n",
    "    \n",
    "    # 冻结非LoRA参数\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_' not in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：简单Transformer模型\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 768) -> None:\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "# 应用LoRA\n",
    "model = SimpleTransformer()\n",
    "model = apply_lora(model, ['q_proj', 'v_proj'], r=8, alpha=16)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'应用LoRA后: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 权重合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora(model: nn.Module) -> None:\n",
    "    \"\"\"合并所有LoRA权重到原始权重。\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            module.merge()\n",
    "\n",
    "def save_lora(model: nn.Module, path: str) -> None:\n",
    "    \"\"\"保存LoRA权重。\"\"\"\n",
    "    lora_state = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_' in name:\n",
    "            lora_state[name] = param.data\n",
    "    torch.save(lora_state, path)\n",
    "    print(f'已保存 {len(lora_state)} 个LoRA参数')\n",
    "\n",
    "# 保存LoRA权重\n",
    "save_lora(model, '/tmp/lora_weights.pt')\n",
    "\n",
    "# 合并权重\n",
    "merge_lora(model)\n",
    "print('✓ LoRA权重已合并')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 验证测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lora_equivalence() -> None:\n",
    "    \"\"\"验证合并前后输出一致性。\"\"\"\n",
    "    linear = nn.Linear(64, 64)\n",
    "    lora = LoRALinear(linear, r=4, alpha=8)\n",
    "    \n",
    "    x = torch.randn(1, 10, 64)\n",
    "    \n",
    "    # 合并前输出\n",
    "    out_before = lora(x)\n",
    "    \n",
    "    # 合并后输出\n",
    "    lora.merge()\n",
    "    out_after = lora(x)\n",
    "    \n",
    "    diff = (out_before - out_after).abs().max().item()\n",
    "    assert diff < 1e-5, f'合并前后输出差异过大: {diff}'\n",
    "    print(f'合并前后最大差异: {diff:.2e}')\n",
    "    print(f'✓ test_lora_equivalence 通过')\n",
    "\n",
    "test_lora_equivalence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "| 特性 | 说明 |\n",
    "|:-----|:-----|\n",
    "| **参数高效** | 只训练0.1-1%参数 |\n",
    "| **无推理开销** | 权重可合并 |\n",
    "| **易于切换** | 可加载不同适配器 |\n",
    "| **效果接近全量微调** | 性能损失小 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
