{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer 架构详解\n",
        "\n",
        "**SOTA 教育标准** | 包含 Self-Attention、Multi-Head、FFN、LayerNorm\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 核心组件\n",
        "\n",
        "| 组件 | 公式 | 功能 |\n",
        "|:-----|:-----|:-----|\n",
        "| **Self-Attn** | softmax(QK^T/√d_k)V | 捕捉依赖 |\n",
        "| **FFN** | ReLU(xW_1)W_2 | 非线性 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. 注意力配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AttentionConfig:\n",
        "    \"\"\"注意力配置。\"\"\"\n",
        "    d_model: int = 512\n",
        "    n_heads: int = 8\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    @property\n",
        "    def d_k(self) -> int:\n",
        "        return self.d_model // self.n_heads\n",
        "\n",
        "\n",
        "config = AttentionConfig()\n",
        "print(f\"d_model={config.d_model}, n_heads={config.n_heads}, d_k={config.d_k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 缩放点积注意力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"缩放点积注意力。\n",
        "    \n",
        "    Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_k: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Q: Tensor, K: Tensor, V: Tensor, mask: Optional[Tensor] = None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
        "        return torch.matmul(attn, V), attn\n",
        "\n",
        "\n",
        "# 测试\n",
        "attn = ScaledDotProductAttention(64)\n",
        "Q = K = V = torch.randn(2, 8, 10, 64)\n",
        "out, weights = attn(Q, K, V)\n",
        "print(f\"输出: {out.shape}, 权重: {weights.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. 多头注意力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"多头注意力。\"\"\"\n",
        "\n",
        "    def __init__(self, config: AttentionConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_k = config.d_k\n",
        "        \n",
        "        self.W_q = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.W_k = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.W_v = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.W_o = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.attention = ScaledDotProductAttention(config.d_k, config.dropout)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None):\n",
        "        B = query.size(0)\n",
        "        Q = self.W_q(query).view(B, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(B, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(B, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        x, attn = self.attention(Q, K, V, mask)\n",
        "        x = x.transpose(1, 2).contiguous().view(B, -1, self.n_heads * self.d_k)\n",
        "        return self.W_o(x), attn\n",
        "\n",
        "\n",
        "# 测试\n",
        "mha = MultiHeadAttention(config)\n",
        "x = torch.randn(2, 10, 512)\n",
        "out, _ = mha(x, x, x)\n",
        "print(f\"MHA 输出: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. FFN 与 Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionwiseFFN(nn.Module):\n",
        "    \"\"\"前馈网络: FFN(x) = ReLU(xW_1)W_2\"\"\"\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff)\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Transformer 编码器层。\"\"\"\n",
        "    def __init__(self, config: AttentionConfig, d_ff: int = 2048):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ffn = PositionwiseFFN(config.d_model, d_ff, config.dropout)\n",
        "        self.norm1 = nn.LayerNorm(config.d_model)\n",
        "        self.norm2 = nn.LayerNorm(config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
        "        attn_out, _ = self.attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "# 测试\n",
        "layer = TransformerEncoderLayer(config)\n",
        "out = layer(x)\n",
        "print(f\"Encoder Layer 输出: {out.shape}\")\n",
        "print(f\"参数量: {sum(p.numel() for p in layer.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. 可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention():\n",
        "    \"\"\"可视化注意力模式。\"\"\"\n",
        "    tokens = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
        "    n = len(tokens)\n",
        "    \n",
        "    patterns = [\n",
        "        (\"Local\", lambda i,j: 1 if abs(i-j)<=1 else 0.1),\n",
        "        (\"Global\", lambda i,j: 1),\n",
        "        (\"Causal\", lambda i,j: 1 if j<=i else 0),\n",
        "        (\"Sparse\", lambda i,j: 1 if i==j or j==0 else 0.1),\n",
        "    ]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
        "    for ax, (name, fn) in zip(axes, patterns):\n",
        "        attn = np.array([[fn(i,j) for j in range(n)] for i in range(n)])\n",
        "        attn = attn / attn.sum(axis=1, keepdims=True)\n",
        "        ax.imshow(attn, cmap='Blues')\n",
        "        ax.set_xticks(range(n))\n",
        "        ax.set_yticks(range(n))\n",
        "        ax.set_xticklabels(tokens, rotation=45)\n",
        "        ax.set_yticklabels(tokens)\n",
        "        ax.set_title(name)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. 总结\n",
        "\n",
        "| 组件 | 参数占比 | 功能 |\n",
        "|:-----|:--------:|:-----|\n",
        "| **Self-Attn** | ~33% | 捕捉依赖 |\n",
        "| **FFN** | ~67% | 非线性变换 |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
