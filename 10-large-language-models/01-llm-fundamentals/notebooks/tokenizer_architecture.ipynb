{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 分词器原理 (Tokenizer Architecture)\n",
        "\n",
        "**SOTA 教育标准** | 包含 BPE、WordPiece、SentencePiece 详解\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 分词器概述\n",
        "\n",
        "| 方法 | 描述 | 使用模型 |\n",
        "|:-----|:-----|:---------|\n",
        "| **BPE** | 最高频对合并 | GPT |\n",
        "| **WordPiece** | 最大化似然 | BERT |\n",
        "| **SentencePiece** | 统一框架 | LLaMA |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"分词器模块加载完成\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. BPE 配置与初始化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BPEConfig:\n",
        "    \"\"\"BPE 配置。\"\"\"\n",
        "    vocab_size: int = 1000\n",
        "    min_frequency: int = 2\n",
        "    special_tokens: List[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.special_tokens is None:\n",
        "            self.special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "\n",
        "class BPETokenizer:\n",
        "    \"\"\"BPE 分词器。\n",
        "    \n",
        "    Core Idea: 迭代合并最频繁的字节对。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BPEConfig = BPEConfig()):\n",
        "        self.config = config\n",
        "        self.vocab: Dict[str, int] = {}\n",
        "        self.merges: List[Tuple[str, str]] = []\n",
        "\n",
        "    def _get_pair_freq(self, splits: Dict[str, List[str]]) -> Dict[Tuple, int]:\n",
        "        \"\"\"统计相邻对频率。\"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "        for split in splits.values():\n",
        "            for i in range(len(split) - 1):\n",
        "                pairs[(split[i], split[i+1])] += 1\n",
        "        return pairs\n",
        "\n",
        "    def _merge_pair(self, splits: Dict, pair: Tuple[str, str]) -> None:\n",
        "        \"\"\"合并指定对。\"\"\"\n",
        "        for word, split in splits.items():\n",
        "            new_split, i = [], 0\n",
        "            while i < len(split):\n",
        "                if i < len(split)-1 and split[i] == pair[0] and split[i+1] == pair[1]:\n",
        "                    new_split.append(pair[0] + pair[1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_split.append(split[i])\n",
        "                    i += 1\n",
        "            splits[word] = new_split\n",
        "\n",
        "\n",
        "# 测试\n",
        "tokenizer = BPETokenizer()\n",
        "print(f\"配置: vocab_size={tokenizer.config.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. BPE 训练与编码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_bpe(tokenizer: BPETokenizer, texts: List[str], num_merges: int = 50) -> None:\n",
        "    \"\"\"训练 BPE。\"\"\"\n",
        "    # 初始化：字符级分割\n",
        "    splits = {w: list(w) for text in texts for w in text.split()}\n",
        "    \n",
        "    for _ in range(num_merges):\n",
        "        pairs = tokenizer._get_pair_freq(splits)\n",
        "        if not pairs:\n",
        "            break\n",
        "        best = max(pairs.items(), key=lambda x: x[1])[0]\n",
        "        tokenizer._merge_pair(splits, best)\n",
        "        tokenizer.merges.append(best)\n",
        "    \n",
        "    # 构建词表\n",
        "    for t in tokenizer.config.special_tokens:\n",
        "        tokenizer.vocab[t] = len(tokenizer.vocab)\n",
        "    for p in tokenizer.merges:\n",
        "        token = p[0] + p[1]\n",
        "        if token not in tokenizer.vocab:\n",
        "            tokenizer.vocab[token] = len(tokenizer.vocab)\n",
        "\n",
        "\n",
        "# 训练演示\n",
        "texts = [\"hello world\", \"hello there\", \"world peace\", \"hello hello\"]\n",
        "train_bpe(tokenizer, texts, num_merges=20)\n",
        "print(f\"合并次数: {len(tokenizer.merges)}\")\n",
        "print(f\"词表大小: {len(tokenizer.vocab)}\")\n",
        "print(f\"前5个合并: {tokenizer.merges[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. WordPiece 对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WordPieceTokenizer:\n",
        "    \"\"\"WordPiece 分词器 (BERT)。\n",
        "    \n",
        "    Core Idea: 选择最大化似然的合并。\n",
        "    Score(pair) = freq(pair) / (freq(a) * freq(b))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int = 1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab: Dict[str, int] = {}\n",
        "\n",
        "    def compute_score(self, pair: Tuple[str, str], freqs: Dict[str, int]) -> float:\n",
        "        \"\"\"计算合并得分。\"\"\"\n",
        "        freq_ab = freqs.get(pair[0] + pair[1], 0)\n",
        "        freq_a = freqs.get(pair[0], 1)\n",
        "        freq_b = freqs.get(pair[1], 1)\n",
        "        return freq_ab / (freq_a * freq_b) if freq_a * freq_b > 0 else 0\n",
        "\n",
        "\n",
        "# 对比演示\n",
        "print(\"BPE vs WordPiece:\")\n",
        "print(\"  BPE: 合并频率最高的对\")\n",
        "print(\"  WordPiece: 合并最大化似然的对\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. 可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_bpe_process():\n",
        "    \"\"\"可视化 BPE 过程。\"\"\"\n",
        "    word = \"unhappiness\"\n",
        "    steps = [\n",
        "        list(word),\n",
        "        ['u', 'n', 'h', 'a', 'pp', 'i', 'n', 'e', 'ss'],\n",
        "        ['un', 'h', 'a', 'pp', 'i', 'ness'],\n",
        "        ['un', 'happi', 'ness'],\n",
        "        ['unhappiness'],\n",
        "    ]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, len(steps), figsize=(15, 3))\n",
        "    for i, (ax, tokens) in enumerate(zip(axes, steps)):\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(tokens)))\n",
        "        x = 0\n",
        "        for t, c in zip(tokens, colors):\n",
        "            ax.barh(0, len(t), left=x, height=0.5, color=c, edgecolor='black')\n",
        "            ax.text(x + len(t)/2, 0, t, ha='center', va='center', fontsize=8)\n",
        "            x += len(t)\n",
        "        ax.set_xlim(0, len(word))\n",
        "        ax.set_ylim(-0.5, 0.5)\n",
        "        ax.set_title(f'Step {i}')\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.suptitle('BPE Tokenization Process')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_bpe_process()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. 总结\n",
        "\n",
        "| 方法 | 策略 | 使用模型 |\n",
        "|:-----|:-----|:---------|\n",
        "| **BPE** | 最高频对 | GPT |\n",
        "| **WordPiece** | 最大似然 | BERT |\n",
        "| **SentencePiece** | 统一框架 | LLaMA |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
