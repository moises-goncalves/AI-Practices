{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA 架构详解\n",
    "\n",
    "**SOTA 教育标准实现** | 现代大语言模型核心技术\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解RMSNorm相比LayerNorm的优势\n",
    "2. 掌握旋转位置编码(RoPE)的数学原理\n",
    "3. 学会SwiGLU激活函数的实现\n",
    "4. 理解分组查询注意力(GQA)的内存优化\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [环境配置](#1-环境配置)\n",
    "2. [RMSNorm](#2-rmsnorm)\n",
    "3. [旋转位置编码RoPE](#3-旋转位置编码rope)\n",
    "4. [SwiGLU激活](#4-swiglu激活)\n",
    "5. [分组查询注意力GQA](#5-分组查询注意力gqa)\n",
    "6. [验证测试](#6-验证测试)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. RMSNorm\n",
    "\n",
    "### 2.1 数学原理\n",
    "\n",
    "RMSNorm相比LayerNorm移除了均值中心化，计算更高效：\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2 + \\epsilon}} \\cdot \\gamma$$\n",
    "\n",
    "### 2.2 优势\n",
    "\n",
    "| 特性 | LayerNorm | RMSNorm |\n",
    "|:-----|:----------|:--------|\n",
    "| 均值计算 | 需要 | 不需要 |\n",
    "| 计算量 | 较高 | 较低 |\n",
    "| 效果 | 基准 | 相当 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization。\n",
    "    \n",
    "    相比LayerNorm移除了均值中心化，计算更高效。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 计算RMS\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "# 验证RMSNorm\n",
    "x = torch.randn(2, 10, 768)\n",
    "rms = RMSNorm(768)\n",
    "out = rms(x)\n",
    "print(f'输入形状: {x.shape}')\n",
    "print(f'输出形状: {out.shape}')\n",
    "print(f'✓ RMSNorm测试通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 旋转位置编码(RoPE)\n",
    "\n",
    "### 3.1 核心思想\n",
    "\n",
    "RoPE将位置信息编码为旋转矩阵，使注意力分数自然包含相对位置信息：\n",
    "\n",
    "$$f(x, m) = x \\cdot \\cos(m\\theta) + \\text{rotate}(x) \\cdot \\sin(m\\theta)$$\n",
    "\n",
    "### 3.2 优势\n",
    "\n",
    "- **相对位置**: 注意力分数只依赖相对位置\n",
    "- **外推能力**: 可处理比训练更长的序列\n",
    "- **高效**: 无需额外参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"旋转位置编码(RoPE)。\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, max_seq_len: int = 4096, theta: float = 10000.0) -> None:\n",
    "        super().__init__()\n",
    "        # 计算频率\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # 预计算cos和sin\n",
    "        t = torch.arange(max_seq_len)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('cos', emb.cos())\n",
    "        self.register_buffer('sin', emb.sin())\n",
    "    \n",
    "    def forward(self, seq_len: int):\n",
    "        return self.cos[:seq_len], self.sin[:seq_len]\n",
    "\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"将张量的后半部分旋转到前半部分。\"\"\"\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    \"\"\"应用旋转位置编码到Q和K。\"\"\"\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "# 验证RoPE\n",
    "rope = RotaryEmbedding(64)\n",
    "cos, sin = rope(10)\n",
    "print(f'cos形状: {cos.shape}, sin形状: {sin.shape}')\n",
    "print(f'✓ RoPE测试通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. SwiGLU激活\n",
    "\n",
    "### 4.1 数学公式\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{Swish}(xW_1) \\otimes xW_3$$\n",
    "$$\\text{Swish}(x) = x \\cdot \\sigma(x)$$\n",
    "\n",
    "### 4.2 优势\n",
    "\n",
    "- 门控机制提升表达能力\n",
    "- 比GELU/ReLU效果更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"SwiGLU激活函数。\n",
    "    \n",
    "    SwiGLU(x) = Swish(x*W1) * (x*W3)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "# 验证SwiGLU\n",
    "swiglu = SwiGLU(768, 2048)\n",
    "x = torch.randn(2, 10, 768)\n",
    "out = swiglu(x)\n",
    "print(f'输入形状: {x.shape}')\n",
    "print(f'输出形状: {out.shape}')\n",
    "print(f'SwiGLU参数量: {sum(p.numel() for p in swiglu.parameters()):,}')\n",
    "print(f'✓ SwiGLU测试通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 分组查询注意力(GQA)\n",
    "\n",
    "### 5.1 核心思想\n",
    "\n",
    "GQA让多个Q头共享同一组KV头，减少KV Cache内存：\n",
    "\n",
    "| 类型 | KV头数 | 内存 |\n",
    "|:-----|:-------|:-----|\n",
    "| MHA | = Q头数 | 高 |\n",
    "| MQA | 1 | 低 |\n",
    "| GQA | 介于两者 | 中 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"分组查询注意力(GQA)。\n",
    "    \n",
    "    多个Q头共享同一组KV头，减少KV Cache内存。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_heads // n_kv_heads  # KV重复次数\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(d_model, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, _ = x.size()\n",
    "        \n",
    "        # 计算Q, K, V\n",
    "        q = self.wq(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 重复KV头以匹配Q头数\n",
    "        k = k.repeat_interleave(self.n_rep, dim=1)\n",
    "        v = v.repeat_interleave(self.n_rep, dim=1)\n",
    "        \n",
    "        # 注意力计算\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        \n",
    "        return self.wo(out)\n",
    "\n",
    "# 对比MHA和GQA参数量\n",
    "d_model, n_heads = 4096, 32\n",
    "mha = GroupedQueryAttention(d_model, n_heads, n_heads)  # MHA\n",
    "gqa = GroupedQueryAttention(d_model, n_heads, 8)        # GQA\n",
    "\n",
    "print(f'MHA参数量: {sum(p.numel() for p in mha.parameters()):,}')\n",
    "print(f'GQA参数量: {sum(p.numel() for p in gqa.parameters()):,}')\n",
    "print(f'参数减少: {(1 - sum(p.numel() for p in gqa.parameters()) / sum(p.numel() for p in mha.parameters())) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 验证测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gqa() -> None:\n",
    "    \"\"\"验证GQA输出形状正确性。\"\"\"\n",
    "    gqa = GroupedQueryAttention(768, 12, 4)\n",
    "    x = torch.randn(2, 10, 768)\n",
    "    out = gqa(x)\n",
    "    \n",
    "    assert out.shape == x.shape, f'输出形状错误: {out.shape}'\n",
    "    print(f'✓ GQA: {x.shape} -> {out.shape}')\n",
    "    print(f'✓ test_gqa 通过')\n",
    "\n",
    "test_gqa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "| 技术 | 作用 |\n",
    "|:-----|:-----|\n",
    "| **RMSNorm** | 更高效的归一化 |\n",
    "| **RoPE** | 更好的位置编码，支持外推 |\n",
    "| **SwiGLU** | 更强的FFN表达能力 |\n",
    "| **GQA** | 减少KV Cache，加速推理 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
