{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 架构详解 (Generative Pre-trained Transformer)\n",
    "\n",
    "**SOTA 教育标准实现** | 大语言模型核心架构\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解GPT的Decoder-only架构设计\n",
    "2. 掌握因果自注意力的数学原理和实现\n",
    "3. 学会构建完整的GPT模型\n",
    "4. 实现自回归文本生成\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [环境配置](#1-环境配置)\n",
    "2. [GPT架构概述](#2-gpt架构概述)\n",
    "3. [因果自注意力](#3-因果自注意力)\n",
    "4. [GPT Block实现](#4-gpt-block实现)\n",
    "5. [完整模型构建](#5-完整模型构建)\n",
    "6. [验证测试](#6-验证测试)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "print(f'PyTorch版本: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GPT架构概述\n",
    "\n",
    "### 2.1 核心架构\n",
    "\n",
    "GPT使用**Decoder-only Transformer**架构：\n",
    "\n",
    "```\n",
    "输入Token IDs\n",
    "    ↓\n",
    "Token Embedding + Position Embedding\n",
    "    ↓\n",
    "[GPT Block × N层]\n",
    "    ├── LayerNorm → 因果自注意力 → 残差连接\n",
    "    └── LayerNorm → FFN → 残差连接\n",
    "    ↓\n",
    "LayerNorm → LM Head → 输出概率\n",
    "```\n",
    "\n",
    "### 2.2 关键特点\n",
    "\n",
    "| 特点 | 说明 |\n",
    "|:-----|:-----|\n",
    "| **因果注意力** | 只能看到当前及之前的token，防止信息泄漏 |\n",
    "| **Pre-LN** | LayerNorm在注意力/FFN之前，训练更稳定 |\n",
    "| **GELU激活** | 比ReLU更平滑，性能更好 |\n",
    "| **权重绑定** | Token Embedding与LM Head共享权重 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"GPT模型配置类。\n",
    "    \n",
    "    使用dataclass管理所有超参数，避免魔术数字散落在代码中。\n",
    "    \n",
    "    Attributes:\n",
    "        vocab_size: 词汇表大小\n",
    "        max_seq_len: 最大序列长度\n",
    "        n_layers: Transformer层数\n",
    "        n_heads: 注意力头数\n",
    "        d_model: 模型隐藏维度\n",
    "        d_ff: FFN中间维度\n",
    "        dropout: Dropout概率\n",
    "    \"\"\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 1024\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    d_model: int = 768\n",
    "    d_ff: int = None\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # FFN中间维度默认为4倍d_model\n",
    "        if self.d_ff is None:\n",
    "            self.d_ff = 4 * self.d_model\n",
    "        # 验证d_model能被n_heads整除\n",
    "        assert self.d_model % self.n_heads == 0, \\\n",
    "            f'd_model({self.d_model})必须能被n_heads({self.n_heads})整除'\n",
    "\n",
    "# 创建GPT-2 Small配置\n",
    "config = GPTConfig()\n",
    "print(f'GPT-2 Small配置: {config}')\n",
    "print(f'每头维度: {config.d_model // config.n_heads}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 因果自注意力\n",
    "\n",
    "### 3.1 数学原理\n",
    "\n",
    "因果注意力确保每个位置只能关注当前及之前的位置：\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
    "\n",
    "其中因果掩码 $M$ 定义为：\n",
    "$$M_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$\n",
    "\n",
    "### 3.2 为什么需要因果掩码？\n",
    "\n",
    "- **自回归生成**: 预测下一个token时，不能看到未来的token\n",
    "- **防止信息泄漏**: 训练时确保模型只使用合法的上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"因果自注意力机制。\n",
    "    \n",
    "    核心思想:\n",
    "        通过因果掩码确保每个位置只能关注当前及之前的位置，\n",
    "        实现自回归语言建模。\n",
    "    \n",
    "    数学原理:\n",
    "        Attention(Q,K,V) = softmax(QK^T/sqrt(d_k) + M) * V\n",
    "        其中M是因果掩码（上三角为-inf）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "        self.d_k = config.d_model // config.n_heads\n",
    "        \n",
    "        # 合并的QKV投影（更高效）\n",
    "        self.c_attn = nn.Linear(config.d_model, 3 * config.d_model)\n",
    "        # 输出投影\n",
    "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # 注册因果掩码（下三角矩阵）\n",
    "        mask = torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "        self.register_buffer('mask', mask.view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"前向传播。\n",
    "        \n",
    "        Args:\n",
    "            x: 输入张量 (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            输出张量 (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Step 1: 计算Q, K, V\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.d_model, dim=2)\n",
    "        \n",
    "        # Step 2: 重塑为多头格式 (B, n_heads, T, d_k)\n",
    "        q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 3: 计算注意力分数并缩放\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Step 4: 应用因果掩码\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Step 5: Softmax归一化 + Dropout\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # Step 6: 加权求和\n",
    "        y = att @ v\n",
    "        \n",
    "        # Step 7: 合并多头并投影\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证因果自注意力\n",
    "def test_causal_attention() -> None:\n",
    "    \"\"\"验证因果自注意力的输出形状正确性。\"\"\"\n",
    "    attn = CausalSelfAttention(config)\n",
    "    x = torch.randn(2, 10, config.d_model)\n",
    "    out = attn(x)\n",
    "    \n",
    "    assert out.shape == x.shape, f'输出形状错误: {out.shape}'\n",
    "    print(f'✓ 输入形状: {x.shape}')\n",
    "    print(f'✓ 输出形状: {out.shape}')\n",
    "    print(f'✓ test_causal_attention 通过')\n",
    "\n",
    "test_causal_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GPT Block实现\n",
    "\n",
    "### 4.1 Pre-LN结构\n",
    "\n",
    "GPT Block使用Pre-LN（LayerNorm在子层之前）：\n",
    "\n",
    "```\n",
    "x → LN → Attention → + → LN → MLP → +\n",
    "    └─────────────────┘   └──────────┘\n",
    "         残差连接             残差连接\n",
    "```\n",
    "\n",
    "**Pre-LN vs Post-LN**:\n",
    "- Pre-LN训练更稳定，不需要warmup\n",
    "- Post-LN理论上性能更好，但训练困难"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"GPT前馈网络。\n",
    "    \n",
    "    结构: Linear → GELU → Linear → Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.c_proj = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.gelu(self.c_fc(x))\n",
    "        return self.dropout(self.c_proj(x))\n",
    "\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    \"\"\"GPT Transformer Block (Pre-LN)。\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.d_model)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pre-LN: LayerNorm在子层之前\n",
    "        x = x + self.attn(self.ln_1(x))  # 残差连接\n",
    "        x = x + self.mlp(self.ln_2(x))   # 残差连接\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证GPT Block\n",
    "def test_gpt_block() -> None:\n",
    "    \"\"\"验证GPT Block的输出形状正确性。\"\"\"\n",
    "    block = GPTBlock(config)\n",
    "    x = torch.randn(2, 10, config.d_model)\n",
    "    out = block(x)\n",
    "    \n",
    "    assert out.shape == x.shape, f'输出形状错误: {out.shape}'\n",
    "    print(f'✓ GPT Block: {x.shape} -> {out.shape}')\n",
    "    print(f'✓ test_gpt_block 通过')\n",
    "\n",
    "test_gpt_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 完整模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT语言模型。\n",
    "    \n",
    "    完整的GPT架构，包含:\n",
    "        - Token Embedding\n",
    "        - Position Embedding\n",
    "        - N × GPT Block\n",
    "        - Layer Norm\n",
    "        - LM Head (与Token Embedding权重绑定)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding层\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)  # Token\n",
    "        self.wpe = nn.Embedding(config.max_seq_len, config.d_model) # Position\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([GPTBlock(config) for _ in range(config.n_layers)])\n",
    "        \n",
    "        # 输出层\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # 权重绑定: Token Embedding与LM Head共享权重\n",
    "        self.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # 统计参数量\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f'GPT参数量: {n_params/1e6:.2f}M')\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"前向传播。\n",
    "        \n",
    "        Args:\n",
    "            idx: 输入token ID (batch, seq_len)\n",
    "            targets: 目标token ID，用于计算损失\n",
    "        Returns:\n",
    "            logits: 预测logits (batch, seq_len, vocab_size)\n",
    "            loss: 交叉熵损失（如果提供targets）\n",
    "        \"\"\"\n",
    "        B, T = idx.size()\n",
    "        \n",
    "        # 生成位置索引\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        \n",
    "        # Embedding: Token + Position\n",
    "        x = self.drop(self.wte(idx) + self.wpe(pos))\n",
    "        \n",
    "        # 通过所有Transformer Block\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # 输出层\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), \n",
    "                targets.view(-1)\n",
    "            )\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建并验证模型\n",
    "model = GPT(config)\n",
    "\n",
    "# 测试前向传播\n",
    "idx = torch.randint(0, config.vocab_size, (2, 10))\n",
    "logits, _ = model(idx)\n",
    "print(f'输入形状: {idx.shape}')\n",
    "print(f'输出Logits形状: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 验证测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    \"\"\"自回归文本生成。\n",
    "    \n",
    "    Args:\n",
    "        model: GPT模型\n",
    "        idx: 输入token ID (batch, seq_len)\n",
    "        max_new_tokens: 最大生成token数\n",
    "        temperature: 采样温度\n",
    "        top_k: Top-K采样\n",
    "    Returns:\n",
    "        生成的token ID (batch, seq_len + max_new_tokens)\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 截断到最大长度\n",
    "        idx_cond = idx if idx.size(1) <= model.config.max_seq_len \\\n",
    "            else idx[:, -model.config.max_seq_len:]\n",
    "        \n",
    "        # 前向传播\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Top-K采样\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float('-inf')\n",
    "        \n",
    "        # 采样下一个token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "# 测试生成\n",
    "start_ids = torch.randint(0, config.vocab_size, (1, 5))\n",
    "generated = generate(model, start_ids, max_new_tokens=20, temperature=0.8, top_k=40)\n",
    "print(f'输入长度: {start_ids.shape[1]}')\n",
    "print(f'生成后长度: {generated.shape[1]}')\n",
    "print(f'✓ 文本生成测试通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### GPT核心组件\n",
    "\n",
    "| 组件 | 作用 |\n",
    "|:-----|:-----|\n",
    "| **因果自注意力** | 确保自回归生成，防止信息泄漏 |\n",
    "| **Pre-LN** | 训练更稳定，不需要warmup |\n",
    "| **权重绑定** | 减少参数量，提升性能 |\n",
    "| **GELU激活** | 更平滑的非线性，性能更好 |\n",
    "\n",
    "### 进阶学习\n",
    "\n",
    "- RoPE位置编码\n",
    "- Flash Attention\n",
    "- KV Cache优化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
