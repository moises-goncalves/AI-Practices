# ä»£ç é£æ ¼æŒ‡å—

æœ¬æ–‡æ¡£å®šä¹‰äº† AI-Practices é¡¹ç›®çš„ä»£ç é£æ ¼å’Œæœ€ä½³å®è·µã€‚

## ğŸ“‹ ç›®å½•

- [Pythonä»£ç è§„èŒƒ](#pythonä»£ç è§„èŒƒ)
- [Jupyter Notebookè§„èŒƒ](#jupyter-notebookè§„èŒƒ)
- [å‘½åçº¦å®š](#å‘½åçº¦å®š)
- [æ³¨é‡Šè§„èŒƒ](#æ³¨é‡Šè§„èŒƒ)
- [æ–‡æ¡£å­—ç¬¦ä¸²](#æ–‡æ¡£å­—ç¬¦ä¸²)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## ğŸ Pythonä»£ç è§„èŒƒ

### åŸºæœ¬è§„åˆ™

éµå¾ª [PEP 8](https://www.python.org/dev/peps/pep-0008/) è§„èŒƒï¼š

1. **ç¼©è¿›**: ä½¿ç”¨4ä¸ªç©ºæ ¼
2. **è¡Œå®½**: æœ€å¤š79ä¸ªå­—ç¬¦ï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²72ä¸ªå­—ç¬¦ï¼‰
3. **ç©ºè¡Œ**: å‡½æ•°å’Œç±»ä¹‹é—´2ä¸ªç©ºè¡Œï¼Œæ–¹æ³•ä¹‹é—´1ä¸ªç©ºè¡Œ
4. **å¯¼å…¥**: æ¯ä¸ªå¯¼å…¥å ä¸€è¡Œï¼ŒæŒ‰æ ‡å‡†åº“ã€ç¬¬ä¸‰æ–¹åº“ã€æœ¬åœ°åº“åˆ†ç»„

### å¯¼å…¥è§„èŒƒ

```python
# æ­£ç¡®çš„å¯¼å…¥é¡ºåº
# 1. æ ‡å‡†åº“
import os
import sys
from typing import List, Tuple

# 2. ç¬¬ä¸‰æ–¹åº“
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split

# 3. æœ¬åœ°æ¨¡å—
from utils import data_loader
from models import create_model
```

### å˜é‡å£°æ˜

```python
# å¥½çš„ä¾‹å­
learning_rate = 0.001
num_epochs = 100
batch_size = 32
model_name = 'resnet50'

# é¿å…
lr = 0.001  # é™¤éæ˜¯å…¬è®¤çš„ç¼©å†™
e = 100
bs = 32
mn = 'resnet50'
```

### å‡½æ•°å®šä¹‰

```python
def train_model(
    X_train: np.ndarray,
    y_train: np.ndarray,
    epochs: int = 100,
    batch_size: int = 32,
    learning_rate: float = 0.001,
    verbose: bool = True
) -> Tuple[tf.keras.Model, dict]:
    """
    è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹

    å‚æ•°:
        X_train: è®­ç»ƒæ•°æ®ï¼Œshape (n_samples, n_features)
        y_train: è®­ç»ƒæ ‡ç­¾ï¼Œshape (n_samples,)
        epochs: è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤100
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤32
        learning_rate: å­¦ä¹ ç‡ï¼Œé»˜è®¤0.001
        verbose: æ˜¯å¦æ‰“å°è®­ç»ƒä¿¡æ¯ï¼Œé»˜è®¤True

    è¿”å›:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        history: åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸

    ç¤ºä¾‹:
        >>> X_train = np.random.rand(1000, 10)
        >>> y_train = np.random.randint(0, 2, 1000)
        >>> model, history = train_model(X_train, y_train)
    """
    # å‡½æ•°å®ç°
    model = create_model(X_train.shape[1])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.2,
        verbose=1 if verbose else 0
    )

    return model, history.history
```

### ç±»å®šä¹‰

```python
class NeuralNetwork:
    """
    è‡ªå®šä¹‰ç¥ç»ç½‘ç»œç±»

    å±æ€§:
        input_dim: è¾“å…¥ç»´åº¦
        hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
        output_dim: è¾“å‡ºç»´åº¦
        activation: æ¿€æ´»å‡½æ•°åç§°

    æ–¹æ³•:
        build(): æ„å»ºæ¨¡å‹
        train(): è®­ç»ƒæ¨¡å‹
        predict(): è¿›è¡Œé¢„æµ‹
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dims: List[int],
        output_dim: int,
        activation: str = 'relu'
    ):
        """
        åˆå§‹åŒ–ç¥ç»ç½‘ç»œ

        å‚æ•°:
            input_dim: è¾“å…¥ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨ï¼Œå¦‚[64, 32]
            output_dim: è¾“å‡ºç»´åº¦
            activation: æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤'relu'
        """
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.activation = activation
        self.model = None

    def build(self) -> tf.keras.Model:
        """æ„å»ºæ¨¡å‹æ¶æ„"""
        model = tf.keras.Sequential()

        # è¾“å…¥å±‚
        model.add(tf.keras.layers.Dense(
            self.hidden_dims[0],
            activation=self.activation,
            input_shape=(self.input_dim,)
        ))

        # éšè—å±‚
        for dim in self.hidden_dims[1:]:
            model.add(tf.keras.layers.Dense(dim, activation=self.activation))

        # è¾“å‡ºå±‚
        model.add(tf.keras.layers.Dense(self.output_dim, activation='softmax'))

        self.model = model
        return model
```

## ğŸ““ Jupyter Notebookè§„èŒƒ

### Notebookç»“æ„

æ¯ä¸ªnotebookåº”éµå¾ªä»¥ä¸‹ç»“æ„ï¼š

```python
# ============================================================
# æ–‡ä»¶å: linear_regression_tutorial.ipynb
# æè¿°: çº¿æ€§å›å½’ç®—æ³•çš„å®Œæ•´æ•™ç¨‹
# ä½œè€…: Your Name
# æ—¥æœŸ: 2024-01-01
# ============================================================
```

#### 1. æ ‡é¢˜å’Œç®€ä»‹

```markdown
# çº¿æ€§å›å½’æ•™ç¨‹

## ğŸ“š å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å°†å­¦ä¼šï¼š
- ç†è§£çº¿æ€§å›å½’çš„æ•°å­¦åŸç†
- ä½¿ç”¨NumPyå®ç°çº¿æ€§å›å½’
- ä½¿ç”¨Scikit-learnå¿«é€Ÿæ„å»ºæ¨¡å‹
- è¯„ä¼°æ¨¡å‹æ€§èƒ½

## ğŸ“‹ å‰ç½®çŸ¥è¯†

- PythonåŸºç¡€
- NumPyåŸºç¡€
- çº¿æ€§ä»£æ•°åŸºç¡€

## â±ï¸ é¢„è®¡æ—¶é—´

30-45åˆ†é’Ÿ
```

#### 2. å¯¼å…¥åº“

```python
# ============================================================
# å¯¼å…¥å¿…è¦çš„åº“
# ============================================================

# æ•°å€¼è®¡ç®—
import numpy as np
import pandas as pd

# å¯è§†åŒ–
import matplotlib.pyplot as plt
import seaborn as sns

# æœºå™¨å­¦ä¹ 
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# è®¾ç½®
np.random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# æ˜¾ç¤ºè®¾ç½®
%matplotlib inline
%config InlineBackend.figure_format = 'retina'  # é«˜æ¸…å›¾åƒ

print("æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼")
```

#### 3. ç†è®ºèƒŒæ™¯

```markdown
## ğŸ“– ç†è®ºèƒŒæ™¯

### ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’ï¼Ÿ

çº¿æ€§å›å½’æ˜¯ä¸€ç§ç”¨äºå»ºç«‹å˜é‡ä¹‹é—´çº¿æ€§å…³ç³»çš„ç»Ÿè®¡æ–¹æ³•ã€‚

### æ•°å­¦å…¬å¼

å¯¹äºå•å˜é‡çº¿æ€§å›å½’ï¼š
$$y = wx + b$$

å…¶ä¸­ï¼š
- $y$ æ˜¯é¢„æµ‹å€¼
- $x$ æ˜¯è¾“å…¥ç‰¹å¾
- $w$ æ˜¯æƒé‡ï¼ˆæ–œç‡ï¼‰
- $b$ æ˜¯åç½®ï¼ˆæˆªè·ï¼‰

### æŸå¤±å‡½æ•°

ä½¿ç”¨å‡æ–¹è¯¯å·®(MSE)ä½œä¸ºæŸå¤±å‡½æ•°ï¼š
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

### ä¼˜åŒ–æ–¹æ³•

1. **æ­£è§„æ–¹ç¨‹æ³•**ï¼šç›´æ¥è®¡ç®—æœ€ä¼˜è§£
2. **æ¢¯åº¦ä¸‹é™æ³•**ï¼šè¿­ä»£ä¼˜åŒ–

> ğŸ’¡ **æç¤º**: å½“ç‰¹å¾æ•°é‡è¾ƒå°‘æ—¶ï¼Œæ­£è§„æ–¹ç¨‹æ³•æ›´å¿«ï¼›ç‰¹å¾æ•°é‡å¾ˆå¤§æ—¶ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ›´é€‚åˆã€‚
```

#### 4. æ•°æ®å‡†å¤‡

```python
# ============================================================
# æ•°æ®å‡†å¤‡
# ============================================================

def generate_linear_data(n_samples=100, noise=0.1):
    """
    ç”Ÿæˆçº¿æ€§å›å½’çš„æ¨¡æ‹Ÿæ•°æ®

    å‚æ•°:
        n_samples: æ ·æœ¬æ•°é‡
        noise: å™ªå£°æ°´å¹³

    è¿”å›:
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
    """
    X = 2 * np.random.rand(n_samples, 1)
    y = 4 + 3 * X + noise * np.random.randn(n_samples, 1)
    return X, y

# ç”Ÿæˆæ•°æ®
X, y = generate_linear_data(n_samples=100, noise=0.5)

print(f"æ•°æ®å½¢çŠ¶ - X: {X.shape}, y: {y.shape}")
print(f"XèŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")
print(f"yèŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
```

#### 5. æ•°æ®å¯è§†åŒ–

```python
# ============================================================
# æ•°æ®å¯è§†åŒ–
# ============================================================

plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.6, s=50, edgecolors='k', linewidth=0.5)
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('ç”Ÿæˆçš„çº¿æ€§æ•°æ®', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, linestyle='--')
plt.tight_layout()
plt.show()

print("âœ“ æ•°æ®å¯è§†åŒ–å®Œæˆ")
```

#### 6. æ¨¡å‹å®ç°

```python
# ============================================================
# æ–¹æ³•1: ä½¿ç”¨æ­£è§„æ–¹ç¨‹
# ============================================================

# æ·»åŠ åç½®é¡¹
X_b = np.c_[np.ones((len(X), 1)), X]  # æ·»åŠ  x0 = 1

# è®¡ç®—æœ€ä¼˜å‚æ•°: Î¸ = (X^T * X)^(-1) * X^T * y
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

print("æ–¹æ³•1: æ­£è§„æ–¹ç¨‹")
print(f"æˆªè· (b): {theta_best[0][0]:.4f}")
print(f"æ–œç‡ (w): {theta_best[1][0]:.4f}")
print()

# ============================================================
# æ–¹æ³•2: ä½¿ç”¨Scikit-learn
# ============================================================

model = LinearRegression()
model.fit(X, y)

print("æ–¹æ³•2: Scikit-learn")
print(f"æˆªè· (b): {model.intercept_[0]:.4f}")
print(f"æ–œç‡ (w): {model.coef_[0][0]:.4f}")
```

#### 7. ç»“æœå¯è§†åŒ–

```python
# ============================================================
# ç»“æœå¯è§†åŒ–
# ============================================================

# ç”Ÿæˆé¢„æµ‹ç‚¹
X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]
y_pred_manual = X_new_b.dot(theta_best)
y_pred_sklearn = model.predict(X_new)

# ç»˜å›¾
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# å·¦å›¾ï¼šæ­£è§„æ–¹ç¨‹ç»“æœ
axes[0].scatter(X, y, alpha=0.6, s=50, edgecolors='k', linewidth=0.5, label='æ•°æ®ç‚¹')
axes[0].plot(X_new, y_pred_manual, 'r-', linewidth=2, label='æ‹Ÿåˆçº¿')
axes[0].set_xlabel('X', fontsize=12)
axes[0].set_ylabel('y', fontsize=12)
axes[0].set_title('æ–¹æ³•1: æ­£è§„æ–¹ç¨‹', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# å³å›¾ï¼šScikit-learnç»“æœ
axes[1].scatter(X, y, alpha=0.6, s=50, edgecolors='k', linewidth=0.5, label='æ•°æ®ç‚¹')
axes[1].plot(X_new, y_pred_sklearn, 'b-', linewidth=2, label='æ‹Ÿåˆçº¿')
axes[1].set_xlabel('X', fontsize=12)
axes[1].set_ylabel('y', fontsize=12)
axes[1].set_title('æ–¹æ³•2: Scikit-learn', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("âœ“ ç»“æœå¯è§†åŒ–å®Œæˆ")
```

#### 8. æ¨¡å‹è¯„ä¼°

```python
# ============================================================
# æ¨¡å‹è¯„ä¼°
# ============================================================

# è®¡ç®—é¢„æµ‹å€¼
y_pred = model.predict(X)

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y, y_pred)

print("æ¨¡å‹è¯„ä¼°ç»“æœ:")
print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f}")
print(f"RÂ² åˆ†æ•°: {r2:.4f}")

# å¯è§†åŒ–æ®‹å·®
residuals = y - y_pred

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# æ®‹å·®å›¾
axes[0].scatter(y_pred, residuals, alpha=0.6, s=50, edgecolors='k', linewidth=0.5)
axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[0].set_xlabel('é¢„æµ‹å€¼', fontsize=12)
axes[0].set_ylabel('æ®‹å·®', fontsize=12)
axes[0].set_title('æ®‹å·®å›¾', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# æ®‹å·®åˆ†å¸ƒ
axes[1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)
axes[1].set_xlabel('æ®‹å·®', fontsize=12)
axes[1].set_ylabel('é¢‘æ•°', fontsize=12)
axes[1].set_title('æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

#### 9. æ€»ç»“

```markdown
## ğŸ“ æ€»ç»“

### å…³é”®è¦ç‚¹

1. âœ… çº¿æ€§å›å½’ç”¨äºå»ºç«‹å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»
2. âœ… å¯ä»¥ä½¿ç”¨æ­£è§„æ–¹ç¨‹æˆ–æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£
3. âœ… Scikit-learnæä¾›äº†ç®€å•æ˜“ç”¨çš„API
4. âœ… æ¨¡å‹è¯„ä¼°ä½¿ç”¨MSEã€RMSEå’ŒRÂ²ç­‰æŒ‡æ ‡

### ä¸‹ä¸€æ­¥

- å­¦ä¹ å¤šå…ƒçº¿æ€§å›å½’
- äº†è§£æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆRidgeã€Lassoï¼‰
- æ¢ç´¢éçº¿æ€§å›å½’æ¨¡å‹

### ç»ƒä¹ é¢˜

1. å°è¯•ä½¿ç”¨å¤šä¸ªç‰¹å¾è¿›è¡Œå›å½’
2. å®ç°æ¢¯åº¦ä¸‹é™ç®—æ³•
3. æ¯”è¾ƒä¸åŒæ­£åˆ™åŒ–æ–¹æ³•çš„æ•ˆæœ

## ğŸ“š å‚è€ƒèµ„æ–™

- [Scikit-learnæ–‡æ¡£](https://scikit-learn.org/stable/)
- [çº¿æ€§å›å½’æ•°å­¦æ¨å¯¼](https://example.com)
```

### Markdownå•å…ƒæ ¼è§„èŒƒ

#### æ ‡é¢˜å±‚æ¬¡

```markdown
# ä¸€çº§æ ‡é¢˜ï¼ˆç« èŠ‚æ ‡é¢˜ï¼‰

## äºŒçº§æ ‡é¢˜ï¼ˆä¸»è¦éƒ¨åˆ†ï¼‰

### ä¸‰çº§æ ‡é¢˜ï¼ˆå­éƒ¨åˆ†ï¼‰

#### å››çº§æ ‡é¢˜ï¼ˆè¯¦ç»†è¯´æ˜ï¼‰
```

#### å¼ºè°ƒå’Œæç¤º

```markdown
**é‡è¦æ¦‚å¿µåŠ ç²—**

*æ–œä½“ç”¨äºå¼ºè°ƒ*

> ğŸ’¡ **æç¤º**: è¿™æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æç¤º

> âš ï¸ **æ³¨æ„**: è¿™éœ€è¦ç‰¹åˆ«æ³¨æ„

> âœ… **æœ€ä½³å®è·µ**: æ¨èçš„åšæ³•

> âŒ **é¿å…**: ä¸æ¨èçš„åšæ³•
```

#### ä»£ç å—

```markdown
è¡Œå†…ä»£ç ï¼šä½¿ç”¨ `model.fit()` è®­ç»ƒæ¨¡å‹

ä»£ç å—ï¼š
\```python
import numpy as np
X = np.array([[1, 2], [3, 4]])
\```
```

#### åˆ—è¡¨

```markdown
æœ‰åºåˆ—è¡¨ï¼š
1. ç¬¬ä¸€æ­¥
2. ç¬¬äºŒæ­¥
3. ç¬¬ä¸‰æ­¥

æ— åºåˆ—è¡¨ï¼š
- é€‰é¡¹A
- é€‰é¡¹B
- é€‰é¡¹C

ä»»åŠ¡åˆ—è¡¨ï¼š
- [x] å·²å®Œæˆä»»åŠ¡
- [ ] å¾…å®Œæˆä»»åŠ¡
```

#### æ•°å­¦å…¬å¼

```markdown
è¡Œå†…å…¬å¼ï¼šæŸå¤±å‡½æ•° $L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$

ç‹¬ç«‹å…¬å¼ï¼š
$$
\theta = (X^TX)^{-1}X^Ty
$$

å¤šè¡Œå…¬å¼ï¼š
$$
\begin{aligned}
y &= wx + b \\
L &= \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
\end{aligned}
$$
```

## ğŸ“› å‘½åçº¦å®š

### å˜é‡å‘½å

```python
# ä½¿ç”¨å°å†™å­—æ¯å’Œä¸‹åˆ’çº¿
learning_rate = 0.001
num_epochs = 100
train_data = load_data()

# å¸¸é‡ä½¿ç”¨å¤§å†™å­—æ¯
MAX_ITERATIONS = 1000
DEFAULT_BATCH_SIZE = 32
PI = 3.14159
```

### å‡½æ•°å‘½å

```python
# ä½¿ç”¨å°å†™å­—æ¯å’Œä¸‹åˆ’çº¿ï¼ŒåŠ¨è¯å¼€å¤´
def calculate_accuracy(y_true, y_pred):
    pass

def load_dataset(file_path):
    pass

def preprocess_text(text):
    pass
```

### ç±»å‘½å

```python
# ä½¿ç”¨é©¼å³°å‘½åæ³•
class NeuralNetwork:
    pass

class DataLoader:
    pass

class ModelTrainer:
    pass
```

### æ–‡ä»¶å‘½å

```python
# Notebookæ–‡ä»¶
linear_regression_tutorial.ipynb
cnn_image_classification.ipynb
lstm_text_generation.ipynb

# Pythonè„šæœ¬
data_preprocessing.py
model_utils.py
evaluation_metrics.py

# Markdownæ–‡æ¡£
å†³ç­–æ ‘ç®—æ³•è¯¦è§£.md
Kerasä½¿ç”¨æŒ‡å—.md
```

## ğŸ’¬ æ³¨é‡Šè§„èŒƒ

### å•è¡Œæ³¨é‡Š

```python
# æ­£ç¡®ï¼šæ³¨é‡Šè¯´æ˜ä¸ºä»€ä¹ˆè¿™æ ·åš
learning_rate = 0.001  # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ä»¥ç¡®ä¿æ”¶æ•›ç¨³å®š

# é”™è¯¯ï¼šæ³¨é‡Šåªæ˜¯é‡å¤ä»£ç 
x = 5  # è®¾ç½®xä¸º5
```

### å¤šè¡Œæ³¨é‡Š

```python
# æ­£ç¡®ï¼šè§£é‡Šå¤æ‚é€»è¾‘
# ä½¿ç”¨Adamä¼˜åŒ–å™¨å› ä¸ºå®ƒç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡
# è¿™å¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒç‰¹åˆ«æœ‰æ•ˆ
# å‚è€ƒï¼šhttps://arxiv.org/abs/1412.6980
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
```

### ä»£ç å—æ³¨é‡Š

```python
# === æ•°æ®é¢„å¤„ç† ===
# 1. æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 3. è½¬æ¢ä¸ºTensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.shuffle(1000).batch(32)
```

### TODOæ³¨é‡Š

```python
# TODO: æ·»åŠ æ•°æ®å¢å¼º
# TODO(username): å®ç°å­¦ä¹ ç‡è°ƒåº¦å™¨
# FIXME: ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜
# NOTE: è¿™é‡Œéœ€è¦è¶³å¤Ÿçš„å†…å­˜
```

## ğŸ“– æ–‡æ¡£å­—ç¬¦ä¸²

### å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²

ä½¿ç”¨Googleé£æ ¼ï¼š

```python
def train_neural_network(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray = None,
    y_val: np.ndarray = None,
    epochs: int = 100,
    batch_size: int = 32
) -> Tuple[tf.keras.Model, dict]:
    """
    è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹

    è¯¥å‡½æ•°ä½¿ç”¨æä¾›çš„è®­ç»ƒæ•°æ®è®­ç»ƒç¥ç»ç½‘ç»œï¼Œæ”¯æŒéªŒè¯é›†
    å’Œæ—©åœæœºåˆ¶ã€‚

    Args:
        X_train: è®­ç»ƒç‰¹å¾ï¼Œshapeä¸º(n_samples, n_features)
        y_train: è®­ç»ƒæ ‡ç­¾ï¼Œshapeä¸º(n_samples,)æˆ–(n_samples, n_classes)
        X_val: éªŒè¯ç‰¹å¾ï¼Œå¯é€‰
        y_val: éªŒè¯æ ‡ç­¾ï¼Œå¯é€‰
        epochs: è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤100
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤32

    Returns:
        model: è®­ç»ƒå¥½çš„Kerasæ¨¡å‹
        history: åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸ï¼Œé”®åŒ…æ‹¬:
            - 'loss': è®­ç»ƒæŸå¤±
            - 'accuracy': è®­ç»ƒå‡†ç¡®ç‡
            - 'val_loss': éªŒè¯æŸå¤±ï¼ˆå¦‚æœæä¾›éªŒè¯é›†ï¼‰
            - 'val_accuracy': éªŒè¯å‡†ç¡®ç‡ï¼ˆå¦‚æœæä¾›éªŒè¯é›†ï¼‰

    Raises:
        ValueError: å¦‚æœX_trainå’Œy_trainçš„æ ·æœ¬æ•°ä¸åŒ¹é…
        ValueError: å¦‚æœæä¾›X_valä½†æœªæä¾›y_val

    Examples:
        >>> X_train = np.random.rand(1000, 10)
        >>> y_train = np.random.randint(0, 2, 1000)
        >>> model, history = train_neural_network(X_train, y_train)
        >>> print(f"æœ€ç»ˆå‡†ç¡®ç‡: {history['accuracy'][-1]:.4f}")

    Note:
        - å»ºè®®æä¾›éªŒè¯é›†ä»¥ç›‘æ§è¿‡æ‹Ÿåˆ
        - å¯¹äºå¤§å‹æ•°æ®é›†ï¼Œè€ƒè™‘ä½¿ç”¨ç”Ÿæˆå™¨
    """
    # å‡½æ•°å®ç°
    pass
```

### ç±»æ–‡æ¡£å­—ç¬¦ä¸²

```python
class ConvolutionalNeuralNetwork:
    """
    å·ç§¯ç¥ç»ç½‘ç»œå®ç°

    è¯¥ç±»æä¾›äº†æ„å»ºå’Œè®­ç»ƒCNNçš„å®Œæ•´åŠŸèƒ½ï¼Œé€‚ç”¨äºå›¾åƒ
    åˆ†ç±»ä»»åŠ¡ã€‚

    Attributes:
        input_shape: è¾“å…¥å›¾åƒå½¢çŠ¶ï¼Œå¦‚(28, 28, 1)
        num_classes: åˆ†ç±»æ•°é‡
        conv_layers: å·ç§¯å±‚é…ç½®åˆ—è¡¨
        dense_layers: å…¨è¿æ¥å±‚é…ç½®åˆ—è¡¨
        model: Kerasæ¨¡å‹å®ä¾‹

    Methods:
        build(): æ„å»ºæ¨¡å‹æ¶æ„
        compile(): ç¼–è¯‘æ¨¡å‹
        train(): è®­ç»ƒæ¨¡å‹
        evaluate(): è¯„ä¼°æ¨¡å‹
        predict(): è¿›è¡Œé¢„æµ‹

    Example:
        >>> cnn = ConvolutionalNeuralNetwork(
        ...     input_shape=(28, 28, 1),
        ...     num_classes=10
        ... )
        >>> cnn.build()
        >>> cnn.compile()
        >>> history = cnn.train(X_train, y_train, epochs=10)
    """

    def __init__(self, input_shape, num_classes):
        """
        åˆå§‹åŒ–CNN

        Args:
            input_shape: è¾“å…¥å½¢çŠ¶ï¼Œå¦‚(height, width, channels)
            num_classes: è¾“å‡ºç±»åˆ«æ•°
        """
        pass
```

## âœ¨ æœ€ä½³å®è·µ

### 1. ä»£ç ç»„ç»‡

```python
# å°†ç›¸å…³åŠŸèƒ½åˆ†ç»„
# === é…ç½®å‚æ•° ===
LEARNING_RATE = 0.001
BATCH_SIZE = 32
EPOCHS = 100

# === æ•°æ®åŠ è½½ ===
def load_data():
    pass

def preprocess_data():
    pass

# === æ¨¡å‹å®šä¹‰ ===
def create_model():
    pass

# === è®­ç»ƒæµç¨‹ ===
def train():
    pass

# === ä¸»ç¨‹åº ===
if __name__ == '__main__':
    main()
```

### 2. é­”æ³•æ•°å­—

```python
# é”™è¯¯ï¼šä½¿ç”¨é­”æ³•æ•°å­—
model.add(Dense(64))
optimizer = Adam(0.001)

# æ­£ç¡®ï¼šä½¿ç”¨å‘½åå¸¸é‡
HIDDEN_SIZE = 64
LEARNING_RATE = 0.001

model.add(Dense(HIDDEN_SIZE))
optimizer = Adam(LEARNING_RATE)
```

### 3. é”™è¯¯å¤„ç†

```python
def load_dataset(file_path):
    """åŠ è½½æ•°æ®é›†"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    try:
        data = pd.read_csv(file_path)
    except Exception as e:
        raise ValueError(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

    if data.empty:
        raise ValueError("æ•°æ®é›†ä¸ºç©º")

    return data
```

### 4. ç±»å‹æç¤º

```python
from typing import List, Tuple, Optional, Union

def process_batch(
    batch: np.ndarray,
    labels: np.ndarray,
    augment: bool = False
) -> Tuple[np.ndarray, np.ndarray]:
    """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    pass

def create_layers(
    layer_sizes: List[int],
    activation: str = 'relu'
) -> List[tf.keras.layers.Layer]:
    """åˆ›å»ºå±‚åˆ—è¡¨"""
    pass
```

### 5. ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
# ä½¿ç”¨withè¯­å¥ç®¡ç†èµ„æº
with open('data.txt', 'r') as f:
    data = f.read()

# ä½¿ç”¨TensorFlowçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with tf.device('/GPU:0'):
    model.fit(X_train, y_train)
```

### 6. åˆ—è¡¨æ¨å¯¼å¼

```python
# å¥½çš„ä¾‹å­
squares = [x**2 for x in range(10)]
even_numbers = [x for x in numbers if x % 2 == 0]

# é¿å…è¿‡äºå¤æ‚çš„æ¨å¯¼å¼
# å¦‚æœé€»è¾‘å¤æ‚ï¼Œä½¿ç”¨ä¼ ç»Ÿå¾ªç¯
```

### 7. ä»£ç å¤ç”¨

```python
# é¿å…é‡å¤ä»£ç 
def evaluate_model(model, X, y, name):
    """è¯„ä¼°æ¨¡å‹å¹¶æ‰“å°ç»“æœ"""
    predictions = model.predict(X)
    accuracy = np.mean(predictions == y)
    print(f"{name} å‡†ç¡®ç‡: {accuracy:.4f}")
    return accuracy

# ä½¿ç”¨
train_acc = evaluate_model(model, X_train, y_train, "è®­ç»ƒé›†")
test_acc = evaluate_model(model, X_test, y_test, "æµ‹è¯•é›†")
```

## ğŸ” ä»£ç å®¡æŸ¥æ¸…å•

åœ¨æäº¤ä»£ç å‰ï¼Œæ£€æŸ¥ä»¥ä¸‹é¡¹ç›®ï¼š

- [ ] ä»£ç éµå¾ªPEP 8è§„èŒƒ
- [ ] æ‰€æœ‰å‡½æ•°å’Œç±»éƒ½æœ‰æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] å˜é‡å‘½åæ¸…æ™°ä¸”æœ‰æ„ä¹‰
- [ ] æ·»åŠ äº†å¿…è¦çš„æ³¨é‡Š
- [ ] æ²¡æœ‰é­”æ³•æ•°å­—
- [ ] ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ
- [ ] Notebookå•å…ƒæ ¼å¯ä»¥é¡ºåºæ‰§è¡Œ
- [ ] å›¾è¡¨æ¸…æ™°ä¸”æœ‰æ ‡é¢˜å’Œæ ‡ç­¾
- [ ] æ²¡æœ‰è°ƒè¯•ä»£ç ï¼ˆprintè¯­å¥é™¤å¤–ï¼‰
- [ ] å¯¼å…¥è¯­å¥æŒ‰è§„èŒƒæ’åº

---

éµå¾ªè¿™äº›è§„èŒƒå°†ä½¿ä½ çš„ä»£ç æ›´åŠ ä¸“ä¸šå’Œæ˜“äºç»´æŠ¤ï¼
