---
title: 序列模型
date: 2024-01-01 00:00:00
permalink: /modules/04-sequence-models/
categories:
  - 学习模块
tags:
  - RNN
  - LSTM
  - Transformer
  - NLP
author:
  name: zimingttkx
  link: https://github.com/zimingttkx
---

# 序列模型

掌握处理序列数据的深度学习模型，从 RNN 到 Transformer。

## 模块概览

| 属性 | 值 |
|:-----|:---|
| **前置要求** | 02-neural-networks |
| **学习时长** | 4-5 周 |
| **Notebooks** | 20+ |
| **难度** | ⭐⭐⭐⭐ 高级 |

## 学习目标

- ✅ 理解 RNN、LSTM、GRU 的工作原理
- ✅ 掌握序列到序列模型和注意力机制
- ✅ 深入理解 Transformer 架构
- ✅ 应用预训练模型（BERT, GPT）

## 子模块详解

### 01. RNN Basics | RNN 基础

| 主题 | 内容 |
|:-----|:-----|
| Vanilla RNN | 基础循环网络 |
| Backpropagation Through Time | 时间反向传播 |
| Gradient Problems | 梯度消失/爆炸 |

**RNN 前向传播：**

$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$
$$y_t = W_{hy}h_t + b_y$$

### 02. LSTM & GRU | 长短期记忆

| 架构 | 门控机制 | 特点 |
|:-----|:---------|:-----|
| LSTM | 遗忘门、输入门、输出门 | 长期依赖建模 |
| GRU | 重置门、更新门 | 参数更少，效率更高 |

**LSTM 门控方程：**

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$
$$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$

### 03. Attention Mechanism | 注意力机制

| 类型 | 描述 | 应用 |
|:-----|:-----|:-----|
| Additive | Bahdanau Attention | Seq2Seq |
| Multiplicative | Luong Attention | 机器翻译 |
| Self-Attention | 自注意力 | Transformer |

**Scaled Dot-Product Attention：**

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### 04. Transformer | Transformer 架构

| 组件 | 功能 |
|:-----|:-----|
| Multi-Head Attention | 多头注意力 |
| Positional Encoding | 位置编码 |
| Feed-Forward Network | 前馈网络 |
| Layer Normalization | 层归一化 |

```python
import tensorflow as tf

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.depth = d_model // num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)

    def call(self, q, k, v, mask=None):
        # 实现多头注意力
        pass
```

### 05. Pretrained Models | 预训练模型

| 模型 | 类型 | 参数量 | 特点 |
|:-----|:-----|:------:|:-----|
| BERT | Encoder | 110M-340M | 双向编码 |
| GPT | Decoder | 117M-175B | 自回归生成 |
| T5 | Encoder-Decoder | 220M-11B | 文本到文本 |

## 参考资料

- Vaswani, A., et al. (2017). Attention Is All You Need. *NeurIPS*.
- Devlin, J., et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers. *NAACL*.
