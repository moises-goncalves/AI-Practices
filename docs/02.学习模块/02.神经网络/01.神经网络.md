---
title: 神经网络
date: 2024-01-01 00:00:00
permalink: /modules/02-neural-networks/
categories:
  - 学习模块
tags:
  - 神经网络
  - 深度学习
  - Keras
author:
  name: zimingttkx
  link: https://github.com/zimingttkx
---

# 神经网络

深入理解神经网络的原理，掌握 TensorFlow/Keras 框架的使用。

## 模块概览

| 属性 | 值 |
|:-----|:---|
| **前置要求** | 01-foundations, 线性代数, 微积分 |
| **学习时长** | 3-4 周 |
| **Notebooks** | 20+ |
| **难度** | ⭐⭐⭐ 中级 |

## 学习目标

- ✅ 理解神经网络的数学原理（前向传播、反向传播）
- ✅ 熟练使用 TensorFlow/Keras 构建和训练模型
- ✅ 掌握各种优化器、激活函数和正则化技术
- ✅ 能够自定义层、损失函数和训练循环

## 子模块详解

### 01. Keras Introduction | Keras 入门

| 主题 | 内容 |
|:-----|:-----|
| Sequential API | 顺序模型构建 |
| Functional API | 函数式 API |
| Model Subclassing | 模型子类化 |

```python
import tensorflow as tf
from tensorflow import keras

# Sequential API
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

### 02. Training Deep Networks | 深度网络训练

| 主题 | 内容 | 关键概念 |
|:-----|:-----|:---------|
| Optimizers | SGD, Adam, RMSprop | 学习率调度 |
| Regularization | Dropout, L2, BatchNorm | 过拟合防止 |
| Callbacks | EarlyStopping, ModelCheckpoint | 训练监控 |

### 03. Custom Models | 自定义模型

| 主题 | 内容 |
|:-----|:-----|
| Custom Layers | 自定义层实现 |
| Custom Loss | 自定义损失函数 |
| Custom Training | 自定义训练循环 |

```python
class CustomDense(keras.layers.Layer):
    def __init__(self, units, activation=None):
        super().__init__()
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )

    def call(self, inputs):
        return self.activation(tf.matmul(inputs, self.w) + self.b)
```

### 04. Data Loading | 数据加载

| 主题 | 内容 |
|:-----|:-----|
| tf.data API | 数据管道构建 |
| TFRecord | 高效数据格式 |
| Data Augmentation | 数据增强 |

## 核心概念

### 反向传播算法

$$\frac{\partial L}{\partial w^{(l)}} = \frac{\partial L}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial w^{(l)}} = \delta^{(l)} \cdot (a^{(l-1)})^T$$

### 常用激活函数

| 函数 | 公式 | 特点 |
|:-----|:-----|:-----|
| ReLU | $\max(0, x)$ | 计算高效，梯度消失少 |
| Sigmoid | $\frac{1}{1+e^{-x}}$ | 输出 (0,1)，用于二分类 |
| Tanh | $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | 输出 (-1,1)，零中心 |
| Softmax | $\frac{e^{x_i}}{\sum_j e^{x_j}}$ | 多分类输出层 |

## 参考资料

- [TensorFlow 官方文档](https://www.tensorflow.org/)
- [Keras 官方指南](https://keras.io/guides/)
- Chollet, F. (2021). *Deep Learning with Python* (2nd ed.)
