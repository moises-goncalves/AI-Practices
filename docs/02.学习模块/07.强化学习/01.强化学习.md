---
title: 强化学习
date: 2024-01-01 00:00:00
permalink: /modules/07-reinforcement-learning/
categories:
  - 学习模块
tags:
  - 强化学习
  - Q-Learning
  - Policy Gradient
  - DQN
author:
  name: zimingttkx
  link: https://github.com/zimingttkx
---

# 强化学习

掌握强化学习的核心算法和应用。

## 模块概览

| 属性 | 值 |
|:-----|:---|
| **前置要求** | 02-neural-networks, 概率论 |
| **学习时长** | 5-6 周 |
| **Notebooks** | 25+ |
| **难度** | ⭐⭐⭐⭐⭐ 专家级 |

## 学习目标

- ✅ 理解马尔可夫决策过程（MDP）
- ✅ 掌握值函数方法（Q-Learning, DQN）
- ✅ 掌握策略梯度方法（REINFORCE, PPO）
- ✅ 应用强化学习解决实际问题

## 子模块详解

### 01. MDP Basics | MDP 基础

**马尔可夫决策过程：**

$$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$$

| 组件 | 描述 |
|:-----|:-----|
| $\mathcal{S}$ | 状态空间 |
| $\mathcal{A}$ | 动作空间 |
| $\mathcal{P}$ | 转移概率 |
| $\mathcal{R}$ | 奖励函数 |
| $\gamma$ | 折扣因子 |

**贝尔曼方程：**

$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]$$

### 02. Q-Learning | Q 学习

**Q-Learning 更新规则：**

$$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$$

| 算法 | 特点 |
|:-----|:-----|
| Q-Learning | 离策略，表格方法 |
| SARSA | 在策略，表格方法 |
| Expected SARSA | 期望更新 |

### 03. Deep Q-Learning | 深度 Q 学习

| 技术 | 作用 |
|:-----|:-----|
| Experience Replay | 打破样本相关性 |
| Target Network | 稳定训练 |
| Double DQN | 减少过估计 |
| Dueling DQN | 分离状态价值和优势 |
| Prioritized Replay | 重要样本优先 |

```python
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

### 04. Policy Gradient | 策略梯度

**策略梯度定理：**

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)]$$

| 算法 | 特点 |
|:-----|:-----|
| REINFORCE | 蒙特卡洛策略梯度 |
| Actor-Critic | 值函数基线 |
| A2C/A3C | 异步训练 |
| PPO | 近端策略优化 |
| SAC | 最大熵强化学习 |

**PPO 目标函数：**

$$L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$$

### 05. Applications | 应用

| 领域 | 应用 | 算法 |
|:-----|:-----|:-----|
| 游戏 | Atari, Go | DQN, AlphaGo |
| 机器人 | 控制、导航 | PPO, SAC |
| 推荐系统 | 个性化推荐 | Contextual Bandits |
| 金融 | 交易策略 | DQN, A2C |

## 参考资料

- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.)
- [Spinning Up in Deep RL](https://spinningup.openai.com/)
- Schulman, J., et al. (2017). Proximal Policy Optimization Algorithms. *arXiv*.
