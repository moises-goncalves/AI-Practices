# 机器学习实战第四章笔记
## 4.1 线性回归
### 可以直接通过标准方程计算线性回归方程的最佳Theta值
### LinearRegression 是基于np.linalg.lstsq函数实现的 原理是最小二乘法
### np.linalg.lstsq(X_b, y, rcond=None) 返回值是theta_best_svd, residuals, rank, s
### 也可以通过奇异值分解找得到伪逆

## 复杂度
### 通过求x的转置乘以x这种方法求逆的计算复杂度是n^2.4到n^3 也就是说当特征数量翻倍 训练时间将提升到5到8倍
### sklearn的LinearRegression复杂度是n**2 当特征数量翻倍 训练时间将提升到4倍

## 4.2 梯度下降
