# Keras神经网络
## 10.1 从生物神经元到人工神经元:
### 10.1.3 感知机:
1. 使用scikit-learn中的Perceptron类可以实现单层神经网络 等效于使用SGDClassifier类loss= "perceptron",learning_rate="constant", eta0=1.0(学习率),penalty(正则化)
2. 感知机不输出分类概率 而是基于硬阈值进行预测 所以逻辑回归胜过感知机
3. 但是可以通过堆叠多层感知机来消除感知机的局限性 所得的ANN称为多层感知机MLP
### 10.1.4多层感知机和反向传播:
1. 多层感知机有由一个输入层 一个或多个隐藏层和一个输出层组成 靠近输入层的隐藏层称为底层 隐藏层之间的连接称为前馈连接
2. 信号仅沿着一个方向流动 所以该架构是前馈神经网络 当ANN包含一个深层的隐藏层时 称为深度神经网络(DNN)
3. 自动计算梯度称为自动微分(autodiff) 反向传播算法是自动微分的一种形式. ;cv 给v发帖。‘gf通过那边v 
#### 多层感知机训练过程：
1. 每次处理一个小批量 然后多次遍历整个数据集 每次遍历称为一个epoch
2. 每个小批量被传入到网络中 进行前向传播 只保留中间结果 因为反向传播需要用到
3. 损失函数衡量网络的输出误差 并返回一些误差测量值
4. 计算每个输出连接对误差的贡献 然后使用链式法则进行分析
5. 算法再次使用链式法则来测量每个隐藏层连接对误差的贡献 并更新权重
### 10.1.5 常见的激活函数:
1. 逻辑函数:sigmoid(z)=1/(1+exp(-z)) ; 优点:平滑 可微分 ; 缺点:梯度消失问题 计算开销大
2. 双曲正切函数:tanh(z)=2sigmoid(2z)-1 ; 连续可微 但是输出范围在-1到1之间 仍然存在梯度消失问题 在训练时倾向将每一层输出以0为中心 加快收敛速度
3. 线性整流单元ReLU: f(z)=max(0,z) ; 计算简单 快速收敛 ; 缺点:在训练过程中 神经元可能会永久失活
### 10.1.6 回归MLP:
1. 预测单个值就只需要单个输出神经元 多个值就每个数据维度需要输出神经元 比如预测正方体大小就要长宽高三个维度 对应三个神经元
2. 通常构建用于回归的MLP时 如果不想应用激活函数那么可以输出任意范围的值 如果要保证输出始终为正数 可以使用ReLU激活函数
3. 或者使用softplus激活函数:softplus(z)=ln(1+exp(z)) ; 该函数是ReLU的平滑版本 要保证输出范围的话就是用逻辑函数或者双曲正切函数
4. 训练期间实用的损失函数通常是均方误差(MSE), 如果离群值比较多就平均绝对误差(MAE) 或者使用两者的总和Huber损失函数 
5. Huber损失函数: 对于误差小于某个阈值δ的实例 使用平方误差 对于误差大于δ的实例 使用绝对误差
### 10.1.7 典型的回归MLP架构:
1. 输入神经元数量: 每个特征输入一个 比如mnist数据集有784个像素点 每个像素点对应一个输入神经元
2. 隐藏层数量: 取决于问题 一般为1-5层
3. 每个隐藏层神经元数量: 取决于问题 一般为输入神经元数量的10-1000倍
4. 输出神经元数量: 每个预测维度输出一个神经元
5. 输出激活: 没有/ReLU/softplus/逻辑函数/双曲正切函数
6. 损失函数: MSE/MAE/Huber损失函数(如果有离群值)
### 10.1.8 分类MLP:
1. softmax函数: 如果每个类都需要预测的话 并且输出值的概率和为1 
2. 分类任务的损失函数通常使用交叉熵损失函数(cross-entropy loss) 也称为对数损失函数(log loss)
### 10.1.9 典型的分类MLP架构:
1. 输入层和输出层: 二进制分类 多标签二进制分类 多累分类都与回归相同
2. 输出神经元数量:二进制分类1个 多标签二进制分类每个标签1个 多类分类每个类1个
3. 输出层激活: 二进制分类逻辑函数 多标签二进制分类逻辑函数 多类分类softmax函数
4. 损失函数:使用交叉熵
## 10.2 使用Keras实现MLP:
1. 密集连接层会随机初始化权重和偏置项 如果不随机 全部为0 则每个神经元都会学习到相同的东西
2. 如果要使用其他初始化方法 可以使用kernel_initializer和bias_initializer参数
3. 权重矩阵的形状取决于输入的个数 这就是为什么在第一层需要指定input_shape参数
4. 如果标签是稀疏编码的 则使用sparse_categorical_crossentropy损失函数
5. 如果标签是独热编码的 则使用categorical_crossentropy损失函数
6. 如果训练集十分不均匀 可以使用class_weight参数来平衡类别和sample_weight参数来平衡每个实例 如果都提供了 则两个权重会相乘 但这些weight由专家指定
7. 如果对于训练结果不满意 可以尝试增加隐藏层神经元数量 或者增加隐藏层数量 或者增加训练轮数epochs 调整batch_size
## 10.3 使用函数式API构建MLP:
1. 函数式API允许构建更复杂的模型 比如共享层或者多输入多输出模型
2. 如果任务既有分类又有回归 可以使用多输出模型 比如判断人物表情和是否佩戴眼睛
3. 作为正则化技术 可以添加一个辅助输出层 来帮助主输出层进行分类 
4. keras假定所有的输出层都具有相同的重要性 如果想要调整某个输出层的权重 可以使用loss_weights参数 我们应该给主要输出层更高的权重
## 10.4 训练早停(callbacks)
1. checkpoints: 在训练过程中保存模型权重 以便在训练中断时可以从保存的权重开始训练
2. early stopping: 在训练过程中监控验证集上的损失 如果损失不再下降 则停止训练
3. save_best_only: 如果使用checkpoints参数 则可以指定只保存最好的模型
## 10.5 使用TensorBoard进行可视化
1. TensorBoard是TensorFlow的可视化工具 可以在训练过程中可视化损失函数和准确率
2. 可以使用TensorBoard回调函数来记录训练过程中的数据
## 10.6 微调神经网络超参数
1. 超参数:学习率 优化器类型 损失函数 激活函数 隐藏层数量 隐藏层神经元数量
2. GridSearch: 网格搜索是一种超参数优化技术 它会尝试所有可能的超参数组合
3. RandomizedSearchCV: 随机搜索是一种超参数优化技术 它会随机选择超参数组合

## 10.7 用于超参数调整的第三方库:
1. Hyperopt: 基于树结构的 Parzen Estimator(TPE)算法 优化各种复杂的搜索空间 
2. Hyperas, kopt, Talos
3. Keras Tuner: Google开源的用于Keras的自动超参数调整库
4. Scikit-Optimize: 基于贝叶斯优化的超参数调整库
5. Spearmnit: 基于贝叶斯优化的超参数调整库
6. Hyperband: 基于贝叶斯优化的超参数调整库
7. Sklearn-Deap: 基于遗传算法的超参数调整库

## 10.8 隐藏层数量
1. 中层会组合底层数据 高层会组合中层数据
2. 所以在比如一个成熟领域类的人脸识别领域内 想训练一个识别人的表情的model 可以借用人脸识别模型的底层来进行初始化 而不是随机初始化

## 10.9 每个隐藏层的神经元数量
1. 通常将隐藏层的神经元个数呈金字塔形状 每一层神经元个数越来越少 因为很多底层特征可以合成更少的高阶特征
2. 像层数一样 可以逐渐添加神经元的数量 直到网络开始过拟合 因为神经元数量太少可能没有足够能力来表现数据特征

## 学习率 批量大小 其他超参数
1. 学习率: 一般来讲 最佳学习率一般是最大学习率的一半 即学习率大于算法发散的学习率一般来讲是不断尝试从小到大的学习率 每次乘以一个固定的因子
2. 也可以绘制损失作为学习率的函数 最佳学习率为在损失开始攀升的点低一些 一般来讲比转折点低10倍
3. 批量大小: 小批量可以更频繁地更新权重 但是可能需要更多的训练轮数 大批量可以减少训练轮数 但是可能需要更多的计算资源 而且大批量训练容易不稳定
4. 迭代次数: 一般直接使用早停机制

## 更多超参数调整:Leslie Smith的“超参数优化指南”