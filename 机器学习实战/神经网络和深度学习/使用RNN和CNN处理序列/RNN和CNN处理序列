# 第十五章：RNN和CNN处理序列
1. 他们可以处理任意长度的序列 而常规的密集神经网络只能处理较小的序列

## 15.1 循环神经元和层：
1. 循环神经元同时处理当前的输入和上一个神经元的隐藏状态 在每个步长t接受输入xt和前一个时间不长y(t-1)的隐藏状态
2. 在第一步因为没有以前的输出 所以没有隐藏状态 也就是隐藏状态为0
3. 每个循环神经元都有两组权重 一组是输入权重wx 一组是隐藏状态权重wy 如果考虑整个循环层 可以将多个权重向量放在两个权重矩阵中计算整个循环层的输出向量

### 15.1.1 记忆单元：
1. 循环神经网络在时间步长上面保留某些状态的神经网络的一部分称为记忆单元 单个记忆单元存储的步长大概在10(具体取决于任务)
2. 在时间步长t的状态 ht是输入xt和前一个时间步长ht-1的函数

### 15.1.2 输入和输出序列：
1. RNN可以同时接受输入序列并产生输出序列 比如预测股票之类的变化
2. 或者可以向网络提供一个输入序列 忽略除了最后一个输出以外的所有输出 比如输入电影评论输出感情得分
3. 可以在每个时间步长中一次又一次的向网络提供相同的输入向量，并输出一个序列 成为向量到序列
4. 编码器和解码器，常见于句子翻译 编码器将语言转化为单个向量 然后解码器将向量解码为另一种语言的句子

## 15.2 训练RNN：
1. 训练的诀窍在于按照时间步逐步展开 然后使用反向传播 这种策略成为时间反向传播
2. 反向传播是通过成本函数的所有输出 而不是最终输出

## 15.3 预测时间序列：
1. 时间序列分为单变量时间序列和多变量时间序列 时间序列的典型任务有预测和根据数据填补空缺值 也就是插帧
2. 在处理时间序列数据时 输入特征通常表示为形状为[批处理大小，时间步长，维度]的三维数组 单变量时间序列维度为1 多变量时间维度更多

### 15.3.1 基准指标：
1. 均方根误差（RMSE）和平均绝对误差（MAE）是衡量预测时间序列性能的常用指标

### 15.3.2 实现一个简单的RNN：
1. SimpleRNN默认使用双曲正切函数 默认下Keras只返回最终输出 如果需要返回每个时间步长的输出 可以设置return_sequences=True
2. 趋势和季节性：可以使用加权移动平均模型或者自回归集成移动平均来进行时间序列预测 这种要求先删除季节性数据 在模型训练并做出预测后要添加季节性模式来得到最终输出

### 15.3.3 深度RNN：
1. 可以使用堆叠的RNN层来创建深度RNN
2. 确保每个层的return_sequences=True来保证每个层之间的数据结构不变 如果只关心最后一个输出则最后一个层可以设置为False
3. 确保最后一层为密集层 这样输出可以使用任何激活函数 同时将倒数第二个RNN层设置为return_sequences=False

### 15.3.4 预测未来几个时间步长:
1. 可以选择已经训练好的model 预测下一个值 然后将值加入到输入里面继续预测下一个值 依次类推
2. 一般来讲这种方法准确性会越来越差 因为误差会逐渐累加
3. 第二种方法是一次性预测10个值, 使用一个序列到向量的model 让他输出10个值而不是一个值 keras.layers.Dense(10)
4. 第三种方法是使用序列到序列的模型 让他输出一个序列 这样损失将包含每个时间步长的损失 这样更多的损失流过模型可以稳定和加速训练
5. 要将模型转化为序列到序列 必须在所有循环层设置return_sequences=True 在每个时间步长应用Dense层 Keras为此提供了一个TimeDistributed层
6. 可以将序列到序列这种方法和第一个方法结合 将序列作为输入和其他一起来预测下一个时间步长的输出 可以利用MC Drop来讲进行处理误差

## 15.4 处理长序列:
1. 长序列的梯度消失和梯度爆炸问题

### 15.4.1 应对不稳定梯度的问题:
1. 良好的参数初始化, 更快的优化器, dropout, 但是非饱和激活函数没什么帮助 甚至为副作用 假设梯度下降在每次时间步长增加输出 由于每个时间步长参数一样 
2. 会导致输出爆炸 而不饱和函数无法解决这个问题 所以一般使用较小的学习率和饱和激活函数来解决 或者进行梯度裁剪
3. 批量归一化对于RNN效果也不是很好 不能在时间步之间使用 只能在递归层之间使用 可以在记忆单元添加一个BN层
4. BN层只有在应用于输入而不是隐藏状态才稍微受益
5. 层归一化和RNN一起使用通常会更好,他和批归一化区别是它不是跨批量维度进行归一化 而是对数据特征上进行归一化
6. 层归一化优点是可以在每个时间步上对每一个实例独立计算所需要的统计信息 也就是在每个时间步行为方式相同
7. 使用keras实现层归一化:

### 15.4.2 解决短期记忆问题:
1. 数据在遍历RNN的时候会进行转换 导致每个时间步都会丢失一段信息 在时间步很长的时候就会几乎完全丢失开头的信息
#### LSTM:
1. LSTM是一种特殊的RNN 它可以学习长期依赖关系 LSTM通过门机制来控制信息流
2. LSTM详细架构如下:

#### 窥视孔连接: 
1. 常规LSTM的门控制器只能查看输入和先前的短期状态 窥视孔连接让控制器可以查看先前的长期状态
2. 但是这个不是一定有效 取决于具体的任务

#### GRU单元:
1. GRU是LSTM的简化版 它只有两个门 一个是更新门 一个是重置门
2. GRU单元的架构如下:

#### 使用一维卷积层处理序列:
1. 虽然LSTM和GRU效果还不错 但是短期记忆问题还是存在 所以使用一维卷积层缩短序列
2. 可以构建由循环层和卷积层混合而成的神经网络 使用步长为1和same填充则输入序列长度等于输出序列长度
3. 使用valid或步幅大于1那么输出比输入短

#### WaveNet:
1. WaveNet是一种使用因果卷积层的深度神经网络 它使用因果卷积层来预测下一个时间步长
2. 堆叠一维卷积层,使每一层的扩散率加倍 这样较低的层学习短期模式 较高的层学习长期模式,由于扩散加倍 可以处理很长的序列
3. 使用causal填充避免窥探未来 
