# 第11章:训练深度神经网络:
1. 梯度消失,梯度下降,梯度爆炸
2. 对于巨大的网络没有足够的训练集或者做label的代价太高
3. 训练缓慢
4. 在百万个参数下model可能会有验证过拟合训练集的情况 尤其是在没有充足训练实例或者噪声太大的情况

## 11.1 梯度消失和梯度爆炸的问题
1. 梯度消失:在反向传播过程中,梯度越来越小,低层的权重几乎不再更新
2. 梯度爆炸:在反向传播过程中,梯度越来越大,导致权重值变得非常大
3. cost function的平均值如果在0处 训练表现会更好

## 权重初始化技术:
### 11.1.1 Glorot and He Initialization
1. 每一层在输入输出的数据应该含有相同的方差 在反向传播时输入输出的方差也应该一样 但是这要求层含有相同数量的输入和神经元
2. 所以最后的方案是:fan_avg = (fan_in + fan_out) / 2
3. Xavier,Glorot初始化:1.正态分布:均值为0,方差为1/fan_avg,或者在-r到r的均匀分布 r = sqrt(3 / fan_avg)
4. 使用Glorot初始化可以大幅加快深度学习的速度

#### 11.1.1.1每种激活函数的初始化参数
1. Glorot: 激活函数有None,logistic,tanh,softmax 方差为1/fan_avg
2. He: 激活函数为ReLU及其变体, 方差为2/fan_in
3. Lecun: 激活函数为SELU, 方差为1/fan_in


### 11.1.2 非饱和激活函数(RELU)
1. RELU对正值有不饱和 而且计算速度很快
#### 11.1.2.1 RELU
1. RELU对正值有不饱和度 而且训练速度很快
2. RELU缺点是会导致一些神经元的死亡 只输出0值 在某些情况可能一半的神经元都会死去 特别是使用较大的学习率
3. 当神经元的权重进行调整时,输入的加权和对于训练集中的所有实例均为负数,神经元会死亡 只会继续输出0,梯度下降不会再影响它 因为RELU函数的输入为负数时其梯度为0
#### 11.1.2.2 Leaky RELU
1. LEAKEY RELU:max(az,z) 超参数a定义了泄露的程度 他是z<0时的函数的斜率 通常设置为0.01
2. 这个小的斜率确保了神经元不会死亡 而是陷入昏迷 有机会醒来 一般来讲效果要好于严格的RELU函数
3. 甚至设置a=0.2的大泄露比0.01的效果还要好
#### 11.1.2.3 随机Leaky RELU(RReLU)
1. RReLU:在训练时随机选择a的值 在测试时使用平均的a值
2. Leaky ReLU和ReLU相似 但是在负值有一个小的斜率
#### 11.1.2.4 Parametric RELU(PReLU) 参数化ReLU
1. PReLU的a可以在训练中学习 不是超参数 但是可以通过梯度下降来进行修正
2. 在大型图像数据上PReLU效果明显优于ReLU 但是在小数据集上面存在过拟合的风险
#### 11.1.2.5 Exponential Linear Unit(ELU) 指数线性单元
1. 在作者的实验里面胜过了所有的ReLU及其变体 训练时间更短 在测试集上面表现更好
2. 当z<0时 取负值 这让单元的平均输出为0 有助于缓解梯度消失 同时超参数a定义一个值 该值为z为较大负数的时候ELU逼近的值 通产设为1
3. 对于z<0 它具有非零梯度 避免了神经元死亡问题
4. 如果a = 1, 则函数在所有位置都是平滑的 加速了梯度下降 因为他在z=0两处弹跳不大
5. 缺点是:比ReLU的所有系列都要慢 因为使用了指数函数 但是在训练过程更快的收敛速度弥补了这种缓慢地计算 但是测试的时候ELU比ReLU慢
#### 11.1.2.6 Scaled ELU(SELU) 缩放指数线性单元
1. 如果训练层只有密集层 并且所有层都是用SELU激活函数 则网络在训练过程是自归一化的 每层的输出都倾向于保留平均值0和标准差1 解决了梯度消失和爆炸的问题
2. 产生自归一化条件:从输入特征必须是标准化的 平均值为0 方差为1 每个隐藏层权重必须使用LeCun正态初始化 网络架构必须是顺序的 无法用于循环网络还有wideand deep网络

### 如何对隐藏层使用激活函数:
1. 一般来讲 SELU > ELU > LeakyReLU及其变体 >ReLU > Tanh > logistic
2. 如果网络架构不能自归一化 则ELU可能要优于SELU 如果关心运行延迟 则使用Leakey Relu
3. 如果时间足够 使用交叉验证来评估其他激活函数 例如过拟合则为RRelu 如果数据集很大则PRELU 如果速度首位则ReLU 因为优化很好

### 11.1.3 批量归一化:
1. 在每个隐藏层的激活函数之前或者之后添加一个操作 将每个输入零中心化并归一化 然后每层使用两个新的参数向量缩放和偏移其结果
2. 该操作可以使模型学习各层输入的最佳缩放和均值 如果将缩放层添加为神经网络第一层 则无需归一化数据集 BN层会完成这个操作
3. 在每一个批量归一化层训练了四个参数:常规反向传播学习输出缩放向量,输出偏移向量,使用移动指数平均计算的最终输入均值向量和最终输入标准差向量 后者仅在训练后使用
4. 批量归一化的作用像正则化一样减少了对其他正则化技术的依赖 即使使用饱和激活函数 比较大一点的学习率 效果也会很好
5. 但是归一化会增加model复杂度 我们可以将隐藏层的权重修改 让隐藏层直接输出需要通过BN层输出的东西 再次简化复杂度
6. BN作者推荐在激活函数之前添加BN层 但是有争议 可以对比一下两种方案给出最佳
7. 超参数1:momentum:用于计算移动均值和标准差 良好的动量值应该接近1 超参数2:axis默认为-1表示哪个轴应该被归一化

### 11.1.4 梯度裁剪:
1. 梯度裁剪:在反向传播过程中,如果梯度的L2范数大于某个阈值,则将梯度缩小到阈值 最常用于循环神经网络因为RNN难以使用BN
2. 在Keras中实现裁剪就是在创建优化器时设置clipvalue和clipnorm参数 
3. 该优化器将梯度向量每个分量都裁剪到-1到1之间 但是应注意裁剪阈值可能改变梯度方向 例如[0.9,100]按照值裁剪后为[0.9,1] 改变了向量方向
4. 所以一般使用clipnorm=1.0来裁剪梯度向量使其L2范数等于1而不是clipvalue



## 11.2 重用预训练层
1. 迁移技术:避免从头开始训练利用相似任务的model底层 这部分权重通常是固定的
2. 如果新任务的输入照片的大小与原始人无使用的照片不同 通常需要进行预处理将数据处理为原来model需要的尺寸
3. 通常应该替换原模型的输出层 添加新层来适应新任务 原始模型上面隐藏层也可能没有什么用 一般来讲任务越相似可以重用的层数就越多
4. 首先尝试冻结所有可以重复使用的层 使其权重不可以训练 这样梯度下降就不会对其进行修改 然后尝试解冻上部隐藏层中的一两层看性能是否提高
### 11.2.1 用Keras进行迁移学习:
1. 迁移网络更适合深度卷积神经网络
### 11.2.2 无监督预训练:
1. 假设要处理一个没有太多标签训练数据的复杂任务 可以尝试收集未标记数据 训练自动编码器和GAN 然后重用GAN或者自动编码器的低层
2. 或者在辅助任务上面训练第一个神经网络 然后将其低层重用致主要任务
3. 自我监督学习指的是自动从数据本身生成标签 然后使用有监督技术在标签上训练数据

## 11.3 更快的优化器:
1. 常见加快训练速度方法:1.良好初始化权重 2.良好的激活函数 3.批量归一化 4.迁移学习 5.使用更好的优化器
2. 常见的优化器算法: 动量优化, Neterov加速梯度(NAG), AdaGrad, RMSProp, Adam,  Nadam

### 11.3.1 动量优化:
1. 动量优化的梯度下降，是在梯度下降更新权重时，引入历史梯度的累积信息（动量），让参数更新既考虑当前梯度，又受之前梯度方向的“惯性”影响，以此加速收敛并抑制震荡。
2. 一般将β设置为0.9 会比普通的梯度下降速度快
3. 只需要在创建优化器的时候将momentum参数设置为0.9
### 11.3.2 Nesterov加速梯度(NAG):
1. Nesterov加速梯度(NAG)是动量梯度下降的改进版,他是沿动量方向提前测量成本函数的梯度进而优化 
2. 原因是通常动量会指向最优解的方向 所以测的更远方向会更加精准一点
3. 只需要在SGD里面将nesterov设置为True
4. 这两个动量的区别就是前者在动量步骤之前计算梯度 后者在使用动量步骤之后计算梯度

### 11.3.3 AdaGard:
1. AdaGard算法可以找出沿最陡峭的维度按比例缩小梯度向量
2. 该算法会降低学习率 但是对于陡峭的维度 执行速度会快很多 称为自适应学习率 
3. 他几乎不需要调整学习率超参数 适用于简单的二次问题 不适合深度学习网络 因为学习率容易提前变得很小 还未到达全局最优点就停止了
### 11.3.4 RMSProP:
1. RMSProp算法是AdaGard的改进版,它不仅对每个参数的梯度进行平均,还计算梯度的平方的平均值 通过指数型衰减控制
2. 衰减率β一般设置为0.9 效果很好 不用修改 除了非常简单的问题以外 几乎表现总是比AdaGrad好

### 11.3.5 Adam,Nadam:
1. Adam算法结合了AdaGard和RMSProp算法,它计算梯度的指数衰减平均值和梯度的平方的指数衰减平均值
2. 也是自适应学习率算法 不用调整
3. Nadam算法是Adam算法的改进版,它结合了Nesterov加速梯度 总体胜过Adam 但是有时候不如RMSProP

### 自适应优化方法: 有时候会导致在某些数据集上面泛化不好 这时候可以尝试普通的Nesterov加速梯度(NAG) 有时候数据集对自适应过敏

### 11.3.6 学习率调度:
1. 学习率太高model会发散 学习率太低model会收敛太慢
2. 幂调度,指数调度,分段恒定调度,性能调度,周期调度
## 11.4 通过正则化避免过拟合:
1.流行的正则化方法:L1,L2正则化, dropout,早停,最大范数正则化,最大范数权重衰减

### 11.4.2 dropout:
1. dropout:在训练过程中 随机将一部分神经元设置为0 然后训练模型
2. 原理是神经元将无法注意到其他神经元 必须要发挥自己的全部作用 实际上每一个步骤都训练了一个神经网络 最终model既是所有的神经网络的集成
3. 由于dropout只在训练期间激活 所以比较训练损失和验证损失可能产生差别 不能使用dropout评估训练损失
4. 模型过拟合则提高drop率 反之减低 如果每一层使用dropout效果太强的话 只在最后一层使用dropout
5. 如果激活函数用的是SELU则要使用alpha dropout,保留了输入的均值和标准差 常规的dropout会破坏归一化

### 11.4.3 蒙特卡洛Dropout:
1. 与普通 Dropout 完全相同。你就像训练一个普通的使用了 Dropout 的模型一样。
2. 关键区别：在推理时，Dropout 仍然保持开启状态！
3. 你将同一个输入样本多次（例如 $T=100$ 次）喂给模型。由于 Dropout 在每次前向传播时都会随机“关闭”不同的神经元，这 $T$ 次推理会产生 $T$ 个略微不同的预测结果。
4. 结果：最终预测：你将这 $T$ 个预测结果取平均值，作为模型最终的预测输出。
5. 不确定性：你可以计算这 $T$ 个结果的方差 (Variance) 或标准差 (Standard Deviation)。这个方差就代表了模型对这个预测有多“自信”。
6. 方差小：意味着 $T$ 次预测结果都非常接近，模型对这个结果非常确定。方差大：意味着 $T$ 次预测结果七零八落，模型对这个结果非常不确定。
7. 一句话总结： MC Dropout 在测试时故意保持 Dropout 开启并多次运行，利用预测结果的抖动来量化模型“懂不懂”或“确不确定”。

### 11.4.4 最大范数正则化:
1. 最大范数正则化:在训练过程中,如果隐藏层权重向量的L2范数超过某个阈值,则将这个向量缩小到阈值
2. Max-Norm 的作用：最大范数正则化通过设置一个硬上限 $c$，完美地解决了这个问题。
3. 它允许权重在 $c$ 范围内自由增长以配合 Dropout，但又严格禁止它们超过 $c$，从而保证了训练的稳定性

## 11.5 总结和使用指南(默认的DNN配置)
### 超参数                 默认值
1. 内核初始化                He初始化
2. 激活函数                  Elu
3. 归一化                   浅层网络 深度网络需要批量归一化
4. 正则化                   提前停止
5. 优化器                   Adam
6. 学习率调度                1周期

## 11.6 用于自归一化网络的DNN配置:
### 超参数                 默认值
1. 内核初始化                LeCun初始化
2. 激活函数                  SELU
3. 归一化                   批量归一化
4. 正则化                   需要的话就Alpha dropout
5. 优化器                   动量优化(RMPROP或者Nadam)
6. 学习率调度                1周期

## 11.7 其他
1. 如果可以找到类似model则可以迁移学习使用低层
2. 需要稀疏矩阵 使用l1正则化
3. 如果要构建风险敏感应用 使用MC Dropout