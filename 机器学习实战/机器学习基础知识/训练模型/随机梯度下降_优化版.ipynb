{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# éšæœºæ¢¯åº¦ä¸‹é™ (Stochastic Gradient Descent)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å°†å­¦ä¼šï¼š\n",
    "- ç†è§£éšæœºæ¢¯åº¦ä¸‹é™ä¸æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„åŒºåˆ«\n",
    "- å®ç°SGDå’ŒMini-batch GD\n",
    "- ç†è§£å­¦ä¹ ç‡è¡°å‡ç­–ç•¥\n",
    "- ä½¿ç”¨Scikit-learnçš„SGDRegressor\n",
    "\n",
    "## ğŸ“‹ å‰ç½®çŸ¥è¯†\n",
    "\n",
    "- æ‰¹é‡æ¢¯åº¦ä¸‹é™\n",
    "- çº¿æ€§å›å½’\n",
    "\n",
    "## â±ï¸ é¢„è®¡æ—¶é—´\n",
    "\n",
    "25-35åˆ†é’Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– ç¬¬1éƒ¨åˆ†ï¼šç†è®ºèƒŒæ™¯\n",
    "\n",
    "### æ¢¯åº¦ä¸‹é™çš„ä¸‰ç§å˜ä½“\n",
    "\n",
    "| ç±»å‹ | æ¯æ¬¡ä½¿ç”¨çš„æ ·æœ¬æ•° | ç‰¹ç‚¹ |\n",
    "|------|----------------|------|\n",
    "| æ‰¹é‡GD | å…¨éƒ¨ (m) | ç¨³å®šä½†æ…¢ï¼Œå†…å­˜éœ€æ±‚å¤§ |\n",
    "| éšæœºGD | 1ä¸ª | å¿«ä½†ä¸ç¨³å®šï¼Œå¯é€ƒç¦»å±€éƒ¨æœ€ä¼˜ |\n",
    "| å°æ‰¹é‡GD | bä¸ª (1<b<m) | å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§ |\n",
    "\n",
    "### éšæœºæ¢¯åº¦ä¸‹é™çš„ä¼˜ç‚¹\n",
    "\n",
    "1. **æ›´å¿«çš„è¿­ä»£**: æ¯æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬\n",
    "2. **å¯å¤„ç†å¤§æ•°æ®**: ä¸éœ€è¦åŠ è½½å…¨éƒ¨æ•°æ®\n",
    "3. **å¯é€ƒç¦»å±€éƒ¨æœ€ä¼˜**: éšæœºæ€§å¸¦æ¥çš„ä¼˜åŠ¿\n",
    "4. **åœ¨çº¿å­¦ä¹ **: å¯å®æ—¶æ›´æ–°æ¨¡å‹\n",
    "\n",
    "### å­¦ä¹ ç‡è¡°å‡\n",
    "\n",
    "ç”±äºSGDçš„éšæœºæ€§ï¼Œéœ€è¦é€æ¸é™ä½å­¦ä¹ ç‡ä»¥ç¡®ä¿æ”¶æ•›ï¼š\n",
    "\n",
    "$$\\eta(t) = \\frac{\\eta_0}{1 + t/t_1}$$\n",
    "\n",
    "> ğŸ’¡ **å…³é”®æ´å¯Ÿ**: SGDçš„éšæœºæ€§æ˜¯ä¸€æŠŠåŒåˆƒå‰‘â€”â€”å¸®åŠ©é€ƒç¦»å±€éƒ¨æœ€ä¼˜ï¼Œä½†ä¹Ÿå¯¼è‡´æ”¶æ•›ä¸ç¨³å®šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» ç¬¬2éƒ¨åˆ†ï¼šä»£ç å®ç°\n",
    "\n",
    "### æ­¥éª¤1: å‡†å¤‡å·¥ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å¯¼å…¥åº“å’Œç”Ÿæˆæ•°æ®\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = 4 + 3 * X + np.random.randn(m, 1) * 0.5\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "TRUE_THETA_0, TRUE_THETA_1 = 4.0, 3.0\n",
    "print(\"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­¥éª¤2: å®ç°éšæœºæ¢¯åº¦ä¸‹é™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# éšæœºæ¢¯åº¦ä¸‹é™å®ç°\n",
    "# ============================================================\n",
    "\n",
    "def learning_schedule(t, t0=5, t1=50):\n",
    "    \"\"\"å­¦ä¹ ç‡è¡°å‡å‡½æ•°\"\"\"\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "def stochastic_gradient_descent(X_b, y, n_epochs=50, t0=5, t1=50, record_history=False):\n",
    "    \"\"\"\n",
    "    éšæœºæ¢¯åº¦ä¸‹é™\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X_b: å¸¦åç½®çš„ç‰¹å¾çŸ©é˜µ\n",
    "        y: ç›®æ ‡å€¼\n",
    "        n_epochs: è¿­ä»£è½®æ•°\n",
    "        t0, t1: å­¦ä¹ ç‡è¡°å‡å‚æ•°\n",
    "        record_history: æ˜¯å¦è®°å½•å†å²\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)\n",
    "    \n",
    "    history = {'theta': [theta.copy()], 'cost': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # æ¯ä¸ªepochéšæœºæ‰“ä¹±æ•°æ®é¡ºåº\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        \n",
    "        for i in range(m):\n",
    "            # éšæœºé€‰å–ä¸€ä¸ªæ ·æœ¬\n",
    "            random_index = shuffled_indices[i]\n",
    "            xi = X_b[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            \n",
    "            # è®¡ç®—æ¢¯åº¦ï¼ˆå•æ ·æœ¬ï¼‰\n",
    "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "            \n",
    "            # è®¡ç®—å½“å‰å­¦ä¹ ç‡\n",
    "            eta = learning_schedule(epoch * m + i, t0, t1)\n",
    "            \n",
    "            # æ›´æ–°å‚æ•°\n",
    "            theta = theta - eta * gradients\n",
    "        \n",
    "        if record_history:\n",
    "            history['theta'].append(theta.copy())\n",
    "            predictions = X_b.dot(theta)\n",
    "            cost = (1/(2*m)) * np.sum((predictions - y)**2)\n",
    "            history['cost'].append(cost)\n",
    "    \n",
    "    if record_history:\n",
    "        return theta, history\n",
    "    return theta\n",
    "\n",
    "print(\"âœ“ SGDå‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# è¿è¡ŒSGD\n",
    "# ============================================================\n",
    "\n",
    "theta_sgd, history_sgd = stochastic_gradient_descent(\n",
    "    X_b, y, n_epochs=50, record_history=True\n",
    ")\n",
    "\n",
    "print(\"SGDç»“æœ:\")\n",
    "print(f\"  Î¸â‚€: {theta_sgd[0][0]:.6f} (çœŸå®: {TRUE_THETA_0})\")\n",
    "print(f\"  Î¸â‚: {theta_sgd[1][0]:.6f} (çœŸå®: {TRUE_THETA_1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­¥éª¤3: å®ç°Mini-batchæ¢¯åº¦ä¸‹é™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Mini-batchæ¢¯åº¦ä¸‹é™\n",
    "# ============================================================\n",
    "\n",
    "def minibatch_gradient_descent(X_b, y, n_epochs=50, batch_size=16, \n",
    "                               eta=0.1, record_history=False):\n",
    "    \"\"\"\n",
    "    Mini-batchæ¢¯åº¦ä¸‹é™\n",
    "    \n",
    "    å‚æ•°:\n",
    "        batch_size: æ¯æ‰¹æ ·æœ¬æ•°\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)\n",
    "    \n",
    "    history = {'theta': [theta.copy()], 'cost': []}\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_b_shuffled = X_b[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, m)\n",
    "            \n",
    "            X_batch = X_b_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # è®¡ç®—æ¢¯åº¦ï¼ˆæ‰¹é‡ï¼‰\n",
    "            gradients = (2/len(y_batch)) * X_batch.T.dot(X_batch.dot(theta) - y_batch)\n",
    "            theta = theta - eta * gradients\n",
    "        \n",
    "        if record_history:\n",
    "            history['theta'].append(theta.copy())\n",
    "            cost = (1/(2*m)) * np.sum((X_b.dot(theta) - y)**2)\n",
    "            history['cost'].append(cost)\n",
    "    \n",
    "    if record_history:\n",
    "        return theta, history\n",
    "    return theta\n",
    "\n",
    "theta_mb, history_mb = minibatch_gradient_descent(\n",
    "    X_b, y, n_epochs=50, batch_size=16, record_history=True\n",
    ")\n",
    "\n",
    "print(\"Mini-batch GDç»“æœ:\")\n",
    "print(f\"  Î¸â‚€: {theta_mb[0][0]:.6f}\")\n",
    "print(f\"  Î¸â‚: {theta_mb[1][0]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ç¬¬3éƒ¨åˆ†ï¼šä¸‰ç§æ–¹æ³•å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# æ‰¹é‡GDç”¨äºå¯¹æ¯”\n",
    "# ============================================================\n",
    "\n",
    "def batch_gradient_descent(X_b, y, n_epochs=50, eta=0.1, record_history=False):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)\n",
    "    history = {'theta': [theta.copy()], 'cost': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "        if record_history:\n",
    "            history['theta'].append(theta.copy())\n",
    "            cost = (1/(2*m)) * np.sum((X_b.dot(theta) - y)**2)\n",
    "            history['cost'].append(cost)\n",
    "    \n",
    "    if record_history:\n",
    "        return theta, history\n",
    "    return theta\n",
    "\n",
    "# è¿è¡Œæ‰¹é‡GD\n",
    "np.random.seed(42)\n",
    "_, history_bgd = batch_gradient_descent(X_b, y, n_epochs=50, record_history=True)\n",
    "\n",
    "# é‡æ–°è¿è¡ŒSGDå’ŒMini-batchç¡®ä¿ç›¸åŒåˆå§‹åŒ–\n",
    "np.random.seed(42)\n",
    "_, history_sgd = stochastic_gradient_descent(X_b, y, n_epochs=50, record_history=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "_, history_mb = minibatch_gradient_descent(X_b, y, n_epochs=50, batch_size=16, record_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æŸå¤±æ›²çº¿å¯¹æ¯”\n",
    "axes[0].plot(history_bgd['cost'], 'b-', linewidth=2, label='æ‰¹é‡GD')\n",
    "axes[0].plot(history_sgd['cost'], 'r-', linewidth=2, alpha=0.7, label='éšæœºGD')\n",
    "axes[0].plot(history_mb['cost'], 'g-', linewidth=2, alpha=0.7, label='Mini-batch GD')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('æŸå¤±')\n",
    "axes[0].set_title('ä¸‰ç§æ¢¯åº¦ä¸‹é™æ–¹æ³•å¯¹æ¯”', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# å‚æ•°ç©ºé—´è·¯å¾„\n",
    "theta0_range = np.linspace(2, 6, 50)\n",
    "theta1_range = np.linspace(1, 5, 50)\n",
    "theta0_grid, theta1_grid = np.meshgrid(theta0_range, theta1_range)\n",
    "cost_grid = np.zeros_like(theta0_grid)\n",
    "\n",
    "for i in range(len(theta0_range)):\n",
    "    for j in range(len(theta1_range)):\n",
    "        theta_temp = np.array([[theta0_range[i]], [theta1_range[j]]])\n",
    "        cost_grid[j, i] = (1/(2*m)) * np.sum((X_b.dot(theta_temp) - y)**2)\n",
    "\n",
    "axes[1].contour(theta0_grid, theta1_grid, cost_grid, levels=20, cmap='viridis')\n",
    "\n",
    "# ç»˜åˆ¶è·¯å¾„\n",
    "for hist, color, label in [(history_bgd, 'b', 'æ‰¹é‡GD'), \n",
    "                           (history_sgd, 'r', 'éšæœºGD'),\n",
    "                           (history_mb, 'g', 'Mini-batch')]:\n",
    "    theta_hist = np.array(hist['theta']).squeeze()\n",
    "    axes[1].plot(theta_hist[:, 0], theta_hist[:, 1], f'{color}.-', \n",
    "                linewidth=1, markersize=3, alpha=0.7, label=label)\n",
    "\n",
    "axes[1].scatter(TRUE_THETA_0, TRUE_THETA_1, c='black', s=100, marker='*', \n",
    "               zorder=5, label='æœ€ä¼˜ç‚¹')\n",
    "axes[1].set_xlabel('Î¸â‚€')\n",
    "axes[1].set_ylabel('Î¸â‚')\n",
    "axes[1].set_title('å‚æ•°ç©ºé—´ä¸­çš„ä¼˜åŒ–è·¯å¾„', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nè§‚å¯Ÿç»“è®º:\")\n",
    "print(\"  - æ‰¹é‡GD: è·¯å¾„å¹³æ»‘ï¼Œç›´æ¥èµ°å‘æœ€ä¼˜\")\n",
    "print(\"  - éšæœºGD: è·¯å¾„éœ‡è¡ï¼Œä½†æœ€ç»ˆæ”¶æ•›\")\n",
    "print(\"  - Mini-batch: ä»‹äºä¸¤è€…ä¹‹é—´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ ç¬¬4éƒ¨åˆ†ï¼šä½¿ç”¨Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ä½¿ç”¨sklearnçš„SGDRegressor\n",
    "# ============================================================\n",
    "\n",
    "sgd_reg = SGDRegressor(\n",
    "    max_iter=1000,          # æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "    tol=1e-3,               # æ”¶æ•›é˜ˆå€¼\n",
    "    penalty=None,           # æ— æ­£åˆ™åŒ–\n",
    "    eta0=0.1,               # åˆå§‹å­¦ä¹ ç‡\n",
    "    learning_rate='invscaling',  # å­¦ä¹ ç‡è¡°å‡ç­–ç•¥\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "print(\"sklearn SGDRegressorç»“æœ:\")\n",
    "print(f\"  Î¸â‚€: {sgd_reg.intercept_[0]:.6f}\")\n",
    "print(f\"  Î¸â‚: {sgd_reg.coef_[0]:.6f}\")\n",
    "\n",
    "print(f\"\\nçœŸå®å€¼:\")\n",
    "print(f\"  Î¸â‚€: {TRUE_THETA_0}\")\n",
    "print(f\"  Î¸â‚: {TRUE_THETA_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ æ€»ç»“\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "1. âœ… **SGD**æ¯æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬æ›´æ–°ï¼Œé€Ÿåº¦å¿«ä½†ä¸ç¨³å®š\n",
    "2. âœ… **Mini-batch GD**å¹³è¡¡äº†é€Ÿåº¦å’Œç¨³å®šæ€§\n",
    "3. âœ… **å­¦ä¹ ç‡è¡°å‡**å¸®åŠ©SGDç¨³å®šæ”¶æ•›\n",
    "4. âœ… åœ¨å¤§æ•°æ®é›†ä¸Šï¼ŒSGDå’ŒMini-batchæ›´å®ç”¨\n",
    "\n",
    "### æ–¹æ³•é€‰æ‹©æŒ‡å—\n",
    "\n",
    "| åœºæ™¯ | æ¨èæ–¹æ³• |\n",
    "|------|----------|\n",
    "| å°æ•°æ®é›† | æ‰¹é‡GD |\n",
    "| å¤§æ•°æ®é›† | Mini-batch GD |\n",
    "| åœ¨çº¿å­¦ä¹  | SGD |\n",
    "| GPUè®­ç»ƒ | Mini-batch GD |\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "- ğŸ“˜ åŠ¨é‡æ³• (Momentum)\n",
    "- ğŸ“™ Adamä¼˜åŒ–å™¨\n",
    "- ğŸ“• å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥\n",
    "\n",
    "## ğŸ“š å‚è€ƒèµ„æ–™\n",
    "\n",
    "- [Scikit-learn: SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "- Bottou, L. (2010). \"Large-Scale Machine Learning with Stochastic Gradient Descent\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
