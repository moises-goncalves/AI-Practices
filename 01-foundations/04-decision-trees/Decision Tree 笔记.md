# 第六章深度笔记：决策树 —— 从可解释模型到集成基石

> **核心主题**：CART 决策树、分裂准则、正则化、防止过拟合、回归树与不稳定性  
> **前置知识**：熵与信息增益、基本概率知识、树结构与递归

---

## 📋 本章知识图谱

```
决策树
├── 模型与直观
│   ├── if–else 规则与划分空间
│   └── 分类树 vs 回归树
├── CART 算法
│   ├── 二叉树结构
│   ├── 分裂准则：Gini / Entropy / MSE
│   └── 训练与预测复杂度
├── 模型容量与正则化
│   ├── max_depth / min_samples_leaf / max_leaf_nodes
│   ├── 预剪枝 vs 后剪枝 (ccp_alpha)
│   └── 偏差–方差权衡
├── 决策树回归
│   ├── 分裂依据与预测方式
│   └── 阶梯状预测与不可外推
└── 不稳定性与集成
    ├── 高方差与对数据扰动的敏感性
    ├── 轴平行分裂与“旋转问题”
    └── 随机森林、梯度提升作为自然扩展
```

---

## 1. 决策树的直观与形式

### 1.1 如果–那么 规则与划分空间

- 决策树通过一系列 “如果特征满足某条件，则走左/右子树” 的规则，将特征空间划分为若干区域；
- 每一个叶子节点对应一个区域：
  - 分类任务：叶子节点存储各类别的经验分布，常用**多数类**作为预测；
  - 回归任务：叶子节点存储目标值的平均数或中位数。

优点：

- 可视化简单，易于解释（白盒模型）；
- 能处理数值特征与离散特征；
- 对特征缩放不敏感。

缺点：

- 对训练数据极敏感，具有**高方差**；
- 易过拟合，需依赖正则化或集成方法。

---

## 2. CART 决策树算法

Scikit‑learn 中的 `DecisionTreeClassifier` / `DecisionTreeRegressor` 使用 CART（Classification And Regression Trees）算法：

- 始终构建**二叉树**：每次分裂成“左子节点 + 右子节点”；
- 使用贪心策略：在当前节点上，枚举特征及其阈值，选择可最大化某种“纯度提升”的划分；
- 递归地对子节点重复该过程，直到不能进一步分裂或达到停止条件。

### 2.1 分类树的分裂准则

假设在某节点 $t$ 中，类别比例为 $p_{k}$（第 $k$ 类样本占比），则：

- **基尼不纯度 (Gini)**：
  $$
  G(t) = 1 - \sum_k p_k^2.
  $$
- **熵 (Entropy)**：
  $$
  H(t) = -\sum_k p_k \log_2 p_k.
  $$

分裂质量由“父节点纯度”与“子节点加权纯度”之间的差值度量。  
在实践中：

- Gini 与 Entropy 的效果通常非常接近；
- Gini 计算更快，是 Scikit‑learn 的默认值；
- Entropy 更贴近信息增益概念，有时会产生稍微不同的划分，但差别不大。

### 2.2 回归树的分裂准则

在回归任务中，节点上预测值通常为该节点目标值的平均：

$$
\hat{y}_t = \frac{1}{|t|} \sum_{i\in t} y_i.
$$

分裂目标为减少节点内的平方误差 (MSE)：

$$
J(t) = \frac{1}{|t|} \sum_{i\in t} (y_i - \hat{y}_t)^2.
$$

算法选择使得整体 MSE 降低最多的划分。

### 2.3 训练与预测复杂度

- 训练：
  - 最坏情况为 $O(m n \log m)$，其中 $m$ 为样本数，$n$ 为特征数；
  - 需要为多个特征尝试多个划分阈值。
- 预测：
  - 每个样本仅需沿树从根走到叶，复杂度近似 $O(\text{深度})$，通常接近 $O(\log m)$；
  - 预测非常快，适合在线推理或大规模预测场景。

---

## 3. 模型容量与正则化

未经约束的决策树可以一直分裂：

- 分类：直至每个叶子纯度为 1 或样本数为 1；
- 回归：直至每个叶子只包含一个样本。

这会带来严重过拟合。需要通过**预剪枝**和/或**后剪枝**来正则化。

### 3.1 预剪枝参数（训练前限制）

常见预剪枝参数：

- `max_depth`：树的最大深度。通常是控制容量最直接、最有效的参数；
- `min_samples_split`：一个内部节点至少有多少样本才能继续分裂；
- `min_samples_leaf`：一个叶子节点至少包含多少样本；
- `max_leaf_nodes`：最大叶子数量。

实践建议：

- 首先调节 `max_depth` 和 `min_samples_leaf`；
- 使用交叉验证搜索小范围的组合，例如：

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {
    "max_depth": [3, 5, 7, None],
    "min_samples_leaf": [1, 5, 10, 20]
}

grid = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring="accuracy"
)
grid.fit(X_train, y_train)
best_tree = grid.best_estimator_
```

### 3.2 后剪枝（成本复杂度剪枝）

Scikit‑learn 提供基于成本复杂度的后剪枝参数 `ccp_alpha`：

$$
R_\alpha(T) = R(T) + \alpha \cdot \lvert T\rvert,
$$

其中 $R(T)$ 为原树的经验风险，$\lvert T\rvert$ 为叶子数。  
增大 $\alpha$ 会鼓励更小更简单的树。

典型流程：

1. 使用 `DecisionTreeClassifier.ccp_path` 得到一系列候选 $\alpha$；  
2. 对不同 $\alpha$ 交叉验证，选取验证集误差最小的值。

---

## 4. 决策树回归的特性

### 4.1 阶梯状预测与局部常数

- 决策树回归在每个叶子输出常数值；
- 整体预测函数呈**阶梯状**，在特征空间不同区域形成“台阶”；
- 与线性回归相比：
  - 可自然表达强非线性关系；
  - 但缺乏平滑性，对小扰动不连续。

### 4.2 无法外推

- 线性模型可以在训练区间外做**外推**（例如延伸趋势线）；  
- 决策树预测始终为训练样本目标值的加权平均：
  - 若训练数据的最大房价为 100 万，树无法预测出 > 100 万的值；
  - 在许多任务中，这种“局部插值”特性是限制因素。

---

## 5. 不稳定性与高方差

### 5.1 对数据扰动的敏感性

- CART 使用贪心策略：在根节点选择当前最优特征和阈值；
- 训练数据稍微改变（添加/删除少量样本），根节点选择可能全变；
- 后续划分完全依赖前面决策，导致整棵树结构截然不同——**高方差**模型。

### 5.2 轴平行分裂与“旋转问题”

- 决策树仅使用形式如 `x_j \le \tau` 的轴平行分裂；
- 若真实决策边界是斜线或曲线，则树必须用大量“阶梯”近似；
- 在特征经过旋转后，这种现象尤为明显；
- 虽可通过 PCA 等方法旋转特征，但会牺牲解释性。

### 5.3 集成方法是自然解法

决策树的不稳定性是**随机森林、梯度提升树**等集成方法存在的主要动机：

- 随机森林通过：
  - 对样本做自助采样（bootstrap）；
  - 在每个分裂处随机子集特征；
  生成大量“多样化”的树，再对它们预测做平均/投票；
- 梯度提升树通过：
  - 串行地拟合残差；
  - 控制每棵树的深度和学习率；
  减少偏差并通过正则化控制方差。

这些方法在下一章中会详细展开。

---

## 6. 实验建议

建议在 `训练和可视化决策树.ipynb` 与 `决策树回归.ipynb` 中系统完成以下实验：

1. **分类树可视化**：  
   - 在二维玩具数据集（如 moons、circles）上训练浅树 (`max_depth=2,3`)；  
   - 可视化决策边界与树结构，直观理解轴平行分裂。

2. **深度与过拟合**：  
   - 固定数据集，逐步增加 `max_depth`；  
   - 绘制训练/验证集准确率随深度变化曲线（学习曲线风格），观察过拟合点。

3. **Gini vs Entropy**：  
   - 在同一数据集上对比两种分裂准则在训练时间、树结构与验证性能上的差异。

4. **回归树的阶梯状预测**：  
   - 用一维非线性函数（如 $y = \sin x + \epsilon$）训练回归树；  
   - 绘制真实曲线与预测曲线，对比不同 `max_depth` 的拟合形状。

5. **决策树 vs 线性回归**：  
   - 在近似线性数据上对比两者的偏差–方差表现；  
   - 在明显非线性数据上再对比一次，感受模型选择的重要性。

通过这些实验，你可以把“决策树是高方差、易过拟合、但非常可解释的基学习器”这一结论真正内化。

