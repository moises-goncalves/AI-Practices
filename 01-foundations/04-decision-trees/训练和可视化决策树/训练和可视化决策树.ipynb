{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 决策树分类：训练与可视化 (Decision Tree Classification)\n",
    "\n",
    "本 notebook 系统性地介绍决策树分类器的训练、可视化与评估，涵盖以下核心内容：\n",
    "\n",
    "1. **CART 算法原理**：二叉树结构与分裂准则\n",
    "2. **决策边界可视化**：直观理解轴平行分裂特性\n",
    "3. **分裂准则对比**：Gini 不纯度 vs 信息熵\n",
    "4. **正则化与过拟合**：超参数对模型复杂度的影响\n",
    "5. **模型评估**：混淆矩阵、分类报告与交叉验证\n",
    "\n",
    "---\n",
    "\n",
    "## 核心知识点\n",
    "\n",
    "- **CART 算法**：Classification And Regression Trees，始终构建二叉树\n",
    "- **分裂准则**：Gini 不纯度 $G(t) = 1 - \\sum_k p_k^2$，信息熵 $H(t) = -\\sum_k p_k \\log_2 p_k$\n",
    "- **轴平行分裂**：决策树只能产生与坐标轴平行的决策边界\n",
    "- **高方差模型**：对训练数据敏感，需要正则化或集成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 数值计算\n",
    "import numpy as np\n",
    "\n",
    "# 可视化\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# scikit-learn 模块\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.datasets import load_iris, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"环境配置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. 数据准备：Iris 数据集\n",
    "\n",
    "使用经典的 Iris（鸢尾花）数据集：\n",
    "- 150 个样本，3 个类别\n",
    "- 4 个特征：花萼长度/宽度、花瓣长度/宽度\n",
    "- 为便于可视化，我们选取花瓣长度和宽度两个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-iris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 Iris 数据集\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:]  # 只取花瓣长度和宽度，便于 2D 可视化\n",
    "y = iris.target\n",
    "\n",
    "feature_names = iris.feature_names[2:]\n",
    "class_names = iris.target_names\n",
    "\n",
    "print(f\"数据集形状: X={X.shape}, y={y.shape}\")\n",
    "print(f\"特征名称: {feature_names}\")\n",
    "print(f\"类别名称: {class_names}\")\n",
    "print(f\"类别分布: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化原始数据分布\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for i, (color, marker, name) in enumerate(zip(colors, markers, class_names)):\n",
    "    mask = y == i\n",
    "    ax.scatter(X[mask, 0], X[mask, 1], c=color, marker=marker, \n",
    "               s=50, label=name, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "ax.set_xlabel(feature_names[0])\n",
    "ax.set_ylabel(feature_names[1])\n",
    "ax.set_title('Iris Dataset: Petal Features')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"训练集: {len(X_train)} 样本\")\n",
    "print(f\"测试集: {len(X_test)} 样本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. 训练决策树分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练决策树分类器\n",
    "tree_clf = DecisionTreeClassifier(\n",
    "    max_depth=3,\n",
    "    criterion='gini',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# 模型基本信息\n",
    "print(\"决策树分类器训练完成\")\n",
    "print(f\"  树深度: {tree_clf.get_depth()}\")\n",
    "print(f\"  叶子节点数: {tree_clf.get_n_leaves()}\")\n",
    "print(f\"  特征数: {tree_clf.n_features_in_}\")\n",
    "print(f\"  类别数: {tree_clf.n_classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. 决策树结构可视化\n",
    "\n",
    "决策树的可解释性是其最大优势之一。下面展示两种可视化方式：\n",
    "1. 图形化树结构\n",
    "2. 文本规则表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tree-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图形化决策树结构\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "plot_tree(\n",
    "    tree_clf,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=11,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Decision Tree Structure (max_depth=3)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n节点说明:\")\n",
    "print(\"  - gini: Gini 不纯度，越小越纯\")\n",
    "print(\"  - samples: 落入该节点的样本数\")\n",
    "print(\"  - value: 各类别的样本数 [setosa, versicolor, virginica]\")\n",
    "print(\"  - class: 该节点的预测类别（多数类）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tree-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本规则表示\n",
    "tree_rules = export_text(tree_clf, feature_names=list(feature_names))\n",
    "print(\"决策规则 (文本格式):\")\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. 决策边界可视化\n",
    "\n",
    "决策树的一个重要特性是**轴平行分裂**：所有决策边界都与坐标轴平行。\n",
    "这使得决策树难以学习斜线或曲线形式的真实决策边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision-boundary-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, ax, title, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    绘制决策边界\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : 已训练的分类器\n",
    "    X : 特征矩阵 (n_samples, 2)\n",
    "    y : 标签向量\n",
    "    ax : matplotlib axes 对象\n",
    "    title : 图标题\n",
    "    feature_names : 特征名称列表\n",
    "    class_names : 类别名称列表\n",
    "    \"\"\"\n",
    "    # 创建网格\n",
    "    h = 0.02  # 网格步长\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # 预测网格点\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # 绘制决策区域\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)\n",
    "    ax.contour(xx, yy, Z, colors='black', linewidths=0.5, alpha=0.5)\n",
    "    \n",
    "    # 绘制样本点\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    for i, (color, name) in enumerate(zip(colors, class_names)):\n",
    "        mask = y == i\n",
    "        ax.scatter(X[mask, 0], X[mask, 1], c=color, s=30, \n",
    "                   label=name, edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_ylabel(feature_names[1])\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper left', fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision-boundary-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化不同深度的决策边界\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "depths = [1, 2, 3, None]  # None 表示无限制\n",
    "\n",
    "for ax, depth in zip(axes.ravel(), depths):\n",
    "    clf = DecisionTreeClassifier(max_depth=depth, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    \n",
    "    depth_str = str(depth) if depth else 'None'\n",
    "    title = f'max_depth={depth_str}\\nTrain Acc={train_acc:.3f}, Test Acc={test_acc:.3f}'\n",
    "    \n",
    "    plot_decision_boundary(clf, X, y, ax, title, feature_names, class_names)\n",
    "\n",
    "plt.suptitle('Decision Boundary Evolution with Increasing Depth', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. 分裂准则对比：Gini vs Entropy\n",
    "\n",
    "两种常用的分裂准则：\n",
    "\n",
    "**Gini 不纯度**：\n",
    "$$G(t) = 1 - \\sum_{k=1}^{K} p_k^2$$\n",
    "\n",
    "**信息熵**：\n",
    "$$H(t) = -\\sum_{k=1}^{K} p_k \\log_2 p_k$$\n",
    "\n",
    "实践中两者效果通常非常接近，Gini 计算更快是默认选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gini-entropy-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比 Gini 和 Entropy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, criterion in zip(axes, ['gini', 'entropy']):\n",
    "    clf = DecisionTreeClassifier(max_depth=3, criterion=criterion, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    \n",
    "    title = f'Criterion: {criterion.upper()}\\nTrain Acc={train_acc:.3f}, Test Acc={test_acc:.3f}'\n",
    "    plot_decision_boundary(clf, X, y, ax, title, feature_names, class_names)\n",
    "\n",
    "plt.suptitle('Gini vs Entropy: Decision Boundary Comparison', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criterion-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 Gini 和 Entropy 函数曲线（二分类情况）\n",
    "p = np.linspace(0.001, 0.999, 100)\n",
    "\n",
    "# 二分类情况下的公式\n",
    "gini = 2 * p * (1 - p)\n",
    "entropy = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(p, gini, 'b-', linewidth=2, label='Gini: 2p(1-p)')\n",
    "ax.plot(p, entropy, 'r-', linewidth=2, label='Entropy: -p·log₂(p) - (1-p)·log₂(1-p)')\n",
    "ax.plot(p, entropy / 2, 'r--', linewidth=1.5, label='Entropy / 2 (scaled)')\n",
    "\n",
    "ax.axvline(x=0.5, color='gray', linestyle=':', alpha=0.7)\n",
    "ax.set_xlabel('Class proportion p')\n",
    "ax.set_ylabel('Impurity')\n",
    "ax.set_title('Comparison of Gini Impurity and Entropy (Binary Classification)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：\")\n",
    "print(\"  - 两种指标在 p=0 和 p=1 时都为 0（纯净）\")\n",
    "print(\"  - 两种指标在 p=0.5 时都达到最大值（最不纯净）\")\n",
    "print(\"  - Entropy 缩放后与 Gini 形状非常接近\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. 概率预测\n",
    "\n",
    "决策树不仅可以预测类别，还可以输出概率。\n",
    "概率 = 叶子节点中各类别样本的比例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probability-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 概率预测示例\n",
    "test_samples = np.array([\n",
    "    [5.0, 1.5],  # 可能是 versicolor\n",
    "    [1.5, 0.5],  # 可能是 setosa\n",
    "    [6.0, 2.2],  # 可能是 virginica\n",
    "])\n",
    "\n",
    "print(\"概率预测示例:\")\n",
    "print(f\"{'Sample':<20} {'setosa':>10} {'versicolor':>12} {'virginica':>12} {'Prediction':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for sample in test_samples:\n",
    "    proba = tree_clf.predict_proba([sample])[0]\n",
    "    pred = tree_clf.predict([sample])[0]\n",
    "    print(f\"{str(sample):<20} {proba[0]:>10.3f} {proba[1]:>12.3f} {proba[2]:>12.3f} {class_names[pred]:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "print(\"分类报告:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# 混淆矩阵\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=class_names,\n",
    "       yticklabels=class_names,\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label',\n",
    "       title='Confusion Matrix')\n",
    "\n",
    "# 在格子中显示数值\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. 超参数调优\n",
    "\n",
    "通过网格搜索找到最佳超参数组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网格搜索\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"网格搜索结果:\")\n",
    "print(f\"  最佳参数: {grid_search.best_params_}\")\n",
    "print(f\"  最佳交叉验证准确率: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 使用最佳模型在测试集上评估\n",
    "best_tree = grid_search.best_estimator_\n",
    "test_acc = accuracy_score(y_test, best_tree.predict(X_test))\n",
    "print(f\"  测试集准确率: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. 决策树的局限性：旋转问题\n",
    "\n",
    "由于决策树只能产生轴平行的决策边界，对于需要斜线分割的数据表现不佳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotation-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建需要斜线分割的数据\n",
    "np.random.seed(RANDOM_STATE)\n",
    "n = 100\n",
    "\n",
    "# 原始数据（可以用直线分割）\n",
    "X_simple = np.random.randn(n, 2)\n",
    "y_simple = (X_simple[:, 0] > 0).astype(int)\n",
    "\n",
    "# 旋转 45 度\n",
    "angle = np.pi / 4\n",
    "rotation_matrix = np.array([\n",
    "    [np.cos(angle), -np.sin(angle)],\n",
    "    [np.sin(angle), np.cos(angle)]\n",
    "])\n",
    "X_rotated = X_simple @ rotation_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, X_data, title in zip(axes, [X_simple, X_rotated], ['Original Data', 'Rotated Data (45°)']):\n",
    "    # 训练决策树\n",
    "    clf = DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_data, y_simple)\n",
    "    \n",
    "    # 绘制决策边界\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    ax.scatter(X_data[y_simple==0, 0], X_data[y_simple==0, 1], c='red', s=30, label='Class 0')\n",
    "    ax.scatter(X_data[y_simple==1, 0], X_data[y_simple==1, 1], c='blue', s=30, label='Class 1')\n",
    "    ax.set_title(f'{title}\\nDepth={clf.get_depth()}, Leaves={clf.get_n_leaves()}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Decision Tree Rotation Problem: Axis-Parallel Splits', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：\")\n",
    "print(\"  - 原始数据只需要一条垂直线即可分割\")\n",
    "print(\"  - 旋转后的数据需要多条阶梯状边界近似斜线\")\n",
    "print(\"  - 这导致模型复杂度增加，泛化能力下降\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "## 11. 特征重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用完整特征训练\n",
    "X_full = iris.data\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "tree_full = DecisionTreeClassifier(max_depth=4, random_state=RANDOM_STATE)\n",
    "tree_full.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 特征重要性\n",
    "importances = tree_full.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(range(len(importances)), importances[indices], color='steelblue')\n",
    "ax.set_yticks(range(len(importances)))\n",
    "ax.set_yticklabels([iris.feature_names[i] for i in indices])\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Decision Tree: Feature Importance (Iris Dataset)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"特征重要性排名:\")\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"  {i+1}. {iris.feature_names[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12",
   "metadata": {},
   "source": [
    "## 12. 总结\n",
    "\n",
    "### 决策树分类器优点\n",
    "\n",
    "1. **可解释性强**：可以可视化树结构，理解决策过程\n",
    "2. **无需特征缩放**：对特征的量纲不敏感\n",
    "3. **能处理非线性关系**：通过多层分裂捕获复杂模式\n",
    "4. **支持多分类**：天然支持多类别问题\n",
    "\n",
    "### 决策树分类器缺点\n",
    "\n",
    "1. **高方差**：对训练数据敏感，容易过拟合\n",
    "2. **轴平行限制**：只能产生与坐标轴平行的决策边界\n",
    "3. **不稳定**：数据小变化可能导致树结构大变化\n",
    "\n",
    "### 实践建议\n",
    "\n",
    "- 优先使用正则化参数（`max_depth`, `min_samples_leaf`）\n",
    "- 使用交叉验证选择超参数\n",
    "- 考虑使用集成方法（随机森林、梯度提升）提升性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unit-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 单元测试：验证代码正确性\n",
    "# ============================================================\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"运行基础功能测试\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"运行单元测试...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 测试 1: 模型训练\n",
    "    try:\n",
    "        clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        assert clf.get_depth() <= 3\n",
    "        print(\"[PASS] 测试 1: 模型训练成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] 测试 1: {e}\")\n",
    "    \n",
    "    # 测试 2: 预测输出\n",
    "    try:\n",
    "        pred = clf.predict(X_test)\n",
    "        assert pred.shape == (len(X_test),)\n",
    "        assert all(p in [0, 1, 2] for p in pred)\n",
    "        print(\"[PASS] 测试 2: 预测输出正确\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] 测试 2: {e}\")\n",
    "    \n",
    "    # 测试 3: 概率预测\n",
    "    try:\n",
    "        proba = clf.predict_proba(X_test)\n",
    "        assert proba.shape == (len(X_test), 3)\n",
    "        assert np.allclose(proba.sum(axis=1), 1.0)\n",
    "        print(\"[PASS] 测试 3: 概率预测正确\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] 测试 3: {e}\")\n",
    "    \n",
    "    # 测试 4: 特征重要性\n",
    "    try:\n",
    "        imp = clf.feature_importances_\n",
    "        assert len(imp) == X_train.shape[1]\n",
    "        assert abs(sum(imp) - 1.0) < 1e-6\n",
    "        print(\"[PASS] 测试 4: 特征重要性正确\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] 测试 4: {e}\")\n",
    "    \n",
    "    # 测试 5: 交叉验证\n",
    "    try:\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=3)\n",
    "        assert len(scores) == 3\n",
    "        assert all(0 <= s <= 1 for s in scores)\n",
    "        print(\"[PASS] 测试 5: 交叉验证成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] 测试 5: {e}\")\n",
    "    \n",
    "    # 测试 6: Gini vs Entropy\n",
    "    try:\n",
    "        clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "        clf_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)\n",
    "        clf_gini.fit(X_train, y_train)\n",
    "        clf_entropy.fit(X_train, y_train)\n",
    "        assert clf_gini.get_depth() <= 3\n",
    "        assert clf_entropy.get_depth() <= 3\n",
    "        print(\"[PASS] 测试 6: Gini/Entropy 准则正常\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] 测试 6: {e}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"测试完成!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "run_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
