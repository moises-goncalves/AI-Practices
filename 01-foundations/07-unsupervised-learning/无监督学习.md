# 第九章深度笔记：无监督学习与聚类

> **核心主题**：聚类、异常检测、密度估计（K‑Means、DBSCAN、Gaussian Mixture）  
> **前置知识**：距离度量、概率密度、上一章降维方法（PCA 等）

---

## 本章知识图谱

```
无监督学习
├── 任务类型
│   ├── 聚类 (Clustering)
│   ├── 异常检测 (Anomaly Detection)
│   └── 密度估计 (Density Estimation)
├── K-Means 及其变体
│   ├── 标准 K-Means
│   ├── Mini-Batch K-Means
│   ├── K 的选择：肘部法则 / 轮廓系数
│   └── 局限性：形状、尺度、不平衡
├── 基于密度的聚类：DBSCAN
│   ├── eps 与 min_samples
│   ├── 噪声点与簇形状
│   └── 预测新样本的“二阶段方案”
└── 概率模型：Gaussian Mixture
    ├── 软聚类与后验概率
    ├── EM 算法直观
    ├── 协方差结构与椭球簇
    └── BIC/AIC 选择成分数
```

---

## 1. 无监督学习的目标

无监督学习在**没有标签**的情况下，从数据中发现结构：

- 聚类：将“相似”的点分组；
- 异常检测：找出“格格不入”的点；
- 密度估计：估计数据的概率分布形状，可为前两者提供基础。

与有监督学习不同，无监督通常没有唯一“正确答案”，更多强调**探索性分析**与辅助建模。

---

## 2. K‑Means 与变体

### 2.1 标准 K‑Means

目标：将 $m$ 个样本划分为 $k$ 个簇，最小化簇内平方误差：

$$
J = \sum_{i=1}^k \sum_{x \in C_i} \|x - \mu_i\|^2,
$$

其中 $\mu_i$ 为簇 $C_i$ 的质心。

算法步骤：

1. 初始化 $k$ 个中心（常用 k‑means++）；
2. 分配：将每个样本分配给最近的中心；
3. 更新：重新计算每个簇的中心为该簇样本的均值；
4. 重复 2–3 步直到收敛（中心不再变化或迭代次数上限）。

典型用法：

```python
from sklearn.cluster import KMeans

kmeans = KMeans(
    n_clusters=5,
    init="k-means++",
    n_init=10,
    max_iter=300,
    random_state=42
)
labels = kmeans.fit_predict(X_scaled)
inertia = kmeans.inertia_
```

### 2.2 K 的选择

1. **肘部法则 (Elbow)**：
   - 绘制 $k$ 与 `inertia_` 的关系曲线；
   - 选择曲线由快速下降转为缓慢下降的“拐点”。

2. **轮廓系数 (Silhouette Score)**：

对每个样本 $i$：

- $a(i)$：与同簇其他点的平均距离（簇内紧凑度）；
- $b(i)$：与最近邻簇中所有点的平均距离（簇间分离度）；

轮廓系数：
$$
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}\in[-1,1].
$$

整体轮廓系数为 $s(i)$ 的平均值，越接近 1 越好。

```python
from sklearn.metrics import silhouette_score

best_k, best_score = None, -1
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    if score > best_score:
        best_k, best_score = k, score
```

注意：轮廓系数要计算点对距离，复杂度较高，不宜在超大数据集上频繁使用。

### 2.3 Mini‑Batch K‑Means

在大规模数据上加速 K‑Means：

- 每次迭代仅使用一个小批量样本更新中心；
- 极大降低每轮计算成本，收敛到相近的局部最优。

```python
from sklearn.cluster import MiniBatchKMeans

mbk = MiniBatchKMeans(
    n_clusters=5,
    batch_size=256,
    n_init=10,
    random_state=42
)
mbk.fit(X_scaled)
```

适用于样本数极大（>10^5）或在线场景。

### 2.4 K‑Means 的局限性

1. **必须预先指定 $k$**；  
2. 隐含假设簇为**大小相近的球形**：
   - 不能表达月牙形、环形等复杂形状；
   - 对簇大小与密度差异敏感；
3. 对异常值敏感（均值受极端点影响明显）；  
4. 对特征缩放非常敏感，必须标准化特征。

这些局限正是 DBSCAN 和 Gaussian Mixture 存在的动机。

---

## 3. DBSCAN：基于密度的聚类与异常检测

### 3.1 核心概念

参数：

- `eps`：邻域半径；
- `min_samples`：成为核心点所需的最少邻居数。

定义：

- **核心点**：其 `eps` 邻域中包含至少 `min_samples` 个样本；
- **边界点**：自身邻居不足以成为核心点，但落在某个核心点邻域内；
- **噪声点**：既不是核心点也不是任何核心点的邻居。

算法通过密度可达概念连通核心点及其邻域，形成任意形状的簇。

### 3.2 优点与局限

优点：

- 无需预先指定簇数 $k$；
- 可以发现任意形状的簇；
- 自然区分噪声点，适合作为异常检测工具。

局限：

- 参数 `eps`、`min_samples` 对结果影响巨大，调参困难；
- 对不同簇密度差异大的数据不鲁棒；
- 标准实现预测新样本比较麻烦（无内置 `predict`）。

### 3.3 实战用法与新样本预测

```python
from sklearn.cluster import DBSCAN
from sklearn.neighbors import KNeighborsClassifier

db = DBSCAN(eps=0.5, min_samples=5)
labels = db.fit_predict(X_scaled)

# 噪声点标签为 -1
core_mask = labels != -1
X_core = X_scaled[core_mask]
y_core = labels[core_mask]

# 二阶段方案：用 KNN 学习“簇标签”以预测新点
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_core, y_core)

# 对新样本 X_new_scaled 预测簇标签（或判断为异常）
pred_label = knn.predict(X_new_scaled)
```

在本章的 `DBSCAN/make_moon_based_on_DBSCAN.ipynb` 中，可以直观看到 DBSCAN 在非凸簇上的优势。

---

## 4. Gaussian Mixture：概率视角下的软聚类

### 4.1 模型与 EM 算法直观

假设数据来自 $k$ 个高斯分布的混合：

$$
p(x) = \sum_{j=1}^k \pi_j \mathcal{N}(x\mid \mu_j, \Sigma_j),
$$

其中 $\pi_j$ 为混合权重，$\mu_j$ 为均值，$\Sigma_j$ 为协方差矩阵。

EM 算法：

1. **E 步**：计算样本属于每个成分的后验概率（“责任度”）；
2. **M 步**：利用责任度加权更新每个成分的参数 $(\pi_j,\mu_j,\Sigma_j)$；
3. 交替迭代直至收敛。

### 4.2 软聚类与异常检测

Gaussian Mixture 与 K‑Means 的本质区别：

- K‑Means：硬聚类，每个点属于唯一簇；
- GMM：软聚类，每个点有属于各簇的概率向量。

```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(
    n_components=3,
    covariance_type="full",
    n_init=10,
    random_state=42
)
gmm.fit(X_scaled)

hard_labels = gmm.predict(X_scaled)        # 硬聚类
proba = gmm.predict_proba(X_scaled)       # 软聚类
log_density = gmm.score_samples(X_scaled) # 对数密度，用于异常检测
```

异常检测思路：将对数密度低于一定分位数（如 1%）的样本标记为异常点。

### 4.3 协方差结构与成分数选择

`covariance_type` 控制簇形状：

- `"full"`：每个成分拥有独立的全协方差矩阵（最灵活）；
- `"tied"`：所有成分共享同一协方差矩阵；
- `"diag"`：每个成分协方差为对角矩阵，轴对齐椭球；
- `"spherical"`：各向同性球体，接近 K‑Means 的假设。

选择成分数 `n_components` 常用 BIC / AIC：

```python
import numpy as np

bics = []
for k in range(1, 11):
    gmm_k = GaussianMixture(n_components=k, covariance_type="full", n_init=5, random_state=42)
    gmm_k.fit(X_scaled)
    bics.append(gmm_k.bic(X_scaled))

best_k = np.argmin(bics) + 1
```

---

## 5. 实验建议

配合本章的各个 Notebook（K‑Means、Mini‑Batch K‑Means、图像颜色聚类、DBSCAN、GMM），建议完成以下实验：

1. **K‑Means 在彩色图像上的颜色压缩**：  
   - 对图像像素 RGB 值运行 K‑Means，尝试不同 `k`；  
   - 可视化压缩后的图像，观察视觉质量与压缩比的权衡。

2. **K 的选择方法比较**：  
   - 在合成数据上对比肘部法则与轮廓系数给出的最优 $k$；  
   - 分析在不同簇形状/密度下两种方法的鲁棒性。

3. **DBSCAN vs K‑Means**：  
   - 在环形、月牙形数据上对比两者的聚类结果；  
   - 调整 `eps` 与 `min_samples`，观察噪声点标记与簇形状变化。

4. **GMM vs K‑Means**：  
   - 在椭圆形簇与方差差异大的簇上比较两者表现；  
   - 观察软聚类概率输出在边界区域的行为。

5. **异常检测实践**：  
   - 使用 GMM 或 DBSCAN 在信用卡交易、网络流量等数据上定位低密度区域样本；  
   - 与简单的 z‑score 或阈值检测对比。

---

## 小结

- K‑Means 提供简单高效的聚类基线，但假设簇为大小相近的球形；
- DBSCAN 在任意形状簇以及同时做聚类与异常检测时非常有用；
- Gaussian Mixture 提供带概率语义的软聚类与密度估计，是异常检测与生成建模的重要工具；
- 实战中常组合使用：先用 PCA 降维，再用 K‑Means / DBSCAN / GMM 聚类，以兼顾效率与表现。
