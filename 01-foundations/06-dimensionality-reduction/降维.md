# 第八章深度笔记：降维与表示学习

> **核心主题**：维度灾难、PCA 系列、核 PCA、流形学习、t‑SNE 与 LDA  
> **前置知识**：线性代数（特征分解 / SVD）、概率与协方差、基础优化

---

## 📋 本章知识图谱

``+
降维
├── 动机与维度灾难
│   ├── 稀疏性与距离失效
│   ├── 过拟合与模型复杂度
│   └── 可视化与特征压缩
├── 线性降维：PCA 家族
│   ├── 标准 PCA（特征向量 / SVD）
│   ├── 解释方差与成分选择
│   ├── 随机 / 增量 PCA
│   └── 重构误差与信息保留
├── 非线性特征映射：Kernel PCA
├── 流形学习
│   ├── LLE
│   ├── Isomap
│   ├── MDS
│   └── t‑SNE（可视化专用）
└── 有监督降维
    └── LDA（线性判别分析）
```

---

## 1. 维度灾难与降维动机

### 1.1 高维空间的稀疏性

- 在低维空间中，少量点即可“填满”空间；
- 在高维空间中，点之间的平均距离趋于接近，最近邻与最远邻的距离差缩小，**距离度量失去区分力**；
- 许多基于距离的算法（KNN、K‑Means、DBSCAN）在高维下表现退化。

### 1.2 过拟合与模型复杂度

- 维度越高，自由度越大，模型越容易完全匹配训练数据中的噪声；
- 在有限样本下，高维特征需要强烈正则化或降维，否则泛化误差大幅上升。

### 1.3 降维的主要目的

1. **加速计算**：降低特征维度，减少训练和预测成本；  
2. **提高泛化**：去除冗余与噪声特征，缓解过拟合；  
3. **可视化**：将高维数据映射到 2D/3D，以进行结构探索；  
4. **特征压缩**：在有限存储/带宽下保留主要信息。

---

## 2. PCA：最常用的线性降维方法

### 2.1 几何与统计视角

- 几何视角：在所有 $k$ 维线性子空间中，寻找使**投影后方差最大**的子空间；
- 统计视角：在均值为 0 的数据上，PCA 寻找协方差矩阵主特征向量：
  $$
  \Sigma = \frac{1}{m} X^\top X,\quad
  \Sigma u_i = \lambda_i u_i,\quad
  \lambda_1 \ge \lambda_2 \ge \dots
  $$
  其中 $u_i$ 为第 $i$ 个主成分方向，$\lambda_i$ 为解释的方差。

### 2.2 SVD 实现与解释方差

实际实现多采用 SVD：

$$
X = U \Sigma V^\top,\quad
V = [u_1,\dots,u_n].
$$

前 $k$ 个列向量 $u_1,\dots,u_k$ 定义了降维子空间：

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # 保留 95% 方差
X_pca = pca.fit_transform(X_scaled)
print("保留维度:", pca.n_components_)
print("解释方差比:", pca.explained_variance_ratio_)
```

要点：

- 设置 `n_components` 为浮点数 ∈ (0,1] 时，表示“保留多少比例的总方差”；
- `explained_variance_ratio_` 数组给出每个主成分解释的方差占比。

### 2.3 重构与信息损失

从低维表示近似重构原始数据：

```python
X_reconstructed = pca.inverse_transform(X_pca)
reconstruction_error = ((X_scaled - X_reconstructed) ** 2).mean()
```

重构误差越小，说明信息损失越少；  
在实践中通常在“压缩比”和“下游任务性能”之间权衡，而非追求极低重构误差。

### 2.4 增量 PCA 与随机 PCA

1. **Incremental PCA**：  
   - 可在小批量数据上逐块更新主成分，支持无法一次载入内存的大数据集；
   - 适用于流式数据或 out‑of‑core 计算。

   ```python
   from sklearn.decomposition import IncrementalPCA

   inc_pca = IncrementalPCA(n_components=k)
   for X_batch in batches:          # X_batch 为分块数据
       inc_pca.partial_fit(X_batch)
   X_reduced = inc_pca.transform(X_full)
   ```

2. **Randomized PCA** (`svd_solver="randomized"`)：  
   - 通过随机投影近似主成分，计算更快；
   - 在高维但 `n_components` 相对较小时非常实用。

---

## 3. Kernel PCA：线性方法的非线性扩展

### 3.1 思想与形式

- 目标：在通过非线性映射 $\phi(x)$ 的特征空间中执行 PCA；
- 直接显式计算 $\phi(x)$ 通常不可行（维度可能极高或无限维）；
- 核技巧：利用核函数
  $$
  K(x_i,x_j) = \langle \phi(x_i), \phi(x_j)\rangle
  $$
  代替显式内积，在对偶空间完成 PCA。

### 3.2 实战用法

```python
from sklearn.decomposition import KernelPCA

kpca = KernelPCA(
    n_components=2,
    kernel="rbf",
    gamma=0.04,
    fit_inverse_transform=True  # 允许近似逆变换
)
X_kpca = kpca.fit_transform(X_scaled)
X_approx = kpca.inverse_transform(X_kpca)
```

常用核：

- `"rbf"`：适合复杂非线性结构；
- `"poly"`：多项式关系；
- `"sigmoid"`：在某些特定任务下使用。

注意：

- 需要通过下游模型性能（如分类准确率）而非直接“解释方差”来调节 `gamma` 等参数；
- 逆变换是近似的，仅用于可视化或粗略重构。

---

## 4. 流形学习：非线性结构的展开

### 4.1 LLE：局部线性嵌入

假设数据分布在一个低维流形上，且局部区域近似线性。

- 对于每个点，寻找其 $K$ 个近邻，并求解线性重构权重：
  $$
  x_i \approx \sum_{j\in \mathcal{N}(i)} w_{ij} x_j.
  $$
- 在低维空间寻找 $y_i$，使这些权重关系仍尽可能成立。

```python
from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_lle = lle.fit_transform(X)
```

要点：

- `n_neighbors` 是关键超参数，过小会导致流形“断裂”，过大则趋近线性方法；
- 主要用于可视化和探索数据流形结构。

### 4.2 Isomap：测地距离 + MDS

思想：

1. 使用 KNN 图近似流形；
2. 在图上计算所有点对的最短路径（测地距离近似）；
3. 在测地距离矩阵上运行 MDS。

```python
from sklearn.manifold import Isomap

isomap = Isomap(n_components=2, n_neighbors=10)
X_iso = isomap.fit_transform(X)
```

相比 LLE，对流形的全局结构更敏感；但计算复杂度较高 ($O(n^2)$ 或更大)。

### 4.3 MDS：基于距离矩阵的嵌入

当只有点对距离/相似度而无显式特征时，可使用 MDS：

```python
from sklearn.manifold import MDS

mds = MDS(n_components=2, dissimilarity="precomputed")
X_mds = mds.fit(D).embedding_      # D 为距离矩阵
```

常用于根据“城市间距离”“物种相似性”等构建低维可视化。

### 4.4 t‑SNE：高维数据可视化利器

特点：

- 强调保持局部邻域结构；
- 适合在 2D/3D 上可视化高维嵌入（如词向量、图像特征）；
- 仅适合可视化，不适合作为下游模型输入。

```python
from sklearn.manifold import TSNE

tsne = TSNE(
    n_components=2,
    perplexity=30.0,
    learning_rate=200.0,
    n_iter=1000,
    random_state=42
)
X_tsne = tsne.fit_transform(X)
```

注意：

- 不要解读簇与簇之间的相对距离与大小，它们并不具有严格量化意义；
- 对超参数敏感、计算代价高，可先用 PCA 降到中等维度再用 t‑SNE。

---

## 5. 有监督降维：LDA

线性判别分析 (Linear Discriminant Analysis)：

- 利用类别标签寻找最能区分类别的低维投影；
- 目标：最大化类间散度、最小化类内散度。

与 PCA 的对比：

- PCA 完全不使用标签，是无监督的，最大化整体方差；
- LDA 使用标签，专注于提高分类可分性。

示例：

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)
```

注意：

- 最大可用 `n_components` 为 `n_classes - 1`；
- 常作为分类任务中的特征提取步骤，与 PCA 比较效果。

---

## 6. 实验建议

在本章的 PCA / KernelPCA / LLE / IncrementalPCA 等 Notebook 中，可设计如下实验：

1. **PCA 压缩与重构**：  
   - 在手写数字数据集上应用 PCA，尝试 `n_components=10, 30, 50, 100`；  
   - 观察重构图像质量与训练线性分类器时准确率之间的关系。

2. **PCA vs 原空间**：  
   - 在同一数据上训练 Logistic 回归或线性 SVM，比较是否在 PCA 空间中更易收敛、是否更易正则化。

3. **KernelPCA vs PCA**：  
   - 在非线性可分玩具数据上对比两者降维后的可视化与线性分类性能。

4. **LLE / Isomap 可视化**：  
   - 对“瑞士卷”等典型流形数据集，比较 PCA 与 LLE/Isomap 的 2D 嵌入图像。

5. **t‑SNE 探索高维嵌入**：  
   - 对预训练的图像/文本特征进行 t‑SNE，观察类内聚类与类间分离情况，辅助理解表示质量。

---

## 小结

- PCA 为默认首选的线性降维工具，配合正则化线性模型构成强大基线；
- KernelPCA 与流形学习方法提供非线性结构探索能力，但计算成本更高；
- t‑SNE 专注高维可视化，不适合直接用于预测模型输入；
- LDA 将“降维”和“监督信号”结合，是分类任务中值得尝试的特征提取手段。

