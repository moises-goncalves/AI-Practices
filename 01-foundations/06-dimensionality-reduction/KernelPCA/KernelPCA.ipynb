{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 核主成分分析（Kernel PCA）\n",
    "\n",
    "## 概述\n",
    "\n",
    "核主成分分析（Kernel PCA）是 PCA 的非线性扩展，通过核技巧将数据隐式映射到高维特征空间，然后在该空间中执行 PCA。\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "标准 PCA 只能捕捉数据中的线性结构。对于非线性可分的数据，Kernel PCA 通过以下步骤工作：\n",
    "\n",
    "1. 使用非线性映射 $\\phi(x)$ 将数据映射到高维特征空间\n",
    "2. 在特征空间中执行标准 PCA\n",
    "3. 利用核函数 $K(x_i, x_j) = \\langle\\phi(x_i), \\phi(x_j)\\rangle$ 避免显式计算 $\\phi(x)$\n",
    "\n",
    "## 常用核函数\n",
    "\n",
    "- **RBF（径向基函数）核**：$K(x, y) = \\exp(-\\gamma\\|x-y\\|^2)$\n",
    "- **多项式核**：$K(x, y) = (\\gamma x^\\top y + c_0)^d$\n",
    "- **Sigmoid 核**：$K(x, y) = \\tanh(\\gamma x^\\top y + c_0)$\n",
    "\n",
    "## 本节内容\n",
    "\n",
    "1. Kernel PCA 基本使用\n",
    "2. 不同核函数的比较\n",
    "3. 超参数调优\n",
    "4. 逆变换（近似重构）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.datasets import make_moons, make_circles, make_swiss_roll\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 2. 生成非线性数据集\n",
    "\n",
    "使用经典的非线性数据集来展示 Kernel PCA 的优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成三种非线性数据集\n",
    "n_samples = 500\n",
    "\n",
    "# 半月形数据\n",
    "X_moons, y_moons = make_moons(n_samples=n_samples, noise=0.1, random_state=42)\n",
    "\n",
    "# 同心圆数据\n",
    "X_circles, y_circles = make_circles(n_samples=n_samples, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# 瑞士卷数据（取前两维用于演示）\n",
    "X_swiss, y_swiss = make_swiss_roll(n_samples=n_samples, noise=0.5, random_state=42)\n",
    "X_swiss_2d = X_swiss[:, [0, 2]]  # 选择 x 和 z 维度\n",
    "y_swiss_discrete = (y_swiss > y_swiss.mean()).astype(int)\n",
    "\n",
    "# 可视化原始数据\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "datasets = [\n",
    "    (X_moons, y_moons, 'Half Moons'),\n",
    "    (X_circles, y_circles, 'Concentric Circles'),\n",
    "    (X_swiss_2d, y_swiss_discrete, 'Swiss Roll (2D projection)')\n",
    "]\n",
    "\n",
    "for ax, (X, y, title) in zip(axes, datasets):\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', \n",
    "                        alpha=0.7, edgecolors='white', linewidth=0.5)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-section",
   "metadata": {},
   "source": [
    "## 3. PCA vs Kernel PCA 对比\n",
    "\n",
    "使用半月形数据集对比标准 PCA 和 Kernel PCA 的降维效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-vs-kpca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "X_moons_scaled = scaler.fit_transform(X_moons)\n",
    "\n",
    "# 标准 PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_moons_scaled)\n",
    "\n",
    "# Kernel PCA with RBF kernel\n",
    "kpca_rbf = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "X_kpca_rbf = kpca_rbf.fit_transform(X_moons_scaled)\n",
    "\n",
    "# 可视化对比\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 原始数据\n",
    "axes[0].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], \n",
    "                c=y_moons, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# 标准 PCA\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                c=y_moons, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "axes[1].set_title('Standard PCA')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "\n",
    "# Kernel PCA\n",
    "axes[2].scatter(X_kpca_rbf[:, 0], X_kpca_rbf[:, 1], \n",
    "                c=y_moons, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "axes[2].set_title('Kernel PCA (RBF)')\n",
    "axes[2].set_xlabel('KPC1')\n",
    "axes[2].set_ylabel('KPC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kernels-section",
   "metadata": {},
   "source": [
    "## 4. 不同核函数的比较\n",
    "\n",
    "比较 RBF、多项式和 Sigmoid 核在同心圆数据上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-kernels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化同心圆数据\n",
    "X_circles_scaled = StandardScaler().fit_transform(X_circles)\n",
    "\n",
    "# 不同核函数的配置\n",
    "kernels = [\n",
    "    ('linear', {'kernel': 'linear'}),\n",
    "    ('rbf', {'kernel': 'rbf', 'gamma': 10}),\n",
    "    ('poly', {'kernel': 'poly', 'gamma': 10, 'degree': 3}),\n",
    "    ('sigmoid', {'kernel': 'sigmoid', 'gamma': 1, 'coef0': 1})\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (name, params) in zip(axes.flat, kernels):\n",
    "    kpca = KernelPCA(n_components=2, **params)\n",
    "    X_kpca = kpca.fit_transform(X_circles_scaled)\n",
    "    \n",
    "    ax.scatter(X_kpca[:, 0], X_kpca[:, 1], \n",
    "               c=y_circles, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "    ax.set_title(f'Kernel: {name}')\n",
    "    ax.set_xlabel('KPC1')\n",
    "    ax.set_ylabel('KPC2')\n",
    "\n",
    "plt.suptitle('Kernel PCA with Different Kernels (Concentric Circles)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gamma-section",
   "metadata": {},
   "source": [
    "## 5. Gamma 参数的影响\n",
    "\n",
    "RBF 核的 gamma 参数控制核函数的宽度，对结果有重要影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gamma-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不同的 gamma 值\n",
    "gamma_values = [0.1, 1, 5, 15, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "\n",
    "for ax, gamma in zip(axes.flat, gamma_values):\n",
    "    kpca = KernelPCA(n_components=2, kernel='rbf', gamma=gamma)\n",
    "    X_kpca = kpca.fit_transform(X_moons_scaled)\n",
    "    \n",
    "    ax.scatter(X_kpca[:, 0], X_kpca[:, 1], \n",
    "               c=y_moons, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "    ax.set_title(f'gamma = {gamma}')\n",
    "    ax.set_xlabel('KPC1')\n",
    "    ax.set_ylabel('KPC2')\n",
    "\n",
    "plt.suptitle('Effect of Gamma Parameter in RBF Kernel PCA', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning-section",
   "metadata": {},
   "source": [
    "## 6. 超参数调优\n",
    "\n",
    "使用网格搜索结合下游分类任务来选择最优的核函数和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_moons_scaled, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 创建 Pipeline：Kernel PCA + Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('kpca', KernelPCA(n_components=2)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 参数网格\n",
    "param_grid = {\n",
    "    'kpca__kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'kpca__gamma': np.linspace(0.01, 0.1, 5)\n",
    "}\n",
    "\n",
    "# 网格搜索\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"最佳参数:\")\n",
    "print(f\"  Kernel: {grid_search.best_params_['kpca__kernel']}\")\n",
    "print(f\"  Gamma: {grid_search.best_params_['kpca__gamma']:.4f}\")\n",
    "print(f\"\\n交叉验证准确率: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 测试集评估\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(f\"测试集准确率: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inverse-section",
   "metadata": {},
   "source": [
    "## 7. 逆变换与重构\n",
    "\n",
    "Kernel PCA 的逆变换是近似的，通过学习一个从降维空间到原始空间的映射来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inverse-transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启用逆变换\n",
    "kpca_fit = KernelPCA(n_components=2, kernel='rbf', gamma=15, \n",
    "                     fit_inverse_transform=True, alpha=0.01)\n",
    "X_kpca = kpca_fit.fit_transform(X_moons_scaled)\n",
    "\n",
    "# 执行逆变换\n",
    "X_reconstructed = kpca_fit.inverse_transform(X_kpca)\n",
    "\n",
    "# 计算重构误差\n",
    "mse = mean_squared_error(X_moons_scaled, X_reconstructed)\n",
    "print(f\"重构 MSE: {mse:.6f}\")\n",
    "\n",
    "# 可视化重构结果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 原始数据\n",
    "axes[0].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], \n",
    "                c=y_moons, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# 降维后的数据\n",
    "axes[1].scatter(X_kpca[:, 0], X_kpca[:, 1], \n",
    "                c=y_moons, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "axes[1].set_title('Kernel PCA Projection')\n",
    "axes[1].set_xlabel('KPC1')\n",
    "axes[1].set_ylabel('KPC2')\n",
    "\n",
    "# 重构的数据\n",
    "axes[2].scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], \n",
    "                c=y_moons, cmap='coolwarm', alpha=0.7, edgecolors='white')\n",
    "axes[2].set_title(f'Reconstructed (MSE={mse:.4f})')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-section",
   "metadata": {},
   "source": [
    "## 8. 分类性能对比\n",
    "\n",
    "对比使用不同降维方法后的分类性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义不同的降维方法\n",
    "methods = [\n",
    "    ('No Reduction', None),\n",
    "    ('PCA', PCA(n_components=2)),\n",
    "    ('KPCA (linear)', KernelPCA(n_components=2, kernel='linear')),\n",
    "    ('KPCA (rbf)', KernelPCA(n_components=2, kernel='rbf', gamma=15)),\n",
    "    ('KPCA (poly)', KernelPCA(n_components=2, kernel='poly', gamma=10, degree=3))\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, reducer in methods:\n",
    "    if reducer is None:\n",
    "        X_train_red, X_test_red = X_train, X_test\n",
    "    else:\n",
    "        X_train_red = reducer.fit_transform(X_train)\n",
    "        X_test_red = reducer.transform(X_test)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train_red, y_train)\n",
    "    \n",
    "    train_acc = clf.score(X_train_red, y_train)\n",
    "    test_acc = clf.score(X_test_red, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Method': name,\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:20s} | Train: {train_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "# 可视化对比\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(results))\n",
    "width = 0.35\n",
    "\n",
    "train_accs = [r['Train Accuracy'] for r in results]\n",
    "test_accs = [r['Test Accuracy'] for r in results]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_accs, width, label='Train', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, test_accs, width, label='Test', color='coral')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Classification Accuracy with Different Dimensionality Reduction Methods')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([r['Method'] for r in results], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0.5, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 9. 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **适用场景**：\n",
    "   - 数据具有非线性结构\n",
    "   - 标准 PCA 无法有效分离类别\n",
    "   - 需要在高维特征空间中进行降维\n",
    "\n",
    "2. **核函数选择**：\n",
    "   - **RBF 核**：最常用，适合大多数非线性数据\n",
    "   - **多项式核**：适合已知多项式关系的数据\n",
    "   - **Sigmoid 核**：类似神经网络激活函数\n",
    "\n",
    "3. **超参数调优**：\n",
    "   - 通过交叉验证结合下游任务性能选择参数\n",
    "   - gamma 控制核函数的局部性，需要仔细调节\n",
    "\n",
    "4. **逆变换**：\n",
    "   - 是近似的，不能完全恢复原始数据\n",
    "   - 主要用于可视化和理解映射\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "- 计算复杂度 $O(n^3)$，不适合大规模数据\n",
    "- 核矩阵需要 $O(n^2)$ 内存\n",
    "- 对于大数据考虑使用 Nyström 近似或随机特征方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
