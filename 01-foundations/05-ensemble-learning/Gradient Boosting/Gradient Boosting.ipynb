{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# 梯度提升 (Gradient Boosting)\n",
    "\n",
    "## 算法原理\n",
    "\n",
    "梯度提升是一种序列集成方法，通过逐步拟合残差来构建强学习器：\n",
    "\n",
    "1. **初始化**: 用一个简单模型（如均值）作为初始预测\n",
    "2. **迭代拟合**: 每轮训练一个新的弱学习器来拟合当前残差\n",
    "3. **累加预测**: 将所有弱学习器的预测加权求和\n",
    "\n",
    "## 数学表达\n",
    "\n",
    "第 $m$ 轮的模型更新：\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
    "\n",
    "其中：\n",
    "- $F_{m-1}(x)$ 是前一轮的累积预测\n",
    "- $h_m(x)$ 是第 $m$ 轮训练的弱学习器\n",
    "- $\\eta$ 是学习率（收缩参数）\n",
    "\n",
    "## 关键超参数\n",
    "\n",
    "- `n_estimators`: 弱学习器数量\n",
    "- `learning_rate`: 学习率，控制每棵树的贡献\n",
    "- `max_depth`: 树的最大深度，通常设为 3-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 1. 数据准备\n",
    "\n",
    "使用鸢尾花数据集进行回归任务演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape[0]}\")\n",
    "print(f\"测试集大小: {X_test.shape[0]}\")\n",
    "print(f\"特征数量: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-section",
   "metadata": {},
   "source": [
    "## 2. 手动实现梯度提升\n",
    "\n",
    "通过手动迭代拟合残差来理解梯度提升的核心思想。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一棵树：拟合原始目标\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=RANDOM_STATE)\n",
    "tree_reg1.fit(X_train, y_train)\n",
    "\n",
    "# 计算第一棵树的残差\n",
    "residual1 = y_train - tree_reg1.predict(X_train)\n",
    "\n",
    "# 第二棵树：拟合第一棵树的残差\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=RANDOM_STATE)\n",
    "tree_reg2.fit(X_train, residual1)\n",
    "\n",
    "# 计算第二棵树的残差\n",
    "residual2 = residual1 - tree_reg2.predict(X_train)\n",
    "\n",
    "# 第三棵树：拟合第二棵树的残差\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=RANDOM_STATE)\n",
    "tree_reg3.fit(X_train, residual2)\n",
    "\n",
    "# 组合预测：所有树的预测之和\n",
    "y_pred_manual = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "# 评估手动实现的梯度提升\n",
    "mse_manual = mean_squared_error(y_test, y_pred_manual)\n",
    "r2_manual = r2_score(y_test, y_pred_manual)\n",
    "\n",
    "print(\"手动实现梯度提升 (3棵树):\")\n",
    "print(f\"  MSE: {mse_manual:.4f}\")\n",
    "print(f\"  R²: {r2_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sklearn-section",
   "metadata": {},
   "source": [
    "## 3. 使用 GradientBoostingRegressor\n",
    "\n",
    "sklearn 的 GradientBoostingRegressor 提供了完整的梯度提升实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sklearn-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建梯度提升回归器\n",
    "gbrt = GradientBoostingRegressor(\n",
    "    n_estimators=100,          # 弱学习器数量\n",
    "    learning_rate=0.1,         # 学习率\n",
    "    max_depth=3,               # 树的最大深度\n",
    "    min_samples_split=2,       # 内部节点再划分所需最小样本数\n",
    "    min_samples_leaf=1,        # 叶子节点最少样本数\n",
    "    subsample=1.0,             # 用于拟合各个基学习器的样本比例\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# 预测和评估\n",
    "y_pred_gbrt = gbrt.predict(X_test)\n",
    "mse_gbrt = mean_squared_error(y_test, y_pred_gbrt)\n",
    "r2_gbrt = r2_score(y_test, y_pred_gbrt)\n",
    "\n",
    "print(\"GradientBoostingRegressor:\")\n",
    "print(f\"  MSE: {mse_gbrt:.4f}\")\n",
    "print(f\"  R²: {r2_gbrt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr-section",
   "metadata": {},
   "source": [
    "## 4. 学习率的影响\n",
    "\n",
    "学习率与树的数量之间存在权衡：\n",
    "- 低学习率需要更多的树，但泛化性能通常更好\n",
    "- 高学习率训练更快，但可能过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lr-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同学习率\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "print(\"学习率对比:\")\n",
    "print(f\"{'学习率':>10} {'训练R²':>10} {'测试R²':>10}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gbrt_lr = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    gbrt_lr.fit(X_train, y_train)\n",
    "    \n",
    "    train_r2 = gbrt_lr.score(X_train, y_train)\n",
    "    test_r2 = gbrt_lr.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"{lr:>10.2f} {train_r2:>10.4f} {test_r2:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "staged-section",
   "metadata": {},
   "source": [
    "## 5. 使用 staged_predict 分析学习曲线\n",
    "\n",
    "`staged_predict` 方法可以获取每个迭代阶段的预测，用于分析最佳树数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "staged-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个有更多树的模型\n",
    "gbrt_staged = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbrt_staged.fit(X_train, y_train)\n",
    "\n",
    "# 计算每个阶段的测试误差\n",
    "errors = [mean_squared_error(y_test, y_pred) \n",
    "          for y_pred in gbrt_staged.staged_predict(X_test)]\n",
    "\n",
    "# 找到最佳树数量\n",
    "best_n_estimators = np.argmin(errors) + 1\n",
    "min_error = min(errors)\n",
    "\n",
    "print(f\"最佳树数量: {best_n_estimators}\")\n",
    "print(f\"最小测试MSE: {min_error:.4f}\")\n",
    "\n",
    "# 使用最佳参数重新训练\n",
    "gbrt_best = GradientBoostingRegressor(\n",
    "    n_estimators=best_n_estimators,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "print(f\"最佳模型 R²: {gbrt_best.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-section",
   "metadata": {},
   "source": [
    "## 6. 单元测试验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient_boosting():\n",
    "    \"\"\"梯度提升功能测试\"\"\"\n",
    "    \n",
    "    # 测试1: 模型应该正确训练\n",
    "    assert hasattr(gbrt, 'estimators_'), \"模型未正确训练\"\n",
    "    \n",
    "    # 测试2: 树的数量应该正确\n",
    "    assert len(gbrt.estimators_) == 100, \"树的数量不正确\"\n",
    "    \n",
    "    # 测试3: R² 分数应该在合理范围\n",
    "    assert r2_gbrt >= 0.9, f\"R² 分数过低: {r2_gbrt}\"\n",
    "    \n",
    "    # 测试4: 手动实现应该有合理性能\n",
    "    assert r2_manual >= 0.8, f\"手动实现 R² 过低: {r2_manual}\"\n",
    "    \n",
    "    # 测试5: 预测结果形状应该正确\n",
    "    assert y_pred_gbrt.shape == y_test.shape, \"预测结果形状不正确\"\n",
    "    \n",
    "    # 测试6: 最佳树数量应该在合理范围\n",
    "    assert 1 <= best_n_estimators <= 200, \"最佳树数量不在预期范围\"\n",
    "    \n",
    "    print(\"所有测试通过!\")\n",
    "\n",
    "test_gradient_boosting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 梯度提升的优势\n",
    "\n",
    "1. **高预测精度**: 在结构化数据上通常表现最佳\n",
    "2. **内置正则化**: 通过学习率和树深度控制过拟合\n",
    "3. **特征重要性**: 自动计算特征重要性\n",
    "\n",
    "### 梯度提升的局限\n",
    "\n",
    "1. **训练较慢**: 必须串行训练，无法并行\n",
    "2. **参数敏感**: 需要仔细调参\n",
    "3. **对异常值敏感**: 特别是使用平方损失时\n",
    "\n",
    "### 调参建议\n",
    "\n",
    "1. 先固定 `learning_rate=0.1`，调整 `n_estimators`\n",
    "2. 使用 `staged_predict` 或早停找到最佳迭代次数\n",
    "3. 适当降低 `learning_rate`（如 0.01-0.1）并增加 `n_estimators`\n",
    "4. `max_depth` 通常设为 3-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
