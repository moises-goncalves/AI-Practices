{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# XGBoost 极端梯度提升\n",
    "\n",
    "## 算法原理\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) 是梯度提升算法的高效实现，具有以下特点：\n",
    "\n",
    "1. **正则化**: 在目标函数中加入 L1/L2 正则化项，防止过拟合\n",
    "2. **二阶导数**: 使用二阶泰勒展开，比传统梯度提升更精确\n",
    "3. **并行化**: 特征级别的并行计算\n",
    "4. **缺失值处理**: 自动学习缺失值的最优处理方向\n",
    "\n",
    "## 目标函数\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = \\sum_{i} l(y_i, \\hat{y}_i) + \\sum_{k} \\Omega(f_k)$$\n",
    "\n",
    "其中 $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda ||w||^2$ 是正则化项。\n",
    "\n",
    "## 与传统 GBDT 的区别\n",
    "\n",
    "| 特性 | 传统 GBDT | XGBoost |\n",
    "|------|-----------|----------|\n",
    "| 优化 | 一阶导数 | 二阶导数 |\n",
    "| 正则化 | 无 | L1 + L2 |\n",
    "| 并行化 | 不支持 | 特征级并行 |\n",
    "| 缺失值 | 需预处理 | 自动处理 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 1. 数据准备\n",
    "\n",
    "使用鸢尾花数据集进行多分类任务演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 三分数据集：训练集、验证集、测试集\n",
    "# 第一步：划分训练集和临时集（8:2）\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# 第二步：将临时集划分为验证集和测试集（1:1）\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"训练集: {X_train.shape[0]} 样本\")\n",
    "print(f\"验证集: {X_val.shape[0]} 样本\")\n",
    "print(f\"测试集: {X_test.shape[0]} 样本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## 2. XGBoost 分类器\n",
    "\n",
    "使用 XGBClassifier 进行训练，并在验证集上监控性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 XGBoost 分类器\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,          # 树的数量\n",
    "    max_depth=3,               # 树的最大深度\n",
    "    learning_rate=0.1,         # 学习率\n",
    "    subsample=0.8,             # 每棵树使用的样本比例\n",
    "    colsample_bytree=0.8,      # 每棵树使用的特征比例\n",
    "    reg_alpha=0,               # L1 正则化系数\n",
    "    reg_lambda=1,              # L2 正则化系数\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='mlogloss'     # 评估指标\n",
    ")\n",
    "\n",
    "# 训练模型，使用验证集进行早停\n",
    "xgb_clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False              # 不打印每轮日志\n",
    ")\n",
    "\n",
    "# 评估\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"测试集准确率: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-stop-section",
   "metadata": {},
   "source": [
    "## 3. 早停策略\n",
    "\n",
    "使用早停可以防止过拟合并节省训练时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-stop-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用早停的 XGBoost\n",
    "xgb_early = xgb.XGBClassifier(\n",
    "    n_estimators=500,          # 设置较大的值\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='mlogloss',\n",
    "    early_stopping_rounds=10   # 10轮无改善则停止\n",
    ")\n",
    "\n",
    "# 训练\n",
    "xgb_early.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"最佳迭代次数: {xgb_early.best_iteration}\")\n",
    "print(f\"最佳验证分数: {xgb_early.best_score:.4f}\")\n",
    "\n",
    "# 评估\n",
    "y_pred_early = xgb_early.predict(X_test)\n",
    "early_accuracy = accuracy_score(y_test, y_pred_early)\n",
    "print(f\"测试集准确率: {early_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "importance-section",
   "metadata": {},
   "source": [
    "## 4. 特征重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "importance-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征重要性\n",
    "feature_names = data.feature_names\n",
    "importances = xgb_clf.feature_importances_\n",
    "\n",
    "# 按重要性排序\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"特征重要性排序:\")\n",
    "for i in indices:\n",
    "    print(f\"  {feature_names[i]}: {importances[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv-section",
   "metadata": {},
   "source": [
    "## 5. 交叉验证评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉验证\n",
    "cv_clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(cv_clf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"交叉验证分数: {cv_scores}\")\n",
    "print(f\"平均准确率: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "param-section",
   "metadata": {},
   "source": [
    "## 6. 参数调优建议\n",
    "\n",
    "XGBoost 的关键参数及其调优策略："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "param-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同学习率的影响\n",
    "learning_rates = [0.01, 0.1, 0.3, 0.5]\n",
    "\n",
    "print(\"学习率对比:\")\n",
    "print(f\"{'学习率':>10} {'训练准确率':>12} {'测试准确率':>12}\")\n",
    "print(\"-\" * 36)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"{lr:>10.2f} {train_acc:>12.4f} {test_acc:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-section",
   "metadata": {},
   "source": [
    "## 7. 单元测试验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_xgboost():\n",
    "    \"\"\"XGBoost 功能测试\"\"\"\n",
    "    \n",
    "    # 测试1: 模型应该正确训练\n",
    "    assert hasattr(xgb_clf, 'feature_importances_'), \"模型未正确训练\"\n",
    "    \n",
    "    # 测试2: 准确率应该在合理范围\n",
    "    assert test_accuracy >= 0.8, f\"测试集准确率过低: {test_accuracy}\"\n",
    "    \n",
    "    # 测试3: 特征重要性应该存在\n",
    "    assert len(importances) == X.shape[1], \"特征重要性维度不正确\"\n",
    "    \n",
    "    # 测试4: 早停应该正常工作\n",
    "    assert xgb_early.best_iteration < 500, \"早停未生效\"\n",
    "    \n",
    "    # 测试5: 预测结果形状应该正确\n",
    "    assert y_pred.shape == y_test.shape, \"预测结果形状不正确\"\n",
    "    \n",
    "    # 测试6: 交叉验证应该返回正确数量的分数\n",
    "    assert len(cv_scores) == 5, \"交叉验证折数不正确\"\n",
    "    \n",
    "    print(\"所有测试通过!\")\n",
    "\n",
    "test_xgboost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### XGBoost 的优势\n",
    "\n",
    "1. **高效**: 并行计算和缓存优化\n",
    "2. **正则化**: 内置 L1/L2 正则化防止过拟合\n",
    "3. **灵活**: 支持自定义目标函数和评估指标\n",
    "4. **鲁棒**: 自动处理缺失值\n",
    "\n",
    "### 调参建议\n",
    "\n",
    "1. **首先调整**: `n_estimators` 和 `learning_rate`（互相关联）\n",
    "2. **然后调整**: `max_depth`（通常 3-10）\n",
    "3. **正则化**: `reg_alpha` 和 `reg_lambda`（防止过拟合）\n",
    "4. **采样**: `subsample` 和 `colsample_bytree`（增加随机性）\n",
    "\n",
    "### 常用参数组合\n",
    "\n",
    "- 小数据集: `learning_rate=0.1`, `n_estimators=100-500`\n",
    "- 大数据集: `learning_rate=0.01-0.05`, `n_estimators=1000+`, 配合早停"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
