{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# 梯度提升回归树 (GBRT) 早停与调优\n",
    "\n",
    "## 算法原理\n",
    "\n",
    "GBRT (Gradient Boosted Regression Trees) 是梯度提升用于回归任务的实现。本节重点介绍：\n",
    "\n",
    "1. **早停策略**: 防止过拟合，找到最优迭代次数\n",
    "2. **staged_predict**: 获取每轮迭代的预测结果\n",
    "3. **warm_start**: 增量训练，动态调整树的数量\n",
    "\n",
    "## 早停的重要性\n",
    "\n",
    "- 梯度提升容易过拟合\n",
    "- 太多的树会导致验证误差上升\n",
    "- 早停可以自动找到最佳迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 1. 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape[0]}\")\n",
    "print(f\"测试集大小: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "staged-section",
   "metadata": {},
   "source": [
    "## 2. 使用 staged_predict 寻找最佳迭代次数\n",
    "\n",
    "`staged_predict` 方法可以获取每轮迭代后的预测，用于分析学习曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "staged-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个有较多树的模型\n",
    "gbrt = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    n_estimators=120,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# 计算每个阶段的测试误差\n",
    "errors = [mean_squared_error(y_test, y_pred) \n",
    "          for y_pred in gbrt.staged_predict(X_test)]\n",
    "\n",
    "# 找到最佳迭代次数（误差最小的点）\n",
    "best_n_estimators = np.argmin(errors) + 1\n",
    "min_error = min(errors)\n",
    "\n",
    "print(f\"最佳迭代次数: {best_n_estimators}\")\n",
    "print(f\"最小测试MSE: {min_error:.6f}\")\n",
    "\n",
    "# 使用最佳参数重新训练\n",
    "gbrt_best = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    n_estimators=best_n_estimators,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "print(f\"最佳模型 R²: {gbrt_best.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warmstart-section",
   "metadata": {},
   "source": [
    "## 3. 使用 warm_start 实现早停\n",
    "\n",
    "`warm_start=True` 允许增量训练，可以在验证误差开始上升时停止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warmstart-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 warm_start 实现早停\n",
    "gbrt_warm = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    warm_start=True,           # 启用增量训练\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "best_iteration = 0\n",
    "\n",
    "# 逐步增加树的数量\n",
    "for n_estimators in range(1, 121):\n",
    "    gbrt_warm.n_estimators = n_estimators\n",
    "    gbrt_warm.fit(X_train, y_train)\n",
    "    y_pred = gbrt_warm.predict(X_test)\n",
    "    val_error = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        best_iteration = n_estimators\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        # 如果连续5轮误差没有改善，停止训练\n",
    "        if error_going_up == 5:\n",
    "            break\n",
    "\n",
    "print(f\"早停迭代次数: {gbrt_warm.n_estimators}\")\n",
    "print(f\"最佳迭代次数: {best_iteration}\")\n",
    "print(f\"最小验证MSE: {min_val_error:.6f}\")\n",
    "print(f\"最终模型 R²: {gbrt_warm.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-section",
   "metadata": {},
   "source": [
    "## 4. 不同策略的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比不同配置的性能\n",
    "configs = [\n",
    "    ('无早停 (120棵树)', 120),\n",
    "    ('staged_predict 最佳', best_n_estimators),\n",
    "    ('warm_start 早停', gbrt_warm.n_estimators)\n",
    "]\n",
    "\n",
    "print(\"不同策略性能对比:\")\n",
    "print(f\"{'策略':<25} {'树数量':>10} {'R²':>10}\")\n",
    "print(\"-\" * 47)\n",
    "\n",
    "for name, n_trees in configs:\n",
    "    model = GradientBoostingRegressor(\n",
    "        max_depth=2,\n",
    "        n_estimators=n_trees,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(f\"{name:<25} {n_trees:>10} {r2:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-section",
   "metadata": {},
   "source": [
    "## 5. 单元测试验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gbrt():\n",
    "    \"\"\"GBRT 功能测试\"\"\"\n",
    "    \n",
    "    # 测试1: staged_predict 应该返回正确数量的预测\n",
    "    assert len(errors) == 120, \"staged_predict 返回数量不正确\"\n",
    "    \n",
    "    # 测试2: 最佳迭代次数应该在合理范围\n",
    "    assert 1 <= best_n_estimators <= 120, \"最佳迭代次数不在预期范围\"\n",
    "    \n",
    "    # 测试3: 最佳模型应该有良好性能\n",
    "    assert gbrt_best.score(X_test, y_test) >= 0.9, \"最佳模型 R² 过低\"\n",
    "    \n",
    "    # 测试4: warm_start 早停应该在合理位置停止\n",
    "    assert gbrt_warm.n_estimators < 120, \"早停未生效\"\n",
    "    \n",
    "    # 测试5: 早停不应该显著损害性能\n",
    "    assert gbrt_warm.score(X_test, y_test) >= 0.9, \"早停后 R² 过低\"\n",
    "    \n",
    "    print(\"所有测试通过!\")\n",
    "\n",
    "test_gbrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 早停策略\n",
    "\n",
    "1. **staged_predict**: 训练完成后分析，找到最佳迭代点\n",
    "2. **warm_start**: 边训练边监控，达到早停条件即停止\n",
    "3. **early_stopping (sklearn 0.24+)**: 内置早停参数\n",
    "\n",
    "### 最佳实践\n",
    "\n",
    "1. 始终使用验证集监控过拟合\n",
    "2. 设置合理的耐心参数（如连续5轮无改善）\n",
    "3. 使用较小的学习率配合更多的树\n",
    "4. 早停后可以用最佳迭代次数重新训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
