{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReduceLROnPlateau学习率动态调整\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "### 学习率调整的重要性\n",
    "\n",
    "学习率是深度学习中最关键的超参数之一：\n",
    "- **过大**: 导致训练不稳定，损失震荡甚至发散\n",
    "- **过小**: 训练速度慢，容易陷入局部最优\n",
    "- **动态调整**: 训练初期用大学习率快速下降，后期用小学习率精细调优\n",
    "\n",
    "### ReduceLROnPlateau原理\n",
    "\n",
    "当监控指标在一定epoch内不再改善时，自动降低学习率：\n",
    "1. 监控验证集指标(如val_loss)\n",
    "2. 连续N个epoch无改善时触发\n",
    "3. 按照指定比例降低学习率\n",
    "4. 重复以上过程直到学习率达到下限\n",
    "\n",
    "## 应用场景\n",
    "\n",
    "1. **训练后期优化**: 当损失降低缓慢时降低学习率进行精细调整\n",
    "2. **避免过拟合**: 小学习率有助于模型更平滑地收敛\n",
    "3. **自适应训练**: 无需手动设置学习率衰减策略\n",
    "4. **迁移学习**: 微调预训练模型时动态调整学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 准备数据集\n",
    "\n",
    "使用Fashion MNIST数据集进行演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Fashion MNIST数据集\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 添加通道维度\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# 转换标签为one-hot编码\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f'训练集形状: {x_train.shape}, 标签形状: {y_train.shape}')\n",
    "print(f'测试集形状: {x_test.shape}, 标签形状: {y_test.shape}')\n",
    "\n",
    "# Fashion MNIST类别\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建模型\n",
    "\n",
    "构建一个卷积神经网络用于图像分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    创建CNN模型\n",
    "    \n",
    "    参数:\n",
    "        learning_rate: 初始学习率\n",
    "    \n",
    "    返回:\n",
    "        编译好的模型\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # 第一个卷积块\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # 第二个卷积块\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # 第三个卷积块\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # 全连接层\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # 使用指定学习率的优化器\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReduceLROnPlateau详解\n",
    "\n",
    "### 关键参数说明\n",
    "\n",
    "- `monitor`: 监控的指标名称(通常是'val_loss')\n",
    "- `factor`: 学习率衰减因子，新学习率 = 旧学习率 × factor\n",
    "- `patience`: 容忍多少个epoch无改善\n",
    "- `min_delta`: 最小改善幅度\n",
    "- `cooldown`: 降低学习率后等待多少个epoch再继续监控\n",
    "- `min_lr`: 学习率下限\n",
    "- `mode`: 'min'(指标越小越好)或'max'(指标越大越好)\n",
    "- `verbose`: 日志详细程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 策略1: 标准配置\n",
    "reduce_lr_standard = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,              # 每次降低50%\n",
    "    patience=3,              # 3个epoch无改善后降低\n",
    "    min_delta=0.001,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 策略2: 激进策略(快速降低学习率)\n",
    "reduce_lr_aggressive = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,              # 每次降低80%\n",
    "    patience=2,              # 2个epoch后就降低\n",
    "    min_delta=0.0001,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 策略3: 保守策略(缓慢降低学习率)\n",
    "reduce_lr_conservative = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.7,              # 每次降低30%\n",
    "    patience=5,              # 5个epoch后才降低\n",
    "    min_delta=0.01,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 策略4: 带cooldown的策略\n",
    "reduce_lr_cooldown = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    cooldown=2,              # 降低学习率后等待2个epoch\n",
    "    min_delta=0.001,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('ReduceLROnPlateau回调函数配置完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 自定义学习率监控回调\n",
    "\n",
    "创建一个回调函数来记录学习率变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    监控和记录学习率变化\n",
    "    \n",
    "    功能:\n",
    "    1. 记录每个epoch的学习率\n",
    "    2. 检测学习率变化\n",
    "    3. 显示学习率调整信息\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.learning_rates = []\n",
    "        self.epochs = []\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"记录当前学习率\"\"\"\n",
    "        lr = float(keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.learning_rates.append(lr)\n",
    "        self.epochs.append(epoch)\n",
    "        \n",
    "        # 检测学习率变化\n",
    "        if len(self.learning_rates) > 1:\n",
    "            prev_lr = self.learning_rates[-2]\n",
    "            if lr != prev_lr:\n",
    "                change_ratio = (lr - prev_lr) / prev_lr * 100\n",
    "                print(f'\\n学习率已调整: {prev_lr:.2e} -> {lr:.2e} ({change_ratio:+.1f}%)')\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"训练结束时总结学习率变化\"\"\"\n",
    "        print(f'\\n学习率变化总结:')\n",
    "        print(f'  初始学习率: {self.learning_rates[0]:.2e}')\n",
    "        print(f'  最终学习率: {self.learning_rates[-1]:.2e}')\n",
    "        print(f'  调整次数: {len(set(self.learning_rates)) - 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 实验1: 无学习率调整的基准\n",
    "\n",
    "先训练一个固定学习率的模型作为对照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验1: 固定学习率基准 ==========')\n",
    "print('学习率: 0.001 (固定)\\n')\n",
    "\n",
    "model_baseline = create_model(learning_rate=0.001)\n",
    "lr_monitor_baseline = LearningRateMonitor()\n",
    "\n",
    "history_baseline = model_baseline.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=3,  # 测试用，实际训练可设为20\n",
    "    validation_split=0.2,\n",
    "    callbacks=[lr_monitor_baseline],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model_baseline.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\n测试结果 - 损失: {test_loss:.4f}, 准确率: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 实验2: 标准ReduceLROnPlateau\n",
    "\n",
    "使用标准配置的学习率调整策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验2: 标准ReduceLROnPlateau ==========')\n",
    "print('初始学习率: 0.001')\n",
    "print('策略: factor=0.5, patience=3\\n')\n",
    "\n",
    "model_standard = create_model(learning_rate=0.001)\n",
    "lr_monitor_standard = LearningRateMonitor()\n",
    "\n",
    "callbacks_standard = [\n",
    "    reduce_lr_standard,\n",
    "    lr_monitor_standard\n",
    "]\n",
    "\n",
    "history_standard = model_standard.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=3,  # 测试用\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_standard,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model_standard.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\n测试结果 - 损失: {test_loss:.4f}, 准确率: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 实验3: 激进策略\n",
    "\n",
    "使用激进的学习率衰减策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验3: 激进策略 ==========')\n",
    "print('初始学习率: 0.001')\n",
    "print('策略: factor=0.2, patience=2\\n')\n",
    "\n",
    "model_aggressive = create_model(learning_rate=0.001)\n",
    "lr_monitor_aggressive = LearningRateMonitor()\n",
    "\n",
    "callbacks_aggressive = [\n",
    "    reduce_lr_aggressive,\n",
    "    lr_monitor_aggressive\n",
    "]\n",
    "\n",
    "history_aggressive = model_aggressive.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=3,  # 测试用\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_aggressive,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model_aggressive.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\n测试结果 - 损失: {test_loss:.4f}, 准确率: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 实验4: 保守策略\n",
    "\n",
    "使用保守的学习率衰减策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验4: 保守策略 ==========')\n",
    "print('初始学习率: 0.001')\n",
    "print('策略: factor=0.7, patience=5\\n')\n",
    "\n",
    "model_conservative = create_model(learning_rate=0.001)\n",
    "lr_monitor_conservative = LearningRateMonitor()\n",
    "\n",
    "callbacks_conservative = [\n",
    "    reduce_lr_conservative,\n",
    "    lr_monitor_conservative\n",
    "]\n",
    "\n",
    "history_conservative = model_conservative.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=3,  # 测试用\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_conservative,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model_conservative.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\n测试结果 - 损失: {test_loss:.4f}, 准确率: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 组合使用：ReduceLROnPlateau + EarlyStopping\n",
    "\n",
    "在实际应用中，通常将学习率调整与早停机制结合使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验5: 组合策略 ==========')\n",
    "print('ReduceLROnPlateau + EarlyStopping + ModelCheckpoint\\n')\n",
    "\n",
    "model_combined = create_model(learning_rate=0.001)\n",
    "lr_monitor_combined = LearningRateMonitor()\n",
    "\n",
    "# 创建保存目录\n",
    "checkpoint_dir = 'lr_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "callbacks_combined = [\n",
    "    # 学习率调整\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # 早停机制\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,              # 比ReduceLR的patience大\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # 模型保存\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # 学习率监控\n",
    "    lr_monitor_combined\n",
    "]\n",
    "\n",
    "history_combined = model_combined.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=3,  # 测试用\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_combined,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model_combined.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\n测试结果 - 损失: {test_loss:.4f}, 准确率: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 对比分析\n",
    "\n",
    "对比不同学习率策略的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_experiments(histories, names):\n",
    "    \"\"\"\n",
    "    对比多个实验的结果\n",
    "    \"\"\"\n",
    "    print('\\n========== 实验对比 ==========')\n",
    "    print(f'{\"策略\":<15} {\"最佳验证损失\":<15} {\"最佳验证准确率\":<15} {\"最终验证准确率\"}')\n",
    "    print('-' * 60)\n",
    "    \n",
    "    results = []\n",
    "    for history, name in zip(histories, names):\n",
    "        best_val_loss = min(history.history['val_loss'])\n",
    "        best_val_acc = max(history.history['val_accuracy'])\n",
    "        final_val_acc = history.history['val_accuracy'][-1]\n",
    "        \n",
    "        print(f'{name:<15} {best_val_loss:<15.4f} {best_val_acc:<15.4f} {final_val_acc:.4f}')\n",
    "        \n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'final_val_acc': final_val_acc\n",
    "        })\n",
    "    \n",
    "    # 找出最佳策略\n",
    "    best_strategy = max(results, key=lambda x: x['best_val_acc'])\n",
    "    print(f'\\n最佳策略: {best_strategy[\"name\"]} (验证准确率: {best_strategy[\"best_val_acc\"]:.4f})')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 对比所有实验\n",
    "all_histories = [\n",
    "    history_baseline,\n",
    "    history_standard,\n",
    "    history_aggressive,\n",
    "    history_conservative,\n",
    "    history_combined\n",
    "]\n",
    "\n",
    "all_names = [\n",
    "    '固定学习率',\n",
    "    '标准策略',\n",
    "    '激进策略',\n",
    "    '保守策略',\n",
    "    '组合策略'\n",
    "]\n",
    "\n",
    "results = compare_experiments(all_histories, all_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 学习率变化可视化\n",
    "\n",
    "可视化不同策略下的学习率变化轨迹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learning_rates(lr_monitors, names):\n",
    "    \"\"\"\n",
    "    可视化学习率变化\n",
    "    \"\"\"\n",
    "    print('\\n========== 学习率变化轨迹 ==========')\n",
    "    \n",
    "    for monitor, name in zip(lr_monitors, names):\n",
    "        print(f'\\n{name}:')\n",
    "        print(f'  Epochs: {monitor.epochs}')\n",
    "        print(f'  学习率: {[f\"{lr:.2e}\" for lr in monitor.learning_rates]}')\n",
    "        \n",
    "        # 统计信息\n",
    "        unique_lrs = len(set(monitor.learning_rates))\n",
    "        print(f'  不同学习率数量: {unique_lrs}')\n",
    "        if unique_lrs > 1:\n",
    "            reduction_ratio = monitor.learning_rates[-1] / monitor.learning_rates[0]\n",
    "            print(f'  总衰减比例: {reduction_ratio:.4f}')\n",
    "\n",
    "# 可视化所有实验的学习率变化\n",
    "all_monitors = [\n",
    "    lr_monitor_baseline,\n",
    "    lr_monitor_standard,\n",
    "    lr_monitor_aggressive,\n",
    "    lr_monitor_conservative,\n",
    "    lr_monitor_combined\n",
    "]\n",
    "\n",
    "visualize_learning_rates(all_monitors, all_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 清理临时文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# 清理测试产生的文件和目录\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f'已删除目录: {checkpoint_dir}')\n",
    "\n",
    "print('清理完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### ReduceLROnPlateau最佳实践\n",
    "\n",
    "1. **参数选择建议**:\n",
    "   - `factor`: 通常设为0.5或0.1\n",
    "     - 0.5: 温和降低，适合大多数场景\n",
    "     - 0.1: 激进降低，适合快速收敛\n",
    "   - `patience`: 根据数据集大小调整\n",
    "     - 小数据集: 2-3\n",
    "     - 大数据集: 5-10\n",
    "   - `min_lr`: 建议设为初始学习率的1/1000到1/10000\n",
    "\n",
    "2. **监控指标选择**:\n",
    "   - 优先监控`val_loss`而非`val_accuracy`\n",
    "   - 损失曲线更平滑，更容易判断plateau\n",
    "   - 准确率在高值时变化不明显\n",
    "\n",
    "3. **与其他回调组合**:\n",
    "   ```python\n",
    "   callbacks = [\n",
    "       ReduceLROnPlateau(patience=3),    # 学习率调整\n",
    "       EarlyStopping(patience=10),       # patience应大于ReduceLR\n",
    "       ModelCheckpoint(...)              # 保存最佳模型\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "4. **常见问题**:\n",
    "   - **学习率降低过快**: 增大patience或factor\n",
    "   - **训练停滞**: 检查min_lr是否过大\n",
    "   - **过早停止**: EarlyStopping的patience应大于ReduceLR\n",
    "\n",
    "### 与其他学习率策略对比\n",
    "\n",
    "| 策略 | 优点 | 缺点 | 适用场景 |\n",
    "|------|------|------|----------|\n",
    "| ReduceLROnPlateau | 自适应，无需预设 | 可能反应滞后 | 训练时间充足 |\n",
    "| CosineAnnealing | 周期性恢复，防止局部最优 | 需要预知总epoch | 研究实验 |\n",
    "| ExponentialDecay | 平滑衰减 | 需要调整衰减率 | 稳定收敛 |\n",
    "| StepDecay | 简单可控 | 需要手动设置步数 | 经验丰富时 |\n",
    "\n",
    "### 高级技巧\n",
    "\n",
    "1. **Warmup + ReduceLR**: 训练初期线性增加学习率，后期动态降低\n",
    "2. **Cyclical LR**: 周期性地调整学习率，有助于跳出局部最优\n",
    "3. **Per-parameter LR**: 对不同层使用不同的学习率策略\n",
    "4. **Gradient Clipping**: 配合学习率调整，防止梯度爆炸\n",
    "\n",
    "### 调试建议\n",
    "\n",
    "1. 使用TensorBoard可视化学习率和损失曲线\n",
    "2. 记录每次学习率变化的epoch和对应的指标值\n",
    "3. 对比不同配置下的训练曲线\n",
    "4. 保存学习率变化历史用于后续分析"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
