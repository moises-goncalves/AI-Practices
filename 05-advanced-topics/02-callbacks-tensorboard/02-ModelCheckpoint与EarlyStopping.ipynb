{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelCheckpoint与EarlyStopping深度解析\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "### ModelCheckpoint\n",
    "在训练过程中自动保存模型权重，支持多种保存策略：\n",
    "- 保存所有epoch的模型\n",
    "- 仅保存最佳性能的模型\n",
    "- 按照指定频率保存\n",
    "- 保存完整模型或仅保存权重\n",
    "\n",
    "### EarlyStopping\n",
    "当监控指标停止改善时自动终止训练，防止：\n",
    "- 过拟合：验证集性能下降\n",
    "- 资源浪费：不必要的训练迭代\n",
    "- 时间浪费：性能已经收敛\n",
    "\n",
    "## 应用场景\n",
    "\n",
    "1. **长时间训练任务**: 防止训练中断导致进度丢失\n",
    "2. **超参数搜索**: 快速淘汰表现不佳的配置\n",
    "3. **模型选择**: 自动保存验证集上最佳的模型\n",
    "4. **资源受限环境**: 在性能饱和时及时停止训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 准备实验数据\n",
    "\n",
    "使用CIFAR-10数据集演示，这是一个更复杂的图像分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载CIFAR-10数据集\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 转换标签为one-hot编码\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 为了快速演示，使用部分数据\n",
    "x_train = x_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "x_test = x_test[:2000]\n",
    "y_test = y_test[:2000]\n",
    "\n",
    "print(f'训练集形状: {x_train.shape}, 标签形状: {y_train.shape}')\n",
    "print(f'测试集形状: {x_test.shape}, 标签形状: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建卷积神经网络\n",
    "\n",
    "构建一个适合CIFAR-10的CNN模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    \"\"\"\n",
    "    创建卷积神经网络模型\n",
    "    \n",
    "    架构:\n",
    "    - Conv2D (32) -> MaxPooling -> Conv2D (64) -> MaxPooling\n",
    "    - Conv2D (128) -> GlobalAveragePooling\n",
    "    - Dense (128) -> Dropout -> Dense (10)\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # 第一个卷积块\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # 第二个卷积块\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # 第三个卷积块\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # 全连接层\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ModelCheckpoint详解\n",
    "\n",
    "### 关键参数说明\n",
    "\n",
    "- `filepath`: 保存路径，支持格式化字符串(如`model_{epoch:02d}_{val_loss:.2f}.h5`)\n",
    "- `monitor`: 监控的指标名称\n",
    "- `save_best_only`: 是否仅保存最佳模型\n",
    "- `save_weights_only`: 是否仅保存权重(False则保存完整模型)\n",
    "- `mode`: 'min'(指标越小越好)或'max'(指标越大越好)\n",
    "- `save_freq`: 保存频率，'epoch'或整数(按batch数)\n",
    "- `verbose`: 日志详细程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存目录\n",
    "checkpoint_dir = 'model_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 策略1: 保存最佳模型(基于验证准确率)\n",
    "checkpoint_best_acc = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'best_accuracy.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 策略2: 保存最佳模型(基于验证损失)\n",
    "checkpoint_best_loss = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'best_loss.keras'),\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 策略3: 保存每个epoch的模型\n",
    "checkpoint_all = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}_val_acc_{val_accuracy:.4f}.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 策略4: 仅保存权重\n",
    "checkpoint_weights = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'best_weights.weights.h5'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('ModelCheckpoint回调函数配置完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EarlyStopping详解\n",
    "\n",
    "### 关键参数说明\n",
    "\n",
    "- `monitor`: 监控的指标名称\n",
    "- `patience`: 容忍多少个epoch无改善\n",
    "- `min_delta`: 最小改善幅度，小于此值视为无改善\n",
    "- `mode`: 'min'或'max'\n",
    "- `baseline`: 基准值，指标未达到基准值时不触发停止\n",
    "- `restore_best_weights`: 是否恢复最佳权重\n",
    "- `start_from_epoch`: 从第几个epoch开始监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 策略1: 基于验证损失的早停\n",
    "early_stop_loss = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,                    # 5个epoch无改善则停止\n",
    "    min_delta=0.001,               # 改善小于0.001视为无改善\n",
    "    mode='min',\n",
    "    restore_best_weights=True,     # 恢复最佳权重\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 策略2: 基于验证准确率的早停\n",
    "early_stop_acc = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    min_delta=0.001,\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 策略3: 设置基准值的早停\n",
    "early_stop_baseline = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    baseline=0.60,                 # 准确率未达到60%不触发早停\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('EarlyStopping回调函数配置完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练模型 - 实验1: 基础配置\n",
    "\n",
    "同时使用ModelCheckpoint和EarlyStopping。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验1: 基础配置 ==========')\n",
    "print('使用: 最佳准确率保存 + 验证损失早停\\n')\n",
    "\n",
    "model_exp1 = create_cnn_model()\n",
    "\n",
    "# 组合回调函数\n",
    "callbacks_exp1 = [\n",
    "    checkpoint_best_acc,   # 保存最佳准确率模型\n",
    "    early_stop_loss        # 基于损失的早停\n",
    "]\n",
    "\n",
    "history_exp1 = model_exp1.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=3,  # 测试用，实际训练可设为50\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_exp1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估最终模型\n",
    "test_loss, test_acc = model_exp1.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\n最终模型 - 测试损失: {test_loss:.4f}, 测试准确率: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练模型 - 实验2: 加载最佳模型\n",
    "\n",
    "演示如何加载保存的最佳模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验2: 加载最佳模型 ==========')\n",
    "\n",
    "# 加载最佳准确率模型\n",
    "best_acc_path = os.path.join(checkpoint_dir, 'best_accuracy.keras')\n",
    "if os.path.exists(best_acc_path):\n",
    "    best_model = keras.models.load_model(best_acc_path)\n",
    "    print(f'成功加载模型: {best_acc_path}')\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    test_loss, test_acc = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f'\\n最佳模型 - 测试损失: {test_loss:.4f}, 测试准确率: {test_acc:.4f}')\n",
    "    \n",
    "    # 与训练历史对比\n",
    "    best_val_acc = max(history_exp1.history['val_accuracy'])\n",
    "    print(f'训练时最佳验证准确率: {best_val_acc:.4f}')\n",
    "else:\n",
    "    print(f'未找到模型文件: {best_acc_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 训练模型 - 实验3: 仅保存权重\n",
    "\n",
    "演示权重保存和加载的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n========== 实验3: 权重保存与加载 ==========')\n",
    "\n",
    "model_exp3 = create_cnn_model()\n",
    "\n",
    "callbacks_exp3 = [\n",
    "    checkpoint_weights,    # 仅保存权重\n",
    "    early_stop_acc         # 基于准确率的早停\n",
    "]\n",
    "\n",
    "history_exp3 = model_exp3.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=3,  # 测试用\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_exp3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 创建新模型并加载权重\n",
    "weights_path = os.path.join(checkpoint_dir, 'best_weights.weights.h5')\n",
    "if os.path.exists(weights_path):\n",
    "    model_from_weights = create_cnn_model()\n",
    "    model_from_weights.load_weights(weights_path)\n",
    "    print(f'\\n成功加载权重: {weights_path}')\n",
    "    \n",
    "    # 评估加载权重后的模型\n",
    "    test_loss, test_acc = model_from_weights.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f'加载权重的模型 - 测试损失: {test_loss:.4f}, 测试准确率: {test_acc:.4f}')\n",
    "else:\n",
    "    print(f'未找到权重文件: {weights_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 自定义保存逻辑\n",
    "\n",
    "实现自定义的模型保存策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    自定义模型保存策略\n",
    "    \n",
    "    功能:\n",
    "    1. 保存top-k个最佳模型\n",
    "    2. 记录每个保存模型的详细信息\n",
    "    3. 自动清理旧的模型文件\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir, monitor='val_accuracy', mode='max', top_k=3):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.top_k = top_k\n",
    "        self.best_models = []  # 存储(score, epoch, filepath)元组\n",
    "        self.metadata_file = os.path.join(save_dir, 'checkpoint_metadata.json')\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current_score = logs.get(self.monitor)\n",
    "        \n",
    "        if current_score is None:\n",
    "            return\n",
    "        \n",
    "        # 生成模型文件名\n",
    "        filename = f'model_epoch_{epoch:02d}_{self.monitor}_{current_score:.4f}.keras'\n",
    "        filepath = os.path.join(self.save_dir, filename)\n",
    "        \n",
    "        # 判断是否应该保存\n",
    "        should_save = False\n",
    "        if len(self.best_models) < self.top_k:\n",
    "            should_save = True\n",
    "        else:\n",
    "            worst_score = min(self.best_models, key=lambda x: x[0])[0]\n",
    "            if self.mode == 'max' and current_score > worst_score:\n",
    "                should_save = True\n",
    "            elif self.mode == 'min' and current_score < worst_score:\n",
    "                should_save = True\n",
    "        \n",
    "        if should_save:\n",
    "            # 保存模型\n",
    "            self.model.save(filepath)\n",
    "            self.best_models.append((current_score, epoch, filepath))\n",
    "            \n",
    "            # 按分数排序\n",
    "            self.best_models.sort(key=lambda x: x[0], reverse=(self.mode == 'max'))\n",
    "            \n",
    "            # 如果超过top_k，删除最差的模型\n",
    "            if len(self.best_models) > self.top_k:\n",
    "                _, _, old_filepath = self.best_models.pop()\n",
    "                if os.path.exists(old_filepath):\n",
    "                    os.remove(old_filepath)\n",
    "                    print(f'\\n删除旧模型: {old_filepath}')\n",
    "            \n",
    "            print(f'\\n保存模型: {filename} ({self.monitor}={current_score:.4f})')\n",
    "            \n",
    "            # 保存元数据\n",
    "            self._save_metadata()\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"保存模型元数据到JSON文件\"\"\"\n",
    "        metadata = {\n",
    "            'monitor': self.monitor,\n",
    "            'mode': self.mode,\n",
    "            'top_k': self.top_k,\n",
    "            'best_models': [\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'score': float(score),\n",
    "                    'filepath': os.path.basename(filepath)\n",
    "                }\n",
    "                for score, epoch, filepath in self.best_models\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "# 测试自定义回调\n",
    "print('\\n========== 实验4: 自定义模型保存 ==========')\n",
    "print('保存top-3最佳模型\\n')\n",
    "\n",
    "custom_checkpoint_dir = os.path.join(checkpoint_dir, 'custom')\n",
    "model_exp4 = create_cnn_model()\n",
    "\n",
    "callbacks_exp4 = [\n",
    "    CustomModelCheckpoint(\n",
    "        save_dir=custom_checkpoint_dir,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        top_k=3\n",
    "    )\n",
    "]\n",
    "\n",
    "history_exp4 = model_exp4.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=3,  # 测试用\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_exp4,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 读取并显示元数据\n",
    "metadata_path = os.path.join(custom_checkpoint_dir, 'checkpoint_metadata.json')\n",
    "if os.path.exists(metadata_path):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print('\\n保存的模型元数据:')\n",
    "    print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 分析训练历史\n",
    "\n",
    "分析不同配置下的训练表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_history(history, title):\n",
    "    \"\"\"\n",
    "    分析训练历史\n",
    "    \"\"\"\n",
    "    print(f'\\n========== {title} ==========')\n",
    "    \n",
    "    # 提取指标\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    # 统计信息\n",
    "    total_epochs = len(train_loss)\n",
    "    best_epoch = np.argmax(val_acc) + 1\n",
    "    best_val_acc = max(val_acc)\n",
    "    final_val_acc = val_acc[-1]\n",
    "    \n",
    "    print(f'总训练轮数: {total_epochs}')\n",
    "    print(f'最佳epoch: {best_epoch} (验证准确率: {best_val_acc:.4f})')\n",
    "    print(f'最终验证准确率: {final_val_acc:.4f}')\n",
    "    \n",
    "    # 检查过拟合\n",
    "    train_val_gap = train_acc[-1] - val_acc[-1]\n",
    "    if train_val_gap > 0.1:\n",
    "        print(f'\\n警告: 存在过拟合迹象 (训练-验证准确率差距: {train_val_gap:.4f})')\n",
    "    \n",
    "    # 检查欠拟合\n",
    "    if val_acc[-1] < 0.5:\n",
    "        print('\\n警告: 可能存在欠拟合，模型性能较差')\n",
    "    \n",
    "    return {\n",
    "        'total_epochs': total_epochs,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'final_val_acc': final_val_acc\n",
    "    }\n",
    "\n",
    "# 分析各个实验的训练历史\n",
    "results = {}\n",
    "results['exp1'] = analyze_training_history(history_exp1, '实验1分析')\n",
    "results['exp3'] = analyze_training_history(history_exp3, '实验3分析')\n",
    "results['exp4'] = analyze_training_history(history_exp4, '实验4分析')\n",
    "\n",
    "# 对比结果\n",
    "print('\\n========== 实验对比 ==========')\n",
    "for exp_name, result in results.items():\n",
    "    print(f'{exp_name}: 最佳验证准确率={result[\"best_val_acc\"]:.4f} @ epoch {result[\"best_epoch\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 清理临时文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# 清理测试产生的文件和目录\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f'已删除目录: {checkpoint_dir}')\n",
    "\n",
    "print('清理完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### ModelCheckpoint最佳实践\n",
    "\n",
    "1. **监控指标选择**:\n",
    "   - 使用验证集指标而非训练集指标\n",
    "   - 对于分类任务，优先监控准确率\n",
    "   - 对于回归任务，监控损失或自定义指标\n",
    "\n",
    "2. **保存策略**:\n",
    "   - 长时间训练: 同时保存最佳模型和定期检查点\n",
    "   - 快速实验: 仅保存最佳模型节省空间\n",
    "   - 生产环境: 使用Keras格式(.keras)而非HDF5格式(.h5)\n",
    "\n",
    "3. **文件命名**:\n",
    "   - 包含关键信息: epoch、指标值、时间戳\n",
    "   - 便于识别和排序\n",
    "\n",
    "### EarlyStopping最佳实践\n",
    "\n",
    "1. **patience设置**:\n",
    "   - 数据量大、模型复杂: patience=5-10\n",
    "   - 数据量小、模型简单: patience=3-5\n",
    "   - 避免过小导致过早停止\n",
    "\n",
    "2. **min_delta设置**:\n",
    "   - 根据指标尺度调整\n",
    "   - 准确率: 0.001-0.01\n",
    "   - 损失: 0.0001-0.001\n",
    "\n",
    "3. **restore_best_weights**:\n",
    "   - 几乎总是设为True\n",
    "   - 确保返回最佳性能的模型\n",
    "\n",
    "### 组合使用建议\n",
    "\n",
    "```python\n",
    "callbacks = [\n",
    "    # 保存最佳模型\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    # 早停机制\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "```\n",
    "\n",
    "### 常见问题\n",
    "\n",
    "1. **模型未保存**: 检查`save_best_only`和`monitor`配置\n",
    "2. **过早停止**: 增大patience值\n",
    "3. **磁盘空间不足**: 使用`save_best_only=True`或定期清理\n",
    "4. **权重加载失败**: 确保模型架构完全一致"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
