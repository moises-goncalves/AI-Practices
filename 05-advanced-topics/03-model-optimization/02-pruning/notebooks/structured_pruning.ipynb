{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 结构化剪枝 (Structured Pruning)\n",
        "\n",
        "**SOTA 教育标准** | 包含通道剪枝、BN Scale 剪枝、硬件友好稀疏\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 结构化剪枝原理\n",
        "\n",
        "**非结构化剪枝的问题**: 稀疏矩阵存储开销、硬件加速困难。\n",
        "\n",
        "**结构化剪枝优势**: 直接减少张量维度，无需特殊硬件支持。\n",
        "\n",
        "**BN Scale 剪枝**: $y = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta$，当 $|\\gamma| \\approx 0$ 时可移除通道。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. 通道重要性计算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChannelImportance:\n",
        "    \"\"\"通道重要性计算器。\n",
        "    \n",
        "    Core Idea: 使用 L1-norm 衡量通道重要性。\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def compute(conv: nn.Conv2d) -> Tensor:\n",
        "        \"\"\"计算每个输出通道的重要性 (L1-norm)。\"\"\"\n",
        "        return conv.weight.data.abs().sum(dim=(1, 2, 3))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_mask(importance: Tensor, sparsity: float) -> Tensor:\n",
        "        \"\"\"获取剪枝掩码。\"\"\"\n",
        "        num_channels = importance.numel()\n",
        "        num_prune = int(num_channels * sparsity)\n",
        "        threshold = torch.kthvalue(importance, max(num_prune, 1)).values\n",
        "        return importance > threshold\n",
        "\n",
        "\n",
        "# 测试\n",
        "conv = nn.Conv2d(64, 128, 3, padding=1)\n",
        "importance = ChannelImportance.compute(conv)\n",
        "mask = ChannelImportance.get_mask(importance, 0.5)\n",
        "print(f\"通道数: {len(importance)}, 保留: {mask.sum().item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 通道剪枝器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChannelPruner:\n",
        "    \"\"\"通道剪枝器。\n",
        "    \n",
        "    Summary: 移除不重要的输出通道，直接减少模型大小。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sparsity: float = 0.5):\n",
        "        self.sparsity = sparsity\n",
        "\n",
        "    def prune_conv(self, conv: nn.Conv2d) -> Tuple[nn.Conv2d, Tensor]:\n",
        "        \"\"\"剪枝单个卷积层。\"\"\"\n",
        "        importance = ChannelImportance.compute(conv)\n",
        "        mask = ChannelImportance.get_mask(importance, self.sparsity)\n",
        "        keep_indices = torch.where(mask)[0]\n",
        "\n",
        "        new_conv = nn.Conv2d(\n",
        "            conv.in_channels, len(keep_indices),\n",
        "            conv.kernel_size, conv.stride, conv.padding,\n",
        "            bias=conv.bias is not None\n",
        "        )\n",
        "        new_conv.weight.data = conv.weight.data[keep_indices]\n",
        "        if conv.bias is not None:\n",
        "            new_conv.bias.data = conv.bias.data[keep_indices]\n",
        "\n",
        "        return new_conv, keep_indices\n",
        "\n",
        "\n",
        "# 测试\n",
        "conv = nn.Conv2d(64, 128, 3, padding=1)\n",
        "pruner = ChannelPruner(sparsity=0.5)\n",
        "new_conv, kept = pruner.prune_conv(conv)\n",
        "\n",
        "print(f\"原始通道数: {conv.out_channels}\")\n",
        "print(f\"剪枝后通道数: {new_conv.out_channels}\")\n",
        "print(f\"压缩率: {1 - new_conv.out_channels/conv.out_channels:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. BN Scale 剪枝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BNScalePruner:\n",
        "    \"\"\"基于 BN Scale 的剪枝器。\n",
        "    \n",
        "    Core Idea: 使用 BatchNorm 的 gamma 参数作为通道重要性。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, sparsity: float = 0.5):\n",
        "        self.model = model\n",
        "        self.sparsity = sparsity\n",
        "\n",
        "    def collect_bn_scales(self) -> Tensor:\n",
        "        \"\"\"收集所有 BN 层的 gamma 参数。\"\"\"\n",
        "        scales = []\n",
        "        for m in self.model.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                scales.append(m.weight.data.abs())\n",
        "        return torch.cat(scales) if scales else torch.tensor([])\n",
        "\n",
        "    def compute_threshold(self) -> float:\n",
        "        \"\"\"计算全局剪枝阈值。\"\"\"\n",
        "        all_scales = self.collect_bn_scales()\n",
        "        if len(all_scales) == 0:\n",
        "            return 0.0\n",
        "        num_prune = int(len(all_scales) * self.sparsity)\n",
        "        return torch.kthvalue(all_scales, max(num_prune, 1)).values.item()\n",
        "\n",
        "\n",
        "# 测试\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "    nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        ")\n",
        "pruner = BNScalePruner(model, sparsity=0.3)\n",
        "threshold = pruner.compute_threshold()\n",
        "print(f\"BN Scale 剪枝阈值: {threshold:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. 可视化分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_channel_importance(conv: nn.Conv2d) -> None:\n",
        "    \"\"\"可视化通道重要性分布。\"\"\"\n",
        "    importance = ChannelImportance.compute(conv)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    axes[0].bar(range(len(importance)), importance.numpy())\n",
        "    axes[0].set_xlabel('Channel Index')\n",
        "    axes[0].set_ylabel('Importance (L1-norm)')\n",
        "    axes[0].set_title('Channel Importance Distribution')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    sorted_imp, _ = torch.sort(importance)\n",
        "    axes[1].plot(sorted_imp.numpy(), 'b-')\n",
        "    mid = len(sorted_imp) // 2\n",
        "    axes[1].axhline(sorted_imp[mid].item(), color='r', linestyle='--', label='50% threshold')\n",
        "    axes[1].set_xlabel('Rank')\n",
        "    axes[1].set_ylabel('Importance')\n",
        "    axes[1].set_title('Sorted Channel Importance')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "conv = nn.Conv2d(64, 128, 3)\n",
        "visualize_channel_importance(conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. 总结\n",
        "\n",
        "| 方法 | 重要性指标 | 优势 |\n",
        "|:-----|:---------|:-----|\n",
        "| **L1-norm** | $\\sum|W|$ | 简单直接 |\n",
        "| **BN Scale** | $|\\gamma|$ | 可学习 |\n",
        "| **Taylor** | $|W \\cdot \\nabla W|$ | 考虑梯度 |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
