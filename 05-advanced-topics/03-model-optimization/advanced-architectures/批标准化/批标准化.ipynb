{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批标准化 (Batch Normalization)\n",
    "\n",
    "## 核心原理\n",
    "\n",
    "批标准化是一种有效的优化技术，主要解决深度神经网络训练过程中的内部协变量偏移问题。\n",
    "\n",
    "### 为什么需要批标准化\n",
    "\n",
    "1. **数据标准化的必要性**：在输入模型前进行数据标准化是标准做法\n",
    "   ```python\n",
    "   normalized_data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "   ```\n",
    "\n",
    "2. **网络内部的标准化**：每一层的输入分布在训练过程中持续变化，批标准化在每层之前进行标准化\n",
    "\n",
    "3. **主要优势**：\n",
    "   - 加速模型收敛速度\n",
    "   - 改善梯度传播，特别是在深层网络中\n",
    "   - 允许使用更大的学习率\n",
    "   - 提供一定的正则化效果\n",
    "   - 减少对权重初始化的敏感度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验设置\n",
    "\n",
    "我们将通过对比实验来展示批标准化的效果：\n",
    "- 模型A：不使用批标准化\n",
    "- 模型B：使用批标准化\n",
    "\n",
    "数据集：MNIST手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow版本: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"训练集形状: {x_train.shape}\")\n",
    "print(f\"测试集形状: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "\n",
    "### 使用批标准化的关键原则\n",
    "\n",
    "1. **位置**：BatchNormalization层通常放在激活函数之前\n",
    "   - Conv2D/Dense → BatchNormalization → Activation\n",
    "   \n",
    "2. **参数axis**：\n",
    "   - 对于channels_last格式（默认），使用axis=-1\n",
    "   - 对于channels_first格式，使用axis=1\n",
    "   \n",
    "3. **小批量数据**：当batch_size较小时，可以考虑使用BatchRenormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_without_bn():\n",
    "    \"\"\"创建不使用批标准化的模型\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_model_with_bn():\n",
    "    \"\"\"创建使用批标准化的模型\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), use_bias=False, input_shape=(28, 28, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 创建两个模型\n",
    "model_without_bn = create_model_without_bn()\n",
    "model_with_bn = create_model_with_bn()\n",
    "\n",
    "print(\"\\n不使用批标准化的模型：\")\n",
    "model_without_bn.summary()\n",
    "\n",
    "print(\"\\n使用批标准化的模型：\")\n",
    "model_with_bn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型编译与训练\n",
    "\n",
    "使用相同的训练配置进行对比实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 编译模型\nmodel_without_bn.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_with_bn.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 训练参数\nepochs = 10\nbatch_size = 128\nvalidation_split = 0.1\n\nprint(\"开始训练不使用批标准化的模型...\")\nhistory_without_bn = model_without_bn.fit(\n    x_train, y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=validation_split,\n    verbose=1\n)\n\nprint(\"\\n开始训练使用批标准化的模型...\")\nhistory_with_bn = model_with_bn.fit(\n    x_train, y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=validation_split,\n    verbose=1\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果分析与可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估测试集性能\n",
    "print(\"\\n测试集性能评估：\")\n",
    "test_loss_without_bn, test_acc_without_bn = model_without_bn.evaluate(x_test, y_test, verbose=0)\n",
    "test_loss_with_bn, test_acc_with_bn = model_with_bn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"不使用批标准化 - 测试准确率: {test_acc_without_bn:.4f}, 测试损失: {test_loss_without_bn:.4f}\")\n",
    "print(f\"使用批标准化 - 测试准确率: {test_acc_with_bn:.4f}, 测试损失: {test_loss_with_bn:.4f}\")\n",
    "print(f\"准确率提升: {(test_acc_with_bn - test_acc_without_bn)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 训练准确率对比\n",
    "axes[0].plot(history_without_bn.history['accuracy'], label='训练准确率（无BN）', marker='o')\n",
    "axes[0].plot(history_without_bn.history['val_accuracy'], label='验证准确率（无BN）', marker='o')\n",
    "axes[0].plot(history_with_bn.history['accuracy'], label='训练准确率（有BN）', marker='s')\n",
    "axes[0].plot(history_with_bn.history['val_accuracy'], label='验证准确率（有BN）', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('准确率')\n",
    "axes[0].set_title('模型准确率对比')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 训练损失对比\n",
    "axes[1].plot(history_without_bn.history['loss'], label='训练损失（无BN）', marker='o')\n",
    "axes[1].plot(history_without_bn.history['val_loss'], label='验证损失（无BN）', marker='o')\n",
    "axes[1].plot(history_with_bn.history['loss'], label='训练损失（有BN）', marker='s')\n",
    "axes[1].plot(history_with_bn.history['val_loss'], label='验证损失（有BN）', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('损失')\n",
    "axes[1].set_title('模型损失对比')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批标准化的工作原理\n",
    "\n",
    "### 训练阶段\n",
    "对于每个小批量数据，BatchNormalization层执行以下操作：\n",
    "\n",
    "1. 计算当前批次的均值和方差\n",
    "2. 标准化数据：$\\hat{x} = \\frac{x - \\mu_{batch}}{\\sqrt{\\sigma_{batch}^2 + \\epsilon}}$\n",
    "3. 应用可学习的缩放和偏移参数：$y = \\gamma \\hat{x} + \\beta$\n",
    "4. 使用移动平均更新全局统计量（用于推理阶段）\n",
    "\n",
    "### 推理阶段\n",
    "使用训练过程中累积的移动平均统计量进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查批标准化层的参数\n",
    "print(\"批标准化层的可学习参数示例：\\n\")\n",
    "for layer in model_with_bn.layers:\n",
    "    if isinstance(layer, layers.BatchNormalization):\n",
    "        print(f\"层名称: {layer.name}\")\n",
    "        print(f\"  - gamma (缩放): 形状 {layer.gamma.shape}\")\n",
    "        print(f\"  - beta (偏移): 形状 {layer.beta.shape}\")\n",
    "        print(f\"  - moving_mean: 形状 {layer.moving_mean.shape}\")\n",
    "        print(f\"  - moving_variance: 形状 {layer.moving_variance.shape}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最佳实践与注意事项\n",
    "\n",
    "### 1. 使用建议\n",
    "- 在卷积层或全连接层之后立即使用\n",
    "- 通常放在激活函数之前（Conv/Dense → BN → Activation）\n",
    "- 使用BN时可以省略层的bias参数（use_bias=False），因为BN有自己的偏移参数\n",
    "\n",
    "### 2. 批量大小的影响\n",
    "- 批量较大（≥32）时效果最佳\n",
    "- 批量较小时，统计量估计不准确，可考虑使用Layer Normalization或Group Normalization\n",
    "\n",
    "### 3. 训练与推理的差异\n",
    "- 训练时使用批次统计量\n",
    "- 推理时使用移动平均统计量\n",
    "- 注意设置正确的training参数\n",
    "\n",
    "### 4. 与其他技术的配合\n",
    "- 可以与Dropout配合使用，但通常放在Dropout之前\n",
    "- 使用BN后可以适当提高学习率\n",
    "- 可以减少对权重初始化方法的依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "批标准化是现代深度学习中的关键技术，通过实验我们观察到：\n",
    "\n",
    "1. **收敛速度**：使用BN的模型通常收敛更快\n",
    "2. **最终性能**：使用BN通常能获得更好的准确率\n",
    "3. **训练稳定性**：BN使训练过程更加稳定，减少了对学习率和初始化的敏感度\n",
    "4. **泛化能力**：BN提供了一定的正则化效果，有助于模型泛化\n",
    "\n",
    "在实际应用中，批标准化已成为深度神经网络的标准配置之一。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}