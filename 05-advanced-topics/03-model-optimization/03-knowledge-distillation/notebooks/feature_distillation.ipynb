{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 特征蒸馏 (Feature Distillation)\n",
        "\n",
        "**SOTA 教育标准** | 包含 FitNets、注意力迁移、中间层蒸馏\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 特征蒸馏原理\n",
        "\n",
        "**核心思想**: 不仅蒸馏输出，还蒸馏中间层特征。\n",
        "\n",
        "**FitNets 损失**:\n",
        "\n",
        "$$\\mathcal{L}_{hint} = \\|W_r \\cdot F_s - F_t\\|_2^2$$\n",
        "\n",
        "其中 $W_r$ 是回归器，用于匹配维度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import Dict, List\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. 特征蒸馏损失"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureDistillationLoss(nn.Module):\n",
        "    \"\"\"特征蒸馏损失。\n",
        "    \n",
        "    Core Idea: 最小化学生和教师中间层特征的差异。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, student_channels: int, teacher_channels: int):\n",
        "        super().__init__()\n",
        "        self.regressor = nn.Conv2d(student_channels, teacher_channels, 1) if student_channels != teacher_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, student_feat: Tensor, teacher_feat: Tensor) -> Tensor:\n",
        "        student_feat = self.regressor(student_feat)\n",
        "        return F.mse_loss(student_feat, teacher_feat)\n",
        "\n",
        "\n",
        "# 测试\n",
        "loss_fn = FeatureDistillationLoss(64, 128)\n",
        "student_feat = torch.randn(4, 64, 8, 8)\n",
        "teacher_feat = torch.randn(4, 128, 8, 8)\n",
        "loss = loss_fn(student_feat, teacher_feat)\n",
        "print(f\"Feature Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 注意力迁移"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionTransferLoss(nn.Module):\n",
        "    \"\"\"注意力迁移损失。\n",
        "    \n",
        "    Core Idea: 迁移教师的注意力图到学生。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p: int = 2):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def attention_map(self, feat: Tensor) -> Tensor:\n",
        "        \"\"\"计算注意力图: 沿通道维度求和。\"\"\"\n",
        "        return feat.pow(self.p).mean(dim=1)\n",
        "\n",
        "    def forward(self, student_feat: Tensor, teacher_feat: Tensor) -> Tensor:\n",
        "        s_attn = self.attention_map(student_feat)\n",
        "        t_attn = self.attention_map(teacher_feat)\n",
        "        \n",
        "        # 归一化\n",
        "        s_attn = s_attn / s_attn.sum(dim=(1, 2), keepdim=True)\n",
        "        t_attn = t_attn / t_attn.sum(dim=(1, 2), keepdim=True)\n",
        "        \n",
        "        return (s_attn - t_attn).pow(2).mean()\n",
        "\n",
        "\n",
        "# 测试\n",
        "loss_fn = AttentionTransferLoss()\n",
        "student_feat = torch.randn(4, 64, 8, 8)\n",
        "teacher_feat = torch.randn(4, 128, 8, 8)\n",
        "loss = loss_fn(student_feat, teacher_feat)\n",
        "print(f\"Attention Transfer Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. 可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention_maps():\n",
        "    \"\"\"可视化注意力图。\"\"\"\n",
        "    teacher_feat = torch.randn(1, 64, 16, 16)\n",
        "    student_feat = torch.randn(1, 32, 16, 16)\n",
        "    \n",
        "    t_attn = teacher_feat.pow(2).mean(dim=1)[0]\n",
        "    s_attn = student_feat.pow(2).mean(dim=1)[0]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    axes[0].imshow(t_attn.numpy(), cmap='hot')\n",
        "    axes[0].set_title('Teacher Attention')\n",
        "    axes[1].imshow(s_attn.numpy(), cmap='hot')\n",
        "    axes[1].set_title('Student Attention')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_attention_maps()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. 总结\n",
        "\n",
        "| 方法 | 损失 | 特点 |\n",
        "|:-----|:-----|:-----|\n",
        "| **FitNets** | MSE | 直接匹配特征 |\n",
        "| **AT** | 注意力差异 | 迁移空间注意力 |\n",
        "| **FSP** | Gram矩阵 | 迁移特征关系 |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
