{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 自蒸馏 (Self-Distillation)\n",
        "\n",
        "**SOTA 教育标准** | 包含 Born-Again Networks、深度监督、辅助分类器\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 自蒸馏理论\n",
        "\n",
        "**传统蒸馏**: 大教师 → 小学生 | **自蒸馏**: 模型自己教自己\n",
        "\n",
        "**Born-Again Networks**: 用训练好的模型作为教师，训练相同架构的学生。\n",
        "\n",
        "$$\\text{Acc}(M_1) < \\text{Acc}(M_2) < \\text{Acc}(M_3)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, Dict, List, Optional, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Born-Again 配置与训练器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BornAgainConfig:\n",
        "    \"\"\"Born-Again Networks 配置。\"\"\"\n",
        "    num_generations: int = 3\n",
        "    temperature: float = 4.0\n",
        "    alpha: float = 0.5\n",
        "    epochs_per_gen: int = 10\n",
        "\n",
        "\n",
        "class BornAgainTrainer:\n",
        "    \"\"\"Born-Again Networks 训练器。\n",
        "    \n",
        "    Core Idea: 迭代自蒸馏，每代模型都比上一代更好。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_fn: Callable[[], nn.Module], config: BornAgainConfig = BornAgainConfig()):\n",
        "        self.model_fn = model_fn\n",
        "        self.config = config\n",
        "        self.generations: List[nn.Module] = []\n",
        "        self.history: Dict[str, List] = {\"generation\": [], \"test_acc\": []}\n",
        "\n",
        "    def get_teacher(self) -> Optional[nn.Module]:\n",
        "        \"\"\"获取上一代模型作为教师。\"\"\"\n",
        "        if not self.generations:\n",
        "            return None\n",
        "        teacher = self.generations[-1]\n",
        "        teacher.eval()\n",
        "        for p in teacher.parameters():\n",
        "            p.requires_grad = False\n",
        "        return teacher\n",
        "\n",
        "\n",
        "# 测试\n",
        "trainer = BornAgainTrainer(lambda: nn.Linear(64, 10))\n",
        "print(f\"配置: {trainer.config}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 深度监督网络"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepSupervisionNet(nn.Module):\n",
        "    \"\"\"深度监督自蒸馏网络。\n",
        "    \n",
        "    Core Idea: 让浅层学习深层的知识。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        \n",
        "        self.aux1 = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(64, num_classes))\n",
        "        self.aux2 = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(128, num_classes))\n",
        "        self.main = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(256, num_classes))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tuple[Tensor, List[Tensor]]:\n",
        "        f1 = self.layer1(x)\n",
        "        f2 = self.layer2(f1)\n",
        "        f3 = self.layer3(f2)\n",
        "        return self.main(f3), [self.aux1(f1), self.aux2(f2)]\n",
        "\n",
        "\n",
        "# 测试\n",
        "model = DeepSupervisionNet(10)\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "main_out, aux_outs = model(x)\n",
        "print(f\"主输出: {main_out.shape}, 辅助输出: {[a.shape for a in aux_outs]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. 深度监督损失"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepSupervisedLoss(nn.Module):\n",
        "    \"\"\"深度监督损失：深层监督浅层。\"\"\"\n",
        "\n",
        "    def __init__(self, temperature: float = 4.0, aux_weights: List[float] = [0.3, 0.3]):\n",
        "        super().__init__()\n",
        "        self.T = temperature\n",
        "        self.aux_weights = aux_weights\n",
        "\n",
        "    def forward(self, main_logits: Tensor, aux_logits: List[Tensor], targets: Tensor) -> Tensor:\n",
        "        loss_main = F.cross_entropy(main_logits, targets)\n",
        "        main_soft = F.softmax(main_logits.detach() / self.T, dim=1)\n",
        "        \n",
        "        loss_aux = 0.0\n",
        "        for aux, w in zip(aux_logits, self.aux_weights):\n",
        "            aux_log = F.log_softmax(aux / self.T, dim=1)\n",
        "            loss_aux += w * F.kl_div(aux_log, main_soft, reduction='batchmean') * (self.T ** 2)\n",
        "        \n",
        "        return loss_main + loss_aux\n",
        "\n",
        "\n",
        "# 测试\n",
        "loss_fn = DeepSupervisedLoss()\n",
        "targets = torch.randint(0, 10, (2,))\n",
        "loss = loss_fn(main_out, aux_outs, targets)\n",
        "print(f\"深度监督损失: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. 可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_born_again_effect() -> None:\n",
        "    \"\"\"可视化 Born-Again 效果。\"\"\"\n",
        "    generations = [1, 2, 3, 4, 5]\n",
        "    test_acc = [85.0, 87.2, 88.5, 89.1, 89.4]\n",
        "    gains = [0] + [test_acc[i] - test_acc[i-1] for i in range(1, len(test_acc))]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    axes[0].plot(generations, test_acc, 'o-', linewidth=2, markersize=8)\n",
        "    axes[0].set_xlabel('Generation')\n",
        "    axes[0].set_ylabel('Test Accuracy (%)')\n",
        "    axes[0].set_title('Born-Again: Accuracy vs Generation')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    colors = ['gray'] + ['green' if g > 0 else 'red' for g in gains[1:]]\n",
        "    axes[1].bar(generations, gains, color=colors, alpha=0.7)\n",
        "    axes[1].axhline(0, color='black', linestyle='-')\n",
        "    axes[1].set_xlabel('Generation')\n",
        "    axes[1].set_ylabel('Accuracy Gain (%)')\n",
        "    axes[1].set_title('Improvement per Generation')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"总提升: {test_acc[-1] - test_acc[0]:+.1f}%\")\n",
        "\n",
        "\n",
        "visualize_born_again_effect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. 总结\n",
        "\n",
        "| 方法 | 特点 | 适用场景 |\n",
        "|:-----|:-----|:---------|\n",
        "| **Born-Again** | 迭代蒸馏 | 有足够训练时间 |\n",
        "| **深度监督** | 深层→浅层 | 深层网络 |\n",
        "| **辅助分类器** | 多出口 | 推理加速 |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
