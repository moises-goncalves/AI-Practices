{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 量化基础理论与数学原理\n",
        "\n",
        "**SOTA 教育标准** | 包含信息论基础、量化误差分析、均匀量化公式推导\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 信息论基础\n",
        "\n",
        "### 1.1 为什么量化？\n",
        "\n",
        "**核心问题**: FP32 模型存储和计算开销大。\n",
        "\n",
        "**量化本质**: 用有限离散值表示连续数值，这是**有损压缩**。\n",
        "\n",
        "### 1.2 量化误差\n",
        "\n",
        "**量化噪声功率**: $\\sigma_q^2 = \\Delta^2/12$\n",
        "\n",
        "**信噪比 (SQNR)**: 每增加 1 bit，SQNR 提升 ~6 dB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. 量化方案定义"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantizationScheme(Enum):\n",
        "    \"\"\"量化方案枚举。\"\"\"\n",
        "    INT8 = (\"int8\", -128, 127)\n",
        "    UINT8 = (\"uint8\", 0, 255)\n",
        "    INT4 = (\"int4\", -8, 7)\n",
        "\n",
        "    def __init__(self, name: str, qmin: int, qmax: int):\n",
        "        self._name = name\n",
        "        self.qmin = qmin\n",
        "        self.qmax = qmax\n",
        "\n",
        "    @property\n",
        "    def levels(self) -> int:\n",
        "        return self.qmax - self.qmin + 1\n",
        "\n",
        "    @property\n",
        "    def bit_width(self) -> int:\n",
        "        return int(np.log2(self.levels))\n",
        "\n",
        "\n",
        "# 测试\n",
        "scheme = QuantizationScheme.INT8\n",
        "print(f\"Scheme: {scheme._name}, Range: [{scheme.qmin}, {scheme.qmax}], Bits: {scheme.bit_width}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 量化参数计算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class QuantizationParams:\n",
        "    \"\"\"量化参数: scale 和 zero_point。\"\"\"\n",
        "    scale: Tensor\n",
        "    zero_point: Tensor\n",
        "    qmin: int\n",
        "    qmax: int\n",
        "\n",
        "    @classmethod\n",
        "    def from_tensor(cls, x: Tensor, scheme: QuantizationScheme, symmetric: bool = True):\n",
        "        \"\"\"从张量统计计算量化参数。\"\"\"\n",
        "        x_min, x_max = x.min().item(), x.max().item()\n",
        "        \n",
        "        if symmetric:\n",
        "            max_abs = max(abs(x_min), abs(x_max), 1e-8)\n",
        "            q_range = min(abs(scheme.qmin), scheme.qmax)\n",
        "            scale = torch.tensor(max_abs / q_range)\n",
        "            zero_point = torch.tensor(0.0)\n",
        "        else:\n",
        "            scale = torch.tensor((x_max - x_min) / (scheme.qmax - scheme.qmin))\n",
        "            scale = torch.clamp(scale, min=1e-8)\n",
        "            zero_point = torch.round(torch.tensor(scheme.qmin - x_min / scale.item()))\n",
        "            zero_point = torch.clamp(zero_point, scheme.qmin, scheme.qmax)\n",
        "        \n",
        "        return cls(scale=scale, zero_point=zero_point, qmin=scheme.qmin, qmax=scheme.qmax)\n",
        "\n",
        "\n",
        "# 测试\n",
        "x = torch.randn(100) * 0.5\n",
        "params = QuantizationParams.from_tensor(x, QuantizationScheme.INT8, symmetric=True)\n",
        "print(f\"Scale: {params.scale:.6f}, Zero-point: {params.zero_point}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. 均匀量化器实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UniformQuantizer:\n",
        "    \"\"\"均匀量化器。\n",
        "    \n",
        "    Core Idea: 实现标准的线性量化/反量化操作。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params: QuantizationParams):\n",
        "        self.params = params\n",
        "\n",
        "    def quantize(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"量化：FP32 -> INT8。\"\"\"\n",
        "        q = x / self.params.scale + self.params.zero_point\n",
        "        q = torch.round(q)\n",
        "        q = torch.clamp(q, self.params.qmin, self.params.qmax)\n",
        "        return q\n",
        "\n",
        "    def dequantize(self, q: Tensor) -> Tensor:\n",
        "        \"\"\"反量化：INT8 -> FP32。\"\"\"\n",
        "        return self.params.scale * (q - self.params.zero_point)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"前向量化：量化 -> 反量化。\"\"\"\n",
        "        return self.dequantize(self.quantize(x))\n",
        "\n",
        "\n",
        "# 测试\n",
        "x = torch.randn(4, 4) * 0.5\n",
        "params = QuantizationParams.from_tensor(x, QuantizationScheme.INT8, symmetric=True)\n",
        "quantizer = UniformQuantizer(params)\n",
        "x_q = quantizer.forward(x)\n",
        "\n",
        "print(f\"原始张量 (部分): {x[0, :2]}\")\n",
        "print(f\"量化后张量 (部分): {x_q[0, :2]}\")\n",
        "print(f\"量化误差 MSE: {torch.mean((x - x_q) ** 2):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. 可视化分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_quantization(x: Tensor, scheme: QuantizationScheme = QuantizationScheme.INT8):\n",
        "    \"\"\"可视化量化效果。\"\"\"\n",
        "    params = QuantizationParams.from_tensor(x, scheme, symmetric=True)\n",
        "    quantizer = UniformQuantizer(params)\n",
        "    x_q = quantizer.forward(x)\n",
        "    error = x - x_q\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    # 分布对比\n",
        "    axes[0].hist(x.flatten().numpy(), bins=50, alpha=0.5, label=\"Original\", color=\"blue\")\n",
        "    axes[0].hist(x_q.flatten().numpy(), bins=50, alpha=0.5, label=\"Quantized\", color=\"red\")\n",
        "    axes[0].set_title(\"Distribution Comparison\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 量化误差\n",
        "    axes[1].hist(error.flatten().numpy(), bins=50, color=\"green\", alpha=0.7)\n",
        "    axes[1].set_title(f\"Error Distribution (MSE={torch.mean(error**2):.6f})\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 散点图\n",
        "    axes[2].scatter(x.flatten().numpy(), x_q.flatten().numpy(), alpha=0.3, s=1)\n",
        "    axes[2].plot([x.min(), x.max()], [x.min(), x.max()], \"r--\", linewidth=1)\n",
        "    axes[2].set_title(\"Original vs Quantized\")\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 测试\n",
        "test_data = torch.randn(10000)\n",
        "visualize_quantization(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. 总结\n",
        "\n",
        "| 概念 | 公式 | 说明 |\n",
        "|:-----|:-----|:-----|\n",
        "| **均匀量化** | $Q(x) = \\text{round}(x/s + z)$ | $s$: scale, $z$: zero-point |\n",
        "| **反量化** | $\\hat{x} = s \\cdot (q - z)$ | 恢复浮点表示 |\n",
        "| **对称量化** | $z=0, s = x_{max}/2^{b-1}$ | 简单高效 |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
