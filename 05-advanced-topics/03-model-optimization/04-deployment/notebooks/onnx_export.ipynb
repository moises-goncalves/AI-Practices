{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ONNX 模型导出与优化\n",
        "\n",
        "**SOTA 教育标准** | 包含 ONNX 导出、验证、优化、推理\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ONNX 概述\n",
        "\n",
        "**ONNX**: 开放神经网络交换格式，支持跨框架模型转换。\n",
        "\n",
        "| 优势 | 说明 |\n",
        "|:-----|:-----|\n",
        "| **跨框架** | PyTorch/TF → ONNX |\n",
        "| **优化推理** | ONNX Runtime 加速 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Optional, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "ONNX_AVAILABLE = False\n",
        "try:\n",
        "    import onnx\n",
        "    import onnxruntime as ort\n",
        "    print(f\"ONNX: {onnx.__version__}, Runtime: {ort.__version__}\")\n",
        "    ONNX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"ONNX not installed. Run: pip install onnx onnxruntime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. 模型定义与导出配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ONNXExportConfig:\n",
        "    \"\"\"ONNX 导出配置。\"\"\"\n",
        "    opset_version: int = 14\n",
        "    dynamic_axes: bool = True\n",
        "    do_constant_folding: bool = True\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"用于导出演示的简单 CNN。\"\"\"\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(64, num_classes))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.classifier(self.features(x))\n",
        "\n",
        "\n",
        "model = SimpleCNN(10)\n",
        "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. ONNX 导出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_onnx(model: nn.Module, input_shape: Tuple[int, ...], output_path: str,\n",
        "                   config: ONNXExportConfig = ONNXExportConfig()) -> str:\n",
        "    \"\"\"导出 PyTorch 模型到 ONNX。\"\"\"\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(*input_shape)\n",
        "    \n",
        "    dynamic_axes = {\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}} if config.dynamic_axes else None\n",
        "    \n",
        "    torch.onnx.export(\n",
        "        model, dummy_input, output_path,\n",
        "        export_params=True, opset_version=config.opset_version,\n",
        "        do_constant_folding=config.do_constant_folding,\n",
        "        input_names=[\"input\"], output_names=[\"output\"],\n",
        "        dynamic_axes=dynamic_axes)\n",
        "    \n",
        "    print(f\"Exported to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "if ONNX_AVAILABLE:\n",
        "    onnx_path = export_to_onnx(model, (1, 3, 32, 32), \"/tmp/model.onnx\")\n",
        "else:\n",
        "    print(\"跳过导出\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. ONNX 验证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_onnx_model(onnx_path: str) -> bool:\n",
        "    \"\"\"验证 ONNX 模型。\"\"\"\n",
        "    if not ONNX_AVAILABLE:\n",
        "        return False\n",
        "    try:\n",
        "        onnx_model = onnx.load(onnx_path)\n",
        "        onnx.checker.check_model(onnx_model)\n",
        "        print(f\"Model valid! IR: {onnx_model.ir_version}, Opset: {onnx_model.opset_import[0].version}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Validation failed: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "if ONNX_AVAILABLE:\n",
        "    verify_onnx_model(\"/tmp/model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. ONNX Runtime 推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ONNXEngine:\n",
        "    \"\"\"ONNX Runtime 推理引擎。\"\"\"\n",
        "    def __init__(self, onnx_path: str):\n",
        "        if not ONNX_AVAILABLE:\n",
        "            raise RuntimeError(\"ONNX Runtime not available\")\n",
        "        self.session = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "        self.input_name = self.session.get_inputs()[0].name\n",
        "        self.output_name = self.session.get_outputs()[0].name\n",
        "\n",
        "    def __call__(self, inputs: np.ndarray) -> np.ndarray:\n",
        "        return self.session.run([self.output_name], {self.input_name: inputs})[0]\n",
        "\n",
        "\n",
        "if ONNX_AVAILABLE:\n",
        "    engine = ONNXEngine(\"/tmp/model.onnx\")\n",
        "    test_input = np.random.randn(1, 3, 32, 32).astype(np.float32)\n",
        "    output = engine(test_input)\n",
        "    print(f\"ONNX 推理输出: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. 性能对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def benchmark(model: nn.Module, onnx_path: str, input_shape: Tuple, runs: int = 100) -> Dict:\n",
        "    \"\"\"对比 PyTorch 和 ONNX Runtime 性能。\"\"\"\n",
        "    dummy = torch.randn(*input_shape)\n",
        "    model.eval()\n",
        "    \n",
        "    # PyTorch\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10): model(dummy)  # warmup\n",
        "        start = time.perf_counter()\n",
        "        for _ in range(runs): model(dummy)\n",
        "        pt_time = (time.perf_counter() - start) / runs * 1000\n",
        "    \n",
        "    # ONNX\n",
        "    if ONNX_AVAILABLE:\n",
        "        engine = ONNXEngine(onnx_path)\n",
        "        np_input = dummy.numpy()\n",
        "        for _ in range(10): engine(np_input)  # warmup\n",
        "        start = time.perf_counter()\n",
        "        for _ in range(runs): engine(np_input)\n",
        "        ort_time = (time.perf_counter() - start) / runs * 1000\n",
        "    else:\n",
        "        ort_time = float('inf')\n",
        "    \n",
        "    return {\"pytorch_ms\": pt_time, \"onnx_ms\": ort_time, \"speedup\": pt_time / ort_time}\n",
        "\n",
        "\n",
        "if ONNX_AVAILABLE:\n",
        "    results = benchmark(model, \"/tmp/model.onnx\", (1, 3, 32, 32))\n",
        "    print(f\"PyTorch: {results['pytorch_ms']:.2f}ms, ONNX: {results['onnx_ms']:.2f}ms, Speedup: {results['speedup']:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. 总结\n",
        "\n",
        "| 步骤 | 函数 | 说明 |\n",
        "|:-----|:-----|:-----|\n",
        "| **导出** | `torch.onnx.export()` | PyTorch → ONNX |\n",
        "| **验证** | `onnx.checker.check_model()` | 检查有效性 |\n",
        "| **推理** | `ort.InferenceSession()` | ONNX Runtime |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
