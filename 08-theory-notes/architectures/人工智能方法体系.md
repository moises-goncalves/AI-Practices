# 人工智能方法体系 - 完整分类与对比

> **知识密度**：⭐⭐⭐⭐⭐ | **实战价值**：⭐⭐⭐⭐⭐

---

## 目录

1. [机器学习方法](#1-机器学习方法)
2. [深度学习架构](#2-深度学习架构)
3. [自然语言处理](#3-自然语言处理)
4. [计算机视觉](#4-计算机视觉)
5. [语音处理](#5-语音处理)
6. [强化学习](#6-强化学习)
7. [高级技术](#7-高级技术)
8. [方法选择指南](#8-方法选择指南)

---

## 1. 机器学习方法

### 1.1 监督学习 (Supervised Learning)

**定义**：从标注数据中学习输入到输出的映射关系

**核心特点**：
- 需要标注数据 $(x_i, y_i)$
- 目标：学习函数 $f: X \to Y$
- 评估：在测试集上验证泛化能力

**主要算法**：

| 算法 | 适用场景 | 优势 | 劣势 |
|------|---------|------|------|
| **线性回归** | 连续值预测 | 简单、可解释 | 只能建模线性关系 |
| **逻辑回归** | 二分类 | 概率输出、快速 | 线性决策边界 |
| **决策树** | 分类/回归 | 可解释、处理非线性 | 易过拟合 |
| **随机森林** | 分类/回归 | 鲁棒、准确 | 黑盒、计算量大 |
| **SVM** | 分类 | 高维有效、核技巧 | 大数据慢 |
| **XGBoost/LightGBM** | 结构化数据 | SOTA性能 | 超参数多 |
| **神经网络** | 复杂模式 | 强大表达能力 | 需要大量数据 |

**典型应用**：
- 房价预测（回归）
- 垃圾邮件检测（分类）
- 信用评分（分类）
- 疾病诊断（分类）

### 1.2 无监督学习 (Unsupervised Learning)

**定义**：从无标注数据中发现隐藏模式和结构

**核心特点**：
- 只有输入数据 $\{x_i\}$
- 目标：发现数据内在结构
- 评估：通常使用内部指标

**主要方法**：

#### 聚类 (Clustering)

| 算法 | 原理 | 适用场景 | 参数 |
|------|------|---------|------|
| **K-Means** | 最小化簇内距离 | 球形簇、大数据 | K（簇数） |
| **DBSCAN** | 密度连接 | 任意形状、噪声数据 | eps, min_samples |
| **层次聚类** | 自底向上/自顶向下 | 小数据、需要层次结构 | 距离度量 |
| **GMM** | 高斯混合模型 | 软聚类、概率输出 | K, 协方差类型 |

#### 降维 (Dimensionality Reduction)

| 算法 | 类型 | 特点 | 用途 |
|------|------|------|------|
| **PCA** | 线性 | 保留最大方差 | 特征压缩、去噪 |
| **t-SNE** | 非线性 | 保留局部结构 | 可视化 |
| **UMAP** | 非线性 | 快速、保留全局结构 | 可视化、预处理 |
| **Autoencoder** | 非线性 | 学习非线性映射 | 特征学习 |

**典型应用**：
- 客户细分（聚类）
- 异常检测（聚类）
- 数据可视化（降维）
- 特征提取（降维）

### 1.3 半监督学习 (Semi-Supervised Learning)

**定义**：利用少量标注数据和大量无标注数据学习

**核心思想**：
- 标注数据：$(x_1, y_1), \ldots, (x_l, y_l)$
- 无标注数据：$x_{l+1}, \ldots, x_{l+u}$（通常 $u \gg l$）

**主要方法**：
1. **自训练 (Self-Training)**：用模型预测无标注数据，选择高置信度样本加入训练集
2. **协同训练 (Co-Training)**：训练多个模型，互相标注对方不确定的样本
3. **伪标签 (Pseudo-Labeling)**：用模型预测作为伪标签
4. **一致性正则化**：对同一样本的不同增强版本预测应一致

**典型应用**：
- 医学图像分析（标注昂贵）
- 文本分类（少量标注）
- 语音识别（大量无标注音频）

### 1.4 强化学习 (Reinforcement Learning)

**定义**：通过与环境交互学习最优策略

**核心要素**：
- 状态 (State)：环境的描述
- 动作 (Action)：智能体的行为
- 奖励 (Reward)：环境的反馈
- 策略 (Policy)：状态到动作的映射

**主要方法**：

| 类别 | 代表算法 | 特点 | 适用场景 |
|------|---------|------|---------|
| **值方法** | Q-Learning, DQN | 学习价值函数 | 离散动作 |
| **策略方法** | REINFORCE, PPO | 直接学习策略 | 连续动作 |
| **Actor-Critic** | A2C, SAC | 结合值和策略 | 通用 |
| **模型方法** | Dyna-Q, MuZero | 学习环境模型 | 样本效率高 |

**典型应用**：
- 游戏AI（AlphaGo）
- 机器人控制
- 自动驾驶
- 推荐系统

---

## 2. 深度学习架构

### 2.1 前馈神经网络 (Feedforward Neural Networks)

**全连接网络 (Dense/MLP)**

```
输入层 → 隐藏层1 → 隐藏层2 → ... → 输出层
```

**特点**：
- 每层神经元与下一层全连接
- 适用于表格数据、向量数据
- 参数量：$O(n \times m)$

**应用**：分类、回归、特征学习

### 2.2 卷积神经网络 (CNNs)

**核心组件**：
- **卷积层**：局部连接、权重共享
- **池化层**：降采样、平移不变性
- **全连接层**：最终分类

**经典架构**：

| 架构 | 年份 | 创新点 | 参数量 |
|------|------|--------|--------|
| **LeNet** | 1998 | 首个CNN | 60K |
| **AlexNet** | 2012 | ReLU、Dropout、GPU | 60M |
| **VGG** | 2014 | 小卷积核堆叠 | 138M |
| **GoogLeNet** | 2014 | Inception模块 | 7M |
| **ResNet** | 2015 | 残差连接 | 25M |
| **EfficientNet** | 2019 | 复合缩放 | 5M |

**应用**：图像分类、目标检测、图像分割

### 2.3 循环神经网络 (RNNs)

**核心思想**：处理序列数据，维护隐藏状态

$$h_t = f(h_{t-1}, x_t)$$

**变体**：

| 类型 | 特点 | 优势 | 劣势 |
|------|------|------|------|
| **Vanilla RNN** | 简单循环 | 简单 | 梯度消失 |
| **LSTM** | 门控机制 | 长期依赖 | 参数多、慢 |
| **GRU** | 简化LSTM | 更快、效果相近 | 仍然串行 |
| **Bidirectional** | 双向处理 | 利用未来信息 | 不适合实时 |

**应用**：语言模型、机器翻译、时序预测

### 2.4 Transformer

**核心机制**：自注意力 (Self-Attention)

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**优势**：
- 并行计算（vs RNN串行）
- 长距离依赖
- 可解释性

**经典模型**：

| 模型 | 类型 | 参数量 | 应用 |
|------|------|--------|------|
| **BERT** | Encoder-only | 110M-340M | 文本理解 |
| **GPT** | Decoder-only | 117M-175B | 文本生成 |
| **T5** | Encoder-Decoder | 60M-11B | 通用NLP |
| **ViT** | Vision | 86M-632M | 图像分类 |

### 2.5 生成对抗网络 (GANs)

**架构**：
```
生成器 G: 噪声 z → 假样本 G(z)
判别器 D: 样本 x → 真假概率 D(x)
```

**训练目标**：
$$\min_G \max_D \mathbb{E}_{x}[\log D(x)] + \mathbb{E}_{z}[\log(1 - D(G(z)))]$$

**变体**：
- **DCGAN**：深度卷积GAN
- **StyleGAN**：风格控制
- **CycleGAN**：无配对图像转换
- **Pix2Pix**：有配对图像转换

**应用**：图像生成、风格迁移、数据增强

### 2.6 变分自编码器 (VAEs)

**架构**：
```
编码器: x → μ, σ → z ~ N(μ, σ²)
解码器: z → x̂
```

**损失函数**：
$$\mathcal{L} = \underbrace{\|x - x̂\|^2}_{\text{重构损失}} + \underbrace{\text{KL}(q(z|x) \| p(z))}_{\text{正则化}}$$

**应用**：图像生成、异常检测、表示学习

---

## 3. 自然语言处理

### 3.1 词嵌入 (Word Embeddings)

| 方法 | 原理 | 维度 | 特点 |
|------|------|------|------|
| **Word2Vec** | Skip-gram/CBOW | 100-300 | 静态、快速 |
| **GloVe** | 全局共现矩阵 | 50-300 | 全局统计 |
| **FastText** | 子词嵌入 | 100-300 | 处理OOV |
| **ELMo** | 双向LSTM | 1024 | 上下文相关 |
| **BERT** | Transformer | 768-1024 | 预训练、双向 |

### 3.2 语言模型 (Language Models)

**发展历程**：
```
N-gram → RNN → LSTM → Transformer → GPT → ChatGPT
```

**关键技术**：
- **预训练**：在大规模语料上学习通用表示
- **微调**：在下游任务上调整
- **提示学习**：通过提示引导模型

### 3.3 主要任务

| 任务 | 输入 | 输出 | 代表模型 |
|------|------|------|---------|
| **文本分类** | 文本 | 类别 | BERT, RoBERTa |
| **命名实体识别** | 文本 | 实体标签 | BERT-CRF |
| **机器翻译** | 源语言 | 目标语言 | Transformer, mBART |
| **问答系统** | 问题+文档 | 答案 | BERT, T5 |
| **文本生成** | 提示 | 文本 | GPT, T5 |
| **情感分析** | 文本 | 情感极性 | BERT, DistilBERT |

---

## 4. 计算机视觉

### 4.1 核心任务

| 任务 | 定义 | 输出 | 难度 |
|------|------|------|------|
| **图像分类** | 整图分类 | 类别标签 | ⭐⭐ |
| **目标检测** | 定位+分类 | 边界框+类别 | ⭐⭐⭐⭐ |
| **语义分割** | 像素级分类 | 分割掩码 | ⭐⭐⭐⭐ |
| **实例分割** | 区分个体 | 实例掩码 | ⭐⭐⭐⭐⭐ |
| **人脸识别** | 身份验证 | 身份ID | ⭐⭐⭐ |
| **姿态估计** | 关键点检测 | 关键点坐标 | ⭐⭐⭐⭐ |

### 4.2 目标检测方法

**两阶段方法**：
- **R-CNN系列**：R-CNN → Fast R-CNN → Faster R-CNN → Mask R-CNN
- 特点：准确但慢

**单阶段方法**：
- **YOLO系列**：YOLOv1-v8
- **SSD**、**RetinaNet**
- 特点：快速但相对不准确

### 4.3 图像分割

| 方法 | 类型 | 特点 | 应用 |
|------|------|------|------|
| **FCN** | 语义分割 | 全卷积 | 场景理解 |
| **U-Net** | 语义分割 | 编码器-解码器 | 医学图像 |
| **DeepLab** | 语义分割 | 空洞卷积 | 高分辨率 |
| **Mask R-CNN** | 实例分割 | 检测+分割 | 通用 |

---

## 5. 语音处理

### 5.1 语音识别 (ASR)

**传统方法**：
```
音频 → 特征提取(MFCC) → 声学模型(HMM) → 语言模型 → 文本
```

**深度学习方法**：
```
音频 → CNN/RNN → CTC/Attention → 文本
```

**代表模型**：
- **DeepSpeech**：端到端CTC
- **Listen, Attend and Spell**：注意力机制
- **Wav2Vec 2.0**：自监督预训练
- **Whisper**：大规模多语言

### 5.2 语音合成 (TTS)

**方法演进**：
```
拼接合成 → 参数合成 → 神经网络合成
```

**代表模型**：
- **Tacotron 2**：Seq2Seq + WaveNet
- **FastSpeech**：非自回归、快速
- **VITS**：端到端、高质量

---

## 6. 强化学习

### 6.1 值方法 (Value-Based)

| 算法 | 类型 | 特点 | 适用 |
|------|------|------|------|
| **Q-Learning** | 表格 | 离策略 | 小状态空间 |
| **DQN** | 深度 | 经验回放 | 离散动作 |
| **Double DQN** | 深度 | 减少过估计 | 离散动作 |
| **Rainbow** | 深度 | 多种改进 | 离散动作 |

### 6.2 策略方法 (Policy-Based)

| 算法 | 类型 | 特点 | 适用 |
|------|------|------|------|
| **REINFORCE** | 蒙特卡洛 | 高方差 | 简单任务 |
| **A2C/A3C** | Actor-Critic | 低方差 | 通用 |
| **PPO** | 近端策略 | 稳定 | 推荐 |
| **TRPO** | 信任区域 | 理论保证 | 复杂任务 |

### 6.3 连续控制

| 算法 | 特点 | 适用场景 |
|------|------|---------|
| **DDPG** | 确定性策略 | 连续动作 |
| **TD3** | 双Q网络 | 更稳定 |
| **SAC** | 最大熵 | SOTA |

---

## 7. 高级技术

### 7.1 迁移学习 (Transfer Learning)

**核心思想**：将在源任务学到的知识迁移到目标任务

**方法**：
1. **特征提取**：冻结预训练模型，只训练新层
2. **微调**：解冻部分层，小学习率训练
3. **领域适应**：减少源域和目标域的分布差异

**典型流程**：
```
ImageNet预训练 → 冻结卷积层 → 训练新分类器 → 微调顶层
```

### 7.2 元学习 (Meta-Learning)

**目标**：学会学习 (Learning to Learn)

**代表方法**：
- **MAML**：模型无关元学习
- **Prototypical Networks**：原型网络
- **Matching Networks**：匹配网络

**应用**：少样本学习 (Few-Shot Learning)

### 7.3 自监督学习 (Self-Supervised Learning)

**核心思想**：从数据本身构造监督信号

**常见任务**：
- **对比学习**：SimCLR, MoCo
- **掩码预测**：BERT, MAE
- **旋转预测**：预测图像旋转角度
- **拼图**：预测图像块顺序

### 7.4 联邦学习 (Federated Learning)

**特点**：
- 数据不离开本地
- 只传输模型参数
- 保护隐私

**应用**：医疗、金融等隐私敏感领域

### 7.5 图神经网络 (GNNs)

**核心思想**：在图结构数据上进行深度学习

**代表方法**：
- **GCN**：图卷积网络
- **GAT**：图注意力网络
- **GraphSAGE**：归纳式学习

**应用**：社交网络、分子性质预测、推荐系统

---

## 8. 方法选择指南

### 8.1 按数据类型选择

| 数据类型 | 推荐方法 | 备选方案 |
|---------|---------|---------|
| **表格数据** | XGBoost, LightGBM | Random Forest, MLP |
| **图像** | CNN (ResNet, EfficientNet) | ViT |
| **文本** | Transformer (BERT, GPT) | LSTM, CNN |
| **音频** | CNN (频谱图) | RNN, Transformer |
| **时序** | LSTM, Transformer | CNN, ARIMA |
| **图** | GNN (GCN, GAT) | 传统图算法 |

### 8.2 按任务类型选择

| 任务 | 数据量 | 推荐方法 |
|------|--------|---------|
| **分类** | 小 | 传统ML (RF, SVM) |
| **分类** | 大 | 深度学习 (CNN, Transformer) |
| **回归** | 小 | 线性回归, GBDT |
| **回归** | 大 | 神经网络 |
| **聚类** | 任意 | K-Means, DBSCAN |
| **生成** | 大 | GAN, VAE, Diffusion |

### 8.3 按资源约束选择

| 约束 | 推荐方案 |
|------|---------|
| **计算受限** | 轻量模型 (MobileNet, DistilBERT) |
| **数据受限** | 迁移学习、数据增强 |
| **时间受限** | 预训练模型微调 |
| **内存受限** | 模型压缩、量化 |

---

## 总结

人工智能方法的选择需要综合考虑：
1. **数据特性**：类型、规模、质量
2. **任务需求**：准确率、速度、可解释性
3. **资源约束**：计算、存储、时间
4. **工程实践**：可维护性、可扩展性

**通用建议**：
- 从简单方法开始
- 建立基线模型
- 逐步增加复杂度
- 持续评估和优化

---

*最后更新：2026年1月*
