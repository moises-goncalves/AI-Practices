# æœºå™¨å­¦ä¹ çš„é€šç”¨æµç¨‹ - ä»é—®é¢˜å®šä¹‰åˆ°æ¨¡å‹éƒ¨ç½²

> **çŸ¥è¯†å¯†åº¦**ï¼šâ­â­â­â­â­ | **å®æˆ˜ä»·å€¼**ï¼šâ­â­â­â­â­
> **æœ€åæ›´æ–°**ï¼š2025-11-30

---

## ğŸ“‹ æœ¬ç« çŸ¥è¯†å›¾è°±

```
æœºå™¨å­¦ä¹ é€šç”¨æµç¨‹
â”œâ”€â”€ 1. é—®é¢˜å®šä¹‰é˜¶æ®µ
â”‚   â”œâ”€â”€ ä¸šåŠ¡ç†è§£
â”‚   â”œâ”€â”€ æ•°æ®å¯ç”¨æ€§åˆ†æ
â”‚   â””â”€â”€ é—®é¢˜ç±»å‹è¯†åˆ«
â”œâ”€â”€ 2. æ•°æ®å‡†å¤‡é˜¶æ®µ
â”‚   â”œâ”€â”€ æ•°æ®æ”¶é›†
â”‚   â”œâ”€â”€ æ•°æ®æ¸…æ´—
â”‚   â”œâ”€â”€ ç‰¹å¾å·¥ç¨‹
â”‚   â””â”€â”€ æ•°æ®åˆ’åˆ†
â”œâ”€â”€ 3. æ¨¡å‹å¼€å‘é˜¶æ®µ
â”‚   â”œâ”€â”€ åŸºçº¿æ¨¡å‹
â”‚   â”œâ”€â”€ æ¨¡å‹é€‰æ‹©
â”‚   â”œâ”€â”€ è®­ç»ƒä¸éªŒè¯
â”‚   â””â”€â”€ è¶…å‚æ•°è°ƒä¼˜
â”œâ”€â”€ 4. æ¨¡å‹è¯„ä¼°é˜¶æ®µ
â”‚   â”œâ”€â”€ è¯„ä¼°æŒ‡æ ‡é€‰æ‹©
â”‚   â”œâ”€â”€ äº¤å‰éªŒè¯
â”‚   â”œâ”€â”€ è¯¯å·®åˆ†æ
â”‚   â””â”€â”€ æ¨¡å‹å¯¹æ¯”
â””â”€â”€ 5. éƒ¨ç½²ä¸ç›‘æ§
    â”œâ”€â”€ æ¨¡å‹éƒ¨ç½²
    â”œâ”€â”€ A/Bæµ‹è¯•
    â”œâ”€â”€ æ€§èƒ½ç›‘æ§
    â””â”€â”€ æ¨¡å‹è¿­ä»£
```

---

## ç¬¬ä¸€é˜¶æ®µï¼šé—®é¢˜å®šä¹‰ ğŸ¯

### 1.1 ä¸šåŠ¡ç†è§£

**æ ¸å¿ƒé—®é¢˜**ï¼š
- è¦è§£å†³ä»€ä¹ˆä¸šåŠ¡é—®é¢˜ï¼Ÿ
- æˆåŠŸçš„æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ
- æœ‰å“ªäº›çº¦æŸæ¡ä»¶ï¼Ÿï¼ˆæ—¶é—´ã€æˆæœ¬ã€è®¡ç®—èµ„æºï¼‰

**é—®é¢˜ç±»å‹è¯†åˆ«**ï¼š

| é—®é¢˜ç±»å‹ | ç‰¹å¾ | å…¸å‹åº”ç”¨ | å¸¸ç”¨ç®—æ³• |
|---------|------|---------|---------|
| **ç›‘ç£å­¦ä¹ -åˆ†ç±»** | ç¦»æ•£æ ‡ç­¾ | åƒåœ¾é‚®ä»¶æ£€æµ‹ã€ç–¾ç—…è¯Šæ–­ | LR, SVM, RF, XGBoost |
| **ç›‘ç£å­¦ä¹ -å›å½’** | è¿ç»­å€¼é¢„æµ‹ | æˆ¿ä»·é¢„æµ‹ã€é”€é‡é¢„æµ‹ | Linear Reg, SVR, GBDT |
| **æ— ç›‘ç£å­¦ä¹ -èšç±»** | æ— æ ‡ç­¾åˆ†ç»„ | å®¢æˆ·ç»†åˆ†ã€å¼‚å¸¸æ£€æµ‹ | K-Means, DBSCAN |
| **æ— ç›‘ç£å­¦ä¹ -é™ç»´** | ç‰¹å¾å‹ç¼© | æ•°æ®å¯è§†åŒ–ã€å»å™ª | PCA, t-SNE, UMAP |
| **å¼ºåŒ–å­¦ä¹ ** | åºåˆ—å†³ç­– | æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ | Q-Learning, PPO |

### 1.2 æ•°æ®å¯ç”¨æ€§åˆ†æ

**å…³é”®é—®é¢˜**ï¼š
```python
# æ•°æ®å¯ç”¨æ€§æ£€æŸ¥æ¸…å•
checklist = {
    'æ•°æ®é‡': 'æ˜¯å¦è¶³å¤Ÿï¼Ÿï¼ˆé€šå¸¸éœ€è¦ > 1000æ ·æœ¬ï¼‰',
    'æ•°æ®è´¨é‡': 'ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€å™ªå£°æ¯”ä¾‹ï¼Ÿ',
    'æ ‡ç­¾': 'æ˜¯å¦éœ€è¦äººå·¥æ ‡æ³¨ï¼Ÿæˆæœ¬å¦‚ä½•ï¼Ÿ',
    'ç‰¹å¾': 'ç°æœ‰ç‰¹å¾æ˜¯å¦å……åˆ†ï¼Ÿéœ€è¦è¡ç”Ÿç‰¹å¾å—ï¼Ÿ',
    'æ•°æ®å¹³è¡¡': 'ç±»åˆ«æ˜¯å¦å¹³è¡¡ï¼Ÿï¼ˆåˆ†ç±»é—®é¢˜ï¼‰',
    'æ—¶é—´æ€§': 'æ•°æ®æ˜¯å¦æœ‰æ—¶é—´ä¾èµ–ï¼Ÿ',
}
```

**æ•°æ®é‡ä¼°ç®—**ï¼š
- **ç®€å•é—®é¢˜**ï¼š1,000 - 10,000 æ ·æœ¬
- **ä¸­ç­‰å¤æ‚åº¦**ï¼š10,000 - 100,000 æ ·æœ¬
- **å¤æ‚é—®é¢˜**ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰ï¼š100,000+ æ ·æœ¬

---

## ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®å‡†å¤‡ ğŸ“Š

### 2.1 æ•°æ®æ”¶é›†ä¸æ¸…æ´—

**æ•°æ®æ¸…æ´—æµç¨‹**ï¼š

```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('data.csv')

# 2. åˆæ­¥æ¢ç´¢
print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
print(f"ç¼ºå¤±å€¼:\n{df.isnull().sum()}")
print(f"æ•°æ®ç±»å‹:\n{df.dtypes}")

# 3. å¤„ç†ç¼ºå¤±å€¼
# ç­–ç•¥Aï¼šåˆ é™¤ï¼ˆç¼ºå¤±ç‡>30%çš„åˆ—ï¼‰
df = df.dropna(thresh=len(df)*0.7, axis=1)

# ç­–ç•¥Bï¼šå¡«å……ï¼ˆæ•°å€¼å‹ç”¨ä¸­ä½æ•°ï¼Œç±»åˆ«å‹ç”¨ä¼—æ•°ï¼‰
num_cols = df.select_dtypes(include=[np.number]).columns
cat_cols = df.select_dtypes(include=['object']).columns

imputer_num = SimpleImputer(strategy='median')
df[num_cols] = imputer_num.fit_transform(df[num_cols])

imputer_cat = SimpleImputer(strategy='most_frequent')
df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])

# 4. å¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# 5. æ•°æ®ç±»å‹è½¬æ¢
df['date'] = pd.to_datetime(df['date'])
df['category'] = df['category'].astype('category')
```

### 2.2 ç‰¹å¾å·¥ç¨‹

**ç‰¹å¾å·¥ç¨‹æŠ€å·§**ï¼š

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. æ•°å€¼ç‰¹å¾å¤„ç†
# æ ‡å‡†åŒ–ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰
scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

# 2. ç±»åˆ«ç‰¹å¾ç¼–ç 
# æ–¹æ³•Aï¼šLabel Encodingï¼ˆæœ‰åºç±»åˆ«ï¼‰
le = LabelEncoder()
df['education'] = le.fit_transform(df['education'])

# æ–¹æ³•Bï¼šOne-Hot Encodingï¼ˆæ— åºç±»åˆ«ï¼‰
df = pd.get_dummies(df, columns=['city', 'gender'])

# 3. æ—¶é—´ç‰¹å¾æå–
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

# 4. äº¤äº’ç‰¹å¾
df['age_income'] = df['age'] * df['income']
df['price_per_sqft'] = df['price'] / df['sqft']

# 5. æ–‡æœ¬ç‰¹å¾ï¼ˆTF-IDFï¼‰
tfidf = TfidfVectorizer(max_features=100)
text_features = tfidf.fit_transform(df['description'])
```

### 2.3 æ•°æ®åˆ’åˆ†ç­–ç•¥

**æ ‡å‡†åˆ’åˆ†**ï¼š

```python
from sklearn.model_selection import train_test_split

# æ–¹æ³•1ï¼šç®€å•éšæœºåˆ’åˆ†ï¼ˆ70/15/15ï¼‰
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y  # stratifyä¿æŒç±»åˆ«æ¯”ä¾‹
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.176, random_state=42  # 0.176 * 0.85 â‰ˆ 0.15
)

print(f"è®­ç»ƒé›†: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"éªŒè¯é›†: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)")
print(f"æµ‹è¯•é›†: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
```

**æ—¶é—´åºåˆ—åˆ’åˆ†**ï¼š

```python
# æ—¶é—´åºåˆ—æ•°æ®ï¼šå¿…é¡»æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
split_date_1 = '2023-01-01'
split_date_2 = '2023-07-01'

train_data = df[df['date'] < split_date_1]
val_data = df[(df['date'] >= split_date_1) & (df['date'] < split_date_2)]
test_data = df[df['date'] >= split_date_2]
```

---

## ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹å¼€å‘ ğŸ¤–

### 3.1 å»ºç«‹åŸºçº¿æ¨¡å‹

**ä¸ºä»€ä¹ˆéœ€è¦åŸºçº¿ï¼Ÿ**
- æä¾›æ€§èƒ½ä¸‹é™
- å¿«é€ŸéªŒè¯æ•°æ®è´¨é‡
- ä½œä¸ºå¯¹æ¯”æ ‡å‡†

**å¸¸ç”¨åŸºçº¿æ¨¡å‹**ï¼š

```python
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, mean_squared_error

# åˆ†ç±»åŸºçº¿ï¼šæœ€é¢‘ç¹ç±»åˆ«
baseline_clf = DummyClassifier(strategy='most_frequent')
baseline_clf.fit(X_train, y_train)
baseline_acc = accuracy_score(y_val, baseline_clf.predict(X_val))
print(f"åŸºçº¿å‡†ç¡®ç‡: {baseline_acc:.3f}")

# å›å½’åŸºçº¿ï¼šå‡å€¼é¢„æµ‹
baseline_reg = DummyRegressor(strategy='mean')
baseline_reg.fit(X_train, y_train)
baseline_mse = mean_squared_error(y_val, baseline_reg.predict(X_val))
print(f"åŸºçº¿MSE: {baseline_mse:.3f}")
```

### 3.2 æ¨¡å‹é€‰æ‹©ç­–ç•¥

**ç®—æ³•é€‰æ‹©å†³ç­–æ ‘**ï¼š

```
æ•°æ®é‡ < 1000ï¼Ÿ
â”œâ”€ æ˜¯ â†’ ç®€å•æ¨¡å‹ï¼ˆLR, Decision Treeï¼‰
â””â”€ å¦ â†’ ç»§ç»­

ç‰¹å¾æ•°é‡ > æ ·æœ¬æ•°é‡ï¼Ÿ
â”œâ”€ æ˜¯ â†’ æ­£åˆ™åŒ–æ¨¡å‹ï¼ˆRidge, Lasso, Elastic Netï¼‰
â””â”€ å¦ â†’ ç»§ç»­

éœ€è¦å¯è§£é‡Šæ€§ï¼Ÿ
â”œâ”€ æ˜¯ â†’ çº¿æ€§æ¨¡å‹ã€å†³ç­–æ ‘ã€è§„åˆ™æ¨¡å‹
â””â”€ å¦ â†’ é›†æˆæ¨¡å‹ã€ç¥ç»ç½‘ç»œ

æ•°æ®æ˜¯å¦çº¿æ€§å¯åˆ†ï¼Ÿ
â”œâ”€ æ˜¯ â†’ çº¿æ€§æ¨¡å‹ï¼ˆLR, Linear SVMï¼‰
â”œâ”€ å¦ â†’ éçº¿æ€§æ¨¡å‹ï¼ˆRBF SVM, RF, XGBoostï¼‰
â””â”€ ä¸ç¡®å®š â†’ éƒ½è¯•è¯•
```

**æ¨¡å‹å¯¹æ¯”å®éªŒ**ï¼š

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# å®šä¹‰å€™é€‰æ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, random_state=42)
}

# è®­ç»ƒå¹¶è¯„ä¼°
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    acc = accuracy_score(y_val, y_pred)
    results[name] = acc
    print(f"{name}: {acc:.4f}")

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_model_name = max(results, key=results.get)
print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} ({results[best_model_name]:.4f})")
```

### 3.3 è¶…å‚æ•°è°ƒä¼˜

**ç½‘æ ¼æœç´¢**ï¼š

```python
from sklearn.model_selection import GridSearchCV

# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
    scoring='accuracy',
    n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    verbose=2
)

grid_search.fit(X_train, y_train)

print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
```

**éšæœºæœç´¢ï¼ˆæ›´é«˜æ•ˆï¼‰**ï¼š

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# å®šä¹‰å‚æ•°åˆ†å¸ƒ
param_distributions = {
    'n_estimators': randint(50, 300),
    'max_depth': randint(3, 15),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.1, 0.9)
}

# éšæœºæœç´¢
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions,
    n_iter=50,  # å°è¯•50ç§ç»„åˆ
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)
```

---

## ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹è¯„ä¼° ğŸ“ˆ

### 4.1 è¯„ä¼°æŒ‡æ ‡é€‰æ‹©

**åˆ†ç±»é—®é¢˜æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | å…¬å¼ | é€‚ç”¨åœºæ™¯ | ä¼˜ç¼ºç‚¹ |
|------|------|---------|--------|
| **Accuracy** | $\frac{TP+TN}{TP+TN+FP+FN}$ | ç±»åˆ«å¹³è¡¡ | âœ…ç®€å•ç›´è§‚<br>âŒä¸é€‚åˆä¸å¹³è¡¡æ•°æ® |
| **Precision** | $\frac{TP}{TP+FP}$ | å…³æ³¨è¯¯æŠ¥ | âœ…å‡å°‘å‡é˜³æ€§<br>âŒå¯èƒ½æ¼æ£€ |
| **Recall** | $\frac{TP}{TP+FN}$ | å…³æ³¨æ¼æŠ¥ | âœ…å‡å°‘å‡é˜´æ€§<br>âŒå¯èƒ½è¯¯æŠ¥å¤š |
| **F1-Score** | $2 \cdot \frac{P \cdot R}{P + R}$ | å¹³è¡¡På’ŒR | âœ…ç»¼åˆæŒ‡æ ‡<br>âŒä¸ç›´è§‚ |
| **AUC-ROC** | ROCæ›²çº¿ä¸‹é¢ç§¯ | æ¦‚ç‡é¢„æµ‹ | âœ…é˜ˆå€¼æ— å…³<br>âŒè®¡ç®—å¤æ‚ |

**å›å½’é—®é¢˜æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | å…¬å¼ | ç‰¹ç‚¹ |
|------|------|------|
| **MAE** | $\frac{1}{n}\sum\|y_i - \hat{y}_i\|$ | å¯¹ç¦»ç¾¤ç‚¹ä¸æ•æ„Ÿ |
| **MSE** | $\frac{1}{n}\sum(y_i - \hat{y}_i)^2$ | æƒ©ç½šå¤§è¯¯å·® |
| **RMSE** | $\sqrt{MSE}$ | ä¸ç›®æ ‡åŒå•ä½ |
| **RÂ²** | $1 - \frac{SS_{res}}{SS_{tot}}$ | è§£é‡Šæ–¹å·®æ¯”ä¾‹ |

### 4.2 äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score, cross_validate

# KæŠ˜äº¤å‰éªŒè¯
cv_scores = cross_val_score(
    model, X_train, y_train,
    cv=5,  # 5æŠ˜
    scoring='accuracy'
)

print(f"CVå‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# å¤šæŒ‡æ ‡äº¤å‰éªŒè¯
scoring = ['accuracy', 'precision', 'recall', 'f1']
cv_results = cross_validate(
    model, X_train, y_train,
    cv=5,
    scoring=scoring,
    return_train_score=True
)

for metric in scoring:
    train_score = cv_results[f'train_{metric}'].mean()
    test_score = cv_results[f'test_{metric}'].mean()
    print(f"{metric}: Train={train_score:.4f}, Test={test_score:.4f}")
```

### 4.3 è¯¯å·®åˆ†æ

```python
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# æ··æ·†çŸ©é˜µ
y_pred = model.predict(X_val)
cm = confusion_matrix(y_val, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print(classification_report(y_val, y_pred))

# åˆ†æé”™è¯¯æ ·æœ¬
errors = X_val[y_val != y_pred]
print(f"é”™è¯¯æ ·æœ¬æ•°: {len(errors)}")
print(f"é”™è¯¯ç‡: {len(errors)/len(X_val)*100:.2f}%")
```

---

## ç¬¬äº”é˜¶æ®µï¼šéƒ¨ç½²ä¸ç›‘æ§ ğŸš€

### 5.1 æ¨¡å‹ä¿å­˜ä¸åŠ è½½

```python
import joblib
import pickle

# æ–¹æ³•1ï¼šjoblibï¼ˆæ¨èï¼Œé€‚åˆå¤§æ¨¡å‹ï¼‰
joblib.dump(model, 'model.pkl')
loaded_model = joblib.load('model.pkl')

# æ–¹æ³•2ï¼špickle
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

# ä¿å­˜é¢„å¤„ç†å™¨
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(imputer, 'imputer.pkl')
```

### 5.2 æ¨¡å‹éƒ¨ç½²

```python
# Flask APIç¤ºä¾‹
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

# åŠ è½½æ¨¡å‹
model = joblib.load('model.pkl')
scaler = joblib.load('scaler.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    # è·å–è¾“å…¥æ•°æ®
    data = request.get_json()
    features = np.array(data['features']).reshape(1, -1)

    # é¢„å¤„ç†
    features_scaled = scaler.transform(features)

    # é¢„æµ‹
    prediction = model.predict(features_scaled)
    probability = model.predict_proba(features_scaled)

    return jsonify({
        'prediction': int(prediction[0]),
        'probability': float(probability[0][1])
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000)
```

### 5.3 æ€§èƒ½ç›‘æ§

```python
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    filename='model_monitoring.log',
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)

def monitor_prediction(features, prediction, actual=None):
    """ç›‘æ§é¢„æµ‹æ€§èƒ½"""
    log_data = {
        'timestamp': datetime.now().isoformat(),
        'prediction': prediction,
        'features': features.tolist()
    }

    if actual is not None:
        log_data['actual'] = actual
        log_data['correct'] = (prediction == actual)

    logging.info(log_data)

# å®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½
def evaluate_model_drift(recent_predictions, recent_actuals):
    """æ£€æµ‹æ¨¡å‹æ¼‚ç§»"""
    recent_accuracy = accuracy_score(recent_actuals, recent_predictions)

    if recent_accuracy < 0.8:  # é˜ˆå€¼
        logging.warning(f"æ¨¡å‹æ€§èƒ½ä¸‹é™: {recent_accuracy:.3f}")
        # è§¦å‘é‡è®­ç»ƒæµç¨‹
        return True
    return False
```

---

## ğŸ“Š å®Œæ•´æµç¨‹å¯¹æ¯”

| é˜¶æ®µ | ä¼ ç»Ÿæ–¹æ³• | ç°ä»£æœ€ä½³å®è·µ | æ—¶é—´å æ¯” |
|------|---------|-------------|---------|
| **é—®é¢˜å®šä¹‰** | æ¨¡ç³Šéœ€æ±‚ | æ˜ç¡®ä¸šåŠ¡æŒ‡æ ‡ | 10% |
| **æ•°æ®å‡†å¤‡** | æ‰‹åŠ¨æ¸…æ´— | è‡ªåŠ¨åŒ–Pipeline | 40% |
| **ç‰¹å¾å·¥ç¨‹** | ç»éªŒé©±åŠ¨ | è‡ªåŠ¨ç‰¹å¾é€‰æ‹© | 20% |
| **æ¨¡å‹è®­ç»ƒ** | å•ä¸€æ¨¡å‹ | æ¨¡å‹é›†æˆ | 15% |
| **è¶…å‚æ•°è°ƒä¼˜** | æ‰‹åŠ¨è°ƒæ•´ | è‡ªåŠ¨åŒ–æœç´¢ | 10% |
| **éƒ¨ç½²ç›‘æ§** | ä¸€æ¬¡æ€§éƒ¨ç½² | æŒç»­ç›‘æ§è¿­ä»£ | 5% |

---

## âœ… æœ€ä½³å®è·µæ€»ç»“

### æ•°æ®å‡†å¤‡é˜¶æ®µ

| å®è·µ | è¯´æ˜ | é‡è¦æ€§ |
|------|------|--------|
| **æ•°æ®ç‰ˆæœ¬æ§åˆ¶** | ä½¿ç”¨DVCæˆ–Git LFS | â­â­â­â­â­ |
| **æ•°æ®è´¨é‡æ£€æŸ¥** | è‡ªåŠ¨åŒ–æ•°æ®éªŒè¯ | â­â­â­â­â­ |
| **ç‰¹å¾æ–‡æ¡£åŒ–** | è®°å½•æ¯ä¸ªç‰¹å¾å«ä¹‰ | â­â­â­â­ |
| **æ•°æ®æ³„æ¼æ£€æµ‹** | ä¸¥æ ¼æ—¶é—´åˆ’åˆ† | â­â­â­â­â­ |

### æ¨¡å‹å¼€å‘é˜¶æ®µ

| å®è·µ | è¯´æ˜ | é‡è¦æ€§ |
|------|------|--------|
| **ä»ç®€å•å¼€å§‹** | å…ˆç”¨çº¿æ€§æ¨¡å‹ | â­â­â­â­â­ |
| **å»ºç«‹åŸºçº¿** | å¯¹æ¯”åŸºå‡†æ€§èƒ½ | â­â­â­â­â­ |
| **äº¤å‰éªŒè¯** | é¿å…è¿‡æ‹Ÿåˆ | â­â­â­â­â­ |
| **è¶…å‚æ•°æœç´¢** | è‡ªåŠ¨åŒ–è°ƒä¼˜ | â­â­â­â­ |

### å¸¸è§é™·é˜±

| é™·é˜± | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **æ•°æ®æ³„æ¼** | æµ‹è¯•æ•°æ®ä¿¡æ¯æ³„éœ²åˆ°è®­ç»ƒé›† | ä¸¥æ ¼åˆ’åˆ†æ•°æ®ï¼Œå…ˆsplitå†preprocess |
| **è¿‡æ‹Ÿåˆ** | æ¨¡å‹è¿‡äºå¤æ‚ï¼Œè®°ä½è®­ç»ƒæ•°æ® | æ­£åˆ™åŒ–ã€äº¤å‰éªŒè¯ã€å¢åŠ æ•°æ® |
| **ç±»åˆ«ä¸å¹³è¡¡** | å°‘æ•°ç±»æ ·æœ¬å¤ªå°‘ | SMOTEã€ç±»æƒé‡ã€é‡é‡‡æ · |
| **ç‰¹å¾ç¼©æ”¾é—æ¼** | å¿˜è®°å¯¹æµ‹è¯•é›†ç¼©æ”¾ | ä½¿ç”¨Pipelineè‡ªåŠ¨åŒ– |
| **æ—¶é—´æ³„æ¼** | æ—¶åºæ•°æ®éšæœºåˆ’åˆ† | æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ† |

---

## ğŸ¯ å®æˆ˜æ£€æŸ¥æ¸…å•

### å¼€å§‹é¡¹ç›®å‰
- [ ] æ˜ç¡®ä¸šåŠ¡ç›®æ ‡å’ŒæˆåŠŸæŒ‡æ ‡
- [ ] è¯„ä¼°æ•°æ®å¯ç”¨æ€§å’Œè´¨é‡
- [ ] ç¡®å®šé—®é¢˜ç±»å‹ï¼ˆåˆ†ç±»/å›å½’/èšç±»ç­‰ï¼‰
- [ ] è®¾å®šæ€§èƒ½åŸºçº¿

### æ•°æ®å‡†å¤‡
- [ ] å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
- [ ] ç‰¹å¾å·¥ç¨‹å’Œç‰¹å¾é€‰æ‹©
- [ ] æ•°æ®æ ‡å‡†åŒ–/å½’ä¸€åŒ–
- [ ] æ­£ç¡®åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†

### æ¨¡å‹å¼€å‘
- [ ] å»ºç«‹ç®€å•åŸºçº¿æ¨¡å‹
- [ ] å°è¯•å¤šç§ç®—æ³•
- [ ] äº¤å‰éªŒè¯è¯„ä¼°
- [ ] è¶…å‚æ•°è°ƒä¼˜

### æ¨¡å‹è¯„ä¼°
- [ ] é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡
- [ ] æ··æ·†çŸ©é˜µåˆ†æ
- [ ] è¯¯å·®åˆ†æ
- [ ] åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°

### éƒ¨ç½²ä¸Šçº¿
- [ ] æ¨¡å‹åºåˆ—åŒ–ä¿å­˜
- [ ] APIæ¥å£å¼€å‘
- [ ] æ€§èƒ½ç›‘æ§
- [ ] å®šæœŸé‡è®­ç»ƒ

---

## ğŸ“š å‚è€ƒèµ„æº

- **ä¹¦ç±**ï¼šã€ŠHands-On Machine Learningã€‹by AurÃ©lien GÃ©ron
- **è¯¾ç¨‹**ï¼šAndrew Ngçš„Machine Learningè¯¾ç¨‹
- **å·¥å…·**ï¼šScikit-learn, XGBoost, MLflow
- **ç¤¾åŒº**ï¼šKaggle, Papers with Code

---

**æ€»ç»“**ï¼šæœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œä¸è¦æœŸæœ›ä¸€æ¬¡å°±å®Œç¾ã€‚ä»ç®€å•å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–ï¼ŒæŒç»­ç›‘æ§å’Œæ”¹è¿›ï¼
