# Loss Functions - Complete Guide

> **çŸ¥è¯†å¯†åº¦**ï¼šâ­â­â­â­â­ | **å®æˆ˜ä»·å€¼**ï¼šâ­â­â­â­â­
> **æœ€åæ›´æ–°**ï¼š2025-11-30

---



## ğŸ“‹ æœ¬ç« çŸ¥è¯†å›¾è°±

```
Loss Functions - Complete Guide
â”œâ”€â”€ æ ¸å¿ƒæ¦‚å¿µ
â”‚   â”œâ”€â”€ åŸºæœ¬åŸç†
â”‚   â”œâ”€â”€ æ•°å­¦åŸºç¡€
â”‚   â””â”€â”€ åº”ç”¨åœºæ™¯
â”œâ”€â”€ ç®—æ³•è¯¦è§£
â”‚   â”œâ”€â”€ ç®—æ³•æµç¨‹
â”‚   â”œâ”€â”€ æ—¶é—´å¤æ‚åº¦
â”‚   â””â”€â”€ ç©ºé—´å¤æ‚åº¦
â”œâ”€â”€ å®æˆ˜æŠ€å·§
â”‚   â”œâ”€â”€ å‚æ•°è°ƒä¼˜
â”‚   â”œâ”€â”€ æ€§èƒ½ä¼˜åŒ–
â”‚   â””â”€â”€ å¸¸è§é™·é˜±
â””â”€â”€ ä»£ç å®ç°
    â”œâ”€â”€ åŸºç¡€å®ç°
    â”œâ”€â”€ é«˜çº§æŠ€å·§
    â””â”€â”€ å®Œæ•´ç¤ºä¾‹
```

---

## ğŸ“š Overview

Loss functions (also called cost functions or objective functions) are the compass that guides neural network training. They quantify how well the model's predictions match the true labels, providing the feedback signal for optimization.

## ğŸ¯ Table of Contents

1. [What Are Loss Functions?](#what-are-loss-functions)
2. [Regression Loss Functions](#regression-loss-functions)
3. [Classification Loss Functions](#classification-loss-functions)
4. [Ranking & Similarity Loss Functions](#ranking--similarity-loss-functions)
5. [Advanced & Specialized Loss Functions](#advanced--specialized-loss-functions)
6. [Selection Guide](#selection-guide)
7. [Best Practices](#best-practices)

---

## What Are Loss Functions?

### Definition

A **loss function** L(Å·, y) measures the discrepancy between predicted values Å· and true values y. The goal of training is to minimize this loss:

```
Î¸* = argmin_Î¸ (1/N) Î£áµ¢ L(f(xáµ¢; Î¸), yáµ¢)
```

where:
- Î¸ = model parameters
- f(x; Î¸) = model prediction
- N = number of samples

### Key Properties

1. **Non-negative**: L(Å·, y) â‰¥ 0
2. **Zero at perfect prediction**: L(y, y) = 0
3. **Differentiable**: Required for gradient-based optimization
4. **Task-appropriate**: Must match the problem type

### Loss vs Cost vs Objective

- **Loss**: Error for a single example
- **Cost**: Average loss over entire dataset
- **Objective**: General term (may include regularization)

```
Cost = (1/N) Î£áµ¢ Loss(Å·áµ¢, yáµ¢) + Î» Ã— Regularization
```

---

## Regression Loss Functions

### 1. Mean Squared Error (MSE) / L2 Loss

**Formula**:
```
MSE = (1/N) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
```

**Per-sample**:
```
L(y, Å·) = (y - Å·)Â²
```

**Gradient**:
```
âˆ‚L/âˆ‚Å· = -2(y - Å·)
```

**Properties**:
- **Range**: [0, âˆ)
- **Sensitivity**: Very sensitive to outliers (quadratic penalty)
- **Units**: Squared units of target variable

**When to Use**:
- âœ… Default choice for regression
- âœ… When outliers should be heavily penalized
- âœ… Gaussian noise assumption
- âŒ When data has many outliers

**Advantages**:
- Smooth, continuous gradient
- Convex (for linear models)
- Penalizes large errors heavily
- Well-studied, stable optimization

**Disadvantages**:
- Sensitive to outliers
- Not robust to noise
- Squared units (harder to interpret)

**Example Use Cases**:
- House price prediction
- Temperature forecasting
- Stock price prediction

---

### 2. Mean Absolute Error (MAE) / L1 Loss

**Formula**:
```
MAE = (1/N) Î£áµ¢ |yáµ¢ - Å·áµ¢|
```

**Per-sample**:
```
L(y, Å·) = |y - Å·|
```

**Gradient**:
```
âˆ‚L/âˆ‚Å· = -sign(y - Å·)
```

**Properties**:
- **Range**: [0, âˆ)
- **Sensitivity**: Robust to outliers (linear penalty)
- **Units**: Same units as target variable

**When to Use**:
- âœ… When data has outliers
- âœ… When all errors should be weighted equally
- âœ… Laplacian noise assumption
- âŒ When you need smooth gradients

**Advantages**:
- Robust to outliers
- Same units as target (interpretable)
- Linear penalty (treats all errors equally)

**Disadvantages**:
- Non-smooth at zero (gradient discontinuity)
- Slower convergence than MSE
- May not converge to exact minimum

**Comparison with MSE**:
```
Error = 1:  MSE = 1,   MAE = 1
Error = 2:  MSE = 4,   MAE = 2
Error = 10: MSE = 100, MAE = 10
```
MSE penalizes large errors much more heavily!

---

### 3. Huber Loss (Smooth L1)

**Formula**:
```
L_Î´(y, Å·) = {
    0.5(y - Å·)Â²           if |y - Å·| â‰¤ Î´
    Î´|y - Å·| - 0.5Î´Â²      otherwise
}
```

**Properties**:
- Combines MSE (small errors) and MAE (large errors)
- Smooth everywhere
- Robust to outliers

**When to Use**:
- âœ… When you want robustness AND smooth gradients
- âœ… Object detection (bounding box regression)
- âœ… Data with moderate outliers

**Advantages**:
- Best of both worlds (MSE + MAE)
- Smooth gradients
- Robust to outliers
- Tunable via Î´ parameter

**Disadvantages**:
- Extra hyperparameter Î´ to tune
- More complex than MSE/MAE

**Choosing Î´**:
- Small Î´ â†’ More like MAE (robust)
- Large Î´ â†’ More like MSE (smooth)
- Typical: Î´ = 1.0

---

### 4. Log-Cosh Loss

**Formula**:
```
L(y, Å·) = Î£áµ¢ log(cosh(Å·áµ¢ - yáµ¢))
```

**Properties**:
- Smooth approximation of MAE
- Approximately MSE for small errors
- Approximately MAE for large errors

**When to Use**:
- âœ… Alternative to Huber loss
- âœ… When you want smooth gradients everywhere
- âœ… XGBoost, LightGBM regression

**Advantages**:
- Smooth everywhere (twice differentiable)
- Robust to outliers
- No hyperparameters

**Disadvantages**:
- Computationally expensive (cosh, log)
- Less interpretable

---

### 5. Quantile Loss (Pinball Loss)

**Formula**:
```
L_Ï„(y, Å·) = {
    Ï„(y - Å·)      if y â‰¥ Å·
    (Ï„-1)(y - Å·)  if y < Å·
}
```

where Ï„ âˆˆ (0, 1) is the quantile

**When to Use**:
- âœ… Quantile regression
- âœ… Prediction intervals
- âœ… Asymmetric cost of errors

**Special Cases**:
- Ï„ = 0.5: Equivalent to MAE (median regression)
- Ï„ = 0.9: 90th percentile prediction

**Example**: Inventory management
- Overestimation cost â‰  Underestimation cost
- Use Ï„ to balance costs

---

## Classification Loss Functions

### 1. Binary Cross-Entropy (BCE) / Log Loss

**Formula** (for single sample):
```
BCE = -[y log(Å·) + (1-y) log(1-Å·)]
```

where:
- y âˆˆ {0, 1} (true label)
- Å· âˆˆ (0, 1) (predicted probability)

**Batch form**:
```
BCE = -(1/N) Î£áµ¢ [yáµ¢ log(Å·áµ¢) + (1-yáµ¢) log(1-Å·áµ¢)]
```

**Gradient**:
```
âˆ‚L/âˆ‚Å· = -(y/Å· - (1-y)/(1-Å·))
```

**Properties**:
- **Range**: [0, âˆ)
- **Interpretation**: Negative log-likelihood
- **Requires**: Å· âˆˆ (0, 1) (use sigmoid activation)

**When to Use**:
- âœ… Binary classification (REQUIRED)
- âœ… Multi-label classification (each label independently)
- âŒ Multi-class classification (use categorical CE instead)

**Advantages**:
- Probabilistic interpretation
- Smooth, continuous gradient
- Penalizes confident wrong predictions heavily
- Well-suited for logistic regression

**Disadvantages**:
- Sensitive to class imbalance
- Requires probability outputs
- Can be numerically unstable (log(0))

**Numerical Stability**:
```python
# Unstable
loss = -y * log(sigmoid(z)) - (1-y) * log(1 - sigmoid(z))

# Stable (use logits directly)
loss = log(1 + exp(-z)) if y==1 else log(1 + exp(z))
```

---

### 2. Categorical Cross-Entropy

**Formula**:
```
CCE = -Î£â±¼ yâ±¼ log(Å·â±¼)
```

where:
- y = one-hot encoded true label [0, 0, 1, 0, ...]
- Å· = predicted probability distribution (from softmax)

**Simplified** (since only one yâ±¼ = 1):
```
CCE = -log(Å·_c)
```
where c is the true class

**When to Use**:
- âœ… Multi-class classification (REQUIRED)
- âœ… Mutually exclusive classes
- âŒ Multi-label problems (use BCE instead)

**Advantages**:
- Standard for multi-class problems
- Probabilistic interpretation
- Works well with softmax

**Disadvantages**:
- Sensitive to class imbalance
- Doesn't account for class similarity

**With Logits** (more stable):
```python
# Instead of: softmax â†’ cross_entropy
# Use: cross_entropy_with_logits (combines operations)
loss = log(Î£â±¼ exp(zâ±¼)) - z_c
```

---

### 3. Sparse Categorical Cross-Entropy

**Same as Categorical CE**, but:
- Input: Integer class labels (not one-hot)
- More memory efficient
- Identical mathematically

**When to Use**:
- âœ… Multi-class with many classes (e.g., 1000+ classes)
- âœ… Save memory (no one-hot encoding)

---

### 4. Focal Loss

**Formula**:
```
FL(p_t) = -Î±_t (1 - p_t)^Î³ log(p_t)
```

where:
- p_t = model's estimated probability for true class
- Î³ â‰¥ 0 (focusing parameter, typically 2)
- Î±_t = class weight

**Intuition**:
- Down-weights easy examples (high p_t)
- Focuses on hard examples (low p_t)
- Reduces impact of class imbalance

**When to Use**:
- âœ… Severe class imbalance (e.g., 1:1000)
- âœ… Object detection (RetinaNet)
- âœ… When easy examples dominate training

**Advantages**:
- Handles extreme imbalance
- Focuses on hard examples
- Improves rare class performance

**Disadvantages**:
- Extra hyperparameters (Î³, Î±)
- More complex than standard CE
- Requires tuning

**Comparison with CE**:
```
p_t = 0.9 (easy example):
  CE:    -log(0.9) = 0.105
  Focal: -(1-0.9)Â² log(0.9) = 0.001  (99% reduction!)

p_t = 0.1 (hard example):
  CE:    -log(0.1) = 2.303
  Focal: -(1-0.1)Â² log(0.1) = 1.863  (19% reduction)
```

---

### 5. Hinge Loss (SVM Loss)

**Formula** (binary):
```
L(y, Å·) = max(0, 1 - yÂ·Å·)
```

where:
- y âˆˆ {-1, +1}
- Å· âˆˆ â„ (decision function output, not probability)

**Multi-class** (one-vs-all):
```
L = Î£â±¼â‰ c max(0, Å·â±¼ - Å·_c + Î”)
```

**When to Use**:
- âœ… Support Vector Machines (SVMs)
- âœ… Maximum margin classification
- âœ… When you want margin-based learning

**Advantages**:
- Encourages large margin
- Sparse solutions (only support vectors matter)
- Robust to outliers

**Disadvantages**:
- Not probabilistic
- Non-smooth at margin boundary
- Less common in deep learning

---

### 6. Kullback-Leibler Divergence (KL Divergence)

**Formula**:
```
KL(P || Q) = Î£áµ¢ P(i) log(P(i) / Q(i))
```

**Properties**:
- Measures how one probability distribution differs from another
- **Not symmetric**: KL(P||Q) â‰  KL(Q||P)
- **Non-negative**: KL(P||Q) â‰¥ 0
- **Zero iff identical**: KL(P||Q) = 0 âŸº P = Q

**When to Use**:
- âœ… Variational Autoencoders (VAE)
- âœ… Knowledge distillation
- âœ… Comparing distributions
- âœ… Reinforcement learning (policy optimization)

**Interpretation**:
- Forward KL: KL(P||Q) - mean-seeking (covers all modes of P)
- Reverse KL: KL(Q||P) - mode-seeking (focuses on single mode)

---

## Ranking & Similarity Loss Functions

### 1. Contrastive Loss

**Formula**:
```
L = (1-Y) Ã— 0.5 Ã— DÂ² + Y Ã— 0.5 Ã— max(0, m - D)Â²
```

where:
- D = distance between embeddings
- Y = 1 if similar, 0 if dissimilar
- m = margin

**When to Use**:
- âœ… Siamese networks
- âœ… Face verification
- âœ… Signature verification
- âœ… Learning embeddings

**Intuition**:
- Similar pairs: Minimize distance
- Dissimilar pairs: Push apart (at least margin m)

---

### 2. Triplet Loss

**Formula**:
```
L = max(0, D(a, p) - D(a, n) + margin)
```

where:
- a = anchor
- p = positive (same class as anchor)
- n = negative (different class)
- D = distance function (usually L2)

**When to Use**:
- âœ… Face recognition (FaceNet)
- âœ… Person re-identification
- âœ… Metric learning
- âœ… Embedding learning

**Advantages**:
- Learns relative similarities
- No need for explicit class labels during training
- Powerful for few-shot learning

**Disadvantages**:
- Requires triplet mining (hard negatives)
- Slow convergence
- Sensitive to margin hyperparameter

**Triplet Mining Strategies**:
- **Hard**: D(a,p) > D(a,n) (hardest negatives)
- **Semi-hard**: D(a,p) < D(a,n) < D(a,p) + margin
- **Easy**: D(a,n) > D(a,p) + margin (too easy, not useful)

---

### 3. Cosine Embedding Loss

**Formula**:
```
L = {
    1 - cos(xâ‚, xâ‚‚)           if y = 1
    max(0, cos(xâ‚, xâ‚‚) - m)   if y = -1
}
```

where:
- cos(xâ‚, xâ‚‚) = xâ‚Â·xâ‚‚ / (||xâ‚|| ||xâ‚‚||)
- y = 1 (similar), y = -1 (dissimilar)
- m = margin

**When to Use**:
- âœ… Text similarity
- âœ… Sentence embeddings
- âœ… Document similarity

---

## Advanced & Specialized Loss Functions

### 1. Dice Loss

**Formula**:
```
Dice = 1 - (2|X âˆ© Y| + Îµ) / (|X| + |Y| + Îµ)
```

**When to Use**:
- âœ… Image segmentation
- âœ… Medical imaging
- âœ… Imbalanced segmentation tasks

**Advantages**:
- Handles class imbalance well
- Directly optimizes overlap metric
- Works well for small objects

---

### 2. IoU Loss (Intersection over Union)

**Formula**:
```
IoU = |X âˆ© Y| / |X âˆª Y|
Loss = 1 - IoU
```

**When to Use**:
- âœ… Object detection (bounding boxes)
- âœ… Instance segmentation
- âœ… When IoU is the evaluation metric

**Variants**:
- **GIoU**: Generalized IoU (handles non-overlapping boxes)
- **DIoU**: Distance IoU (considers center distance)
- **CIoU**: Complete IoU (adds aspect ratio)

---

### 3. CTC Loss (Connectionist Temporal Classification)

**When to Use**:
- âœ… Speech recognition
- âœ… Handwriting recognition
- âœ… Sequence-to-sequence without alignment

**Advantages**:
- No need for frame-level alignment
- Handles variable-length sequences
- Standard for ASR (Automatic Speech Recognition)

---

### 4. Wasserstein Loss (Earth Mover's Distance)

**When to Use**:
- âœ… Generative Adversarial Networks (WGAN)
- âœ… Comparing distributions
- âœ… When KL divergence is problematic

**Advantages**:
- More stable than standard GAN loss
- Meaningful gradient everywhere
- Better for disjoint distributions

---

## Selection Guide

### By Task Type

| Task | Primary Loss | Alternative | Notes |
|------|-------------|-------------|-------|
| **Binary Classification** | Binary Cross-Entropy | Focal Loss (imbalanced) | Use sigmoid output |
| **Multi-class Classification** | Categorical Cross-Entropy | Focal Loss (imbalanced) | Use softmax output |
| **Multi-label Classification** | Binary Cross-Entropy | - | Apply per label |
| **Regression** | MSE | MAE, Huber | MSE for Gaussian noise |
| **Robust Regression** | MAE, Huber | Log-Cosh | When outliers present |
| **Object Detection** | Focal Loss + IoU Loss | - | Combine classification + localization |
| **Semantic Segmentation** | Cross-Entropy + Dice | Focal Loss | Pixel-wise classification |
| **Face Recognition** | Triplet Loss | Contrastive Loss | Metric learning |
| **Sequence Labeling** | CTC Loss | Cross-Entropy | No alignment needed |
| **GANs** | Wasserstein Loss | BCE | More stable training |

---

### Decision Tree

```
START
â”‚
â”œâ”€ Classification?
â”‚  â”œâ”€ Binary â†’ BCE
â”‚  â”œâ”€ Multi-class (exclusive) â†’ Categorical CE
â”‚  â”œâ”€ Multi-label â†’ BCE (per label)
â”‚  â””â”€ Imbalanced â†’ Focal Loss
â”‚
â”œâ”€ Regression?
â”‚  â”œâ”€ No outliers â†’ MSE
â”‚  â”œâ”€ With outliers â†’ MAE or Huber
â”‚  â”œâ”€ Quantile prediction â†’ Quantile Loss
â”‚  â””â”€ Robust â†’ Log-Cosh
â”‚
â”œâ”€ Segmentation?
â”‚  â”œâ”€ Balanced â†’ CE
â”‚  â”œâ”€ Imbalanced â†’ Dice + CE
â”‚  â””â”€ Small objects â†’ Focal + Dice
â”‚
â”œâ”€ Object Detection?
â”‚  â””â”€ Focal Loss (classification) + IoU Loss (bbox)
â”‚
â”œâ”€ Metric Learning?
â”‚  â”œâ”€ Pairs â†’ Contrastive Loss
â”‚  â””â”€ Triplets â†’ Triplet Loss
â”‚
â””â”€ Sequence (no alignment)?
    â””â”€ CTC Loss
```

---

## Best Practices

### 1. Loss Function Design Principles

**Match the Task**:
- Classification â†’ Cross-Entropy (probabilistic)
- Regression â†’ MSE/MAE (distance-based)
- Ranking â†’ Triplet/Contrastive (relative)

**Consider the Data**:
- Imbalanced â†’ Focal Loss, class weights
- Outliers â†’ MAE, Huber
- Small objects â†’ Dice, Focal

**Evaluation Metric Alignment**:
- If evaluating with IoU â†’ use IoU loss
- If evaluating with F1 â†’ consider Dice loss
- If evaluating with accuracy â†’ CE is fine

---

### 2. Handling Class Imbalance

**Method 1: Class Weights**
```python
# Inverse frequency weighting
class_weights = N_total / (N_classes * N_per_class)

# Example: [100, 900] samples
weights = [1000/(2*100), 1000/(2*900)] = [5.0, 0.56]
```

**Method 2: Focal Loss**
```python
# Automatically down-weights easy examples
focal_loss = -(1 - p_t)^Î³ * log(p_t)
```

**Method 3: Oversampling/Undersampling**
- SMOTE (Synthetic Minority Over-sampling)
- Random undersampling of majority class

---

### 3. Numerical Stability

**Problem**: log(0) = -âˆ, exp(large) = âˆ

**Solutions**:

**For Cross-Entropy**:
```python
# Bad: separate softmax + log
probs = softmax(logits)
loss = -log(probs[target])

# Good: combined operation
loss = log_sum_exp(logits) - logits[target]
```

**For BCE**:
```python
# Bad: log(sigmoid(x))
loss = -log(sigmoid(x))

# Good: log-sum-exp trick
loss = log(1 + exp(-x))  # if y=1
loss = log(1 + exp(x))   # if y=0
```

**Add Epsilon**:
```python
# Prevent log(0)
loss = -log(pred + 1e-7)
```

---

### 4. Loss Scaling and Weighting

**Multi-task Learning**:
```python
total_loss = Î»â‚ * lossâ‚ + Î»â‚‚ * lossâ‚‚ + Î»â‚ƒ * lossâ‚ƒ
```

**Balancing Strategies**:
- **Manual**: Set Î» based on importance
- **Uncertainty weighting**: Learn Î» during training
- **GradNorm**: Balance gradient magnitudes

**Example** (Object Detection):
```python
loss = Î»_cls * classification_loss + Î»_box * bbox_loss
# Typical: Î»_cls = 1.0, Î»_box = 5.0
```

---

### 5. Gradient Clipping

For losses with unbounded gradients:

```python
# PyTorch
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# TensorFlow
optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)
```

---

### 6. Loss Monitoring

**Track Multiple Metrics**:
```python
# Don't just track loss
metrics = {
    'loss': loss.item(),
    'accuracy': accuracy,
    'precision': precision,
    'recall': recall,
    'f1': f1_score
}
```

**Separate Train/Val Loss**:
- Diverging â†’ Overfitting
- Both high â†’ Underfitting
- Train low, val high â†’ Overfitting

---

### 7. Custom Loss Functions

**Template**:
```python
def custom_loss(y_true, y_pred):
    # 1. Compute base loss
    base_loss = some_loss(y_true, y_pred)

    # 2. Add regularization/penalty
    penalty = compute_penalty(y_pred)

    # 3. Combine
    total_loss = base_loss + Î» * penalty

    return total_loss
```

**Example** (Smooth L1 with penalty):
```python
def smooth_l1_with_penalty(y_true, y_pred, delta=1.0, Î»=0.01):
    error = y_true - y_pred
    abs_error = torch.abs(error)

    # Smooth L1
    smooth_l1 = torch.where(
        abs_error < delta,
        0.5 * error ** 2,
        delta * abs_error - 0.5 * delta ** 2
    )

    # Penalty for large predictions
    penalty = torch.mean(y_pred ** 2)

    return torch.mean(smooth_l1) + Î» * penalty
```

---

## Common Pitfalls

### âŒ Don't Do This

1. **Using MSE for classification**
   - MSE doesn't match probabilistic interpretation
   - Use Cross-Entropy instead

2. **Forgetting to apply activation before loss**
   ```python
   # Wrong
   logits = model(x)
   loss = cross_entropy(logits, y)  # Expects probabilities!

   # Correct
   loss = cross_entropy_with_logits(logits, y)
   ```

3. **Ignoring class imbalance**
   - 99% accuracy on 99:1 imbalanced data is meaningless
   - Use Focal Loss or class weights

4. **Not normalizing multi-task losses**
   - Different losses have different scales
   - Normalize or use learned weights

5. **Using wrong reduction**
   ```python
   # Be explicit about reduction
   loss = F.cross_entropy(pred, target, reduction='mean')  # or 'sum', 'none'
   ```

---

## ğŸ“– References

### Papers

1. **Focal Loss**: Lin et al. (2017) - "Focal Loss for Dense Object Detection"
2. **Triplet Loss**: Schroff et al. (2015) - "FaceNet: A Unified Embedding for Face Recognition"
3. **Dice Loss**: Milletari et al. (2016) - "V-Net: Fully Convolutional Neural Networks"
4. **CTC Loss**: Graves et al. (2006) - "Connectionist Temporal Classification"
5. **Wasserstein Loss**: Arjovsky et al. (2017) - "Wasserstein GAN"

### Books

1. **"Deep Learning"** - Goodfellow, Bengio, Courville (Chapter 5)
2. **"Pattern Recognition and Machine Learning"** - Bishop (Chapter 1.5)
3. **"Hands-On Machine Learning"** - GÃ©ron (Chapter 10)

---

## ğŸ¯ Key Takeaways

1. **Match loss to task**:
   - Classification â†’ Cross-Entropy
   - Regression â†’ MSE/MAE
   - Metric learning â†’ Triplet/Contrastive

2. **Consider data characteristics**:
   - Imbalanced â†’ Focal Loss, weights
   - Outliers â†’ MAE, Huber
   - Small objects â†’ Dice, Focal

3. **Numerical stability matters**:
   - Use log-sum-exp tricks
   - Combine operations (softmax + log)
   - Add epsilon to prevent log(0)

4. **Monitor beyond loss**:
   - Track task-specific metrics
   - Watch train/val divergence
   - Use multiple evaluation metrics

5. **Hyperparameters matter**:
   - Focal Loss: Î³, Î±
   - Huber: Î´
   - Triplet: margin
   - Multi-task: Î» weights

6. **Start simple, add complexity**:
   - Begin with standard losses (CE, MSE)
   - Add complexity only if needed
   - Validate improvements empirically

---

*Last updated: 2025-11-29*
*Related notebook: See PyTorch implementation in `æŸå¤±å‡½æ•°.md`*
*Framework-agnostic guide - applicable to PyTorch, TensorFlow, JAX*


## âœ… æœ€ä½³å®è·µ

### ä½¿ç”¨å»ºè®®
1. **æ•°æ®é¢„å¤„ç†**ï¼š
   - âš ï¸ æ³¨æ„äº‹é¡¹1
   - âœ… æ¨èåšæ³•1

2. **å‚æ•°é€‰æ‹©**ï¼š
   - âš ï¸ æ³¨æ„äº‹é¡¹2
   - âœ… æ¨èåšæ³•2

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - âš ï¸ æ³¨æ„äº‹é¡¹3
   - âœ… æ¨èåšæ³•3

### å¸¸è§é™·é˜±

| é™·é˜± | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| é™·é˜±1 | åŸå› è¯´æ˜ | è§£å†³æ–¹æ³• |
| é™·é˜±2 | åŸå› è¯´æ˜ | è§£å†³æ–¹æ³• |
| é™·é˜±3 | åŸå› è¯´æ˜ | è§£å†³æ–¹æ³• |

---
