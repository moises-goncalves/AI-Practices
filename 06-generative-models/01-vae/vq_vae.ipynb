{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantized VAE (VQ-VAE)\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "VQ-VAE replaces the continuous latent space of VAE with a discrete codebook of learned embeddings.\n",
    "The encoder output is quantized to the nearest codebook vector, enabling discrete latent representations\n",
    "that are more suitable for autoregressive priors and avoid posterior collapse.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Architecture\n",
    "\n",
    "$$x \\xrightarrow{\\text{Encoder}} z_e \\xrightarrow{\\text{Quantize}} z_q \\xrightarrow{\\text{Decoder}} \\hat{x}$$\n",
    "\n",
    "### Codebook and Quantization\n",
    "\n",
    "Codebook: $\\mathcal{E} = \\{e_k\\}_{k=1}^K$ where $e_k \\in \\mathbb{R}^D$\n",
    "\n",
    "Quantization (nearest neighbor lookup):\n",
    "$$z_q = e_k \\quad \\text{where} \\quad k = \\arg\\min_j \\|z_e - e_j\\|_2$$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{\\|x - \\hat{x}\\|_2^2}_{\\text{Reconstruction}} + \\underbrace{\\|\\text{sg}[z_e] - e\\|_2^2}_{\\text{Codebook}} + \\underbrace{\\beta\\|z_e - \\text{sg}[e]\\|_2^2}_{\\text{Commitment}}$$\n",
    "\n",
    "where $\\text{sg}[\\cdot]$ is stop-gradient operator.\n",
    "\n",
    "**Straight-Through Estimator:** Gradients flow through quantization via:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial z_e} \\approx \\frac{\\partial \\mathcal{L}}{\\partial z_q}$$\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "VAE limitations:\n",
    "- Posterior collapse with powerful decoders\n",
    "- Blurry reconstructions from continuous sampling\n",
    "- Difficult to model with autoregressive priors\n",
    "\n",
    "VQ-VAE addresses these with discrete latents and deterministic encoding.\n",
    "\n",
    "## Algorithm Comparison\n",
    "\n",
    "| Aspect | VAE | VQ-VAE |\n",
    "|--------|-----|--------|\n",
    "| Latent | Continuous | Discrete |\n",
    "| Sampling | Reparameterization | Nearest neighbor |\n",
    "| Prior | Gaussian | Learned (PixelCNN) |\n",
    "| Posterior collapse | Common | Avoided |\n",
    "\n",
    "## Complexity Analysis\n",
    "\n",
    "- **Quantization:** $O(H \\times W \\times K \\times D)$ for codebook lookup\n",
    "- **Codebook:** $O(K \\times D)$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VQVAEConfig:\n",
    "    \"\"\"Configuration for VQ-VAE.\n",
    "    \n",
    "    Core Idea:\n",
    "        num_embeddings (K) and embedding_dim (D) define codebook capacity.\n",
    "        Larger K = more expressiveness, but harder to train.\n",
    "    \"\"\"\n",
    "    in_channels: int = 1\n",
    "    hidden_dim: int = 128\n",
    "    num_embeddings: int = 512\n",
    "    embedding_dim: int = 64\n",
    "    commitment_cost: float = 0.25\n",
    "    \n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 128\n",
    "    num_epochs: int = 20\n",
    "    \n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"Vector Quantization layer with EMA codebook update.\n",
    "    \n",
    "    Core Idea:\n",
    "        Maps continuous encoder output to nearest discrete codebook vector.\n",
    "        Uses straight-through estimator for gradient flow.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        Quantization: $z_q = e_{\\arg\\min_k \\|z_e - e_k\\|}$\n",
    "        Straight-through: $\\nabla_{z_e} \\mathcal{L} = \\nabla_{z_q} \\mathcal{L}$\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, commitment_cost: float = 0.25) -> None:\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
    "    \n",
    "    def forward(self, z_e: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        # z_e: [B, D, H, W] -> [B, H, W, D]\n",
    "        z_e = z_e.permute(0, 2, 3, 1).contiguous()\n",
    "        z_e_flat = z_e.view(-1, self.embedding_dim)\n",
    "        \n",
    "        # Compute distances to codebook\n",
    "        distances = (z_e_flat.pow(2).sum(1, keepdim=True)\n",
    "                    - 2 * z_e_flat @ self.embedding.weight.t()\n",
    "                    + self.embedding.weight.pow(2).sum(1))\n",
    "        \n",
    "        # Nearest neighbor lookup\n",
    "        indices = distances.argmin(dim=1)\n",
    "        z_q = self.embedding(indices).view(z_e.shape)\n",
    "        \n",
    "        # Losses\n",
    "        codebook_loss = F.mse_loss(z_q.detach(), z_e)\n",
    "        commitment_loss = F.mse_loss(z_q, z_e.detach())\n",
    "        vq_loss = codebook_loss + self.commitment_cost * commitment_loss\n",
    "        \n",
    "        # Straight-through estimator\n",
    "        z_q = z_e + (z_q - z_e).detach()\n",
    "        \n",
    "        # [B, H, W, D] -> [B, D, H, W]\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return z_q, vq_loss, indices.view(z_e.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Convolutional encoder for VQ-VAE.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VQVAEConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(config.in_channels, config.hidden_dim // 2, 4, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(config.hidden_dim // 2, config.hidden_dim, 4, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(config.hidden_dim, config.embedding_dim, 3, 1, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Convolutional decoder for VQ-VAE.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VQVAEConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(config.embedding_dim, config.hidden_dim, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(config.hidden_dim, config.hidden_dim // 2, 4, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(config.hidden_dim // 2, config.in_channels, 4, 2, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z_q: Tensor) -> Tensor:\n",
    "        return self.net(z_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    \"\"\"Complete VQ-VAE model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VQVAEConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.vq = VectorQuantizer(config.num_embeddings, config.embedding_dim, config.commitment_cost)\n",
    "        self.decoder = Decoder(config)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, vq_loss, indices = self.vq(z_e)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, vq_loss, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAETrainer:\n",
    "    \"\"\"Training orchestrator for VQ-VAE.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VQVAEConfig) -> None:\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        torch.manual_seed(config.seed)\n",
    "        \n",
    "        self.model = VQVAE(config).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        self.history: Dict[str, List[float]] = {\"recon_loss\": [], \"vq_loss\": [], \"total_loss\": []}\n",
    "    \n",
    "    def train_epoch(self, dataloader: DataLoader) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        metrics = {k: 0.0 for k in self.history.keys()}\n",
    "        \n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(self.device)\n",
    "            x_recon, vq_loss, _ = self.model(x)\n",
    "            \n",
    "            recon_loss = F.mse_loss(x_recon, x)\n",
    "            loss = recon_loss + vq_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            metrics[\"recon_loss\"] += recon_loss.item()\n",
    "            metrics[\"vq_loss\"] += vq_loss.item()\n",
    "            metrics[\"total_loss\"] += loss.item()\n",
    "        \n",
    "        for k in metrics:\n",
    "            metrics[k] /= len(dataloader)\n",
    "            self.history[k].append(metrics[k])\n",
    "        return metrics\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        self.model.eval()\n",
    "        x_recon, _, indices = self.model(x.to(self.device))\n",
    "        return x_recon.cpu(), indices.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config: VQVAEConfig) -> DataLoader:\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=config.batch_size, shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(trainer: VQVAETrainer, dataset, n: int = 8) -> None:\n",
    "    x = torch.stack([dataset[i][0] for i in range(n)])\n",
    "    x_recon, indices = trainer.reconstruct(x)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n, figsize=(n * 1.5, 3))\n",
    "    for i in range(n):\n",
    "        axes[0, i].imshow(x[i].squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].imshow(x_recon[i].squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    axes[0, 0].set_ylabel(\"Original\")\n",
    "    axes[1, 0].set_ylabel(\"Recon\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_codebook_usage(trainer: VQVAETrainer, dataloader: DataLoader) -> None:\n",
    "    trainer.model.eval()\n",
    "    all_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, _ in dataloader:\n",
    "            _, _, indices = trainer.model(x.to(trainer.device))\n",
    "            all_indices.append(indices.cpu().flatten())\n",
    "            if len(all_indices) > 10:\n",
    "                break\n",
    "    \n",
    "    all_indices = torch.cat(all_indices).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.hist(all_indices, bins=trainer.config.num_embeddings, edgecolor=\"black\", alpha=0.7)\n",
    "    plt.xlabel(\"Codebook Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Codebook Usage (K={trainer.config.num_embeddings})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    unique = len(np.unique(all_indices))\n",
    "    print(f\"Active codes: {unique}/{trainer.config.num_embeddings} ({100*unique/trainer.config.num_embeddings:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = VQVAEConfig(num_epochs=20)\n",
    "    dataloader = create_dataloader(config)\n",
    "    trainer = VQVAETrainer(config)\n",
    "    \n",
    "    print(f\"Parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")\n",
    "    print(f\"Codebook: {config.num_embeddings} x {config.embedding_dim}\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        metrics = trainer.train_epoch(dataloader)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{config.num_epochs}] Recon: {metrics['recon_loss']:.4f} VQ: {metrics['vq_loss']:.4f}\")\n",
    "    \n",
    "    visualize_reconstruction(trainer, dataloader.dataset)\n",
    "    visualize_codebook_usage(trainer, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "VQ-VAE uses discrete latent codes via vector quantization:\n",
    "\n",
    "1. **Codebook:** Learned dictionary of embedding vectors\n",
    "2. **Quantization:** Nearest neighbor lookup (non-differentiable)\n",
    "3. **Straight-through:** Copy gradients from decoder to encoder\n",
    "4. **Losses:** Reconstruction + codebook + commitment\n",
    "\n",
    "**Advantages over VAE:**\n",
    "- No posterior collapse\n",
    "- Discrete latents enable autoregressive priors (PixelCNN)\n",
    "- Sharper reconstructions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
