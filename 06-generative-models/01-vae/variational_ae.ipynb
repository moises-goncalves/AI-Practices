{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变分自编码器 (VAE) 深度实现\n",
    "\n",
    "**SOTA 教育标准** | 包含完整数学推导、KL 散度解析解、流形可视化\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 数学理论基础\n",
    "\n",
    "### 1.1 核心思想\n",
    "\n",
    "**问题**: 数据 $x$ 是由潜在变量 $z$ 生成的，但 $z$ 不可观测，如何学习 $p(z|x)$？\n",
    "\n",
    "**变分推断思路**: 不直接计算 $p(z|x)$，而是用一组分布 $q_\\phi(z|x)$ 去近似它。\n",
    "\n",
    "### 1.2 ELBO 推导 ⭐\n",
    "\n",
    "**目标**: 最大化对数似然 $\\log p(x)$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\log p(x) &= \\log \\int_z p(x, z) dz \\\\\n",
    "&= \\log \\int_z q(z|x) \\frac{p(x, z)}{q(z|x)} dz \\\\\n",
    "&= \\log \\mathbb{E}_{z \\sim q}[\\frac{p(x, z)}{q(z|x)}] \\\\\n",
    "&\\geq \\mathbb{E}_{z \\sim q}[\\log \\frac{p(x, z)}{q(z|x)}] \\quad \\text{(Jensen不等式)} \\\\\n",
    "&= \\mathbb{E}_{z \\sim q}[\\log \\frac{p(x|z)p(z)}{q(z|x)}] \\\\\n",
    "&= \\mathbb{E}_{z \\sim q}[\\log p(x|z)] + \\mathbb{E}_{z \\sim q}[\\log \\frac{p(z)}{q(z|x)}] \\\\\n",
    "&= \\mathbb{E}_{z \\sim q}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))\n",
    "\\end{aligned}$$\n",
    "\n",
    "这就是 **ELBO (Evidence Lower Bound)**:\n",
    "$$\\mathcal{L}_{ELBO} = \\underbrace{\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{重构项}} - \\underbrace{D_{KL}(q(z|x) \\| p(z))}_{\\text{KL散度}}$$\n",
    "\n",
    "**直觉**:\n",
    "- **重构项**: 让 $z$ 能够解码出 $x$（编码器能提取有效信息）\n",
    "- **KL项**: 让 $q(z|x)$ 接近先验 $p(z)$（正则化，防止过拟合）\n",
    "\n",
    "### 1.3 KL 散度解析解推导 ⭐\n",
    "\n",
    "设:\n",
    "- $q(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "- $p(z) = \\mathcal{N}(0, 1)$\n",
    "\n",
    "**多维高斯分布的 KL 散度公式**:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "D_{KL}(q \\| p) &= \\int q(z) \\log \\frac{q(z)}{p(z)} dz \\\\\n",
    "&= -\\frac{1}{2} \\sum_{j=1}^J \\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**推导过程**:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\log \\frac{q(z)}{p(z)} &= \\log \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(z-\\mu)^2}{2\\sigma^2}}}{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}} \\\\\n",
    "&= \\log \\left(\\frac{1}{\\sigma} \\cdot e^{-\\frac{(z-\\mu)^2}{2\\sigma^2} + \\frac{z^2}{2}}\\right) \\\\\n",
    "&= -\\log \\sigma - \\frac{(z-\\mu)^2}{2\\sigma^2} + \\frac{z^2}{2}\n",
    "\\end{aligned}$$\n",
    "\n",
    "取期望 $\\mathbb{E}_q[\\cdot]$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "D_{KL}(q \\| p) &= \\mathbb{E}_q\\left[-\\log \\sigma - \\frac{(Z-\\mu)^2}{2\\sigma^2} + \\frac{Z^2}{2}\\right] \\\\\n",
    "&= -\\log \\sigma - \\frac{\\mathbb{E}_q[(Z-\\mu)^2]}{2\\sigma^2} + \\frac{\\mathbb{E}_q[Z^2]}{2} \\\\\n",
    "&= -\\log \\sigma - \\frac{\\sigma^2}{2\\sigma^2} + \\frac{\\mu^2 + \\sigma^2}{2} \\\\\n",
    "&= -\\frac{1}{2}\\left(1 + 2\\log \\sigma - \\mu^2 - \\sigma^2\\right) \\\\\n",
    "&= -\\frac{1}{2}\\left(1 + \\log \\sigma^2 - \\mu^2 - \\sigma^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "多维情况下求和:\n",
    "$$D_{KL}(q \\| p) = -\\frac{1}{2}\\sum_{j=1}^{J}\\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)$$\n",
    "\n",
    "**直觉解释**:\n",
    "- 当 $\\mu \\to 0, \\sigma \\to 1$ 时，KL 散度 $\\to 0$（$q$ 接近 $p$）\n",
    "- KL 项作为正则化，迫使潜在空间接近标准正态分布\n",
    "\n",
    "### 1.4 重参数化技巧 (Reparameterization Trick)\n",
    "\n",
    "**问题**: 需要从 $q(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$ 采样，但采样不可微。\n",
    "\n",
    "**解决方案**:\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "**关键**:\n",
    "- $\\epsilon$ 是噪声，与参数无关（随机源）\n",
    "- $\\mu, \\sigma$ 是网络输出，可微\n",
    "- 梯度可以反向传播到 $\\mu, \\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VAEConfig:\n",
    "    \"\"\"VAE 配置类。\"\"\"\n",
    "\n",
    "    input_dim: int = 784\n",
    "    hidden_dim: int = 400\n",
    "    latent_dim: int = 2  # 2D for visualization\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 20\n",
    "    beta: float = 1.0  # KL 权重 (beta-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"变分自编码器。\n",
    "\n",
    "    核心思想:\n",
    "        编码器学习 q_phi(z|x) ~ N(mu, sigma^2)\n",
    "        解码器学习 p_theta(x|z)\n",
    "        通过 ELBO = E[log p(x|z)] - KL(q||p) 端到端训练\n",
    "\n",
    "    数学原理:\n",
    "        重参数化: z = mu + sigma * epsilon, epsilon ~ N(0, I)\n",
    "        KL散度: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: VAEConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Encoder: x -> hidden -> (mu, log_var)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(config.input_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "\n",
    "        # Decoder: z -> hidden -> x\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(config.latent_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.input_dim),\n",
    "            nn.Sigmoid(),  # MNIST pixels in [0, 1]\n",
    "        )\n",
    "\n",
    "    def encode(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"编码器输出 mu 和 log_var。\n",
    "\n",
    "        使用 log_var 而非 var 保证数值稳定性:\n",
    "            sigma = exp(0.5 * log_var) > 0\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_logvar(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, log_var: Tensor) -> Tensor:\n",
    "        \"\"\"重参数化技巧: z = mu + sigma * epsilon。\n",
    "\n",
    "        关键: epsilon 从标准正态采样，与 mu, sigma 独立，\n",
    "             使得梯度可以反向传播到 mu, sigma。\n",
    "\n",
    "        推导:\n",
    "            设 Z ~ N(mu, sigma^2), E ~ N(0, 1)\n",
    "            则 Z = mu + sigma * E\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)  # sigma = exp(0.5 * log(sigma^2))\n",
    "        eps = torch.randn_like(std)  # epsilon ~ N(0, I)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"从潜在变量 z 重构 x。\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"前向传播: x -> (x_recon, mu, log_var)。\"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, log_var\n",
    "\n",
    "    def loss_function(\n",
    "        self,\n",
    "        x_recon: Tensor,\n",
    "        x: Tensor,\n",
    "        mu: Tensor,\n",
    "        log_var: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"计算 ELBO 损失。\n",
    "\n",
    "        ELBO = E[log p(x|z)] - beta * KL(q(z|x) || p(z))\n",
    "\n",
    "        其中:\n",
    "            重构项: Binary Cross Entropy (适合 MNIST 二值像素)\n",
    "            KL散度: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        \"\"\"\n",
    "        # 重构损失 (Binary Cross Entropy)\n",
    "        recon_loss = F.binary_cross_entropy(x_recon, x, reduction=\"sum\")\n",
    "\n",
    "        # KL 散度: D_KL(N(mu, sigma^2) || N(0, 1))\n",
    "        # 推导见上文 \"KL 散度解析解推导\"\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        total_loss = recon_loss + self.config.beta * kl_loss\n",
    "\n",
    "        return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "# 创建配置和模型\n",
    "config = VAEConfig(latent_dim=2, epochs=5)  # 减少训练时间用于演示\n",
    "model = VAE(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "print(f\"VAE 参数量: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"训练集: {len(train_dataset):,}, 测试集: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer) -> Tuple[float, float, float]:\n",
    "    model.train()\n",
    "    total_loss = total_recon = total_kl = 0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.view(-1, config.input_dim).to(device)\n",
    "\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        loss, recon_loss, kl_loss = model.loss_function(x_recon, x, mu, log_var)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon_loss.item()\n",
    "        total_kl += kl_loss.item()\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, total_recon / n, total_kl / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.view(-1, config.input_dim).to(device)\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        loss, _, _ = model.loss_function(x_recon, x, mu, log_var)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# 训练\n",
    "print(\"开始训练 VAE...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = {\"train\": [], \"test\": [], \"recon\": [], \"kl\": []}\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    train_loss, recon_loss, kl_loss = train(model, train_loader, optimizer)\n",
    "    test_loss = test(model, test_loader)\n",
    "\n",
    "    history[\"train\"].append(train_loss)\n",
    "    history[\"test\"].append(test_loss)\n",
    "    history[\"recon\"].append(recon_loss)\n",
    "    history[\"kl\"].append(kl_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:2d}/{config.epochs} | \"\n",
    "        f\"Loss: {train_loss:.4f} | \"\n",
    "        f\"Recon: {recon_loss:.4f} | \"\n",
    "        f\"KL: {kl_loss:.4f} | \"\n",
    "        f\"Test: {test_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# 左图: 总损失\n",
    "axes[0].plot(history[\"train\"], label=\"Train Loss\", marker=\"o\")\n",
    "axes[0].plot(history[\"test\"], label=\"Test Loss\", marker=\"s\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Total Loss\")\n",
    "axes[0].set_title(\"VAE Training Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 右图: 重构 vs KL\n",
    "axes[1].plot(history[\"recon\"], label=\"Reconstruction\", marker=\"o\")\n",
    "axes[1].plot(history[\"kl\"], label=\"KL Divergence\", marker=\"s\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Reconstruction vs KL Loss\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_reconstruction(model, loader, n=10):\n",
    "    \"\"\"展示原始 vs 重构图片。\"\"\"\n",
    "    model.eval()\n",
    "    x, _ = next(iter(loader))\n",
    "    x = x[:n].to(device)\n",
    "\n",
    "    x_flat = x.view(-1, config.input_dim)\n",
    "    x_recon, _, _ = model(x_flat)\n",
    "    x_recon = x_recon.view(-1, 1, 28, 28)\n",
    "\n",
    "    fig, axes = plt.subplots(2, n, figsize=(15, 3))\n",
    "\n",
    "    for i in range(n):\n",
    "        axes[0, i].imshow(x[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].imshow(x_recon[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "    axes[0, 0].set_title(\"Original\", fontsize=12)\n",
    "    axes[1, 0].set_title(\"Reconstructed\", fontsize=12)\n",
    "    plt.suptitle(\"VAE Reconstruction\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_reconstruction(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_latent_space(model, loader):\n",
    "    \"\"\"可视化 2D 潜在空间。\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    z_all, labels_all = [], []\n",
    "    for x, labels in loader:\n",
    "        x = x.view(-1, config.input_dim).to(device)\n",
    "        mu, _ = model.encode(x)\n",
    "        z_all.append(mu.cpu())\n",
    "        labels_all.append(labels)\n",
    "\n",
    "    z = torch.cat(z_all).numpy()\n",
    "    labels = torch.cat(labels_all).numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(z[:, 0], z[:, 1], c=labels, cmap=\"tab10\", alpha=0.6, s=5)\n",
    "    plt.colorbar(scatter, label=\"Digit\")\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.title(\"VAE Latent Space (colored by digit)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if config.latent_dim == 2:\n",
    "    visualize_latent_space(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_manifold(model, n=20, range_val=3):\n",
    "    \"\"\"在 2D 潜在空间网格上采样，生成流形图。\n",
    "\n",
    "    这展示了 VAE 学习到的连续潜在空间结构。\n",
    "    \"\"\"\n",
    "    if config.latent_dim != 2:\n",
    "        print(\"流形可视化需要 latent_dim=2\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    grid_x = np.linspace(-range_val, range_val, n)\n",
    "    grid_y = np.linspace(-range_val, range_val, n)[::-1]\n",
    "\n",
    "    figure = np.zeros((28 * n, 28 * n))\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            x_decoded = model.decode(z).cpu().numpy().reshape(28, 28)\n",
    "            figure[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = x_decoded\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(figure, cmap=\"gray\")\n",
    "    plt.title(\"VAE Latent Manifold\", fontsize=14)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "\n",
    "    # 坐标刻度\n",
    "    ticks = np.linspace(0, 28 * n, 7)\n",
    "    tick_labels = [f\"{x:.1f}\" for x in np.linspace(-range_val, range_val, 7)]\n",
    "    plt.xticks(ticks, tick_labels)\n",
    "    plt.yticks(ticks, tick_labels[::-1])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_manifold(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples(model, n=20):\n",
    "    \"\"\"从 N(0, I) 采样生成新图片。\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    z = torch.randn(n, config.latent_dim).to(device)\n",
    "    samples = model.decode(z).view(-1, 1, 28, 28)\n",
    "\n",
    "    fig, axes = plt.subplots(2, n // 2, figsize=(15, 4))\n",
    "    for i in range(n):\n",
    "        row, col = i // (n // 2), i % (n // 2)\n",
    "        axes[row, col].imshow(samples[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Generated Samples (z ~ N(0, I))\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. β-VAE: 解耦表示学习 ⭐⭐\n",
    "\n",
    "### 5.1 核心思想\n",
    "\n",
    "**问题**: 标准 VAE 的潜在空间可能是纠缠的（一个 $z_i$ 影响多个生成因素）。\n",
    "\n",
    "**β-VAE**: 增大 KL 权重 $\\beta > 1$，迫使潜在空间更接近各向同性高斯，从而解耦。\n",
    "\n",
    "$$\\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}[\\log p(x|z)] - \\beta \\cdot D_{KL}(q(z|x) \\| p(z))$$\n",
    "\n",
    "**权衡**:\n",
    "- $\\beta = 1$: 标准 VAE\n",
    "- $\\beta > 1$: 更解耦，但重构质量下降\n",
    "- $\\beta < 1$: 更好重构，但潜在空间可能塌缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_beta_values() -> None:\n",
    "    \"\"\"对比不同 beta 值对潜在空间的影响。\"\"\"\n",
    "    betas = [0.1, 1.0, 4.0, 10.0]\n",
    "    results = {}\n",
    "\n",
    "    for beta in betas:\n",
    "        cfg = VAEConfig(latent_dim=2, epochs=3, beta=beta)\n",
    "        vae = VAE(cfg).to(device)\n",
    "        opt = torch.optim.Adam(vae.parameters(), lr=cfg.lr)\n",
    "\n",
    "        # 快速训练\n",
    "        for _ in range(3):\n",
    "            for x, _ in train_loader:\n",
    "                x = x.view(-1, cfg.input_dim).to(device)\n",
    "                x_recon, mu, log_var = vae(x)\n",
    "                loss, _, _ = vae.loss_function(x_recon, x, mu, log_var)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        # 收集潜在表示\n",
    "        vae.eval()\n",
    "        z_list, y_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x = x.view(-1, cfg.input_dim).to(device)\n",
    "                mu, _ = vae.encode(x)\n",
    "                z_list.append(mu.cpu())\n",
    "                y_list.append(y)\n",
    "                if len(z_list) > 10:\n",
    "                    break\n",
    "\n",
    "        results[beta] = (torch.cat(z_list).numpy(), torch.cat(y_list).numpy())\n",
    "\n",
    "    # 可视化\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    for ax, beta in zip(axes, betas):\n",
    "        z, y = results[beta]\n",
    "        ax.scatter(z[:, 0], z[:, 1], c=y, cmap=\"tab10\", alpha=0.6, s=10)\n",
    "        ax.set_title(f\"β = {beta}\")\n",
    "        ax.set_xlabel(\"z[0]\")\n",
    "        ax.set_ylabel(\"z[1]\")\n",
    "\n",
    "    plt.suptitle(\"β-VAE: 不同 β 值对潜在空间的影响\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n观察:\")\n",
    "    print(\"  β=0.1: 潜在空间分散，重构好但不规则\")\n",
    "    print(\"  β=1.0: 标准 VAE，平衡\")\n",
    "    print(\"  β=4.0: 更紧凑，开始解耦\")\n",
    "    print(\"  β=10.0: 高度压缩，可能过度正则化\")\n",
    "\n",
    "\n",
    "compare_beta_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. CNN-VAE: 更强的特征提取 ⭐⭐\n",
    "\n",
    "使用卷积网络替代全连接网络，更好地捕捉图像的空间结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    \"\"\"卷积变分自编码器。\n",
    "\n",
    "    使用 CNN 替代 MLP，更好地捕捉图像空间结构。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int = 32, beta: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        # Encoder: (1, 28, 28) -> (64, 7, 7) -> latent\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),  # -> (32, 14, 14)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # -> (64, 7, 7)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "\n",
    "        # Decoder: latent -> (64, 7, 7) -> (1, 28, 28)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # -> (32, 14, 14)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),  # -> (1, 28, 28)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, log_var: Tensor) -> Tensor:\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        h = self.fc_decode(z).view(-1, 64, 7, 7)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n",
    "\n",
    "    def loss_function(self, x_recon: Tensor, x: Tensor, mu: Tensor, log_var: Tensor) -> Tensor:\n",
    "        recon_loss = F.binary_cross_entropy(x_recon, x, reduction=\"sum\")\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return recon_loss + self.beta * kl_loss\n",
    "\n",
    "\n",
    "# 测试 CNN-VAE\n",
    "conv_vae = ConvVAE(latent_dim=32).to(device)\n",
    "print(f\"CNN-VAE 参数量: {sum(p.numel() for p in conv_vae.parameters()):,}\")\n",
    "\n",
    "# 测试前向传播\n",
    "x_test = torch.randn(4, 1, 28, 28).to(device)\n",
    "x_recon, mu, log_var = conv_vae(x_test)\n",
    "print(f\"输入: {x_test.shape} -> 重构: {x_recon.shape}, 潜在: {mu.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. VQ-VAE 简介: 离散潜在空间 ⭐⭐⭐\n",
    "\n",
    "### 7.1 核心思想\n",
    "\n",
    "**问题**: 连续潜在空间可能导致 \"posterior collapse\"（解码器忽略 $z$）。\n",
    "\n",
    "**VQ-VAE**: 使用离散码本 (codebook)，将连续编码量化到最近的码向量。\n",
    "\n",
    "$$z_q = \\text{argmin}_{e_k \\in \\mathcal{E}} \\|z_e - e_k\\|_2$$\n",
    "\n",
    "**损失函数**:\n",
    "$$\\mathcal{L} = \\|x - \\hat{x}\\|^2 + \\|\\text{sg}[z_e] - e\\|^2 + \\beta\\|z_e - \\text{sg}[e]\\|^2$$\n",
    "\n",
    "其中 $\\text{sg}[\\cdot]$ 是 stop-gradient 操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"向量量化层 (VQ-VAE 核心)。\n",
    "\n",
    "    将连续编码映射到离散码本中最近的向量。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_embeddings: int, embedding_dim: int, commitment_cost: float = 0.25\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        # 码本 (codebook)\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"量化操作。\n",
    "\n",
    "        Args:\n",
    "            z: 编码器输出 (B, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            z_q: 量化后的向量\n",
    "            loss: VQ 损失\n",
    "            indices: 码本索引\n",
    "        \"\"\"\n",
    "        # (B, C, H, W) -> (B, H, W, C) -> (B*H*W, C)\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flat = z.view(-1, self.embedding_dim)\n",
    "\n",
    "        # 计算到所有码向量的距离\n",
    "        distances = (\n",
    "            torch.sum(z_flat**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(z_flat, self.embedding.weight.t())\n",
    "        )\n",
    "\n",
    "        # 找最近的码向量\n",
    "        indices = torch.argmin(distances, dim=1)\n",
    "        z_q = self.embedding(indices).view(z.shape)\n",
    "\n",
    "        # VQ 损失\n",
    "        codebook_loss = F.mse_loss(z_q.detach(), z)  # 更新编码器\n",
    "        commitment_loss = F.mse_loss(z_q, z.detach())  # 更新码本\n",
    "        loss = codebook_loss + self.commitment_cost * commitment_loss\n",
    "\n",
    "        # Straight-through estimator: 前向用 z_q，反向用 z\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # 恢复形状 (B, H, W, C) -> (B, C, H, W)\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return z_q, loss, indices\n",
    "\n",
    "\n",
    "# 测试 VQ 层\n",
    "vq = VectorQuantizer(num_embeddings=512, embedding_dim=64)\n",
    "z_test = torch.randn(4, 64, 7, 7)\n",
    "z_q, vq_loss, indices = vq(z_test)\n",
    "print(f\"VQ 输入: {z_test.shape} -> 输出: {z_q.shape}\")\n",
    "print(f\"VQ Loss: {vq_loss.item():.4f}\")\n",
    "print(f\"码本使用率: {len(indices.unique())}/{vq.num_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. VAE 变体对比\n",
    "\n",
    "| 变体 | 潜在空间 | 优势 | 劣势 |\n",
    "|:-----|:---------|:-----|:-----|\n",
    "| **VAE** | 连续高斯 | 简单，可微 | 可能模糊 |\n",
    "| **β-VAE** | 连续高斯 | 解耦表示 | 重构质量下降 |\n",
    "| **VQ-VAE** | 离散码本 | 清晰，无 posterior collapse | 需要码本管理 |\n",
    "| **CVAE** | 条件高斯 | 可控生成 | 需要标签 |\n",
    "\n",
    "**进阶学习**: VQ-VAE-2, DALL-E (dVAE), Hierarchical VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 总结\n",
    "\n",
    "| 组件 | 说明 |\n",
    "|:-----|:-----|\n",
    "| **ELBO** | $\\mathbb{E}[\\log p(x|z)] - D_{KL}(q \\| p)$ |\n",
    "| **重参数化** | $z = \\mu + \\sigma \\cdot \\epsilon$ |\n",
    "| **KL 解析解** | $-0.5\\sum(1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2)$ |\n",
    "| **β-VAE** | $\\beta > 1$ 促进解耦 |\n",
    "| **VQ-VAE** | 离散码本，避免 posterior collapse |\n",
    "| **CNN-VAE** | 卷积网络，更好的图像特征 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
