{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM: 去噪扩散概率模型 (深度实现)\n",
    "\n",
    "**SOTA 教育标准** | 包含物理直觉、Closed-form 推导、SNR 分析\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 物理直觉：与非平衡热力学的联系 ⭐\n",
    "\n",
    "### 1.1 扩散过程的物理类比\n",
    "\n",
    "**自然界中的扩散**:\n",
    "- 一滴墨水滴入清水中，会逐渐扩散直至均匀分布\n",
    "- 这是**熵增**过程：系统从有序走向无序\n",
    "- 热力学第二定律：封闭系统的熵永不减少\n",
    "\n",
    "**DDPM 的数学对应**:\n",
    "- 原始数据 $x_0$ = \"有序\"状态（低熵）\n",
    "- 纯噪声 $x_T$ = \"无序\"状态（高熵）\n",
    "- 前向过程 $x_0 \\to x_T$ = **扩散**（熵增，自然发生）\n",
    "- 逆向过程 $x_T \\to x_0$ = **去扩散**（熵减，需要能量/学习）\n",
    "\n",
    "**Fokker-Planck 方程**:\n",
    "\n",
    "在物理学中，粒子的扩散过程由 Fokker-Planck 方程描述：\n",
    "$$\\frac{\\partial p}{\\partial t} = \\nabla \\cdot (D \\nabla p)$$\n",
    "\n",
    "DDPM 的离散版本可以看作是这个方程的数值解法。\n",
    "\n",
    "### 1.2 为什么扩散模型有效？\n",
    "\n",
    "**核心洞察**:\n",
    "\n",
    "1. **前向过程是** **自然的、可逆的**: 添加高斯噪声是物理上可实现的过程\n",
    "\n",
    "2. **逆向过程需要** **学习能量函数**: 就像制冷机需要消耗能量来降低温度（减少熵）\n",
    "\n",
    "3. **Score Function**: 我们学习的不是 $p(x)$ 本身，而是 $\\nabla_x \\log p(x)$（梯度方向）\n",
    "\n",
    "**Score Matching 直觉**:\n",
    "- $\\nabla_x \\log p(x)$ 指向概率密度增加的方向\n",
    "- 如果知道这个梯度，就可以\"逆流而上\"，从噪声恢复数据\n",
    "\n",
    "### 1.3 信噪比 (SNR) 的物理意义\n",
    "\n",
    "在时刻 $t$，我们有:\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "**信噪比定义**:\n",
    "$$\\text{SNR}_t = \\frac{\\text{Signal Power}}{\\text{Noise Power}} = \\frac{\\bar{\\alpha}_t}{1 - \\bar{\\alpha}_t}$$\n",
    "\n",
    "- $t=0$: $\\bar{\\alpha}_0 \\approx 1$, SNR $\\to \\infty$ (纯信号)\n",
    "- $t=T$: $\\bar{\\alpha}_T \\approx 0$, SNR $\\to 0$ (纯噪声)\n",
    "\n",
    "**物理意义**:\n",
    "- 前向过程是 SNR 逐渐降低的过程\n",
    "- 逆向过程是 SNR 逐渐升高的过程\n",
    "- 神经网络需要在不同 SNR 下都能预测噪声"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 数学推导\n",
    "\n",
    "### 2.1 前向过程\n",
    "\n",
    "**马尔可夫链定义**:\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "### 2.2 Closed-form 推导 ⭐\n",
    "\n",
    "**目标**: 从 $x_0$ 直接得到 $x_t$，无需 $t$ 步迭代。\n",
    "\n",
    "**推导**:\n",
    "\n",
    "定义 $\\alpha_t = 1 - \\beta_t$，则:\n",
    "$$x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1-\\alpha_t} \\epsilon_t$$\n",
    "\n",
    "递归展开:\n",
    "$$\\begin{aligned}\n",
    "x_1 &= \\sqrt{\\alpha_1} x_0 + \\sqrt{1-\\alpha_1} \\epsilon_1 \\\\\n",
    "x_2 &= \\sqrt{\\alpha_2} x_1 + \\sqrt{1-\\alpha_2} \\epsilon_2 \\\\\n",
    "&= \\sqrt{\\alpha_2}\\left(\\sqrt{\\alpha_1} x_0 + \\sqrt{1-\\alpha_1} \\epsilon_1\\right) + \\sqrt{1-\\alpha_2} \\epsilon_2 \\\\\n",
    "&= \\sqrt{\\alpha_1\\alpha_2} x_0 + \\sqrt{\\alpha_2(1-\\alpha_1)} \\epsilon_1 + \\sqrt{1-\\alpha_2} \\epsilon_2\n",
    "\\end{aligned}$$\n",
    "\n",
    "继续展开到 $t$ 步，利用高斯分布的可加性:\n",
    "$$x_t = \\sqrt{\\prod_{i=1}^t \\alpha_i} \\cdot x_0 + \\sqrt{1 - \\prod_{i=1}^t \\alpha_i} \\cdot \\epsilon$$\n",
    "\n",
    "定义 $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$，得到:\n",
    "$$\\boxed{x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon}$$\n",
    "\n",
    "其中 $\\epsilon \\sim \\mathcal{N}(0, I)$。\n",
    "\n",
    "**重要性**:\n",
    "- 训练时可以直接采样任意 $t$ 的 $x_t$\n",
    "- 时间复杂度从 $O(t)$ 降到 $O(1)$\n",
    "- 这是 DDPM 高效训练的关键"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DDPMConfig:\n",
    "    \"\"\"DDPM 配置类。\"\"\"\n",
    "\n",
    "    timesteps: int = 1000\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "    schedule: str = \"linear\"  # 'linear' or 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMScheduler:\n",
    "    \"\"\"DDPM 噪声调度器。\n",
    "\n",
    "    核心思想:\n",
    "        实现前向扩散过程 q(x_t|x_0) = N(sqrt(alpha_bar)*x_0, (1-alpha_bar)*I)\n",
    "\n",
    "    物理意义:\n",
    "        控制信号向噪声转换的速率，类似热力学中的温度调度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DDPMConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "        # 生成 beta 调度\n",
    "        if config.schedule == \"linear\":\n",
    "            self.betas = torch.linspace(config.beta_start, config.beta_end, config.timesteps)\n",
    "        elif config.schedule == \"cosine\":\n",
    "            self.betas = self._cosine_schedule(config.timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {config.schedule}\")\n",
    "\n",
    "        # 预计算系数\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # 常用系数\n",
    "        self.sqrt_alpha_bars = torch.sqrt(self.alpha_bars)\n",
    "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1.0 - self.alpha_bars)\n",
    "\n",
    "    def _cosine_schedule(self, timesteps: int, s: float = 0.008) -> Tensor:\n",
    "        \"\"\"余弦调度 (Improved DDPM, Nichol & Dhariwal, 2021)。\n",
    "\n",
    "        优势:\n",
    "            - 在高 SNR 区域保留更多信息\n",
    "            - 避免线性调度在后期过度破坏信号\n",
    "        \"\"\"\n",
    "        steps = timesteps + 1\n",
    "        x = torch.linspace(0, timesteps, steps)\n",
    "        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "    def get_snr(self, t: Tensor) -> Tensor:\n",
    "        \"\"\"计算信噪比 SNR_t = alpha_bar / (1 - alpha_bar)。\"\"\"\n",
    "        alpha_bar = self.alpha_bars[t]\n",
    "        return alpha_bar / (1 - alpha_bar)\n",
    "\n",
    "    def q_sample(\n",
    "        self,\n",
    "        x0: Tensor,\n",
    "        t: Tensor,\n",
    "        noise: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"前向扩散采样 (Closed-form)。\n",
    "\n",
    "        公式: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "\n",
    "        Args:\n",
    "            x0: 原始数据 [B, C, H, W]\n",
    "            t: 时间步，tensor of shape [B]\n",
    "            noise: 可选的噪声，若为 None 则随机采样\n",
    "\n",
    "        Returns:\n",
    "            x_t: 加噪后的数据\n",
    "            noise: 使用的噪声\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "\n",
    "        # 获取系数并调整形状\n",
    "        sqrt_alpha_bar = self.sqrt_alpha_bars[t]\n",
    "        sqrt_one_minus_alpha_bar = self.sqrt_one_minus_alpha_bars[t]\n",
    "\n",
    "        # 广播到 [B, C, H, W]\n",
    "        while len(sqrt_alpha_bar.shape) < len(x0.shape):\n",
    "            sqrt_alpha_bar = sqrt_alpha_bar.unsqueeze(-1)\n",
    "            sqrt_one_minus_alpha_bar = sqrt_one_minus_alpha_bar.unsqueeze(-1)\n",
    "\n",
    "        # Closed-form 计算\n",
    "        x_t = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
    "\n",
    "        return x_t, noise\n",
    "\n",
    "    def p_sample(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        x_t: Tensor,\n",
    "        t: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"单步逆向采样 (去噪)。\n",
    "\n",
    "        q(x_{t-1} | x_t, x_0) 的近似，使用模型预测的噪声。\n",
    "        \"\"\"\n",
    "        # 模型预测噪声\n",
    "        predicted_noise = model(x_t, t)\n",
    "\n",
    "        # 获取系数\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_bar_t = self.alpha_bars[t]\n",
    "        beta_t = self.betas[t]\n",
    "\n",
    "        # 计算均值 (使用预测的噪声恢复 x_0)\n",
    "        # 完整推导见 DDPM 论文附录\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar_t)\n",
    "        sqrt_recip_alpha_bar = torch.sqrt(1 / alpha_bar_t)\n",
    "\n",
    "        # x_0 的估计: x_0_hat = (x_t - sqrt(1-alpha_bar) * noise) / sqrt(alpha_bar)\n",
    "        x_0_hat = sqrt_recip_alpha_bar * (x_t - sqrt_one_minus_alpha_bar * predicted_noise)\n",
    "\n",
    "        # 计算均值\n",
    "        sqrt_recip_alpha = torch.sqrt(1 / alpha_t)\n",
    "        posterior_mean = (\n",
    "            sqrt_recip_alpha * (x_t - beta_t / sqrt_one_minus_alpha_bar * predicted_noise)\n",
    "            + (1 - alpha_t) / sqrt_one_minus_alpha_bar * x_0_hat\n",
    "        ) / 2\n",
    "\n",
    "        return posterior_mean\n",
    "\n",
    "\n",
    "# 创建调度器\n",
    "config = DDPMConfig(timesteps=1000)\n",
    "scheduler = DDPMScheduler(config)\n",
    "\n",
    "print(f\"时间步数: {config.timesteps}\")\n",
    "print(f\"Beta 范围: [{scheduler.betas[0]:.6f}, {scheduler.betas[-1]:.4f}]\")\n",
    "print(f\"Alpha_bar 范围: [{scheduler.alpha_bars[0]:.6f}, {scheduler.alpha_bars[-1]:.8f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scheduler_components(scheduler: DDPMScheduler) -> None:\n",
    "    \"\"\"可视化调度器的各组件。\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    t = np.arange(scheduler.config.timesteps)\n",
    "\n",
    "    axes[0].plot(t, scheduler.betas.numpy())\n",
    "    axes[0].set_xlabel(\"Timestep t\")\n",
    "    axes[0].set_ylabel(r\"$\\beta_t$\")\n",
    "    axes[0].set_title(\"Noise Schedule (Beta)\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(t, scheduler.alphas.numpy())\n",
    "    axes[1].set_xlabel(\"Timestep t\")\n",
    "    axes[1].set_ylabel(r\"$\\alpha_t = 1 - \\beta_t$\")\n",
    "    axes[1].set_title(\"Alpha\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].plot(t, scheduler.alpha_bars.numpy())\n",
    "    axes[2].set_xlabel(\"Timestep t\")\n",
    "    axes[2].set_ylabel(r\"$\\bar{\\alpha}_t = \\prod \\alpha_i$\")\n",
    "    axes[2].set_title(\"Cumulative Alpha (Signal Coefficient)\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    # SNR\n",
    "    snr = scheduler.alpha_bars / (1 - scheduler.alpha_bars)\n",
    "    axes[3].semilogy(t, snr.numpy())\n",
    "    axes[3].set_xlabel(\"Timestep t\")\n",
    "    axes[3].set_ylabel(\"SNR (log scale)\")\n",
    "    axes[3].set_title(\"Signal-to-Noise Ratio\")\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_scheduler_components(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试图像\n",
    "\n",
    "\n",
    "def create_test_image(size: int = 64) -> Tensor:\n",
    "    \"\"\"创建一个简单的测试图像（方块图案）。\"\"\"\n",
    "    img = torch.zeros(1, size, size)\n",
    "    img[:, 16:48, 16:48] = 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "x0 = create_test_image(64)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(x0.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Original Image $x_0$\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"图像形状: {x0.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_diffusion_with_snr(scheduler: DDPMScheduler, x0: Tensor) -> None:\n",
    "    \"\"\"可视化扩散过程，标注 SNR。\"\"\"\n",
    "    timesteps_to_show = [0, 50, 100, 200, 400, 600, 800, 999]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, t in enumerate(timesteps_to_show):\n",
    "        x_t, noise = scheduler.q_sample(x0.unsqueeze(0), torch.tensor([t]))\n",
    "        x_t = x_t.squeeze(0)\n",
    "\n",
    "        # 计算并显示 SNR\n",
    "        alpha_bar = scheduler.alpha_bars[t].item()\n",
    "        snr = alpha_bar / (1 - alpha_bar)\n",
    "        snr_db = 10 * torch.log10(torch.tensor(snr) + 1e-10).item()\n",
    "\n",
    "        axes[i].imshow(x_t.squeeze(), cmap=\"gray\", vmin=-2, vmax=2)\n",
    "        axes[i].set_title(f\"t={t}\\nSNR={snr:.4f} ({snr_db:.1f} dB)\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Forward Diffusion with SNR Annotation\\n\" + \"Lower SNR = More Noise = Less Signal\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_diffusion_with_snr(scheduler, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_schedules() -> None:\n",
    "    \"\"\"对比线性和余弦调度。\"\"\"\n",
    "    config_linear = DDPMConfig(schedule=\"linear\")\n",
    "    config_cosine = DDPMConfig(schedule=\"cosine\")\n",
    "\n",
    "    sched_linear = DDPMScheduler(config_linear)\n",
    "    sched_cosine = DDPMScheduler(config_cosine)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    t = np.arange(1000)\n",
    "\n",
    "    # Alpha_bar 对比\n",
    "    axes[0].plot(t, sched_linear.alpha_bars.numpy(), label=\"Linear\", alpha=0.8)\n",
    "    axes[0].plot(t, sched_cosine.alpha_bars.numpy(), label=\"Cosine\", alpha=0.8)\n",
    "    axes[0].set_xlabel(\"Timestep t\")\n",
    "    axes[0].set_ylabel(r\"$\\bar{\\alpha}_t$\")\n",
    "    axes[0].set_title(\"Cumulative Alpha Comparison\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # SNR 对比\n",
    "    snr_linear = sched_linear.alpha_bars / (1 - sched_linear.alpha_bars)\n",
    "    snr_cosine = sched_cosine.alpha_bars / (1 - sched_cosine.alpha_bars)\n",
    "    axes[1].semilogy(t, snr_linear.numpy(), label=\"Linear\", alpha=0.8)\n",
    "    axes[1].semilogy(t, snr_cosine.numpy(), label=\"Cosine\", alpha=0.8)\n",
    "    axes[1].set_xlabel(\"Timestep t\")\n",
    "    axes[1].set_ylabel(\"SNR (log scale)\")\n",
    "    axes[1].set_title(\"SNR Comparison\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n观察:\")\n",
    "    print(\"- 余弦调度在后期保持更高的 SNR（保留更多信号）\")\n",
    "    print(\"- 这有助于模型在高噪声区域仍能预测准确\")\n",
    "\n",
    "\n",
    "compare_schedules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_physics_analogy() -> None:\n",
    "    \"\"\"解释 DDPM 与物理学的关系。\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"DDPM 的物理直觉\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"1. 热力学类比:\")\n",
    "    print(\"   前向过程 x_0 -> x_T:\")\n",
    "    print(\"     类比: 墨水滴入清水扩散\")\n",
    "    print(\"     物理过程: 扩散 (Diffusion)\")\n",
    "    print(\"     热力学: 熵增 (自然过程)\")\n",
    "    print()\n",
    "    print(\"   逆向过程 x_T -> x_0:\")\n",
    "    print(\"     类比: 从浑浊水中分离出墨水\")\n",
    "    print(\"     物理过程: 去扩散 (Reverse Diffusion)\")\n",
    "    print(\"     热力学: 熵减 (需要能量/学习)\")\n",
    "    print()\n",
    "    print(\"2. Score Function:\")\n",
    "    print(\"   梯度方向 = 概率密度增加的方向\")\n",
    "    print(\"   知道梯度就能\"逆流而上\"恢复数据\")\n",
    "    print()\n",
    "    print(\"3. 信噪比 (SNR):\")\n",
    "    print(\"   前向: SNR 逐渐降低 (信号淹没在噪声中)\")\n",
    "    print(\"   逆向: SNR 逐渐升高 (噪声消退，信号显现)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "explain_physics_analogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 简化 U-Net 噪声预测网络 ⭐⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    \"\"\"正弦位置编码 (用于时间步嵌入)。\n",
    "\n",
    "    与 Transformer 的位置编码类似，将离散时间步映射到连续向量。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t: Tensor) -> Tensor:\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"残差块 with 时间嵌入。\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int, time_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor, t_emb: Tensor) -> Tensor:\n",
    "        h = self.norm1(torch.relu(self.conv1(x)))\n",
    "        h = h + self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = self.norm2(torch.relu(self.conv2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"简化版 U-Net 用于噪声预测。\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int = 1, base_ch: int = 64, time_dim: int = 256) -> None:\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = ResBlock(in_ch, base_ch, time_dim)\n",
    "        self.enc2 = ResBlock(base_ch, base_ch * 2, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bot = ResBlock(base_ch * 2, base_ch * 2, time_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.dec2 = ResBlock(base_ch * 4, base_ch, time_dim)\n",
    "        self.dec1 = ResBlock(base_ch * 2, base_ch, time_dim)\n",
    "\n",
    "        self.out = nn.Conv2d(base_ch, in_ch, 1)\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> Tensor:\n",
    "        t_emb = self.time_mlp(t.float())\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x, t_emb)\n",
    "        e2 = self.enc2(self.pool(e1), t_emb)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bot(self.pool(e2), t_emb)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d2 = self.dec2(torch.cat([self.up(b), e2], dim=1), t_emb)\n",
    "        d1 = self.dec1(torch.cat([self.up(d2), e1], dim=1), t_emb)\n",
    "\n",
    "        return self.out(d1)\n",
    "\n",
    "\n",
    "# 测试 U-Net\n",
    "unet = SimpleUNet(in_ch=1, base_ch=32)\n",
    "x_test = torch.randn(2, 1, 32, 32)\n",
    "t_test = torch.randint(0, 1000, (2,))\n",
    "out = unet(x_test, t_test)\n",
    "print(f\"U-Net 参数量: {sum(p.numel() for p in unet.parameters()):,}\")\n",
    "print(f\"输入: {x_test.shape} -> 输出: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. DDIM: 加速采样 ⭐⭐⭐\n",
    "\n",
    "### 6.1 核心思想\n",
    "\n",
    "**DDPM 的问题**: 需要 1000 步采样，太慢。\n",
    "\n",
    "**DDIM 解决方案**: 使用非马尔可夫过程，可以跳步采样。\n",
    "\n",
    "$$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\cdot \\hat{x}_0 + \\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma_t^2} \\cdot \\epsilon_\\theta + \\sigma_t \\cdot \\epsilon$$\n",
    "\n",
    "当 $\\sigma_t = 0$ 时，采样是确定性的 (deterministic)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMSampler:\n",
    "    \"\"\"DDIM 采样器 (加速采样)。\n",
    "\n",
    "    核心优势:\n",
    "        - 可以用 50-100 步替代 1000 步\n",
    "        - 确定性采样 (eta=0) 或随机采样 (eta>0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scheduler: DDPMScheduler, ddim_steps: int = 50, eta: float = 0.0) -> None:\n",
    "        self.scheduler = scheduler\n",
    "        self.ddim_steps = ddim_steps\n",
    "        self.eta = eta  # 0 = deterministic, 1 = DDPM\n",
    "\n",
    "        # 选择子集时间步\n",
    "        self.timesteps = torch.linspace(\n",
    "            0, scheduler.config.timesteps - 1, ddim_steps, dtype=torch.long\n",
    "        ).flip(0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        shape: Tuple[int, ...],\n",
    "        device: str = \"cpu\",\n",
    "    ) -> Tensor:\n",
    "        \"\"\"DDIM 采样。\"\"\"\n",
    "        # 从纯噪声开始\n",
    "        x = torch.randn(shape, device=device)\n",
    "\n",
    "        for i, t in enumerate(self.timesteps):\n",
    "            t_batch = torch.full((shape[0],), t, device=device, dtype=torch.long)\n",
    "\n",
    "            # 预测噪声\n",
    "            eps_pred = model(x, t_batch)\n",
    "\n",
    "            # 获取系数\n",
    "            alpha_bar_t = self.scheduler.alpha_bars[t]\n",
    "            sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)\n",
    "            sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "            # 预测 x_0\n",
    "            x0_pred = (x - sqrt_one_minus_alpha_bar_t * eps_pred) / sqrt_alpha_bar_t\n",
    "            x0_pred = torch.clamp(x0_pred, -1, 1)  # 稳定性\n",
    "\n",
    "            if i < len(self.timesteps) - 1:\n",
    "                t_prev = self.timesteps[i + 1]\n",
    "                alpha_bar_t_prev = self.scheduler.alpha_bars[t_prev]\n",
    "\n",
    "                # DDIM 公式\n",
    "                sigma_t = self.eta * torch.sqrt(\n",
    "                    (1 - alpha_bar_t_prev)\n",
    "                    / (1 - alpha_bar_t)\n",
    "                    * (1 - alpha_bar_t / alpha_bar_t_prev)\n",
    "                )\n",
    "\n",
    "                dir_xt = torch.sqrt(1 - alpha_bar_t_prev - sigma_t**2) * eps_pred\n",
    "                noise = torch.randn_like(x) if self.eta > 0 else 0\n",
    "\n",
    "                x = torch.sqrt(alpha_bar_t_prev) * x0_pred + dir_xt + sigma_t * noise\n",
    "            else:\n",
    "                x = x0_pred\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 对比 DDPM vs DDIM 步数\n",
    "print(\"采样步数对比:\")\n",
    "print(f\"  DDPM: 1000 步\")\n",
    "print(f\"  DDIM: 50 步 (20x 加速)\")\n",
    "print(f\"  DDIM: 10 步 (100x 加速，质量下降)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Classifier-Free Guidance (CFG) ⭐⭐⭐\n",
    "\n",
    "### 7.1 核心思想\n",
    "\n",
    "**问题**: 如何在不训练分类器的情况下实现条件生成？\n",
    "\n",
    "**CFG 解决方案**: 同时训练条件和无条件模型，推理时混合。\n",
    "\n",
    "$$\\tilde{\\epsilon}_\\theta = \\epsilon_\\theta(x_t, \\emptyset) + w \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset))$$\n",
    "\n",
    "其中 $w$ 是 guidance scale，$c$ 是条件，$\\emptyset$ 是无条件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGWrapper:\n",
    "    \"\"\"Classifier-Free Guidance 包装器。\n",
    "\n",
    "    训练时: 随机 drop 条件 (用 null token 替代)\n",
    "    推理时: 混合条件和无条件预测\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, guidance_scale: float = 7.5) -> None:\n",
    "        self.model = model\n",
    "        self.guidance_scale = guidance_scale\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        t: Tensor,\n",
    "        cond: Tensor,\n",
    "        uncond: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"CFG 推理。\n",
    "\n",
    "        Args:\n",
    "            x: 噪声图像\n",
    "            t: 时间步\n",
    "            cond: 条件嵌入 (如文本)\n",
    "            uncond: 无条件嵌入 (null token)\n",
    "\n",
    "        Returns:\n",
    "            引导后的噪声预测\n",
    "        \"\"\"\n",
    "        # 批量处理: [cond, uncond]\n",
    "        x_in = torch.cat([x, x], dim=0)\n",
    "        t_in = torch.cat([t, t], dim=0)\n",
    "        c_in = torch.cat([cond, uncond], dim=0)\n",
    "\n",
    "        # 模型预测\n",
    "        eps_cond, eps_uncond = self.model(x_in, t_in, c_in).chunk(2)\n",
    "\n",
    "        # CFG 公式\n",
    "        eps_guided = eps_uncond + self.guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "        return eps_guided\n",
    "\n",
    "\n",
    "def visualize_cfg_effect() -> None:\n",
    "    \"\"\"可视化 CFG scale 的影响。\"\"\"\n",
    "    scales = [1.0, 3.0, 7.5, 15.0]\n",
    "\n",
    "    print(\"CFG Scale 影响:\")\n",
    "    print(\"=\" * 50)\n",
    "    for s in scales:\n",
    "        if s == 1.0:\n",
    "            desc = \"无引导，多样性高，可能偏离条件\"\n",
    "        elif s <= 3.0:\n",
    "            desc = \"轻度引导，平衡多样性和条件一致性\"\n",
    "        elif s <= 10.0:\n",
    "            desc = \"标准引导，条件一致性好\"\n",
    "        else:\n",
    "            desc = \"强引导，可能过饱和/失真\"\n",
    "        print(f\"  w={s:5.1f}: {desc}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "visualize_cfg_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 扩散模型变体对比\n",
    "\n",
    "| 模型 | 采样步数 | 特点 |\n",
    "|:-----|:---------|:-----|\n",
    "| **DDPM** | 1000 | 原始方法，质量好但慢 |\n",
    "| **DDIM** | 50-100 | 确定性采样，可加速 |\n",
    "| **LDM** | 50 | 潜在空间扩散，更高效 |\n",
    "| **Consistency** | 1-2 | 蒸馏方法，极速 |\n",
    "\n",
    "**进阶学习**: Stable Diffusion, SDXL, DiT, Flow Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 总结\n",
    "\n",
    "| 概念 | 公式 | 物理意义 |\n",
    "|:-----|:-----|:---------|\n",
    "| **前向过程** | $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon$ | 扩散，熵增 |\n",
    "| **信噪比** | $\\text{SNR} = \\bar{\\alpha}_t / (1-\\bar{\\alpha}_t)$ | 信号强度 |\n",
    "| **DDIM** | 非马尔可夫，跳步采样 | 20-100x 加速 |\n",
    "| **CFG** | $\\tilde{\\epsilon} = \\epsilon_{\\emptyset} + w(\\epsilon_c - \\epsilon_{\\emptyset})$ | 条件引导 |\n",
    "| **U-Net** | 编码器-解码器 + 时间嵌入 | 噪声预测 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
