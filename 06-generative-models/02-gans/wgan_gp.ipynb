{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN with Gradient Penalty (WGAN-GP)\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "WGAN-GP replaces the JS divergence in vanilla GAN with the Wasserstein-1 distance (Earth Mover's Distance),\n",
    "which provides meaningful gradients even when the generator and data distributions have disjoint supports.\n",
    "The Lipschitz constraint required by the Kantorovich-Rubinstein duality is enforced via a gradient penalty\n",
    "rather than weight clipping.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Wasserstein-1 Distance\n",
    "\n",
    "$$W_1(p_{\\text{data}}, p_g) = \\inf_{\\gamma \\in \\Pi(p_{\\text{data}}, p_g)} \\mathbb{E}_{(x,y) \\sim \\gamma}[\\|x - y\\|]$$\n",
    "\n",
    "**Interpretation:** Minimum cost to transport mass from $p_g$ to $p_{\\text{data}}$, where cost = distance moved.\n",
    "\n",
    "### Kantorovich-Rubinstein Duality\n",
    "\n",
    "$$W_1(p_{\\text{data}}, p_g) = \\sup_{\\|f\\|_L \\leq 1} \\mathbb{E}_{x \\sim p_{\\text{data}}}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)]$$\n",
    "\n",
    "where $\\|f\\|_L \\leq 1$ denotes 1-Lipschitz functions: $|f(x_1) - f(x_2)| \\leq \\|x_1 - x_2\\|$ for all $x_1, x_2$.\n",
    "\n",
    "**Proof Sketch:**\n",
    "1. Primal: Optimal transport problem over joint distributions $\\gamma$\n",
    "2. Dual: Maximize over 1-Lipschitz functions (Kantorovich potentials)\n",
    "3. Strong duality holds for compact metric spaces\n",
    "\n",
    "### Gradient Penalty (Gulrajani et al., 2017)\n",
    "\n",
    "Instead of weight clipping, enforce Lipschitz constraint via:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{GP}} = \\lambda \\mathbb{E}_{\\hat{x} \\sim p_{\\hat{x}}}\\left[(\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2\\right]$$\n",
    "\n",
    "where $\\hat{x} = \\epsilon x_{\\text{real}} + (1-\\epsilon) x_{\\text{fake}}$, $\\epsilon \\sim U[0,1]$.\n",
    "\n",
    "**Why interpolated points?** The optimal critic has gradient norm 1 almost everywhere along straight lines\n",
    "between $p_{\\text{data}}$ and $p_g$ (Proposition 1 in original paper).\n",
    "\n",
    "### Complete Objective\n",
    "\n",
    "**Critic (minimize):**\n",
    "$$\\mathcal{L}_D = \\underbrace{\\mathbb{E}_{\\tilde{x} \\sim p_g}[D(\\tilde{x})] - \\mathbb{E}_{x \\sim p_{\\text{data}}}[D(x)]}_{-W_1 \\text{ estimate}} + \\underbrace{\\lambda \\mathbb{E}_{\\hat{x}}[(\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2]}_{\\text{Gradient Penalty}}$$\n",
    "\n",
    "**Generator (minimize):**\n",
    "$$\\mathcal{L}_G = -\\mathbb{E}_{z \\sim p_z}[D(G(z))]$$\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Vanilla GAN suffers from:\n",
    "1. **Gradient vanishing:** When supports are disjoint, $\\text{JSD}(p_{\\text{data}} \\| p_g) = \\log 2$ (constant)\n",
    "2. **Mode collapse:** Generator finds few modes that fool discriminator\n",
    "3. **Training instability:** Sensitive to architecture and hyperparameters\n",
    "\n",
    "WGAN-GP addresses these by providing gradients proportional to actual distribution distance.\n",
    "\n",
    "## Algorithm Comparison\n",
    "\n",
    "| Aspect | GAN | WGAN (clip) | WGAN-GP |\n",
    "|--------|-----|-------------|----------|\n",
    "| Distance | JS Divergence | Wasserstein | Wasserstein |\n",
    "| Lipschitz | None | Weight clipping | Gradient penalty |\n",
    "| Output | Probability | Unbounded | Unbounded |\n",
    "| Stability | Poor | Better | Best |\n",
    "| Capacity | Full | Limited | Full |\n",
    "\n",
    "## Complexity Analysis\n",
    "\n",
    "- **Time per iteration:** $O(n_{\\text{critic}} \\cdot (T_D + T_{\\text{GP}}) + T_G)$\n",
    "  - $T_{\\text{GP}} = O(d)$ for gradient computation via autograd\n",
    "  - Typically $n_{\\text{critic}} = 5$\n",
    "- **Space:** $O(|\\theta_D| + |\\theta_G|)$ plus intermediate activations for gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WGANGPConfig:\n",
    "    \"\"\"Configuration for WGAN-GP.\n",
    "    \n",
    "    Core Idea:\n",
    "        Hyperparameters tuned for stable Wasserstein training.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        - gp_lambda: Weight $\\lambda$ in GP term. Default 10 from original paper.\n",
    "        - n_critic: Train critic n times per generator update for accurate $W_1$ estimate.\n",
    "    \"\"\"\n",
    "    latent_dim: int = 100\n",
    "    hidden_dim: int = 256\n",
    "    image_channels: int = 1\n",
    "    image_size: int = 28\n",
    "    \n",
    "    lr: float = 1e-4\n",
    "    beta1: float = 0.0\n",
    "    beta2: float = 0.9\n",
    "    \n",
    "    gp_lambda: float = 10.0\n",
    "    n_critic: int = 5\n",
    "    \n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 50\n",
    "    \n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed: int = 42\n",
    "    \n",
    "    @property\n",
    "    def image_dim(self) -> int:\n",
    "        return self.image_channels * self.image_size * self.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network for WGAN-GP.\n",
    "    \n",
    "    Core Idea:\n",
    "        Maps latent code $z$ to image space. Architecture identical to vanilla GAN;\n",
    "        WGAN-GP changes only affect the critic and loss computation.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        $G: \\mathbb{R}^{d_z} \\to [-1, 1]^{d_x}$ via composition of affine + nonlinear layers.\n",
    "        BatchNorm stabilizes training by normalizing intermediate activations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: WGANGPConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.latent_dim, config.hidden_dim),\n",
    "            nn.BatchNorm1d(config.hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim * 2),\n",
    "            nn.BatchNorm1d(config.hidden_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(config.hidden_dim * 2, config.hidden_dim * 4),\n",
    "            nn.BatchNorm1d(config.hidden_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(config.hidden_dim * 4, config.image_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        out = self.net(z)\n",
    "        return out.view(-1, self.config.image_channels, self.config.image_size, self.config.image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic network for WGAN-GP (not 'Discriminator').\n",
    "    \n",
    "    Core Idea:\n",
    "        Outputs unbounded scalar (not probability). Higher values indicate\n",
    "        'more real'. The critic approximates the optimal Kantorovich potential.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        $D: \\mathbb{R}^{d_x} \\to \\mathbb{R}$ approximates 1-Lipschitz function.\n",
    "        No sigmoid: output is Wasserstein potential, not probability.\n",
    "        No BatchNorm: can interfere with gradient penalty computation.\n",
    "    \n",
    "    Problem Statement:\n",
    "        BatchNorm in critic correlates samples within batch, violating\n",
    "        the independence assumption in GP. Use LayerNorm or no normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: WGANGPConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.image_dim, config.hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(config.hidden_dim * 4, config.hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(config.hidden_dim * 2, config.hidden_dim),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(config.hidden_dim, 1),\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.net(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(\n",
    "    critic: Critic,\n",
    "    real: Tensor,\n",
    "    fake: Tensor,\n",
    "    device: torch.device\n",
    ") -> Tensor:\n",
    "    \"\"\"Compute gradient penalty for WGAN-GP.\n",
    "    \n",
    "    Core Idea:\n",
    "        Penalize critic gradients that deviate from unit norm along\n",
    "        interpolated points between real and fake samples.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        For 1-Lipschitz $f$: $\\|\\nabla f(x)\\| \\leq 1$ almost everywhere.\n",
    "        Optimal critic has $\\|\\nabla D^*(\\hat{x})\\| = 1$ on interpolation lines.\n",
    "        \n",
    "        $$\\text{GP} = \\mathbb{E}_{\\hat{x}}[(\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2]$$\n",
    "        \n",
    "        where $\\hat{x} = \\epsilon x_{\\text{real}} + (1-\\epsilon) x_{\\text{fake}}$\n",
    "    \n",
    "    Complexity:\n",
    "        Time: O(d) for gradient computation via reverse-mode autodiff\n",
    "        Space: O(d) for storing gradient tensor\n",
    "    \n",
    "    Args:\n",
    "        critic: Critic network\n",
    "        real: Real samples [B, C, H, W]\n",
    "        fake: Generated samples [B, C, H, W]\n",
    "        device: Computation device\n",
    "    \n",
    "    Returns:\n",
    "        Scalar gradient penalty loss\n",
    "    \"\"\"\n",
    "    batch_size = real.size(0)\n",
    "    \n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    interpolated = (epsilon * real + (1 - epsilon) * fake).requires_grad_(True)\n",
    "    \n",
    "    d_interpolated = critic(interpolated)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_interpolated),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGPTrainer:\n",
    "    \"\"\"Training orchestrator for WGAN-GP.\n",
    "    \n",
    "    Core Idea:\n",
    "        Alternating optimization with n_critic critic updates per generator update.\n",
    "        More critic updates = better Wasserstein estimate = better generator gradients.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        Critic update: $\\theta_D \\leftarrow \\theta_D - \\alpha \\nabla_{\\theta_D} \\mathcal{L}_D$\n",
    "        Generator update: $\\theta_G \\leftarrow \\theta_G - \\alpha \\nabla_{\\theta_G} \\mathcal{L}_G$\n",
    "        \n",
    "        Adam with $\\beta_1=0$ (no momentum) recommended for WGAN-GP.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: WGANGPConfig) -> None:\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        \n",
    "        torch.manual_seed(config.seed)\n",
    "        \n",
    "        self.generator = Generator(config).to(self.device)\n",
    "        self.critic = Critic(config).to(self.device)\n",
    "        \n",
    "        self.optimizer_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        self.optimizer_c = torch.optim.Adam(\n",
    "            self.critic.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        \n",
    "        self.fixed_noise = torch.randn(64, config.latent_dim, device=self.device)\n",
    "        self.history: Dict[str, List[float]] = {\n",
    "            \"loss_c\": [], \"loss_g\": [], \"gp\": [], \"wasserstein\": []\n",
    "        }\n",
    "    \n",
    "    def _train_critic(self, real: Tensor) -> Tuple[float, float, float]:\n",
    "        batch_size = real.size(0)\n",
    "        self.optimizer_c.zero_grad()\n",
    "        \n",
    "        z = torch.randn(batch_size, self.config.latent_dim, device=self.device)\n",
    "        fake = self.generator(z).detach()\n",
    "        \n",
    "        c_real = self.critic(real).mean()\n",
    "        c_fake = self.critic(fake).mean()\n",
    "        wasserstein = c_real - c_fake\n",
    "        \n",
    "        gp = compute_gradient_penalty(self.critic, real, fake, self.device)\n",
    "        \n",
    "        loss_c = -wasserstein + self.config.gp_lambda * gp\n",
    "        loss_c.backward()\n",
    "        self.optimizer_c.step()\n",
    "        \n",
    "        return loss_c.item(), wasserstein.item(), gp.item()\n",
    "    \n",
    "    def _train_generator(self, batch_size: int) -> float:\n",
    "        self.optimizer_g.zero_grad()\n",
    "        \n",
    "        z = torch.randn(batch_size, self.config.latent_dim, device=self.device)\n",
    "        fake = self.generator(z)\n",
    "        \n",
    "        loss_g = -self.critic(fake).mean()\n",
    "        loss_g.backward()\n",
    "        self.optimizer_g.step()\n",
    "        \n",
    "        return loss_g.item()\n",
    "    \n",
    "    def train_epoch(self, dataloader: DataLoader) -> Dict[str, float]:\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "        \n",
    "        epoch_metrics = {k: 0.0 for k in self.history.keys()}\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for real, _ in dataloader:\n",
    "            real = real.to(self.device)\n",
    "            batch_size = real.size(0)\n",
    "            \n",
    "            for _ in range(self.config.n_critic):\n",
    "                loss_c, w_dist, gp = self._train_critic(real)\n",
    "            \n",
    "            loss_g = self._train_generator(batch_size)\n",
    "            \n",
    "            epoch_metrics[\"loss_c\"] += loss_c\n",
    "            epoch_metrics[\"loss_g\"] += loss_g\n",
    "            epoch_metrics[\"gp\"] += gp\n",
    "            epoch_metrics[\"wasserstein\"] += w_dist\n",
    "        \n",
    "        for k in epoch_metrics:\n",
    "            epoch_metrics[k] /= num_batches\n",
    "            self.history[k].append(epoch_metrics[k])\n",
    "        \n",
    "        return epoch_metrics\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_samples(self, num_samples: int = 64) -> Tensor:\n",
    "        self.generator.eval()\n",
    "        z = torch.randn(num_samples, self.config.latent_dim, device=self.device)\n",
    "        return self.generator(z).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config: WGANGPConfig) -> DataLoader:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=config.batch_size, shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(samples: Tensor, title: str = \"Generated Samples\") -> None:\n",
    "    grid = make_grid(samples, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history: Dict[str, List[float]]) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].plot(history[\"wasserstein\"], label=\"Wasserstein Distance\", alpha=0.8)\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Distance\")\n",
    "    axes[0].set_title(\"Wasserstein Distance (should increase then stabilize)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(history[\"gp\"], label=\"Gradient Penalty\", alpha=0.8, color=\"orange\")\n",
    "    axes[1].axhline(y=0, color=\"r\", linestyle=\"--\", alpha=0.5)\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"GP Value\")\n",
    "    axes[1].set_title(\"Gradient Penalty (should stay near 0)\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = WGANGPConfig(num_epochs=50, batch_size=64)\n",
    "    dataloader = create_dataloader(config)\n",
    "    trainer = WGANGPTrainer(config)\n",
    "    \n",
    "    print(f\"Generator parameters: {sum(p.numel() for p in trainer.generator.parameters()):,}\")\n",
    "    print(f\"Critic parameters: {sum(p.numel() for p in trainer.critic.parameters()):,}\")\n",
    "    print(f\"Device: {config.device}\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        metrics = trainer.train_epoch(dataloader)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{config.num_epochs}] \"\n",
    "                  f\"W: {metrics['wasserstein']:.4f} GP: {metrics['gp']:.4f} \"\n",
    "                  f\"Loss_C: {metrics['loss_c']:.4f} Loss_G: {metrics['loss_g']:.4f}\")\n",
    "            samples = trainer.generate_samples(64)\n",
    "            visualize_samples(samples, f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    plot_training_curves(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "WGAN-GP improves upon vanilla GAN by:\n",
    "\n",
    "1. **Wasserstein distance:** Provides meaningful gradients even when distributions don't overlap\n",
    "2. **Gradient penalty:** Enforces Lipschitz constraint without limiting model capacity\n",
    "3. **Multiple critic updates:** Better distance estimate leads to better generator gradients\n",
    "\n",
    "Key implementation details:\n",
    "- Critic outputs unbounded scalar (no sigmoid)\n",
    "- No BatchNorm in critic (interferes with GP)\n",
    "- Adam with $\\beta_1 = 0$ for stability\n",
    "- $\\lambda = 10$ for gradient penalty weight\n",
    "- $n_{\\text{critic}} = 5$ critic updates per generator update\n",
    "\n",
    "The Wasserstein distance should increase during training (distributions getting closer in W-distance sense),\n",
    "while gradient penalty should stay near zero (Lipschitz constraint satisfied)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
