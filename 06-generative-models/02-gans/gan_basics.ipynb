{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks: Theory and Implementation\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "GAN frames generative modeling as a two-player minimax game between a Generator $G$ and Discriminator $D$.\n",
    "The Generator learns to map noise $z \\sim p_z$ to data space, while the Discriminator learns to distinguish\n",
    "real samples from generated ones. At Nash equilibrium, $G$ produces samples indistinguishable from real data.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Value Function (Goodfellow et al., 2014)\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "**Variable Definitions:**\n",
    "- $p_{\\text{data}}$: True data distribution\n",
    "- $p_z$: Prior noise distribution (typically $\\mathcal{N}(0, I)$)\n",
    "- $p_g$: Implicit distribution defined by $G(z)$ where $z \\sim p_z$\n",
    "- $D(x) \\in [0, 1]$: Probability that $x$ is real\n",
    "\n",
    "### Optimal Discriminator (Theorem 1)\n",
    "\n",
    "For fixed $G$, the optimal discriminator is:\n",
    "\n",
    "$$D^*(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n",
    "\n",
    "**Proof:** The value function can be written as:\n",
    "$$V(D, G) = \\int_x \\left[ p_{\\text{data}}(x) \\log D(x) + p_g(x) \\log(1 - D(x)) \\right] dx$$\n",
    "\n",
    "For any $(a, b) \\in \\mathbb{R}^2 \\setminus \\{0, 0\\}$, the function $f(y) = a \\log y + b \\log(1-y)$\n",
    "achieves maximum at $y = \\frac{a}{a+b}$. Setting $a = p_{\\text{data}}(x)$, $b = p_g(x)$ yields the result.\n",
    "\n",
    "### Global Optimum (Theorem 2)\n",
    "\n",
    "Substituting $D^*$ into $V$:\n",
    "\n",
    "$$C(G) = V(D^*, G) = -\\log 4 + 2 \\cdot \\text{JSD}(p_{\\text{data}} \\| p_g)$$\n",
    "\n",
    "where JSD is the Jensen-Shannon Divergence. Since $\\text{JSD} \\geq 0$ with equality iff $p_{\\text{data}} = p_g$,\n",
    "the global minimum $C(G) = -\\log 4$ is achieved iff $p_g = p_{\\text{data}}$.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Traditional generative models (VAE, flow-based) require explicit density estimation or architectural constraints.\n",
    "GAN circumvents this by using an implicit density model trained via adversarial learning, enabling:\n",
    "- High-fidelity sample generation without tractable likelihood\n",
    "- Flexible generator architectures\n",
    "- Sharp, realistic outputs (vs. blurry VAE reconstructions)\n",
    "\n",
    "## Algorithm Comparison\n",
    "\n",
    "| Method | Likelihood | Sample Quality | Training Stability | Mode Coverage |\n",
    "|--------|------------|----------------|-------------------|---------------|\n",
    "| VAE | Tractable ELBO | Blurry | Stable | Good |\n",
    "| Flow | Exact | Good | Stable | Good |\n",
    "| GAN | Implicit | Sharp | Unstable | Mode collapse risk |\n",
    "| Diffusion | Tractable | Excellent | Stable | Excellent |\n",
    "\n",
    "## Complexity Analysis\n",
    "\n",
    "- **Time:** $O(n \\cdot (T_G + T_D))$ per epoch, where $n$ = samples, $T_G$, $T_D$ = forward pass costs\n",
    "- **Space:** $O(|\\theta_G| + |\\theta_D|)$ for model parameters\n",
    "- **Convergence:** No theoretical guarantees for non-convex case; empirically requires careful tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GANConfig:\n",
    "    \"\"\"Configuration for GAN training.\n",
    "    \n",
    "    Core Idea:\n",
    "        Centralized hyperparameter management following the principle of separation\n",
    "        of concerns. All training dynamics are controlled from a single source.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        - latent_dim: Dimension of $z \\sim p_z = \\mathcal{N}(0, I_{d_z})$\n",
    "        - label_smoothing: Replaces hard labels $y=1$ with $y=0.9$ to prevent\n",
    "          discriminator overconfidence, improving gradient flow to generator.\n",
    "    \"\"\"\n",
    "    latent_dim: int = 100\n",
    "    hidden_dim: int = 256\n",
    "    image_channels: int = 1\n",
    "    image_size: int = 28\n",
    "    \n",
    "    lr_discriminator: float = 2e-4\n",
    "    lr_generator: float = 2e-4\n",
    "    beta1: float = 0.5\n",
    "    beta2: float = 0.999\n",
    "    \n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 50\n",
    "    label_smoothing: float = 0.9\n",
    "    \n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed: int = 42\n",
    "    \n",
    "    @property\n",
    "    def image_dim(self) -> int:\n",
    "        return self.image_channels * self.image_size * self.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"MLP-based Generator Network.\n",
    "    \n",
    "    Core Idea:\n",
    "        Maps low-dimensional noise $z \\in \\mathbb{R}^{d_z}$ to high-dimensional\n",
    "        data space $x \\in \\mathbb{R}^{d_x}$ through a series of learned nonlinear\n",
    "        transformations. The mapping implicitly defines $p_g(x)$.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        $G: \\mathbb{R}^{d_z} \\to \\mathbb{R}^{d_x}$ where\n",
    "        $G(z) = \\tanh(W_L \\cdot \\text{LReLU}(W_{L-1} \\cdots \\text{LReLU}(W_1 z)))$\n",
    "        \n",
    "        Output activation $\\tanh$ ensures $G(z) \\in [-1, 1]^{d_x}$ matching\n",
    "        normalized image range.\n",
    "    \n",
    "    Architecture:\n",
    "        - Progressive dimension expansion: $d_z \\to 256 \\to 512 \\to 1024 \\to d_x$\n",
    "        - BatchNorm after each hidden layer for training stability\n",
    "        - LeakyReLU(0.2) to prevent dead neurons\n",
    "    \n",
    "    Complexity:\n",
    "        Time: O(d_z * h + h^2 * L + h * d_x) where h=hidden_dim, L=num_layers\n",
    "        Space: O(sum of weight matrices)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GANConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            self._block(config.latent_dim, config.hidden_dim, normalize=False),\n",
    "            self._block(config.hidden_dim, config.hidden_dim * 2),\n",
    "            self._block(config.hidden_dim * 2, config.hidden_dim * 4),\n",
    "            nn.Linear(config.hidden_dim * 4, config.image_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _block(self, in_features: int, out_features: int, normalize: bool = True) -> nn.Sequential:\n",
    "        layers = [nn.Linear(in_features, out_features)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(out_features))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _init_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        if z.dim() == 1:\n",
    "            z = z.unsqueeze(0)\n",
    "        out = self.net(z)\n",
    "        return out.view(-1, self.config.image_channels, self.config.image_size, self.config.image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"MLP-based Discriminator Network.\n",
    "    \n",
    "    Core Idea:\n",
    "        Binary classifier that estimates $D(x) = P(x \\text{ is real})$.\n",
    "        Trained to maximize classification accuracy, providing gradient\n",
    "        signal to the generator.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        $D: \\mathbb{R}^{d_x} \\to [0, 1]$ where\n",
    "        $D(x) = \\sigma(W_L \\cdot \\text{LReLU}(W_{L-1} \\cdots \\text{LReLU}(W_1 x)))$\n",
    "        \n",
    "        At optimum: $D^*(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}$\n",
    "    \n",
    "    Architecture:\n",
    "        - Progressive dimension reduction: $d_x \\to 1024 \\to 512 \\to 256 \\to 1$\n",
    "        - Dropout(0.3) for regularization and to prevent discriminator dominance\n",
    "        - No BatchNorm (can cause training instability in discriminator)\n",
    "    \n",
    "    Complexity:\n",
    "        Time: O(d_x * h + h^2 * L) where h=hidden_dim, L=num_layers\n",
    "        Space: O(sum of weight matrices)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GANConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            self._block(config.image_dim, config.hidden_dim * 4),\n",
    "            self._block(config.hidden_dim * 4, config.hidden_dim * 2),\n",
    "            self._block(config.hidden_dim * 2, config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _block(self, in_features: int, out_features: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.net(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANTrainer:\n",
    "    \"\"\"Training orchestrator for GAN.\n",
    "    \n",
    "    Core Idea:\n",
    "        Implements alternating optimization: fix G, update D; fix D, update G.\n",
    "        Uses non-saturating generator loss for stable gradients.\n",
    "    \n",
    "    Mathematical Theory:\n",
    "        Discriminator update (maximize):\n",
    "        $\\nabla_{\\theta_D} \\frac{1}{m} \\sum_{i=1}^m [\\log D(x^{(i)}) + \\log(1 - D(G(z^{(i)})))]$\n",
    "        \n",
    "        Generator update (non-saturating form):\n",
    "        $\\nabla_{\\theta_G} \\frac{1}{m} \\sum_{i=1}^m [-\\log D(G(z^{(i)}))]$\n",
    "        \n",
    "        The non-saturating loss $-\\log D(G(z))$ provides stronger gradients\n",
    "        early in training compared to $\\log(1 - D(G(z)))$ which saturates\n",
    "        when $D(G(z)) \\approx 0$.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GANConfig) -> None:\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        \n",
    "        torch.manual_seed(config.seed)\n",
    "        \n",
    "        self.generator = Generator(config).to(self.device)\n",
    "        self.discriminator = Discriminator(config).to(self.device)\n",
    "        \n",
    "        self.optimizer_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=config.lr_generator,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        self.optimizer_d = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=config.lr_discriminator,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.fixed_noise = torch.randn(64, config.latent_dim, device=self.device)\n",
    "        self.history: Dict[str, List[float]] = {\n",
    "            \"loss_d\": [], \"loss_g\": [], \"d_real\": [], \"d_fake\": []\n",
    "        }\n",
    "    \n",
    "    def _train_discriminator(self, real_images: Tensor) -> Tuple[float, float, float]:\n",
    "        batch_size = real_images.size(0)\n",
    "        real_labels = torch.full((batch_size, 1), self.config.label_smoothing, device=self.device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=self.device)\n",
    "        \n",
    "        self.optimizer_d.zero_grad()\n",
    "        \n",
    "        output_real = self.discriminator(real_images)\n",
    "        loss_real = self.criterion(output_real, real_labels)\n",
    "        \n",
    "        z = torch.randn(batch_size, self.config.latent_dim, device=self.device)\n",
    "        fake_images = self.generator(z).detach()\n",
    "        output_fake = self.discriminator(fake_images)\n",
    "        loss_fake = self.criterion(output_fake, fake_labels)\n",
    "        \n",
    "        loss_d = loss_real + loss_fake\n",
    "        loss_d.backward()\n",
    "        self.optimizer_d.step()\n",
    "        \n",
    "        return loss_d.item(), output_real.mean().item(), output_fake.mean().item()\n",
    "    \n",
    "    def _train_generator(self, batch_size: int) -> float:\n",
    "        real_labels = torch.full((batch_size, 1), self.config.label_smoothing, device=self.device)\n",
    "        \n",
    "        self.optimizer_g.zero_grad()\n",
    "        \n",
    "        z = torch.randn(batch_size, self.config.latent_dim, device=self.device)\n",
    "        fake_images = self.generator(z)\n",
    "        output = self.discriminator(fake_images)\n",
    "        \n",
    "        loss_g = self.criterion(output, real_labels)\n",
    "        loss_g.backward()\n",
    "        self.optimizer_g.step()\n",
    "        \n",
    "        return loss_g.item()\n",
    "    \n",
    "    def train_epoch(self, dataloader: DataLoader) -> Dict[str, float]:\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        epoch_loss_d, epoch_loss_g = 0.0, 0.0\n",
    "        epoch_d_real, epoch_d_fake = 0.0, 0.0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for real_images, _ in dataloader:\n",
    "            real_images = real_images.to(self.device)\n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            loss_d, d_real, d_fake = self._train_discriminator(real_images)\n",
    "            loss_g = self._train_generator(batch_size)\n",
    "            \n",
    "            epoch_loss_d += loss_d\n",
    "            epoch_loss_g += loss_g\n",
    "            epoch_d_real += d_real\n",
    "            epoch_d_fake += d_fake\n",
    "        \n",
    "        metrics = {\n",
    "            \"loss_d\": epoch_loss_d / num_batches,\n",
    "            \"loss_g\": epoch_loss_g / num_batches,\n",
    "            \"d_real\": epoch_d_real / num_batches,\n",
    "            \"d_fake\": epoch_d_fake / num_batches,\n",
    "        }\n",
    "        \n",
    "        for key, value in metrics.items():\n",
    "            self.history[key].append(value)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_samples(self, num_samples: int = 64) -> Tensor:\n",
    "        self.generator.eval()\n",
    "        z = torch.randn(num_samples, self.config.latent_dim, device=self.device)\n",
    "        return self.generator(z).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config: GANConfig) -> DataLoader:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=config.batch_size, shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(samples: Tensor, title: str = \"Generated Samples\") -> None:\n",
    "    grid = make_grid(samples, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history: Dict[str, List[float]]) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].plot(history[\"loss_d\"], label=\"Discriminator\", alpha=0.8)\n",
    "    axes[0].plot(history[\"loss_g\"], label=\"Generator\", alpha=0.8)\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(history[\"d_real\"], label=\"D(real)\", alpha=0.8)\n",
    "    axes[1].plot(history[\"d_fake\"], label=\"D(fake)\", alpha=0.8)\n",
    "    axes[1].axhline(y=0.5, color=\"r\", linestyle=\"--\", label=\"Equilibrium\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Discriminator Output\")\n",
    "    axes[1].set_title(\"Discriminator Confidence\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = GANConfig(num_epochs=50, batch_size=64)\n",
    "    dataloader = create_dataloader(config)\n",
    "    trainer = GANTrainer(config)\n",
    "    \n",
    "    print(f\"Generator parameters: {sum(p.numel() for p in trainer.generator.parameters()):,}\")\n",
    "    print(f\"Discriminator parameters: {sum(p.numel() for p in trainer.discriminator.parameters()):,}\")\n",
    "    print(f\"Device: {config.device}\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        metrics = trainer.train_epoch(dataloader)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{config.num_epochs}] \"\n",
    "                  f\"Loss_D: {metrics['loss_d']:.4f} Loss_G: {metrics['loss_g']:.4f} \"\n",
    "                  f\"D(real): {metrics['d_real']:.3f} D(fake): {metrics['d_fake']:.3f}\")\n",
    "            samples = trainer.generate_samples(64)\n",
    "            visualize_samples(samples, f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    plot_training_curves(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "GAN training alternates between:\n",
    "1. **Discriminator step**: Maximize $\\log D(x) + \\log(1 - D(G(z)))$ to improve real/fake classification\n",
    "2. **Generator step**: Maximize $\\log D(G(z))$ (non-saturating) to fool the discriminator\n",
    "\n",
    "Key implementation details:\n",
    "- Label smoothing (0.9 instead of 1.0) prevents discriminator overconfidence\n",
    "- Adam with $\\beta_1=0.5$ reduces momentum, improving stability\n",
    "- LeakyReLU prevents dead neurons in both networks\n",
    "- Dropout in discriminator prevents overfitting to training data\n",
    "\n",
    "At convergence, $D(x) \\to 0.5$ for all $x$, indicating the generator has learned $p_g \\approx p_{\\text{data}}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
